{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports:\n",
    "import numpy as np\n",
    "import math, os, pickle\n",
    "from numpy import genfromtxt\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "class color:\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   END = '\\033[0m'\n",
    "\n",
    "def softmax(A):  \n",
    "    expA = np.exp(A)\n",
    "    return expA / expA.sum()\n",
    "\n",
    "def load_objects(file):\n",
    "    with open(file, 'rb') as input:\n",
    "        return pickle.load(input)\n",
    "\n",
    "# functions to use:\n",
    "def sigmoid(matrix):\n",
    "    #print(\"SIGMOID: \\n\" +str(matrix)+\"\\n\")\n",
    "    return 1/(1+np.exp(-matrix))\n",
    "\n",
    "def getInputs(inputs):\n",
    "    #add bias to layerOne and inputs\n",
    "    row = inputs.shape[0] if np.ndim(inputs) != 1 else 1\n",
    "    inputBias = np.ones((row,1)) if np.ndim(inputs) != 1 else np.ones((1))\n",
    "    inputsWithBias = np.append(inputBias, inputs, 1) if np.ndim(inputs) != 1 else np.append(inputBias, inputs) \n",
    "    return inputsWithBias\n",
    "\n",
    "def networkError(target, netResult):\n",
    "    print(\"Target: \" + str(target) + \" Net Result: \" + str(netResult))\n",
    "    return .5*np.square(target - netResult)\n",
    "\n",
    "def learning(weights, lr, error, activationsForLayer):\n",
    "    print(\"Weights:  \" + str(weights.shape))\n",
    "    print(\"Learning Rate:  \" + str(lr))\n",
    "    print(\"Error:  \" + str(error.shape))\n",
    "    print(\"Activations: \"+str(activationsForLayer.shape))\n",
    "    return weights+lr*error*activationsForLayer\n",
    "\n",
    "def hiddenUnitError(temp, activations, error):\n",
    "    return temp*(1-temp)*(activations*error)\n",
    "\n",
    "def outputError(target, output):\n",
    "    # Eouput = output(1-output)(target - output)\n",
    "    return output*(1-output)*(target - output)\n",
    "\n",
    "def save_it_all(obj, filename):\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    with open(filename, 'wb') as output:  # Overwrites any existing file.\n",
    "        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "def saveNet(theta1, theta2, fileName):\n",
    "    weights = {\"theta1\":theta1, \"theta2\":theta2}\n",
    "    save_it_all(weights, fileName)\n",
    "    \n",
    "def sigmoidDerivative(target, output):\n",
    "    #E = (t − y) * y *  (1− y) // note: derivative of sigmoid func\n",
    "    return (target - output) * output * (1 - output)\n",
    "\n",
    "def netPlot(instance, error):\n",
    "    instance = list(range(0, instance))\n",
    "\n",
    "    with plt.rc_context({'axes.edgecolor':'orange', 'xtick.color':'red', 'ytick.color':'green', 'figure.facecolor':'white'}):\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(instance, error)\n",
    "\n",
    "        ax.set(xlabel='instance numebr (s)', ylabel='error (net)',\n",
    "               title='Average Net Error for each batch run')\n",
    "        ax.grid()\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def load_objects(file):\n",
    "    with open(file, 'rb') as input:\n",
    "        return pickle.load(input)\n",
    "    \n",
    "def classifyFish(inputInstance, theta1, theta2):\n",
    "    # just need to feed forward \n",
    "    layerOneActivations = theta1.dot(np.transpose(inputInstance))\n",
    "    print(\"\\t\\tLayer One activations: \\n\"+str(layerOneActivations)+\"\\n\")\n",
    "    layerOneSig = sigmoid(layerOneActivations)\n",
    "    print(\"\\t\\tSigmoid Result: \"+ str(layerOneSig) + \"\\n\")\n",
    "    inputsforhiddenlayer = getInputs(np.transpose(layerOneSig)) \n",
    "    print(\"\\t\\tInputs for the hiden layer is: (b,h1,h2)\\n\"+str(inputsforhiddenlayer)+\"\\n\")\n",
    "\n",
    "    outputActivation = theta2.dot(np.transpose(inputsforhiddenlayer)) \n",
    "    print(\"\\t\\tActivation for output layer: (h1,h2)\\n\" + str(outputActivation)+\"\\n\")\n",
    "        # inplace of sigmoid use softmax?? http://dataaspirant.com/2017/03/07/difference-between-softmax-function-and-sigmoid-function/\n",
    "    outputFinal = sigmoid(outputActivation)\n",
    "    print(\"\\t\\tFinal output: \\n\"+ str(outputFinal)+\"\\n\")\n",
    "    \n",
    "    return \"Yes\" if outputFinal >= .5 else \"No\"\n",
    "\n",
    "\n",
    "def getArrayFromFile(name):\n",
    "    array = genfromtxt(name, delimiter=',')\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural Net function:\n",
    "def nn(learningRate, theta1, theta2, inputInstance, targetInstance):\n",
    "    \n",
    "    # feed forward:\n",
    "    layerOneActivations = theta1.dot(np.transpose(inputInstance))\n",
    "    print(\"\\t\\tLayer One activations: \\n\"+str(layerOneActivations.shape)+\"\\n\")\n",
    "    layerOneSig = sigmoid(layerOneActivations)\n",
    "    print(\"\\t\\tSigmoid Result: \"+ str(layerOneSig.shape) + \"\\n\")\n",
    "    inputsforhiddenlayer = getInputs(np.transpose(layerOneSig)) \n",
    "    print(\"\\t\\tInputs for the hiden layer is: (b,h1,h2)\\n\"+str(inputsforhiddenlayer.shape)+\"\\n\")\n",
    "\n",
    "    outputActivation = theta2.dot(np.transpose(inputsforhiddenlayer)) \n",
    "    print(\"\\t\\tActivation for output layer: (h1,h2)\\n\" + str(outputActivation.shape)+\"\\n\")\n",
    "        # inplace of sigmoid use softmax?? http://dataaspirant.com/2017/03/07/difference-between-softmax-function-and-sigmoid-function/\n",
    "    outputFinal = sigmoid(outputActivation)\n",
    "    print(\"\\t\\tFinal output: \\n\"+ str(outputFinal.shape)+\"\\n\")\n",
    "\n",
    "    # network error:\n",
    "    netError = networkError(targetInstance, outputFinal)\n",
    "    print(\"\\t\\tNetwork Error: \\n\" + str(netError.shape)+\"\\n\")\n",
    "\n",
    "    # BACKPROPAGATE\n",
    "    outputErr = outputError(targetInstance, outputFinal[0])\n",
    "    print(\"\\t\\tOutput Error: \\n\"+str(outputErr.shape)+\"\\n\")\n",
    "\n",
    "    hidUnitErr = hiddenUnitError(layerOneSig, layerOneActivations, outputErr)\n",
    "    print(\"\\t\\tHidden unit errors: \\n\"+str(hidUnitErr.shape)+\"\\n\")\n",
    "\n",
    "    # learning:\n",
    "    theta1 = learning(theta1, learningRate, hidUnitErr, inputInstance)\n",
    "    print(\"\\t\\tNext round of weights for layerOne: (b,x1,x2) \\n\"+ str(theta1.shape)+\"\\n\")\n",
    "\n",
    "    theta2 = learning(theta2, learningRate, outputErr, inputsforhiddenlayer)\n",
    "    print(\"\\t\\tNext round of weights for layer 2: (b,h1,h2) \\n\"+ str(theta2.shape)+\"\\n\")\n",
    "    \n",
    "    return theta1, theta2, netError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input nodes with bias node: 9\n",
      "Output layer: 1\n",
      "Hidden nodes with bias node: 6\n",
      "Theta1 dims: (6, 9)\n",
      "Theta2 dims: (1, 7)\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 0 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.12849294 -0.22804686 -0.00297363  0.29226226 -0.07660649 -0.12789675\n",
      "   0.07686363  0.0749551   0.13195334]\n",
      " [ 0.25691971  0.26077057 -0.1043421  -0.28624448 -0.21153169 -0.07793044\n",
      "  -0.28985571 -0.19063245 -0.14977845]\n",
      " [ 0.29026041 -0.33307254  0.04354129 -0.22182266  0.28704634 -0.19694447\n",
      "  -0.12994697  0.02173683  0.27173571]\n",
      " [ 0.01746822 -0.1431561   0.09744372 -0.0093446   0.25286677  0.08318129\n",
      "   0.29925692 -0.24105671 -0.22569127]\n",
      " [-0.0625484   0.13144147  0.02333495  0.08687601 -0.30822278 -0.24011207\n",
      "  -0.30224214 -0.07471252 -0.09750579]\n",
      " [ 0.26613735  0.18439254  0.1701771  -0.29386453 -0.05848052 -0.02431909\n",
      "  -0.13604006 -0.08517676 -0.08401914]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.07603219  0.0074263  -0.1464606   0.22685029 -0.22267373 -0.18249391\n",
      "   0.12624683]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:0 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.49975538]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 0 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.12934589 -0.22804686 -0.00382658  0.29226226 -0.07660649 -0.12789675\n",
      "   0.07601068  0.0749551   0.13195334]\n",
      " [ 0.25478377  0.26077057 -0.10647804 -0.28624448 -0.21153169 -0.07793044\n",
      "  -0.29199165 -0.19063245 -0.14977845]\n",
      " [ 0.29341432 -0.33307254  0.04669519 -0.22182266  0.28704634 -0.19694447\n",
      "  -0.12679306  0.02173683  0.27173571]\n",
      " [ 0.02367286 -0.1431561   0.10364837 -0.0093446   0.25286677  0.08318129\n",
      "   0.30546157 -0.24105671 -0.22569127]\n",
      " [-0.06773364  0.13144147  0.01814971  0.08687601 -0.30822278 -0.24011207\n",
      "  -0.30742738 -0.07471252 -0.09750579]\n",
      " [ 0.27072719  0.18439254  0.17476694 -0.29386453 -0.05848052 -0.02431909\n",
      "  -0.13145022 -0.08517676 -0.08401914]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.13856275  0.0378382  -0.11733797  0.26129137 -0.18502489 -0.15651521\n",
      "   0.16217124]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:0 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.53522525]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 0 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.12469446 -0.22339543  0.00082485  0.29226226 -0.07660649 -0.12789675\n",
      "   0.08066211  0.0749551   0.13195334]\n",
      " [ 0.25284182  0.25882862 -0.10841999 -0.28624448 -0.21153169 -0.07793044\n",
      "  -0.2939336  -0.19063245 -0.14977845]\n",
      " [ 0.29540027 -0.3310866   0.04868114 -0.22182266  0.28704634 -0.19694447\n",
      "  -0.12480712  0.02173683  0.27173571]\n",
      " [ 0.01895236 -0.14787661  0.09892787 -0.0093446   0.25286677  0.08318129\n",
      "   0.30074107 -0.24105671 -0.22569127]\n",
      " [-0.06402688  0.13514823  0.02185647  0.08687601 -0.30822278 -0.24011207\n",
      "  -0.30372062 -0.07471252 -0.09750579]\n",
      " [ 0.26292644  0.1765918   0.16696619 -0.29386453 -0.05848052 -0.02431909\n",
      "  -0.13925097 -0.08517676 -0.08401914]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.07199166  0.0092674  -0.15256991  0.22999652 -0.22309721 -0.18606249\n",
      "   0.1207579 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:0 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.50404655]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 0 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-1.25452065e-01 -2.23395426e-01  6.72469640e-05  2.92262264e-01\n",
      "  -7.66064858e-02 -1.27896749e-01  8.06621083e-02  7.41974995e-02\n",
      "   1.31953344e-01]\n",
      " [ 2.52126050e-01  2.58828616e-01 -1.09135755e-01 -2.86244482e-01\n",
      "  -2.11531692e-01 -7.79304355e-02 -2.93933600e-01 -1.91348222e-01\n",
      "  -1.49778452e-01]\n",
      " [ 3.00884028e-01 -3.31086597e-01  5.41648983e-02 -2.21822657e-01\n",
      "   2.87046341e-01 -1.96944467e-01 -1.24807117e-01  2.72205924e-02\n",
      "   2.71735705e-01]\n",
      " [ 1.70506497e-02 -1.47876609e-01  9.70261566e-02 -9.34460102e-03\n",
      "   2.52866768e-01  8.31812890e-02  3.00741068e-01 -2.42958416e-01\n",
      "  -2.25691272e-01]\n",
      " [-6.58321050e-02  1.35148226e-01  2.00512424e-02  8.68760109e-02\n",
      "  -3.08222776e-01 -2.40112070e-01 -3.03720620e-01 -7.65177466e-02\n",
      "  -9.75057895e-02]\n",
      " [ 2.68113074e-01  1.76591796e-01  1.72152825e-01 -2.93864534e-01\n",
      "  -5.84805181e-02 -2.43190869e-02 -1.39250971e-01 -7.99901328e-02\n",
      "  -8.40191381e-02]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.13398178  0.03950456 -0.12229087  0.26659847 -0.19400867 -0.15687677\n",
      "   0.15704293]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:0 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.54236533]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 0 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-1.21303366e-01 -2.23395426e-01  6.72469640e-05  2.96410963e-01\n",
      "  -7.66064858e-02 -1.27896749e-01  8.06621083e-02  7.41974995e-02\n",
      "   1.36102043e-01]\n",
      " [ 2.49536958e-01  2.58828616e-01 -1.09135755e-01 -2.88833574e-01\n",
      "  -2.11531692e-01 -7.79304355e-02 -2.93933600e-01 -1.91348222e-01\n",
      "  -1.52367545e-01]\n",
      " [ 3.05714647e-01 -3.31086597e-01  5.41648983e-02 -2.16992038e-01\n",
      "   2.87046341e-01 -1.96944467e-01 -1.24807117e-01  2.72205924e-02\n",
      "   2.76566324e-01]\n",
      " [ 1.39920837e-02 -1.47876609e-01  9.70261566e-02 -1.24031670e-02\n",
      "   2.52866768e-01  8.31812890e-02  3.00741068e-01 -2.42958416e-01\n",
      "  -2.28749838e-01]\n",
      " [-6.69161571e-02  1.35148226e-01  2.00512424e-02  8.57919588e-02\n",
      "  -3.08222776e-01 -2.40112070e-01 -3.03720620e-01 -7.65177466e-02\n",
      "  -9.85898416e-02]\n",
      " [ 2.66559191e-01  1.76591796e-01  1.72152825e-01 -2.95418417e-01\n",
      "  -5.84805181e-02 -2.43190869e-02 -1.39250971e-01 -7.99901328e-02\n",
      "  -8.55730207e-02]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.19077542  0.07211208 -0.09649776  0.2999256  -0.1686947  -0.12956505\n",
      "   0.18388274]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:0 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.58486074]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 0 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-1.17966144e-01 -2.23395426e-01  6.72469640e-05  2.96410963e-01\n",
      "  -7.32692642e-02 -1.24559528e-01  8.06621083e-02  7.41974995e-02\n",
      "   1.39439265e-01]\n",
      " [ 2.52918876e-01  2.58828616e-01 -1.09135755e-01 -2.88833574e-01\n",
      "  -2.08149774e-01 -7.45485174e-02 -2.93933600e-01 -1.91348222e-01\n",
      "  -1.48985626e-01]\n",
      " [ 2.95033027e-01 -3.31086597e-01  5.41648983e-02 -2.16992038e-01\n",
      "   2.76364722e-01 -2.07626086e-01 -1.24807117e-01  2.72205924e-02\n",
      "   2.65884705e-01]\n",
      " [ 1.18470281e-02 -1.47876609e-01  9.70261566e-02 -1.24031670e-02\n",
      "   2.50721712e-01  8.10362334e-02  3.00741068e-01 -2.42958416e-01\n",
      "  -2.30894893e-01]\n",
      " [-5.57315514e-02  1.35148226e-01  2.00512424e-02  8.57919588e-02\n",
      "  -2.97038170e-01 -2.28927464e-01 -3.03720620e-01 -7.65177466e-02\n",
      "  -8.74052359e-02]\n",
      " [ 2.64820532e-01  1.76591796e-01  1.72152825e-01 -2.95418417e-01\n",
      "  -6.02191775e-02 -2.60577464e-02 -1.39250971e-01 -7.99901328e-02\n",
      "  -8.73116802e-02]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.11977372  0.0399685  -0.12859581  0.25291988 -0.20634587 -0.15290691\n",
      "   0.14664044]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:0 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.53478191]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 0 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-1.11576542e-01 -2.17005824e-01  6.72469640e-05  2.96410963e-01\n",
      "  -6.68796619e-02 -1.18169925e-01  8.06621083e-02  7.41974995e-02\n",
      "   1.45828867e-01]\n",
      " [ 2.51589465e-01  2.57499205e-01 -1.09135755e-01 -2.88833574e-01\n",
      "  -2.09479185e-01 -7.58779282e-02 -2.93933600e-01 -1.91348222e-01\n",
      "  -1.50315037e-01]\n",
      " [ 2.90176532e-01 -3.35943091e-01  5.41648983e-02 -2.16992038e-01\n",
      "   2.71508227e-01 -2.12482581e-01 -1.24807117e-01  2.72205924e-02\n",
      "   2.61028210e-01]\n",
      " [ 1.24317041e-02 -1.47291933e-01  9.70261566e-02 -1.24031670e-02\n",
      "   2.51306388e-01  8.16209094e-02  3.00741068e-01 -2.42958416e-01\n",
      "  -2.30310217e-01]\n",
      " [-4.74553737e-02  1.43424403e-01  2.00512424e-02  8.57919588e-02\n",
      "  -2.88761992e-01 -2.20651287e-01 -3.03720620e-01 -7.65177466e-02\n",
      "  -7.91290582e-02]\n",
      " [ 2.60445267e-01  1.72216532e-01  1.72152825e-01 -2.95418417e-01\n",
      "  -6.45944419e-02 -3.04330107e-02 -1.39250971e-01 -7.99901328e-02\n",
      "  -9.16869445e-02]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.05324947  0.01326752 -0.16318877  0.21472878 -0.2390232  -0.17749395\n",
      "   0.10895055]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:0 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.48351717]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 0 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-1.05338463e-01 -2.10767745e-01  6.72469640e-05  2.96410963e-01\n",
      "  -6.06415831e-02 -1.11931847e-01  8.69001871e-02  7.41974995e-02\n",
      "   1.45828867e-01]\n",
      " [ 2.52647755e-01  2.58557494e-01 -1.09135755e-01 -2.88833574e-01\n",
      "  -2.08420895e-01 -7.48196387e-02 -2.92875310e-01 -1.91348222e-01\n",
      "  -1.50315037e-01]\n",
      " [ 2.91854955e-01 -3.34264669e-01  5.41648983e-02 -2.16992038e-01\n",
      "   2.73186650e-01 -2.10804158e-01 -1.23128695e-01  2.72205924e-02\n",
      "   2.61028210e-01]\n",
      " [ 5.35249764e-03 -1.54371139e-01  9.70261566e-02 -1.24031670e-02\n",
      "   2.44227182e-01  7.45417029e-02  2.93661861e-01 -2.42958416e-01\n",
      "  -2.30310217e-01]\n",
      " [-3.79115204e-02  1.52968257e-01  2.00512424e-02  8.57919588e-02\n",
      "  -2.79218139e-01 -2.11107433e-01 -2.94176767e-01 -7.65177466e-02\n",
      "  -7.91290582e-02]\n",
      " [ 2.57480238e-01  1.69251503e-01  1.72152825e-01 -2.95418417e-01\n",
      "  -6.75594709e-02 -3.33980397e-02 -1.42216000e-01 -7.99901328e-02\n",
      "  -9.16869445e-02]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.00712449 -0.01048465 -0.19231659  0.1862237  -0.27658662 -0.19729768\n",
      "   0.07577906]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:0 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.44701054]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 0 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-1.00679373e-01 -2.10767745e-01  6.72469640e-05  3.01070053e-01\n",
      "  -6.06415831e-02 -1.11931847e-01  9.15592773e-02  7.41974995e-02\n",
      "   1.45828867e-01]\n",
      " [ 2.47174644e-01  2.58557494e-01 -1.09135755e-01 -2.94306685e-01\n",
      "  -2.08420895e-01 -7.48196387e-02 -2.98348421e-01 -1.91348222e-01\n",
      "  -1.50315037e-01]\n",
      " [ 2.91030726e-01 -3.34264669e-01  5.41648983e-02 -2.17816267e-01\n",
      "   2.73186650e-01 -2.10804158e-01 -1.23952924e-01  2.72205924e-02\n",
      "   2.61028210e-01]\n",
      " [ 1.01505624e-02 -1.54371139e-01  9.70261566e-02 -7.60510230e-03\n",
      "   2.44227182e-01  7.45417029e-02  2.98459926e-01 -2.42958416e-01\n",
      "  -2.30310217e-01]\n",
      " [-4.20567608e-02  1.52968257e-01  2.00512424e-02  8.16467184e-02\n",
      "  -2.79218139e-01 -2.11107433e-01 -2.98322007e-01 -7.65177466e-02\n",
      "  -7.91290582e-02]\n",
      " [ 2.54426817e-01  1.69251503e-01  1.72152825e-01 -2.98471839e-01\n",
      "  -6.75594709e-02 -3.33980397e-02 -1.45269422e-01 -7.99901328e-02\n",
      "  -9.16869445e-02]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.06122282  0.02840833 -0.16371535  0.21957281 -0.23754894 -0.16731129\n",
      "   0.10688275]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:0 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.49404926]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 0 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-1.03528139e-01 -2.10767745e-01  6.72469640e-05  3.01070053e-01\n",
      "  -6.34903494e-02 -1.14780613e-01  8.87105110e-02  7.41974995e-02\n",
      "   1.45828867e-01]\n",
      " [ 2.42033075e-01  2.58557494e-01 -1.09135755e-01 -2.94306685e-01\n",
      "  -2.13562464e-01 -7.99612075e-02 -3.03489990e-01 -1.91348222e-01\n",
      "  -1.50315037e-01]\n",
      " [ 2.94610867e-01 -3.34264669e-01  5.41648983e-02 -2.17816267e-01\n",
      "   2.76766791e-01 -2.07224017e-01 -1.20372782e-01  2.72205924e-02\n",
      "   2.61028210e-01]\n",
      " [ 1.91533058e-02 -1.54371139e-01  9.70261566e-02 -7.60510230e-03\n",
      "   2.53229925e-01  8.35444463e-02  3.07462670e-01 -2.42958416e-01\n",
      "  -2.30310217e-01]\n",
      " [-5.31607729e-02  1.52968257e-01  2.00512424e-02  8.16467184e-02\n",
      "  -2.90322151e-01 -2.22211446e-01 -3.09426019e-01 -7.65177466e-02\n",
      "  -7.91290582e-02]\n",
      " [ 2.54556444e-01  1.69251503e-01  1.72152825e-01 -2.98471839e-01\n",
      "  -6.74298434e-02 -3.32684122e-02 -1.45139794e-01 -7.99901328e-02\n",
      "  -9.16869445e-02]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.12445771  0.05716131 -0.13733585  0.2548019  -0.19632643 -0.14811975\n",
      "   0.13862982]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:0 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.53516423]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 0 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-1.07282882e-01 -2.10767745e-01  6.72469640e-05  2.97315310e-01\n",
      "  -6.34903494e-02 -1.18535356e-01  8.87105110e-02  7.41974995e-02\n",
      "   1.42074124e-01]\n",
      " [ 2.46642424e-01  2.58557494e-01 -1.09135755e-01 -2.89697336e-01\n",
      "  -2.13562464e-01 -7.53518587e-02 -3.03489990e-01 -1.91348222e-01\n",
      "  -1.45705688e-01]\n",
      " [ 2.92446792e-01 -3.34264669e-01  5.41648983e-02 -2.19980342e-01\n",
      "   2.76766791e-01 -2.09388092e-01 -1.20372782e-01  2.72205924e-02\n",
      "   2.58864135e-01]\n",
      " [ 2.13932294e-02 -1.54371139e-01  9.70261566e-02 -5.36517872e-03\n",
      "   2.53229925e-01  8.57843699e-02  3.07462670e-01 -2.42958416e-01\n",
      "  -2.28070294e-01]\n",
      " [-4.87036295e-02  1.52968257e-01  2.00512424e-02  8.61038619e-02\n",
      "  -2.90322151e-01 -2.17754302e-01 -3.09426019e-01 -7.65177466e-02\n",
      "  -7.46719147e-02]\n",
      " [ 2.57346710e-01  1.69251503e-01  1.72152825e-01 -2.95681573e-01\n",
      "  -6.74298434e-02 -3.04781464e-02 -1.45139794e-01 -7.99901328e-02\n",
      "  -8.88966786e-02]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.05789305  0.02009145 -0.16594725  0.21934934 -0.22736201 -0.17688942\n",
      "   0.10815104]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:0 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.48018729]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 0 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-1.08102376e-01 -2.11587238e-01  6.72469640e-05  2.96495817e-01\n",
      "  -6.34903494e-02 -1.19354849e-01  8.78910178e-02  7.41974995e-02\n",
      "   1.42074124e-01]\n",
      " [ 2.44010874e-01  2.55925945e-01 -1.09135755e-01 -2.92328886e-01\n",
      "  -2.13562464e-01 -7.79834083e-02 -3.06121539e-01 -1.91348222e-01\n",
      "  -1.45705688e-01]\n",
      " [ 2.83645235e-01 -3.43066226e-01  5.41648983e-02 -2.28781899e-01\n",
      "   2.76766791e-01 -2.18189649e-01 -1.29174339e-01  2.72205924e-02\n",
      "   2.58864135e-01]\n",
      " [ 2.54609897e-02 -1.50303379e-01  9.70261566e-02 -1.29741836e-03\n",
      "   2.53229925e-01  8.98521303e-02  3.11530430e-01 -2.42958416e-01\n",
      "  -2.28070294e-01]\n",
      " [-5.40142192e-02  1.47657667e-01  2.00512424e-02  8.07932722e-02\n",
      "  -2.90322151e-01 -2.23064892e-01 -3.14736609e-01 -7.65177466e-02\n",
      "  -7.46719147e-02]\n",
      " [ 2.56622078e-01  1.68526870e-01  1.72152825e-01 -2.96406205e-01\n",
      "  -6.74298434e-02 -3.12027786e-02 -1.45864426e-01 -7.99901328e-02\n",
      "  -8.88966786e-02]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.12276761  0.05170889 -0.13615324  0.24246266 -0.19081277 -0.14986371\n",
      "   0.13986345]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:0 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.51910294]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 0 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-1.07132538e-01 -2.10617401e-01  6.72469640e-05  2.97465654e-01\n",
      "  -6.34903494e-02 -1.19354849e-01  8.88608553e-02  7.41974995e-02\n",
      "   1.42074124e-01]\n",
      " [ 2.42536150e-01  2.54451221e-01 -1.09135755e-01 -2.93803610e-01\n",
      "  -2.13562464e-01 -7.79834083e-02 -3.07596263e-01 -1.91348222e-01\n",
      "  -1.45705688e-01]\n",
      " [ 2.77647072e-01 -3.49064389e-01  5.41648983e-02 -2.34780062e-01\n",
      "   2.76766791e-01 -2.18189649e-01 -1.35172503e-01  2.72205924e-02\n",
      "   2.58864135e-01]\n",
      " [ 2.82192117e-02 -1.47545157e-01  9.70261566e-02  1.46080355e-03\n",
      "   2.53229925e-01  8.98521303e-02  3.14288652e-01 -2.42958416e-01\n",
      "  -2.28070294e-01]\n",
      " [-5.61092463e-02  1.45562640e-01  2.00512424e-02  7.86982450e-02\n",
      "  -2.90322151e-01 -2.23064892e-01 -3.16831636e-01 -7.65177466e-02\n",
      "  -7.46719147e-02]\n",
      " [ 2.56365167e-01  1.68269959e-01  1.72152825e-01 -2.96663116e-01\n",
      "  -6.74298434e-02 -3.12027786e-02 -1.46121337e-01 -7.99901328e-02\n",
      "  -8.88966786e-02]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.182792    0.0826916  -0.10761815  0.26630102 -0.15802653 -0.12195342\n",
      "   0.16961872]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:0 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.5641497]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 0 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.10896661 -0.2106174  -0.00176682  0.29746565 -0.06349035 -0.12118892\n",
      "   0.08702678  0.0741975   0.14207412]\n",
      " [ 0.23921107  0.25445122 -0.11246083 -0.29380361 -0.21356246 -0.08130849\n",
      "  -0.31092134 -0.19134822 -0.14570569]\n",
      " [ 0.27735842 -0.34906439  0.05387624 -0.23478006  0.27676679 -0.2184783\n",
      "  -0.13546116  0.02722059  0.25886413]\n",
      " [ 0.03483639 -0.14754516  0.10364333  0.0014608   0.25322993  0.09646931\n",
      "   0.32090583 -0.24295842 -0.22807029]\n",
      " [-0.06321873  0.14556264  0.01294176  0.07869825 -0.29032215 -0.23017437\n",
      "  -0.32394112 -0.07651775 -0.07467191]\n",
      " [ 0.25967766  0.16826996  0.17546532 -0.29666312 -0.06742984 -0.02789028\n",
      "  -0.14280884 -0.07999013 -0.08889668]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.23637649  0.10764398 -0.08418635  0.29280459 -0.12430367 -0.1026703\n",
      "   0.1997584 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:0 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.59624536]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 1 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.11111116 -0.21276196 -0.00176682  0.2953211  -0.06349035 -0.12118892\n",
      "   0.08702678  0.0741975   0.13992957]\n",
      " [ 0.23824015  0.2534803  -0.11246083 -0.29477453 -0.21356246 -0.08130849\n",
      "  -0.31092134 -0.19134822 -0.1466766 ]\n",
      " [ 0.27821238 -0.34821043  0.05387624 -0.2339261   0.27676679 -0.2184783\n",
      "  -0.13546116  0.02722059  0.2597181 ]\n",
      " [ 0.0407526  -0.14162895  0.10364333  0.00737701  0.25322993  0.09646931\n",
      "   0.32090583 -0.24295842 -0.22215408]\n",
      " [-0.06476552  0.14401585  0.01294176  0.07715145 -0.29032215 -0.23017437\n",
      "  -0.32394112 -0.07651775 -0.07621871]\n",
      " [ 0.25891747  0.16750977  0.17546532 -0.29742331 -0.06742984 -0.02789028\n",
      "  -0.14280884 -0.07999013 -0.08965687]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.16460738  0.06960972 -0.12104229  0.25777432 -0.15415783 -0.14010357\n",
      "   0.16311343]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:1 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.55654978]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 1 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.11146476 -0.21276196 -0.00212042  0.2953211  -0.06349035 -0.12118892\n",
      "   0.08667318  0.0741975   0.13992957]\n",
      " [ 0.23572889  0.2534803  -0.1149721  -0.29477453 -0.21356246 -0.08130849\n",
      "  -0.31343261 -0.19134822 -0.1466766 ]\n",
      " [ 0.28087652 -0.34821043  0.05654038 -0.2339261   0.27676679 -0.2184783\n",
      "  -0.13279702  0.02722059  0.2597181 ]\n",
      " [ 0.0467857  -0.14162895  0.10967644  0.00737701  0.25322993  0.09646931\n",
      "   0.32693893 -0.24295842 -0.22215408]\n",
      " [-0.06972891  0.14401585  0.00797837  0.07715145 -0.29032215 -0.23017437\n",
      "  -0.32890451 -0.07651775 -0.07621871]\n",
      " [ 0.26282277  0.16750977  0.17937062 -0.29742331 -0.06742984 -0.02789028\n",
      "  -0.13890354 -0.07999013 -0.08965687]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.21932961  0.0966172  -0.09620681  0.28781677 -0.12054355 -0.11782348\n",
      "   0.19443541]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:1 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.58502563]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 1 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.10727027 -0.20856746  0.00207407  0.2953211  -0.06349035 -0.12118892\n",
      "   0.09086768  0.0741975   0.13992957]\n",
      " [ 0.2346504   0.25240182 -0.11605059 -0.29477453 -0.21356246 -0.08130849\n",
      "  -0.3145111  -0.19134822 -0.1466766 ]\n",
      " [ 0.28341264 -0.34567431  0.05907651 -0.2339261   0.27676679 -0.2184783\n",
      "  -0.1302609   0.02722059  0.2597181 ]\n",
      " [ 0.04089188 -0.14752277  0.10378262  0.00737701  0.25322993  0.09646931\n",
      "   0.32104511 -0.24295842 -0.22215408]\n",
      " [-0.06541616  0.14832861  0.01229113  0.07715145 -0.29032215 -0.23017437\n",
      "  -0.32459175 -0.07651775 -0.07621871]\n",
      " [ 0.25491106  0.15959805  0.17145891 -0.29742331 -0.06742984 -0.02789028\n",
      "  -0.14681525 -0.07999013 -0.08965687]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.14831608  0.06534521 -0.13279273  0.25485485 -0.16205954 -0.14897362\n",
      "   0.1507214 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:1 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.5520076]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 1 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.10769944 -0.20856746  0.0016449   0.2953211  -0.06349035 -0.12118892\n",
      "   0.09086768  0.07376832  0.13992957]\n",
      " [ 0.23364429  0.25240182 -0.1170567  -0.29477453 -0.21356246 -0.08130849\n",
      "  -0.3145111  -0.19235433 -0.1466766 ]\n",
      " [ 0.28836145 -0.34567431  0.06402531 -0.2339261   0.27676679 -0.2184783\n",
      "  -0.1302609   0.0321694   0.2597181 ]\n",
      " [ 0.0395341  -0.14752277  0.10242484  0.00737701  0.25322993  0.09646931\n",
      "   0.32104511 -0.2443162  -0.22215408]\n",
      " [-0.06720396  0.14832861  0.01050332  0.07715145 -0.29032215 -0.23017437\n",
      "  -0.32459175 -0.07830556 -0.07621871]\n",
      " [ 0.25956678  0.15959805  0.17611463 -0.29742331 -0.06742984 -0.02789028\n",
      "  -0.14681525 -0.07533441 -0.08965687]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.20370927  0.09261256 -0.10610313  0.28761377 -0.13572291 -0.12306985\n",
      "   0.18316738]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:1 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.58431445]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 1 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.10367442 -0.20856746  0.0016449   0.29934612 -0.06349035 -0.12118892\n",
      "   0.09086768  0.07376832  0.14395459]\n",
      " [ 0.23104972  0.25240182 -0.1170567  -0.2973691  -0.21356246 -0.08130849\n",
      "  -0.3145111  -0.19235433 -0.14927118]\n",
      " [ 0.29223007 -0.34567431  0.06402531 -0.23005748  0.27676679 -0.2184783\n",
      "  -0.1302609   0.0321694   0.26358672]\n",
      " [ 0.03733929 -0.14752277  0.10242484  0.0051822   0.25322993  0.09646931\n",
      "   0.32104511 -0.2443162  -0.2243489 ]\n",
      " [-0.06803944  0.14832861  0.01050332  0.07631598 -0.29032215 -0.23017437\n",
      "  -0.32459175 -0.07830556 -0.07705418]\n",
      " [ 0.25796398  0.15959805  0.17611463 -0.2990261  -0.06742984 -0.02789028\n",
      "  -0.14681525 -0.07533441 -0.09125967]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.25419242  0.12195152 -0.08347484  0.31678792 -0.1126874  -0.09866436\n",
      "   0.20680181]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:1 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.62050694]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 1 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.10105075 -0.20856746  0.0016449   0.29934612 -0.06086668 -0.11856525\n",
      "   0.09086768  0.07376832  0.14657827]\n",
      " [ 0.23489789  0.25240182 -0.1170567  -0.2973691  -0.20971429 -0.07746032\n",
      "  -0.3145111  -0.19235433 -0.14542301]\n",
      " [ 0.28200817 -0.34567431  0.06402531 -0.23005748  0.26654489 -0.22870021\n",
      "  -0.1302609   0.0321694   0.25336482]\n",
      " [ 0.03438742 -0.14752277  0.10242484  0.0051822   0.25027806  0.09351744\n",
      "   0.32104511 -0.2443162  -0.22730076]\n",
      " [-0.05713566  0.14832861  0.01050332  0.07631598 -0.27941837 -0.21927059\n",
      "  -0.32459175 -0.07830556 -0.0661504 ]\n",
      " [ 0.25666185  0.15959805  0.17611463 -0.2990261  -0.06873198 -0.02919242\n",
      "  -0.14681525 -0.07533441 -0.0925618 ]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.18113453  0.08805538 -0.11612643  0.26938235 -0.15218125 -0.12346642\n",
      "   0.16896963]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:1 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.5691483]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 1 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.0952479  -0.20276461  0.0016449   0.29934612 -0.05506383 -0.1127624\n",
      "   0.09086768  0.07376832  0.15238111]\n",
      " [ 0.23394429  0.25144821 -0.1170567  -0.2973691  -0.2106679  -0.07841392\n",
      "  -0.3145111  -0.19235433 -0.14637661]\n",
      " [ 0.27808946 -0.34959302  0.06402531 -0.23005748  0.26262617 -0.23261892\n",
      "  -0.1302609   0.0321694   0.2494461 ]\n",
      " [ 0.03432882 -0.14758137  0.10242484  0.0051822   0.25021945  0.09345884\n",
      "   0.32104511 -0.2443162  -0.22735937]\n",
      " [-0.0493192   0.15614506  0.01050332  0.07631598 -0.27160191 -0.21145413\n",
      "  -0.32459175 -0.07830556 -0.05833395]\n",
      " [ 0.25277283  0.15570904  0.17611463 -0.2990261  -0.07262099 -0.03308143\n",
      "  -0.14681525 -0.07533441 -0.09645081]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.11135168  0.0590809  -0.15197193  0.23053831 -0.18713128 -0.15024584\n",
      "   0.13015606]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:1 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.51771703]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 1 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.08939707 -0.19691378  0.0016449   0.29934612 -0.049213   -0.10691157\n",
      "   0.09671851  0.07376832  0.15238111]\n",
      " [ 0.23584755  0.25335148 -0.1170567  -0.2973691  -0.20876463 -0.07651065\n",
      "  -0.31260783 -0.19235433 -0.14637661]\n",
      " [ 0.2808444  -0.34683807  0.06402531 -0.23005748  0.26538112 -0.22986398\n",
      "  -0.12750595  0.0321694   0.2494461 ]\n",
      " [ 0.02606254 -0.15584765  0.10242484  0.0051822   0.24195317  0.08519256\n",
      "   0.31277883 -0.2443162  -0.22735937]\n",
      " [-0.03927917  0.1661851   0.01050332  0.07631598 -0.26156188 -0.2014141\n",
      "  -0.31455172 -0.07830556 -0.05833395]\n",
      " [ 0.25026798  0.15320418  0.17611463 -0.2990261  -0.07512585 -0.03558629\n",
      "  -0.14932011 -0.07533441 -0.09645081]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.04671831  0.03275311 -0.18238092  0.20099014 -0.22813965 -0.1716802\n",
      "   0.09532435]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:1 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.4808475]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 1 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.08454414 -0.19691378  0.0016449   0.30419905 -0.049213   -0.10691157\n",
      "   0.10157144  0.07376832  0.15238111]\n",
      " [ 0.229994    0.25335148 -0.1170567  -0.30322265 -0.20876463 -0.07651065\n",
      "  -0.31846138 -0.19235433 -0.14637661]\n",
      " [ 0.2796034  -0.34683807  0.06402531 -0.23129847  0.26538112 -0.22986398\n",
      "  -0.12874695  0.0321694   0.2494461 ]\n",
      " [ 0.03147392 -0.15584765  0.10242484  0.01059359  0.24195317  0.08519256\n",
      "   0.31819022 -0.2443162  -0.22735937]\n",
      " [-0.04368937  0.1661851   0.01050332  0.07190577 -0.26156188 -0.2014141\n",
      "  -0.31896192 -0.07830556 -0.05833395]\n",
      " [ 0.24709043  0.15320418  0.17611463 -0.30220364 -0.07512585 -0.03558629\n",
      "  -0.15249766 -0.07533441 -0.09645081]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.11151715  0.07008189 -0.15597256  0.23214734 -0.19022147 -0.14374781\n",
      "   0.12452541]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:1 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.52479502]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 1 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.08659474 -0.19691378  0.0016449   0.30419905 -0.0512636  -0.10896217\n",
      "   0.09952084  0.07376832  0.15238111]\n",
      " [ 0.22464644  0.25335148 -0.1170567  -0.30322265 -0.21411219 -0.08185822\n",
      "  -0.32380894 -0.19235433 -0.14637661]\n",
      " [ 0.28234044 -0.34683807  0.06402531 -0.23129847  0.26811815 -0.22712694\n",
      "  -0.12600992  0.0321694   0.2494461 ]\n",
      " [ 0.04043407 -0.15584765  0.10242484  0.01059359  0.25091332  0.0941527\n",
      "   0.32715037 -0.2443162  -0.22735937]\n",
      " [-0.05405144  0.1661851   0.01050332  0.07190577 -0.27192395 -0.21177617\n",
      "  -0.32932399 -0.07830556 -0.05833395]\n",
      " [ 0.24685166  0.15320418  0.17611463 -0.30220364 -0.07536462 -0.03582506\n",
      "  -0.15273643 -0.07533441 -0.09645081]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.1707717   0.09765194 -0.13181822  0.26452752 -0.15093415 -0.12570063\n",
      "   0.1539139 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:1 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.56207704]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 1 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.09103289 -0.19691378  0.0016449   0.2997609  -0.0512636  -0.11340032\n",
      "   0.09952084  0.07376832  0.14794296]\n",
      " [ 0.22982954  0.25335148 -0.1170567  -0.29803955 -0.21411219 -0.07667511\n",
      "  -0.32380894 -0.19235433 -0.14119351]\n",
      " [ 0.28107342 -0.34683807  0.06402531 -0.23256549  0.26811815 -0.22839396\n",
      "  -0.12600992  0.0321694   0.24817909]\n",
      " [ 0.04185289 -0.15584765  0.10242484  0.01201241  0.25091332  0.09557152\n",
      "   0.32715037 -0.2443162  -0.22594055]\n",
      " [-0.04975756  0.1661851   0.01050332  0.07619965 -0.27192395 -0.20748229\n",
      "  -0.32932399 -0.07830556 -0.05404007]\n",
      " [ 0.25006814  0.15320418  0.17611463 -0.29898717 -0.07536462 -0.03260859\n",
      "  -0.15273643 -0.07533441 -0.09323434]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.10159507  0.05857491 -0.16114173  0.22867105 -0.18410205 -0.15594938\n",
      "   0.12256097]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:1 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.50615903]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 1 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.09106476 -0.19694565  0.0016449   0.29972904 -0.0512636  -0.11343219\n",
      "   0.09948897  0.07376832  0.14794296]\n",
      " [ 0.22654499  0.25006693 -0.1170567  -0.3013241  -0.21411219 -0.07995966\n",
      "  -0.32709349 -0.19235433 -0.14119351]\n",
      " [ 0.27200259 -0.3559089   0.06402531 -0.24163632  0.26811815 -0.23746479\n",
      "  -0.13508075  0.0321694   0.24817909]\n",
      " [ 0.04667683 -0.15102371  0.10242484  0.01683635  0.25091332  0.10039546\n",
      "   0.33197431 -0.2443162  -0.22594055]\n",
      " [-0.05491409  0.16102857  0.01050332  0.07104313 -0.27192395 -0.21263881\n",
      "  -0.33448052 -0.07830556 -0.05404007]\n",
      " [ 0.24881942  0.15195546  0.17611463 -0.30023589 -0.07536462 -0.0338573\n",
      "  -0.15398514 -0.07533441 -0.09323434]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.16331582  0.08940342 -0.13359135  0.24980262 -0.14833459 -0.13034794\n",
      "   0.15217126]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:1 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.5439377]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 1 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.08949693 -0.19537782  0.0016449   0.30129687 -0.0512636  -0.11343219\n",
      "   0.10105681  0.07376832  0.14794296]\n",
      " [ 0.22441049  0.24793243 -0.1170567  -0.3034586  -0.21411219 -0.07995966\n",
      "  -0.32922799 -0.19235433 -0.14119351]\n",
      " [ 0.26582217 -0.36208932  0.06402531 -0.24781674  0.26811815 -0.23746479\n",
      "  -0.14126117  0.0321694   0.24817909]\n",
      " [ 0.05008287 -0.14761767  0.10242484  0.02024238  0.25091332  0.10039546\n",
      "   0.33538034 -0.2443162  -0.22594055]\n",
      " [-0.05712522  0.15881744  0.01050332  0.06883199 -0.27192395 -0.21263881\n",
      "  -0.33669165 -0.07830556 -0.05404007]\n",
      " [ 0.24806413  0.15120017  0.17611463 -0.30099118 -0.07536462 -0.0338573\n",
      "  -0.15474043 -0.07533441 -0.09323434]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.21988339  0.11925827 -0.10745027  0.2716851  -0.11661074 -0.10428442\n",
      "   0.1796994 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:1 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.58703703]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 1 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.09074803 -0.19537782  0.00039379  0.30129687 -0.0512636  -0.1146833\n",
      "   0.0998057   0.07376832  0.14794296]\n",
      " [ 0.22071807  0.24793243 -0.12074912 -0.3034586  -0.21411219 -0.08365209\n",
      "  -0.33292042 -0.19235433 -0.14119351]\n",
      " [ 0.26521087 -0.36208932  0.06341401 -0.24781674  0.26811815 -0.23807609\n",
      "  -0.14187247  0.0321694   0.24817909]\n",
      " [ 0.05684275 -0.14761767  0.10918472  0.02024238  0.25091332  0.10715534\n",
      "   0.34214023 -0.2443162  -0.22594055]\n",
      " [-0.06395813  0.15881744  0.0036704   0.06883199 -0.27192395 -0.21947173\n",
      "  -0.34352456 -0.07830556 -0.05404007]\n",
      " [ 0.25097167  0.15120017  0.17902217 -0.30099118 -0.07536462 -0.03094976\n",
      "  -0.15183289 -0.07533441 -0.09323434]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26993957  0.14303316 -0.08617092  0.29610165 -0.08442606 -0.08650095\n",
      "   0.207662  ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:1 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.61472283]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 2 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.09369685 -0.19832663  0.00039379  0.29834805 -0.0512636  -0.1146833\n",
      "   0.0998057   0.07376832  0.14499415]\n",
      " [ 0.22028139  0.24749575 -0.12074912 -0.30389528 -0.21411219 -0.08365209\n",
      "  -0.33292042 -0.19235433 -0.14163019]\n",
      " [ 0.26696326 -0.36033693  0.06341401 -0.24606435  0.26811815 -0.23807609\n",
      "  -0.14187247  0.0321694   0.24993148]\n",
      " [ 0.06212135 -0.14233907  0.10918472  0.02552098  0.25091332  0.10715534\n",
      "   0.34214023 -0.2443162  -0.22066195]\n",
      " [-0.06594766  0.15682791  0.0036704   0.06684246 -0.27192395 -0.21947173\n",
      "  -0.34352456 -0.07830556 -0.0560296 ]\n",
      " [ 0.25082706  0.15105556  0.17902217 -0.30113579 -0.07536462 -0.03094976\n",
      "  -0.15183289 -0.07533441 -0.09337895]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.1971445   0.10367371 -0.12300518  0.26145922 -0.11546733 -0.124892\n",
      "   0.17111985]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:2 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.57761723]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 2 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.09361308 -0.19832663  0.00047755  0.29834805 -0.0512636  -0.1146833\n",
      "   0.09988946  0.07376832  0.14499415]\n",
      " [ 0.2173156   0.24749575 -0.12371491 -0.30389528 -0.21411219 -0.08365209\n",
      "  -0.33588621 -0.19235433 -0.14163019]\n",
      " [ 0.26937002 -0.36033693  0.06582077 -0.24606435  0.26811815 -0.23807609\n",
      "  -0.13946571  0.0321694   0.24993148]\n",
      " [ 0.06831781 -0.14233907  0.11538118  0.02552098  0.25091332  0.10715534\n",
      "   0.34833669 -0.2443162  -0.22066195]\n",
      " [-0.07096552  0.15682791 -0.00134745  0.06684246 -0.27192395 -0.21947173\n",
      "  -0.34854242 -0.07830556 -0.0560296 ]\n",
      " [ 0.25433998  0.15105556  0.18253509 -0.30113579 -0.07536462 -0.03094976\n",
      "  -0.14831997 -0.07533441 -0.09337895]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.24867004  0.12952025 -0.1002352   0.28964303 -0.08323223 -0.10428595\n",
      "   0.20044097]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:2 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.60274239]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 2 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.09018856 -0.19490211  0.00390208  0.29834805 -0.0512636  -0.1146833\n",
      "   0.10331399  0.07376832  0.14499415]\n",
      " [ 0.2172216   0.24740176 -0.12380891 -0.30389528 -0.21411219 -0.08365209\n",
      "  -0.3359802  -0.19235433 -0.14163019]\n",
      " [ 0.27231965 -0.3573873   0.0687704  -0.24606435  0.26811815 -0.23807609\n",
      "  -0.13651607  0.0321694   0.24993148]\n",
      " [ 0.06154783 -0.14910905  0.1086112   0.02552098  0.25091332  0.10715534\n",
      "   0.34156671 -0.2443162  -0.22066195]\n",
      " [-0.06628442  0.16150901  0.00333365  0.06684246 -0.27192395 -0.21947173\n",
      "  -0.34386132 -0.07830556 -0.0560296 ]\n",
      " [ 0.24678039  0.14349597  0.1749755  -0.30113579 -0.07536462 -0.03094976\n",
      "  -0.15587956 -0.07533441 -0.09337895]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.17650852  0.096885   -0.13640996  0.25652524 -0.12625563 -0.13563103\n",
      "   0.15655476]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:2 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.56945313]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 2 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.09035373 -0.19490211  0.00373691  0.29834805 -0.0512636  -0.1146833\n",
      "   0.10331399  0.07360315  0.14499415]\n",
      " [ 0.21591926  0.24740176 -0.12511125 -0.30389528 -0.21411219 -0.08365209\n",
      "  -0.3359802  -0.19365667 -0.14163019]\n",
      " [ 0.27707717 -0.3573873   0.07352792 -0.24606435  0.26811815 -0.23807609\n",
      "  -0.13651607  0.03692691  0.24993148]\n",
      " [ 0.06057067 -0.14910905  0.10763404  0.02552098  0.25091332  0.10715534\n",
      "   0.34156671 -0.24529336 -0.22066195]\n",
      " [-0.06813903  0.16150901  0.00147904  0.06684246 -0.27192395 -0.21947173\n",
      "  -0.34386132 -0.08016016 -0.0560296 ]\n",
      " [ 0.25121698  0.14349597  0.17941208 -0.30113579 -0.07536462 -0.03094976\n",
      "  -0.15587956 -0.07089783 -0.09337895]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.22928845  0.12310979 -0.11132446  0.28778396 -0.10084371 -0.11110185\n",
      "   0.18747058]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:2 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.59924164]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 2 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.08623674 -0.19490211  0.00373691  0.30246504 -0.0512636  -0.1146833\n",
      "   0.10331399  0.07360315  0.14911113]\n",
      " [ 0.21319311  0.24740176 -0.12511125 -0.30662143 -0.21411219 -0.08365209\n",
      "  -0.3359802  -0.19365667 -0.14435633]\n",
      " [ 0.28039119 -0.3573873   0.07352792 -0.24275032  0.26811815 -0.23807609\n",
      "  -0.13651607  0.03692691  0.25324551]\n",
      " [ 0.05895906 -0.14910905  0.10763404  0.02390937  0.25091332  0.10715534\n",
      "   0.34156671 -0.24529336 -0.22227356]\n",
      " [-0.06882812  0.16150901  0.00147904  0.06615338 -0.27192395 -0.21947173\n",
      "  -0.34386132 -0.08016016 -0.05671868]\n",
      " [ 0.24950188  0.14349597  0.17941208 -0.30285089 -0.07536462 -0.03094976\n",
      "  -0.15587956 -0.07089783 -0.09509405]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.27740973  0.15137344 -0.09001398  0.3152024  -0.07839956 -0.08773067\n",
      "   0.20981025]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:2 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.63271358]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 2 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.08434737 -0.19490211  0.00373691  0.30246504 -0.04937422 -0.11279392\n",
      "   0.10331399  0.07360315  0.15100051]\n",
      " [ 0.21734599  0.24740176 -0.12511125 -0.30662143 -0.20995931 -0.07949921\n",
      "  -0.3359802  -0.19365667 -0.14020345]\n",
      " [ 0.2708124  -0.3573873   0.07352792 -0.24275032  0.25853936 -0.24765488\n",
      "  -0.13651607  0.03692691  0.24366672]\n",
      " [ 0.05541334 -0.14910905  0.10763404  0.02390937  0.2473676   0.10360963\n",
      "   0.34156671 -0.24529336 -0.22581928]\n",
      " [-0.05850316  0.16150901  0.00147904  0.06615338 -0.261599   -0.20914678\n",
      "  -0.34386132 -0.08016016 -0.04639373]\n",
      " [ 0.24861846  0.14349597  0.17941208 -0.30285089 -0.07624804 -0.03183317\n",
      "  -0.15587956 -0.07089783 -0.09597746]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.2038925   0.11650755 -0.12258335  0.26834962 -0.11872635 -0.11349678\n",
      "   0.17216788]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:2 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.58066203]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 2 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.07932142 -0.18987616  0.00373691  0.30246504 -0.04434828 -0.10776797\n",
      "   0.10331399  0.07360315  0.15602645]\n",
      " [ 0.2167261   0.24678186 -0.12511125 -0.30662143 -0.21057921 -0.0801191\n",
      "  -0.3359802  -0.19365667 -0.14082335]\n",
      " [ 0.26786453 -0.36033517  0.07352792 -0.24275032  0.25559149 -0.25060275\n",
      "  -0.13651607  0.03692691  0.24071884]\n",
      " [ 0.05485743 -0.14966496  0.10763404  0.02390937  0.24681169  0.10305372\n",
      "   0.34156671 -0.24529336 -0.22637519]\n",
      " [-0.05148906  0.16852311  0.00147904  0.06615338 -0.2545849  -0.20213267\n",
      "  -0.34386132 -0.08016016 -0.03937963]\n",
      " [ 0.24532408  0.1402016   0.17941208 -0.30285089 -0.07954241 -0.03512755\n",
      "  -0.15587956 -0.07089783 -0.09927184]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.13319874  0.08625757 -0.15855025  0.23004098 -0.15462923 -0.14162734\n",
      "   0.13350717]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:2 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.53052565]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 2 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.07419955 -0.18475429  0.00373691  0.30246504 -0.03922641 -0.10264611\n",
      "   0.10843586  0.07360315  0.15602645]\n",
      " [ 0.21940335  0.24945911 -0.12511125 -0.30662143 -0.20790196 -0.07744185\n",
      "  -0.33330295 -0.19365667 -0.14082335]\n",
      " [ 0.27151831 -0.35668139  0.07352792 -0.24275032  0.25924527 -0.24694897\n",
      "  -0.13286229  0.03692691  0.24071884]\n",
      " [ 0.04583034 -0.15869205  0.10763404  0.02390937  0.2377846   0.09402663\n",
      "   0.33253962 -0.24529336 -0.22637519]\n",
      " [-0.04142136  0.17859081  0.00147904  0.06615338 -0.24451719 -0.19206497\n",
      "  -0.33379362 -0.08016016 -0.03937963]\n",
      " [ 0.24343127  0.13830878  0.17941208 -0.30285089 -0.08143523 -0.03702037\n",
      "  -0.15777237 -0.07089783 -0.09927184]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.06713021  0.05843193 -0.18889537  0.20069113 -0.19723575 -0.16379139\n",
      "   0.09857592]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:2 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.49447409]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 2 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.06902903 -0.18475429  0.00373691  0.30763556 -0.03922641 -0.10264611\n",
      "   0.11360638  0.07360315  0.15602645]\n",
      " [ 0.21304612  0.24945911 -0.12511125 -0.31297866 -0.20790196 -0.07744185\n",
      "  -0.33966018 -0.19365667 -0.14082335]\n",
      " [ 0.2698785  -0.35668139  0.07352792 -0.24439012  0.25924527 -0.24694897\n",
      "  -0.1345021   0.03692691  0.24071884]\n",
      " [ 0.05193434 -0.15869205  0.10763404  0.03001337  0.2377846   0.09402663\n",
      "   0.33864362 -0.24529336 -0.22637519]\n",
      " [-0.04618848  0.17859081  0.00147904  0.06138626 -0.24451719 -0.19206497\n",
      "  -0.33856073 -0.08016016 -0.03937963]\n",
      " [ 0.2400407   0.13830878  0.17941208 -0.30624146 -0.08143523 -0.03702037\n",
      "  -0.16116294 -0.07089783 -0.09927184]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.13031323  0.09529221 -0.16385012  0.23063987 -0.15937426 -0.13704325\n",
      "   0.12675014]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:2 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.53672241]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 2 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.07042671 -0.18475429  0.00373691  0.30763556 -0.04062409 -0.10404378\n",
      "   0.1122087   0.07360315  0.15602645]\n",
      " [ 0.20735892  0.24945911 -0.12511125 -0.31297866 -0.21358915 -0.08312905\n",
      "  -0.34534738 -0.19365667 -0.14082335]\n",
      " [ 0.27199334 -0.35668139  0.07352792 -0.24439012  0.26136011 -0.24483414\n",
      "  -0.13238726  0.03692691  0.24071884]\n",
      " [ 0.06108907 -0.15869205  0.10763404  0.03001337  0.24693933  0.10318136\n",
      "   0.34779835 -0.24529336 -0.22637519]\n",
      " [-0.05622513  0.17859081  0.00147904  0.06138626 -0.25455385 -0.20210163\n",
      "  -0.34859739 -0.08016016 -0.03937963]\n",
      " [ 0.23947103  0.13830878  0.17941208 -0.30624146 -0.0820049  -0.03759004\n",
      "  -0.16173261 -0.07089783 -0.09927184]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.18791056  0.12269099 -0.14090089  0.26156107 -0.1206036  -0.11944835\n",
      "   0.15497898]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:2 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57175842]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 2 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.07538305 -0.18475429  0.00373691  0.30267922 -0.04062409 -0.10900012\n",
      "   0.1122087   0.07360315  0.15107012]\n",
      " [ 0.21297243  0.24945911 -0.12511125 -0.30736515 -0.21358915 -0.07751554\n",
      "  -0.34534738 -0.19365667 -0.13520984]\n",
      " [ 0.27158237 -0.35668139  0.07352792 -0.24480109  0.26136011 -0.24524511\n",
      "  -0.13238726  0.03692691  0.24030787]\n",
      " [ 0.06165051 -0.15869205  0.10763404  0.03057481  0.24693933  0.10374279\n",
      "   0.34779835 -0.24529336 -0.22581375]\n",
      " [-0.05214687  0.17859081  0.00147904  0.06546453 -0.25455385 -0.19802337\n",
      "  -0.34859739 -0.08016016 -0.03530137]\n",
      " [ 0.24299779  0.13830878  0.17941208 -0.30271469 -0.0820049  -0.03406328\n",
      "  -0.16173261 -0.07089783 -0.09574508]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.11791282  0.08266641 -0.17018407  0.2261512  -0.15504094 -0.15033089\n",
      "   0.1235313 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:2 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.51624463]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 2 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.07469251 -0.18406376  0.00373691  0.30336975 -0.04062409 -0.10830958\n",
      "   0.11289923  0.07360315  0.15107012]\n",
      " [ 0.20899998  0.24548666 -0.12511125 -0.3113376  -0.21358915 -0.08148799\n",
      "  -0.34931983 -0.19365667 -0.13520984]\n",
      " [ 0.2621307  -0.36613307  0.07352792 -0.25425276  0.26136011 -0.25469678\n",
      "  -0.14183893  0.03692691  0.24030787]\n",
      " [ 0.06725532 -0.15308724  0.10763404  0.03617962  0.24693933  0.1093476\n",
      "   0.35340316 -0.24529336 -0.22581375]\n",
      " [-0.0573385   0.17339918  0.00147904  0.0602729  -0.25455385 -0.203215\n",
      "  -0.35378902 -0.08016016 -0.03530137]\n",
      " [ 0.24123391  0.1365449   0.17941208 -0.30447857 -0.0820049  -0.03582716\n",
      "  -0.16349649 -0.07089783 -0.09574508]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.17831842  0.11355999 -0.14400138  0.24609376 -0.11909379 -0.12542928\n",
      "   0.15196618]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:2 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.55347807]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 2 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.07253317 -0.18190442  0.00373691  0.30552909 -0.04062409 -0.10830958\n",
      "   0.11505857  0.07360315  0.15107012]\n",
      " [ 0.20618603  0.24267271 -0.12511125 -0.31415155 -0.21358915 -0.08148799\n",
      "  -0.35213378 -0.19365667 -0.13520984]\n",
      " [ 0.25564626 -0.3726175   0.07352792 -0.2607372   0.26136011 -0.25469678\n",
      "  -0.14832337  0.03692691  0.24030787]\n",
      " [ 0.07135014 -0.14899243  0.10763404  0.04027443  0.24693933  0.1093476\n",
      "   0.35749798 -0.24529336 -0.22581375]\n",
      " [-0.05976718  0.1709705   0.00147904  0.05784421 -0.25455385 -0.203215\n",
      "  -0.3562177  -0.08016016 -0.03530137]\n",
      " [ 0.23999226  0.13530324  0.17941208 -0.30572023 -0.0820049  -0.03582716\n",
      "  -0.16473815 -0.07089783 -0.09574508]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.23349515  0.14331663 -0.11924694  0.266924   -0.08734734 -0.10028236\n",
      "   0.1783112 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:2 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.59596583]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 2 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.073287   -0.18190442  0.00298308  0.30552909 -0.04062409 -0.10906341\n",
      "   0.11430474  0.07360315  0.15107012]\n",
      " [ 0.20202923  0.24267271 -0.12926805 -0.31415155 -0.21358915 -0.08564479\n",
      "  -0.35629058 -0.19365667 -0.13520984]\n",
      " [ 0.25474945 -0.3726175   0.0726311  -0.2607372   0.26136011 -0.25559359\n",
      "  -0.14922018  0.03692691  0.24030787]\n",
      " [ 0.07843882 -0.14899243  0.11472273  0.04027443  0.24693933  0.11643629\n",
      "   0.36458666 -0.24529336 -0.22581375]\n",
      " [-0.06660587  0.1709705  -0.00535965  0.05784421 -0.25455385 -0.21005369\n",
      "  -0.36305639 -0.08016016 -0.03530137]\n",
      " [ 0.24262194  0.13530324  0.18204177 -0.30572023 -0.0820049  -0.03319747\n",
      "  -0.16210847 -0.07089783 -0.09574508]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.28213896  0.16688422 -0.09916848  0.29034828 -0.0554336  -0.08324243\n",
      "   0.20528383]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:2 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: [0.] Net Result: [[0.62072501]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 3 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.07692902 -0.18554644  0.00298308  0.30188707 -0.04062409 -0.10906341\n",
      "   0.11430474  0.07360315  0.1474281 ]\n",
      " [ 0.20211434  0.24275782 -0.12926805 -0.31406644 -0.21358915 -0.08564479\n",
      "  -0.35629058 -0.19365667 -0.13512473]\n",
      " [ 0.25726366 -0.37010329  0.0726311  -0.25822299  0.26136011 -0.25559359\n",
      "  -0.14922018  0.03692691  0.24282208]\n",
      " [ 0.08304095 -0.1443903   0.11472273  0.04487657  0.24693933  0.11643629\n",
      "   0.36458666 -0.24529336 -0.22121162]\n",
      " [-0.06891476  0.16866161 -0.00535965  0.05553533 -0.25455385 -0.21005369\n",
      "  -0.36305639 -0.08016016 -0.03761025]\n",
      " [ 0.24305188  0.13573319  0.18204177 -0.30529029 -0.0820049  -0.03319747\n",
      "  -0.16210847 -0.07089783 -0.09531514]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.20907172  0.12668391 -0.13561699  0.25633689 -0.08731462 -0.12209114\n",
      "   0.16918019]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:3 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.58595524]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 3 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.07642246 -0.18554644  0.00348964  0.30188707 -0.04062409 -0.10906341\n",
      "   0.1148113   0.07360315  0.1474281 ]\n",
      " [ 0.1986258   0.24275782 -0.13275659 -0.31406644 -0.21358915 -0.08564479\n",
      "  -0.35977912 -0.19365667 -0.13512473]\n",
      " [ 0.25951389 -0.37010329  0.07488133 -0.25822299  0.26136011 -0.25559359\n",
      "  -0.14696995  0.03692691  0.24282208]\n",
      " [ 0.08957202 -0.1443903   0.12125379  0.04487657  0.24693933  0.11643629\n",
      "   0.37111773 -0.24529336 -0.22121162]\n",
      " [-0.07415169  0.16866161 -0.01059658  0.05553533 -0.25455385 -0.21005369\n",
      "  -0.36829332 -0.08016016 -0.03761025]\n",
      " [ 0.24629761  0.13573319  0.1852875  -0.30529029 -0.0820049  -0.03319747\n",
      "  -0.15886274 -0.07089783 -0.09531514]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.25929777  0.15230363 -0.1140394   0.28371241 -0.05532081 -0.10238359\n",
      "   0.19757649]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:3 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.6093239]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 3 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.073831   -0.18295498  0.0060811   0.30188707 -0.04062409 -0.10906341\n",
      "   0.11740276  0.07360315  0.1474281 ]\n",
      " [ 0.19955264  0.24368466 -0.13182975 -0.31406644 -0.21358915 -0.08564479\n",
      "  -0.35885228 -0.19365667 -0.13512473]\n",
      " [ 0.26279856 -0.36681863  0.078166   -0.25822299  0.26136011 -0.25559359\n",
      "  -0.14368529  0.03692691  0.24282208]\n",
      " [ 0.08200663 -0.15195568  0.11368841  0.04487657  0.24693933  0.11643629\n",
      "   0.36355234 -0.24529336 -0.22121162]\n",
      " [-0.06909843  0.17371487 -0.00554333  0.05553533 -0.25455385 -0.21005369\n",
      "  -0.36324007 -0.08016016 -0.03761025]\n",
      " [ 0.23919238  0.12862795  0.17818226 -0.30529029 -0.0820049  -0.03319747\n",
      "  -0.16596798 -0.07089783 -0.09531514]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.18677351  0.11864189 -0.14937428  0.25075324 -0.09939205 -0.13352407\n",
      "   0.1540099 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:3 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.57571569]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 3 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.07375517 -0.18295498  0.00615693  0.30188707 -0.04062409 -0.10906341\n",
      "   0.11740276  0.07367898  0.1474281 ]\n",
      " [ 0.19792764  0.24368466 -0.13345475 -0.31406644 -0.21358915 -0.08564479\n",
      "  -0.35885228 -0.19528168 -0.13512473]\n",
      " [ 0.26752338 -0.36681863  0.08289083 -0.25822299  0.26136011 -0.25559359\n",
      "  -0.14368529  0.04165174  0.24282208]\n",
      " [ 0.08136449 -0.15195568  0.11304626  0.04487657  0.24693933  0.11643629\n",
      "   0.36355234 -0.2459355  -0.22121162]\n",
      " [-0.0710919   0.17371487 -0.00753679  0.05553533 -0.25455385 -0.21005369\n",
      "  -0.36324007 -0.08215363 -0.03761025]\n",
      " [ 0.24354887  0.12862795  0.18253875 -0.30529029 -0.0820049  -0.03319747\n",
      "  -0.16596798 -0.06654133 -0.09531514]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.23859287  0.1446274  -0.1250939   0.28150101 -0.07412478 -0.10961583\n",
      "   0.18436376]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:3 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.60451202]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 3 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.06946933 -0.18295498  0.00615693  0.30617292 -0.04062409 -0.10906341\n",
      "   0.11740276  0.07367898  0.15171394]\n",
      " [ 0.19500433  0.24368466 -0.13345475 -0.31698974 -0.21358915 -0.08564479\n",
      "  -0.35885228 -0.19528168 -0.13804803]\n",
      " [ 0.27045637 -0.36681863  0.08289083 -0.25529001  0.26136011 -0.25559359\n",
      "  -0.14368529  0.04165174  0.24575507]\n",
      " [ 0.08024456 -0.15195568  0.11304626  0.04375663  0.24693933  0.11643629\n",
      "   0.36355234 -0.2459355  -0.22233155]\n",
      " [-0.07171984  0.17371487 -0.00753679  0.05490739 -0.25455385 -0.21005369\n",
      "  -0.36324007 -0.08215363 -0.03823819]\n",
      " [ 0.24170401  0.12862795  0.18253875 -0.30713514 -0.0820049  -0.03319747\n",
      "  -0.16596798 -0.06654133 -0.09715999]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.28586896  0.17265274 -0.10441002  0.30810321 -0.05160835 -0.08660602\n",
      "   0.20614936]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:3 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.63679662]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 3 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.0682291  -0.18295498  0.00615693  0.30617292 -0.03938385 -0.10782318\n",
      "   0.11740276  0.07367898  0.15295418]\n",
      " [ 0.19939992  0.24368466 -0.13345475 -0.31698974 -0.20919356 -0.0812492\n",
      "  -0.35885228 -0.19528168 -0.13365244]\n",
      " [ 0.26147254 -0.36681863  0.08289083 -0.25529001  0.25237628 -0.26457742\n",
      "  -0.14368529  0.04165174  0.23677124]\n",
      " [ 0.07622003 -0.15195568  0.11304626  0.04375663  0.24291481  0.11241176\n",
      "   0.36355234 -0.2459355  -0.22635608]\n",
      " [-0.06196903  0.17371487 -0.00753679  0.05490739 -0.24480305 -0.20030288\n",
      "  -0.36324007 -0.08215363 -0.02848738]\n",
      " [ 0.24116394  0.12862795  0.18253875 -0.30713514 -0.08254497 -0.03373755\n",
      "  -0.16596798 -0.06654133 -0.09770006]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.21222767  0.13707327 -0.13679195  0.26188519 -0.09248645 -0.11313044\n",
      "   0.16878856]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:3 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.58397912]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 3 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.06394129 -0.17866717  0.00615693  0.30617292 -0.03509604 -0.10353537\n",
      "   0.11740276  0.07367898  0.15724199]\n",
      " [ 0.19906319  0.24334792 -0.13345475 -0.31698974 -0.2095303  -0.08158594\n",
      "  -0.35885228 -0.19528168 -0.13398918]\n",
      " [ 0.25936566 -0.36892551  0.08289083 -0.25529001  0.25026939 -0.2666843\n",
      "  -0.14368529  0.04165174  0.23466436]\n",
      " [ 0.0752766  -0.15289911  0.11304626  0.04375663  0.24197138  0.11146834\n",
      "   0.36355234 -0.2459355  -0.22729951]\n",
      " [-0.05575739  0.17992651 -0.00753679  0.05490739 -0.23859141 -0.19409124\n",
      "  -0.36324007 -0.08215363 -0.02227574]\n",
      " [ 0.23841744  0.12588145  0.18253875 -0.30713514 -0.08529148 -0.03648405\n",
      "  -0.16596798 -0.06654133 -0.10044657]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.14128953  0.1059352  -0.17259778  0.22430424 -0.12889939 -0.14225142\n",
      "   0.13056187]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:3 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.53553012]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 3 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05962376 -0.17434964  0.00615693  0.30617292 -0.03077851 -0.09921784\n",
      "   0.12172029  0.07367898  0.15724199]\n",
      " [ 0.20248223  0.24676696 -0.13345475 -0.31698974 -0.20611125 -0.0781669\n",
      "  -0.35543324 -0.19528168 -0.13398918]\n",
      " [ 0.26377507 -0.3645161   0.08289083 -0.25529001  0.2546788  -0.26227489\n",
      "  -0.13927587  0.04165174  0.23466436]\n",
      " [ 0.06564854 -0.16252718  0.11304626  0.04375663  0.23234331  0.10184027\n",
      "   0.35392427 -0.2459355  -0.22729951]\n",
      " [-0.04574482  0.18993908 -0.00753679  0.05490739 -0.22857884 -0.18407867\n",
      "  -0.3532275  -0.08215363 -0.02227574]\n",
      " [ 0.2371446   0.1246086   0.18253875 -0.30713514 -0.08656432 -0.03775689\n",
      "  -0.16724082 -0.06654133 -0.10044657]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.07468629  0.07700137 -0.20245575  0.19546566 -0.1724986  -0.16477026\n",
      "   0.09598616]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:3 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.50044337]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 3 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05406526 -0.17434964  0.00615693  0.31173142 -0.03077851 -0.09921784\n",
      "   0.12727879  0.07367898  0.15724199]\n",
      " [ 0.19553651  0.24676696 -0.13345475 -0.32393546 -0.20611125 -0.0781669\n",
      "  -0.36237895 -0.19528168 -0.13398918]\n",
      " [ 0.26174198 -0.3645161   0.08289083 -0.25732309  0.2546788  -0.26227489\n",
      "  -0.14130896  0.04165174  0.23466436]\n",
      " [ 0.07250692 -0.16252718  0.11304626  0.05061502  0.23234331  0.10184027\n",
      "   0.36078266 -0.2459355  -0.22729951]\n",
      " [-0.05096019  0.18993908 -0.00753679  0.04969203 -0.22857884 -0.18407867\n",
      "  -0.35844286 -0.08215363 -0.02227574]\n",
      " [ 0.23349277  0.1246086   0.18253875 -0.31078697 -0.08656432 -0.03775689\n",
      "  -0.17089265 -0.06654133 -0.10044657]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.13713082  0.11390863 -0.17843769  0.22464904 -0.13416991 -0.13886687\n",
      "   0.12352224]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:3 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.54178131]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 3 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05487202 -0.17434964  0.00615693  0.31173142 -0.03158528 -0.1000246\n",
      "   0.12647203  0.07367898  0.15724199]\n",
      " [ 0.18943747  0.24676696 -0.13345475 -0.32393546 -0.21221029 -0.08426594\n",
      "  -0.36847799 -0.19528168 -0.13398918]\n",
      " [ 0.26334135 -0.3645161   0.08289083 -0.25732309  0.25627818 -0.26067552\n",
      "  -0.13970959  0.04165174  0.23466436]\n",
      " [ 0.08195845 -0.16252718  0.11304626  0.05061502  0.24179484  0.11129179\n",
      "   0.37023418 -0.2459355  -0.22729951]\n",
      " [-0.06087738  0.18993908 -0.00753679  0.04969203 -0.23849603 -0.19399586\n",
      "  -0.36836005 -0.08215363 -0.02227574]\n",
      " [ 0.23261597  0.1246086   0.18253875 -0.31078697 -0.08744112 -0.03863369\n",
      "  -0.17176945 -0.06654133 -0.10044657]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.19400821  0.14154013 -0.15630702  0.2546905  -0.09532414 -0.12150071\n",
      "   0.15108358]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:3 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57512779]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 3 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.06025554 -0.17434964  0.00615693  0.30634789 -0.03158528 -0.10540812\n",
      "   0.12647203  0.07367898  0.15185847]\n",
      " [ 0.19544543  0.24676696 -0.13345475 -0.3179275  -0.21221029 -0.07825798\n",
      "  -0.36847799 -0.19528168 -0.12798122]\n",
      " [ 0.26369253 -0.3645161   0.08289083 -0.25697191  0.25627818 -0.26032434\n",
      "  -0.13970959  0.04165174  0.23501553]\n",
      " [ 0.08166746 -0.16252718  0.11304626  0.05032403  0.24179484  0.1110008\n",
      "   0.37023418 -0.2459355  -0.2275905 ]\n",
      " [-0.05693288  0.18993908 -0.00753679  0.05363652 -0.23849603 -0.19005137\n",
      "  -0.36836005 -0.08215363 -0.01833125]\n",
      " [ 0.23638774  0.1246086   0.18253875 -0.3070152  -0.08744112 -0.03486192\n",
      "  -0.17176945 -0.06654133 -0.0966748 ]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.1237403   0.1009337  -0.18530764  0.21990775 -0.1307491  -0.15265607\n",
      "   0.11975113]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:3 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.52049628]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 3 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05887021 -0.1729643   0.00615693  0.30773323 -0.03158528 -0.10402279\n",
      "   0.12785736  0.07367898  0.15185847]\n",
      " [ 0.19074503  0.24206656 -0.13345475 -0.3226279  -0.21221029 -0.08295838\n",
      "  -0.3731784  -0.19528168 -0.12798122]\n",
      " [ 0.25383959 -0.37436904  0.08289083 -0.26682486  0.25627818 -0.27017728\n",
      "  -0.14956253  0.04165174  0.23501553]\n",
      " [ 0.08807849 -0.15611614  0.11304626  0.05673506  0.24179484  0.11741184\n",
      "   0.37664522 -0.2459355  -0.2275905 ]\n",
      " [-0.06230647  0.1845655  -0.00753679  0.04826294 -0.23849603 -0.19542495\n",
      "  -0.37373363 -0.08215363 -0.01833125]\n",
      " [ 0.23411744  0.12233831  0.18253875 -0.30928549 -0.08744112 -0.03713222\n",
      "  -0.17403974 -0.06654133 -0.0966748 ]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.18357754  0.13223965 -0.1601713   0.23900287 -0.09420018 -0.12823567\n",
      "   0.14739064]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:3 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.55743453]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 3 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05611807 -0.17021217  0.00615693  0.31048537 -0.03158528 -0.10402279\n",
      "   0.1306095   0.07367898  0.15185847]\n",
      " [ 0.18721712  0.23853865 -0.13345475 -0.32615582 -0.21221029 -0.08295838\n",
      "  -0.37670631 -0.19528168 -0.12798122]\n",
      " [ 0.24701565 -0.38119299  0.08289083 -0.2736488   0.25627818 -0.27017728\n",
      "  -0.15638647  0.04165174  0.23501553]\n",
      " [ 0.09290182 -0.15129281  0.11304626  0.0615584   0.24179484  0.11741184\n",
      "   0.38146855 -0.2459355  -0.2275905 ]\n",
      " [-0.0650514   0.18182057 -0.00753679  0.045518   -0.23849603 -0.19542495\n",
      "  -0.37647857 -0.08215363 -0.01833125]\n",
      " [ 0.23239292  0.12061379  0.18253875 -0.31101002 -0.08744112 -0.03713222\n",
      "  -0.17576426 -0.06654133 -0.0966748 ]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.23816827  0.16230624 -0.13644466  0.25914167 -0.06197346 -0.10370417\n",
      "   0.17295685]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:3 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.59992038]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 3 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.0563986  -0.17021217  0.0058764   0.31048537 -0.03158528 -0.10430331\n",
      "   0.13032897  0.07367898  0.15185847]\n",
      " [ 0.18254029  0.23853865 -0.13813158 -0.32615582 -0.21221029 -0.0876352\n",
      "  -0.38138314 -0.19528168 -0.12798122]\n",
      " [ 0.24585816 -0.38119299  0.08173334 -0.2736488   0.25627818 -0.27133477\n",
      "  -0.15754396  0.04165174  0.23501553]\n",
      " [ 0.10039254 -0.15129281  0.12053698  0.0615584   0.24179484  0.12490255\n",
      "   0.38895927 -0.2459355  -0.2275905 ]\n",
      " [-0.07203654  0.18182057 -0.01452194  0.045518   -0.23849603 -0.20241009\n",
      "  -0.38346371 -0.08215363 -0.01833125]\n",
      " [ 0.23479341  0.12061379  0.18493924 -0.31101002 -0.08744112 -0.03473173\n",
      "  -0.17336378 -0.06654133 -0.0966748 ]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.28618101  0.18603205 -0.1172446   0.28198875 -0.02984058 -0.08717665\n",
      "   0.19938007]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:3 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.6220892]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 4 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.06064876 -0.17446233  0.0058764   0.3062352  -0.03158528 -0.10430331\n",
      "   0.13032897  0.07367898  0.1476083 ]\n",
      " [ 0.18314447  0.23914283 -0.13813158 -0.32555164 -0.21221029 -0.0876352\n",
      "  -0.38138314 -0.19528168 -0.12737705]\n",
      " [ 0.24901456 -0.37803658  0.08173334 -0.27049239  0.25627818 -0.27133477\n",
      "  -0.15754396  0.04165174  0.23817194]\n",
      " [ 0.10431203 -0.14737332  0.12053698  0.06547789  0.24179484  0.12490255\n",
      "   0.38895927 -0.2459355  -0.22367101]\n",
      " [-0.07452882  0.17932828 -0.01452194  0.04302572 -0.23849603 -0.20241009\n",
      "  -0.38346371 -0.08215363 -0.02082353]\n",
      " [ 0.23574845  0.12156883  0.18493924 -0.31005497 -0.08744112 -0.03473173\n",
      "  -0.17336378 -0.06654133 -0.09571975]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.21305622  0.14518002 -0.15320271  0.2485987  -0.06245267 -0.12623913\n",
      "   0.16377315]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:4 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.58965739]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 4 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05971238 -0.17446233  0.00681279  0.3062352  -0.03158528 -0.10430331\n",
      "   0.13126536  0.07367898  0.1476083 ]\n",
      " [ 0.17908571  0.23914283 -0.14219034 -0.32555164 -0.21221029 -0.0876352\n",
      "  -0.3854419  -0.19528168 -0.12737705]\n",
      " [ 0.25114814 -0.37803658  0.08386691 -0.27049239  0.25627818 -0.27133477\n",
      "  -0.15541039  0.04165174  0.23817194]\n",
      " [ 0.11125517 -0.14737332  0.12748012  0.06547789  0.24179484  0.12490255\n",
      "   0.39590241 -0.2459355  -0.22367101]\n",
      " [-0.08007762  0.17932828 -0.02007074  0.04302572 -0.23849603 -0.20241009\n",
      "  -0.38901251 -0.08215363 -0.02082353]\n",
      " [ 0.2387715   0.12156883  0.18796228 -0.31005497 -0.08744112 -0.03473173\n",
      "  -0.17034073 -0.06654133 -0.09571975]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26269979  0.17093908 -0.13251666  0.27556474 -0.03024348 -0.10717494\n",
      "   0.1916489 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:4 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.61187955]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 4 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05797091 -0.17272087  0.00855425  0.3062352  -0.03158528 -0.10430331\n",
      "   0.13300682  0.07367898  0.1476083 ]\n",
      " [ 0.18106697  0.24112409 -0.14020908 -0.32555164 -0.21221029 -0.0876352\n",
      "  -0.38346064 -0.19528168 -0.12737705]\n",
      " [ 0.25471718 -0.37446754  0.08743595 -0.27049239  0.25627818 -0.27133477\n",
      "  -0.15184135  0.04165174  0.23817194]\n",
      " [ 0.10290979 -0.1557187   0.11913474  0.06547789  0.24179484  0.12490255\n",
      "   0.38755703 -0.2459355  -0.22367101]\n",
      " [-0.07458279  0.18482311 -0.01457591  0.04302572 -0.23849603 -0.20241009\n",
      "  -0.38351768 -0.08215363 -0.02082353]\n",
      " [ 0.23214571  0.11494304  0.1813365  -0.31005497 -0.08744112 -0.03473173\n",
      "  -0.17696652 -0.06654133 -0.09571975]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.1900443   0.13635548 -0.16685919  0.24282951 -0.07525077 -0.13791952\n",
      "   0.14853648]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:4 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.57768122]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 4 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05765849 -0.17272087  0.00886668  0.3062352  -0.03158528 -0.10430331\n",
      "   0.13300682  0.07399141  0.1476083 ]\n",
      " [ 0.17908997  0.24112409 -0.14218608 -0.32555164 -0.21221029 -0.0876352\n",
      "  -0.38346064 -0.19725868 -0.12737705]\n",
      " [ 0.25948251 -0.37446754  0.09220129 -0.27049239  0.25627818 -0.27133477\n",
      "  -0.15184135  0.04641707  0.23817194]\n",
      " [ 0.10260215 -0.1557187   0.1188271   0.06547789  0.24179484  0.12490255\n",
      "   0.38755703 -0.24624315 -0.22367101]\n",
      " [-0.076773    0.18482311 -0.01676611  0.04302572 -0.23849603 -0.20241009\n",
      "  -0.38351768 -0.08434384 -0.02082353]\n",
      " [ 0.23648212  0.11494304  0.18567291 -0.31005497 -0.08744112 -0.03473173\n",
      "  -0.17696652 -0.06220492 -0.09571975]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.24155994  0.16242576 -0.14308624  0.27347052 -0.04980063 -0.11436264\n",
      "   0.17871823]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:4 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.60611906]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 4 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05317966 -0.17272087  0.00886668  0.31071403 -0.03158528 -0.10430331\n",
      "   0.13300682  0.07399141  0.15208713]\n",
      " [ 0.17593077  0.24112409 -0.14218608 -0.32871084 -0.21221029 -0.0876352\n",
      "  -0.38346064 -0.19725868 -0.13053624]\n",
      " [ 0.2621185  -0.37446754  0.09220129 -0.26785641  0.25627818 -0.27133477\n",
      "  -0.15184135  0.04641707  0.24080792]\n",
      " [ 0.10194922 -0.1557187   0.1188271   0.06482496  0.24179484  0.12490255\n",
      "   0.38755703 -0.24624315 -0.22432394]\n",
      " [-0.07741396  0.18482311 -0.01676611  0.04238476 -0.23849603 -0.20241009\n",
      "  -0.38351768 -0.08434384 -0.0214645 ]\n",
      " [ 0.23450639  0.11494304  0.18567291 -0.3120307  -0.08744112 -0.03473173\n",
      "  -0.17696652 -0.06220492 -0.09769549]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.28857726  0.19053133 -0.1227764   0.29963789 -0.02694524 -0.09149526\n",
      "   0.20024171]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:4 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.63787895]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 4 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05249878 -0.17272087  0.00886668  0.31071403 -0.03090439 -0.10362243\n",
      "   0.13300682  0.07399141  0.15276801]\n",
      " [ 0.18054218  0.24112409 -0.14218608 -0.32871084 -0.20759888 -0.0830238\n",
      "  -0.38346064 -0.19725868 -0.12592483]\n",
      " [ 0.25364712 -0.37446754  0.09220129 -0.26785641  0.24780679 -0.27980615\n",
      "  -0.15184135  0.04641707  0.23233654]\n",
      " [ 0.0975158  -0.1557187   0.1188271   0.06482496  0.23736142  0.12046913\n",
      "   0.38755703 -0.24624315 -0.22875736]\n",
      " [-0.06816267  0.18482311 -0.01676611  0.04238476 -0.22924473 -0.19315879\n",
      "  -0.38351768 -0.08434384 -0.0122132 ]\n",
      " [ 0.2342368   0.11494304  0.18567291 -0.3120307  -0.08771071 -0.03500132\n",
      "  -0.17696652 -0.06220492 -0.09796507]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.21490562  0.15437655 -0.15495089  0.25399062 -0.06825871 -0.11862393\n",
      "   0.16313629]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:4 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.58419161]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 4 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.04886636 -0.16908845  0.00886668  0.31071403 -0.02727197 -0.09999001\n",
      "   0.13300682  0.07399141  0.15640043]\n",
      " [ 0.18045138  0.24103329 -0.14218608 -0.32871084 -0.20768968 -0.08311459\n",
      "  -0.38346064 -0.19725868 -0.12601563]\n",
      " [ 0.25223884 -0.37587581  0.09220129 -0.26785641  0.24639852 -0.28121442\n",
      "  -0.15184135  0.04641707  0.23092827]\n",
      " [ 0.09626025 -0.15697424  0.1188271   0.06482496  0.23610587  0.11921358\n",
      "   0.38755703 -0.24624315 -0.2300129 ]\n",
      " [-0.06266282  0.19032296 -0.01676611  0.04238476 -0.22374488 -0.18765894\n",
      "  -0.38351768 -0.08434384 -0.00671335]\n",
      " [ 0.23196676  0.112673    0.18567291 -0.3120307  -0.08998075 -0.03727136\n",
      "  -0.17696652 -0.06220492 -0.10023512]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.14395211  0.12255821 -0.19051844  0.21710411 -0.10499207 -0.1485077\n",
      "   0.12538324]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:4 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.53772188]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 4 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.04536034 -0.16558243  0.00886668  0.31071403 -0.02376596 -0.09648399\n",
      "   0.13651284  0.07399141  0.15640043]\n",
      " [ 0.18460812  0.24519003 -0.14218608 -0.32871084 -0.20353294 -0.07895786\n",
      "  -0.3793039  -0.19725868 -0.12601563]\n",
      " [ 0.25730047 -0.37081418  0.09220129 -0.26785641  0.25146015 -0.27615279\n",
      "  -0.14677972  0.04641707  0.23092827]\n",
      " [ 0.08609208 -0.16714241  0.1188271   0.06482496  0.2259377   0.10904542\n",
      "   0.37738886 -0.24624315 -0.2300129 ]\n",
      " [-0.05266846  0.20031732 -0.01676611  0.04238476 -0.21375052 -0.17766459\n",
      "  -0.37352333 -0.08434384 -0.00671335]\n",
      " [ 0.23129167  0.11199791  0.18567291 -0.3120307  -0.09065584 -0.03794645\n",
      "  -0.17764161 -0.06220492 -0.10023512]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.07711945  0.09267427 -0.21973362  0.18883102 -0.14938374 -0.17117135\n",
      "   0.09129164]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:4 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.50358489]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 4 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.03937158 -0.16558243  0.00886668  0.31670279 -0.02376596 -0.09648399\n",
      "   0.1425016   0.07399141  0.15640043]\n",
      " [ 0.17702058  0.24519003 -0.14218608 -0.33629838 -0.20353294 -0.07895786\n",
      "  -0.38689144 -0.19725868 -0.12601563]\n",
      " [ 0.2548749  -0.37081418  0.09220129 -0.27028199  0.25146015 -0.27615279\n",
      "  -0.14920529  0.04641707  0.23092827]\n",
      " [ 0.09374101 -0.16714241  0.1188271   0.07247388  0.2259377   0.10904542\n",
      "   0.38503779 -0.24624315 -0.2300129 ]\n",
      " [-0.05840816  0.20031732 -0.01676611  0.03664506 -0.21375052 -0.17766459\n",
      "  -0.37926302 -0.08434384 -0.00671335]\n",
      " [ 0.22734978  0.11199791  0.18567291 -0.31597259 -0.09065584 -0.03794645\n",
      "  -0.18158349 -0.06220492 -0.10023512]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.13916815  0.12984988 -0.19664803  0.21741978 -0.11034965 -0.14602865\n",
      "   0.11833009]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:4 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.5443667]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 4 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.0396134  -0.16558243  0.00886668  0.31670279 -0.02400778 -0.09672582\n",
      "   0.14225977  0.07399141  0.15640043]\n",
      " [ 0.17047033  0.24519003 -0.14218608 -0.33629838 -0.21008319 -0.08550811\n",
      "  -0.39344169 -0.19725868 -0.12601563]\n",
      " [ 0.25601694 -0.37081418  0.09220129 -0.27028199  0.2526022  -0.27501075\n",
      "  -0.14806325  0.04641707  0.23092827]\n",
      " [ 0.10352536 -0.16714241  0.1188271   0.07247388  0.23572205  0.11882977\n",
      "   0.39482214 -0.24624315 -0.2300129 ]\n",
      " [-0.0683175   0.20031732 -0.01676611  0.03664506 -0.22365986 -0.18757393\n",
      "  -0.38917237 -0.08434384 -0.00671335]\n",
      " [ 0.22618161  0.11199791  0.18567291 -0.31597259 -0.09182401 -0.03911462\n",
      "  -0.18275167 -0.06220492 -0.10023512]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.19567388  0.15786091 -0.1752133   0.24681593 -0.07119624 -0.12886005\n",
      "   0.14541345]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:4 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57610119]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 4 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.04537098 -0.16558243  0.00886668  0.31094521 -0.02400778 -0.1024834\n",
      "   0.14225977  0.07399141  0.15064285]\n",
      " [ 0.17687574  0.24519003 -0.14218608 -0.32989297 -0.21008319 -0.0791027\n",
      "  -0.39344169 -0.19725868 -0.11961022]\n",
      " [ 0.25704217 -0.37081418  0.09220129 -0.26925675  0.2526022  -0.27398552\n",
      "  -0.14806325  0.04641707  0.2319535 ]\n",
      " [ 0.10238669 -0.16714241  0.1188271   0.07133522  0.23572205  0.1176911\n",
      "   0.39482214 -0.24624315 -0.23115157]\n",
      " [-0.06439404  0.20031732 -0.01676611  0.04056852 -0.22365986 -0.18365047\n",
      "  -0.38917237 -0.08434384 -0.00278989]\n",
      " [ 0.23015887  0.11199791  0.18567291 -0.31199533 -0.09182401 -0.03513736\n",
      "  -0.18275167 -0.06220492 -0.09625786]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.12532944  0.11682167 -0.20382701  0.21266953 -0.10750792 -0.16007534\n",
      "   0.11425338]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:4 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.52267315]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 4 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.04330055 -0.163512    0.00886668  0.31301565 -0.02400778 -0.10041296\n",
      "   0.14433021  0.07399141  0.15064285]\n",
      " [ 0.17141357  0.23972786 -0.14218608 -0.33535514 -0.21008319 -0.08456487\n",
      "  -0.39890386 -0.19725868 -0.11961022]\n",
      " [ 0.24680783 -0.38104852  0.09220129 -0.27949109  0.2526022  -0.28421986\n",
      "  -0.15829759  0.04641707  0.2319535 ]\n",
      " [ 0.10961578 -0.15991332  0.1188271   0.07856431  0.23572205  0.12492019\n",
      "   0.40205123 -0.24624315 -0.23115157]\n",
      " [-0.070068    0.19464336 -0.01676611  0.03489456 -0.22365986 -0.18932443\n",
      "  -0.39484632 -0.08434384 -0.00278989]\n",
      " [ 0.22738887  0.10922791  0.18567291 -0.31476533 -0.09182401 -0.03790736\n",
      "  -0.18552166 -0.06220492 -0.09625786]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.18487261  0.14867043 -0.17965026  0.23106482 -0.07017819 -0.13612742\n",
      "   0.14123867]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:4 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.55937662]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 4 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.0399523  -0.16016375  0.00886668  0.31636389 -0.02400778 -0.10041296\n",
      "   0.14767845  0.07399141  0.15064285]\n",
      " [ 0.16713968  0.23545397 -0.14218608 -0.33962903 -0.21008319 -0.08456487\n",
      "  -0.40317775 -0.19725868 -0.11961022]\n",
      " [ 0.23964451 -0.38821184  0.09220129 -0.28665442  0.2526022  -0.28421986\n",
      "  -0.16546091  0.04641707  0.2319535 ]\n",
      " [ 0.11519518 -0.15433392  0.1188271   0.0841437   0.23572205  0.12492019\n",
      "   0.40763062 -0.24624315 -0.23115157]\n",
      " [-0.07321945  0.19149191 -0.01676611  0.03174311 -0.22365986 -0.18932443\n",
      "  -0.39799778 -0.08434384 -0.00278989]\n",
      " [ 0.22518181  0.10702085  0.18567291 -0.31697239 -0.09182401 -0.03790736\n",
      "  -0.18772872 -0.06220492 -0.09625786]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.2391738   0.17920441 -0.15684831  0.25065499 -0.0372744  -0.11215746\n",
      "   0.16617234]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:4 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.60213899]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 4 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.03975954 -0.16016375  0.00905943  0.31636389 -0.02400778 -0.10022021\n",
      "   0.14787121  0.07399141  0.15064285]\n",
      " [ 0.16191086  0.23545397 -0.1474149  -0.33962903 -0.21008319 -0.08979369\n",
      "  -0.40840657 -0.19725868 -0.11961022]\n",
      " [ 0.23824545 -0.38821184  0.09080222 -0.28665442  0.2526022  -0.28561892\n",
      "  -0.16685998  0.04641707  0.2319535 ]\n",
      " [ 0.12310788 -0.15433392  0.1267398   0.0841437   0.23572205  0.1328329\n",
      "   0.41554333 -0.24624315 -0.23115157]\n",
      " [-0.08043006  0.19149191 -0.02397672  0.03174311 -0.22365986 -0.19653503\n",
      "  -0.40520838 -0.08434384 -0.00278989]\n",
      " [ 0.22736975  0.10702085  0.18786085 -0.31697239 -0.09182401 -0.03571942\n",
      "  -0.18554079 -0.06220492 -0.09625786]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.28683111  0.20322584 -0.13843713  0.27308134 -0.00473498 -0.0961035\n",
      "   0.19220147]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:4 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.62157045]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 5 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.04455468 -0.16495888  0.00905943  0.31156876 -0.02400778 -0.10022021\n",
      "   0.14787121  0.07399141  0.14584772]\n",
      " [ 0.16304058  0.23658369 -0.1474149  -0.33849931 -0.21008319 -0.08979369\n",
      "  -0.40840657 -0.19725868 -0.1184805 ]\n",
      " [ 0.241947   -0.38451029  0.09080222 -0.28295286  0.2526022  -0.28561892\n",
      "  -0.16685998  0.04641707  0.23565506]\n",
      " [ 0.12633951 -0.15110229  0.1267398   0.08737533  0.23572205  0.1328329\n",
      "   0.41554333 -0.24624315 -0.22791994]\n",
      " [-0.08297644  0.18894552 -0.02397672  0.02919672 -0.22365986 -0.19653503\n",
      "  -0.40520838 -0.08434384 -0.00533627]\n",
      " [ 0.22880837  0.10845947  0.18786085 -0.31553377 -0.09182401 -0.03571942\n",
      "  -0.18554079 -0.06220492 -0.09481924]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.21372802  0.16182194 -0.17385824  0.24025725 -0.03803776 -0.13520976\n",
      "   0.15709003]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:5 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.59168783]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 5 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.04317337 -0.16495888  0.01044074  0.31156876 -0.02400778 -0.10022021\n",
      "   0.14925252  0.07399141  0.14584772]\n",
      " [ 0.15837942  0.23658369 -0.15207606 -0.33849931 -0.21008319 -0.08979369\n",
      "  -0.41306773 -0.19725868 -0.1184805 ]\n",
      " [ 0.24397852 -0.38451029  0.09283374 -0.28295286  0.2526022  -0.28561892\n",
      "  -0.16482846  0.04641707  0.23565506]\n",
      " [ 0.13372719 -0.15110229  0.13412748  0.08737533  0.23572205  0.1328329\n",
      "   0.42293101 -0.24624315 -0.22791994]\n",
      " [-0.08889506  0.18894552 -0.02989534  0.02919672 -0.22365986 -0.19653503\n",
      "  -0.411127   -0.08434384 -0.00533627]\n",
      " [ 0.23162061  0.10845947  0.19067309 -0.31553377 -0.09182401 -0.03571942\n",
      "  -0.18272854 -0.06220492 -0.09481924]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26305077  0.18786753 -0.1539788   0.26695947 -0.00542582 -0.11672917\n",
      "   0.18458876]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:5 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.61298145]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 5 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.04229338 -0.16407889  0.01132073  0.31156876 -0.02400778 -0.10022021\n",
      "   0.15013251  0.07399141  0.14584772]\n",
      " [ 0.16145061  0.23965487 -0.14900487 -0.33849931 -0.21008319 -0.08979369\n",
      "  -0.40999654 -0.19725868 -0.1184805 ]\n",
      " [ 0.24779845 -0.38069037  0.09665367 -0.28295286  0.2526022  -0.28561892\n",
      "  -0.16100853  0.04641707  0.23565506]\n",
      " [ 0.12459807 -0.16023142  0.12499836  0.08737533  0.23572205  0.1328329\n",
      "   0.41380189 -0.24624315 -0.22791994]\n",
      " [-0.08287373  0.19496686 -0.023874    0.02919672 -0.22365986 -0.19653503\n",
      "  -0.40510567 -0.08434384 -0.00533627]\n",
      " [ 0.22548212  0.10232098  0.1845346  -0.31553377 -0.09182401 -0.03571942\n",
      "  -0.18886703 -0.06220492 -0.09481924]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.19034038  0.15239267 -0.18724796  0.23445302 -0.05135979 -0.14694567\n",
      "   0.1419704 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:5 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.57787049]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 5 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.04173992 -0.16407889  0.01187419  0.31156876 -0.02400778 -0.10022021\n",
      "   0.15013251  0.07454487  0.14584772]\n",
      " [ 0.15909197  0.23965487 -0.15136351 -0.33849931 -0.21008319 -0.08979369\n",
      "  -0.40999654 -0.19961731 -0.1184805 ]\n",
      " [ 0.25264218 -0.38069037  0.1014974  -0.28295286  0.2526022  -0.28561892\n",
      "  -0.16100853  0.05126081  0.23565506]\n",
      " [ 0.12464123 -0.16023142  0.12504152  0.08737533  0.23572205  0.1328329\n",
      "   0.41380189 -0.24619998 -0.22791994]\n",
      " [-0.08531106  0.19496686 -0.02631134  0.02919672 -0.22365986 -0.19653503\n",
      "  -0.40510567 -0.08678117 -0.00533627]\n",
      " [ 0.2298263   0.10232098  0.18887878 -0.31553377 -0.09182401 -0.03571942\n",
      "  -0.18886703 -0.05786074 -0.09481924]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.24182671  0.17868947 -0.16387688  0.26516421 -0.02557347 -0.1236547\n",
      "   0.17214586]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:5 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.6062348]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 5 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.0370609  -0.16407889  0.01187419  0.31624778 -0.02400778 -0.10022021\n",
      "   0.15013251  0.07454487  0.15052674]\n",
      " [ 0.1556684   0.23965487 -0.15136351 -0.34192288 -0.21008319 -0.08979369\n",
      "  -0.40999654 -0.19961731 -0.12190407]\n",
      " [ 0.25502965 -0.38069037  0.1014974  -0.28056539  0.2526022  -0.28561892\n",
      "  -0.16100853  0.05126081  0.23804253]\n",
      " [ 0.12445438 -0.16023142  0.12504152  0.08718848  0.23572205  0.1328329\n",
      "   0.41380189 -0.24619998 -0.22810679]\n",
      " [-0.0860324   0.19496686 -0.02631134  0.02847538 -0.22365986 -0.19653503\n",
      "  -0.40510567 -0.08678117 -0.00605762]\n",
      " [ 0.22772236  0.10232098  0.18887878 -0.31763771 -0.09182401 -0.03571942\n",
      "  -0.18886703 -0.05786074 -0.09692318]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.28882538  0.20700374 -0.14385198  0.29106782 -0.00226099 -0.10087716\n",
      "   0.19352981]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:5 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.63775643]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 5 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.0368627  -0.16407889  0.01187419  0.31624778 -0.02380958 -0.100022\n",
      "   0.15013251  0.07454487  0.15072494]\n",
      " [ 0.16048366  0.23965487 -0.15136351 -0.34192288 -0.20526794 -0.08497843\n",
      "  -0.40999654 -0.19961731 -0.11708881]\n",
      " [ 0.24698975 -0.38069037  0.1014974  -0.28056539  0.2445623  -0.29365882\n",
      "  -0.16100853  0.05126081  0.23000263]\n",
      " [ 0.11966025 -0.16023142  0.12504152  0.08718848  0.23092792  0.12803877\n",
      "   0.41380189 -0.24619998 -0.23290092]\n",
      " [-0.07719052  0.19496686 -0.02631134  0.02847538 -0.21481799 -0.18769315\n",
      "  -0.40510567 -0.08678117  0.00278426]\n",
      " [ 0.2276624   0.10232098  0.18887878 -0.31763771 -0.09188397 -0.03577939\n",
      "  -0.18886703 -0.05786074 -0.09698314]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.21515713  0.17036781 -0.17581381  0.24590717 -0.04394551 -0.12847757\n",
      "   0.15663572]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:5 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.58309613]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 5 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.03380207 -0.16101826  0.01187419  0.31624778 -0.02074895 -0.09696137\n",
      "   0.15013251  0.07454487  0.15378557]\n",
      " [ 0.16061117  0.23978239 -0.15136351 -0.34192288 -0.20514042 -0.08485092\n",
      "  -0.40999654 -0.19961731 -0.1169613 ]\n",
      " [ 0.24615381 -0.38152631  0.1014974  -0.28056539  0.24372635 -0.29449477\n",
      "  -0.16100853  0.05126081  0.22916669]\n",
      " [ 0.11814818 -0.16174348  0.12504152  0.08718848  0.22941585  0.1265267\n",
      "   0.41380189 -0.24619998 -0.23441299]\n",
      " [-0.07229278  0.19986461 -0.02631134  0.02847538 -0.20992024 -0.18279541\n",
      "  -0.40510567 -0.08678117  0.00768201]\n",
      " [ 0.22580116  0.10045974  0.18887878 -0.31763771 -0.09374521 -0.03764063\n",
      "  -0.18886703 -0.05786074 -0.09884438]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.14428324  0.13800698 -0.21112324  0.20963397 -0.08089637 -0.15895161\n",
      "   0.11933409]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:5 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.53896175]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 5 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.03110132 -0.15831752  0.01187419  0.31624778 -0.0180482  -0.09426063\n",
      "   0.15283326  0.07454487  0.15378557]\n",
      " [ 0.16551559  0.2446868  -0.15136351 -0.34192288 -0.20023601 -0.0799465\n",
      "  -0.40509212 -0.19961731 -0.1169613 ]\n",
      " [ 0.25179356 -0.37588656  0.1014974  -0.28056539  0.2493661  -0.28885501\n",
      "  -0.15536878  0.05126081  0.22916669]\n",
      " [ 0.10746369 -0.17242798  0.12504152  0.08718848  0.21873136  0.11584221\n",
      "   0.40311739 -0.24619998 -0.23441299]\n",
      " [-0.06224405  0.20991333 -0.02631134  0.02847538 -0.19987151 -0.17274668\n",
      "  -0.39505694 -0.08678117  0.00768201]\n",
      " [ 0.22570058  0.10035917  0.18887878 -0.31763771 -0.09384579 -0.0377412\n",
      "  -0.18896761 -0.05786074 -0.09884438]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.0773221   0.10723904 -0.2396257   0.18190712 -0.12602547 -0.18161401\n",
      "   0.08575295]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:5 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.50578345]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 5 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.02465221 -0.15831752  0.01187419  0.32269689 -0.0180482  -0.09426063\n",
      "   0.15928237  0.07454487  0.15378557]\n",
      " [ 0.15725424  0.2446868  -0.15136351 -0.35018423 -0.20023601 -0.0799465\n",
      "  -0.41335347 -0.19961731 -0.1169613 ]\n",
      " [ 0.24897399 -0.37588656  0.1014974  -0.28338496  0.2493661  -0.28885501\n",
      "  -0.15818835  0.05126081  0.22916669]\n",
      " [ 0.11591669 -0.17242798  0.12504152  0.09564148  0.21873136  0.11584221\n",
      "   0.41157039 -0.24619998 -0.23441299]\n",
      " [-0.06857072  0.20991333 -0.02631134  0.02214871 -0.19987151 -0.17274668\n",
      "  -0.40138361 -0.08678117  0.00768201]\n",
      " [ 0.22144725  0.10035917  0.18887878 -0.32189104 -0.09384579 -0.0377412\n",
      "  -0.19322094 -0.05786074 -0.09884438]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.1390909   0.14478073 -0.21747617  0.20995599 -0.08617559 -0.15725197\n",
      "   0.11232786]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:5 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.54612888]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 5 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.02433835 -0.15831752  0.01187419  0.32269689 -0.01773434 -0.09394677\n",
      "   0.15959623  0.07454487  0.15378557]\n",
      " [ 0.15022991  0.2446868  -0.15136351 -0.35018423 -0.20726034 -0.08697083\n",
      "  -0.4203778  -0.19961731 -0.1169613 ]\n",
      " [ 0.24969489 -0.37588656  0.1014974  -0.28338496  0.250087   -0.28813412\n",
      "  -0.15746745  0.05126081  0.22916669]\n",
      " [ 0.12603954 -0.17242798  0.12504152  0.09564148  0.22885421  0.12596506\n",
      "   0.42169325 -0.24619998 -0.23441299]\n",
      " [-0.07854257  0.20991333 -0.02631134  0.02214871 -0.20984336 -0.18271853\n",
      "  -0.41135546 -0.08678117  0.00768201]\n",
      " [ 0.21999759  0.10035917  0.18887878 -0.32189104 -0.09529545 -0.03919086\n",
      "  -0.1946706  -0.05786074 -0.09884438]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.1953419   0.17322012 -0.19671657  0.2388027  -0.04662602 -0.1403208\n",
      "   0.13900112]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:5 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57612982]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 5 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.03044002 -0.15831752  0.01187419  0.31659522 -0.01773434 -0.10004844\n",
      "   0.15959623  0.07454487  0.14768389]\n",
      " [ 0.15705095  0.2446868  -0.15136351 -0.34336318 -0.20726034 -0.08014978\n",
      "  -0.4203778  -0.19961731 -0.11014025]\n",
      " [ 0.25132093 -0.37588656  0.1014974  -0.28175891  0.250087   -0.28650807\n",
      "  -0.15746745  0.05126081  0.23079273]\n",
      " [ 0.12405452 -0.17242798  0.12504152  0.09365646  0.22885421  0.12398004\n",
      "   0.42169325 -0.24619998 -0.23639801]\n",
      " [-0.0745265   0.20991333 -0.02631134  0.02616478 -0.20984336 -0.17870246\n",
      "  -0.41135546 -0.08678117  0.01169808]\n",
      " [ 0.22415699  0.10035917  0.18887878 -0.31773164 -0.09529545 -0.03503146\n",
      "  -0.1946706  -0.05786074 -0.09468499]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.12499522  0.13181378 -0.2248819   0.20525774 -0.08378862 -0.17144212\n",
      "   0.1080272 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:5 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.52422827]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 5 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.0276848  -0.15556229  0.01187419  0.31935044 -0.01773434 -0.09729322\n",
      "   0.16235146  0.07454487  0.14768389]\n",
      " [ 0.1508029   0.23843875 -0.15136351 -0.34961124 -0.20726034 -0.08639784\n",
      "  -0.42662586 -0.19961731 -0.11014025]\n",
      " [ 0.2407392  -0.38646829  0.1014974  -0.29234065  0.250087   -0.29708981\n",
      "  -0.16804918  0.05126081  0.23079273]\n",
      " [ 0.13209728 -0.16438522  0.12504152  0.10169922  0.22885421  0.1320228\n",
      "   0.42973601 -0.24619998 -0.23639801]\n",
      " [-0.08059945  0.20384038 -0.02631134  0.02009183 -0.20984336 -0.18477541\n",
      "  -0.41742841 -0.08678117  0.01169808]\n",
      " [ 0.2208912   0.09709338  0.18887878 -0.32099743 -0.09529545 -0.03829725\n",
      "  -0.19793638 -0.05786074 -0.09468499]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.18432705  0.16425107 -0.20166962  0.2230199  -0.04560358 -0.14803672\n",
      "   0.13440021]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:5 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.56068053]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 5 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.02373632 -0.15161382  0.01187419  0.32329892 -0.01773434 -0.09729322\n",
      "   0.16629993  0.07454487  0.14768389]\n",
      " [ 0.1457594   0.23339525 -0.15136351 -0.35465474 -0.20726034 -0.08639784\n",
      "  -0.43166936 -0.19961731 -0.11014025]\n",
      " [ 0.23324971 -0.39395779  0.1014974  -0.29983014  0.250087   -0.29708981\n",
      "  -0.17553868  0.05126081  0.23079273]\n",
      " [ 0.13844533 -0.15803716  0.12504152  0.10804727  0.22885421  0.1320228\n",
      "   0.43608406 -0.24619998 -0.23639801]\n",
      " [-0.08423824  0.20020159 -0.02631134  0.01645304 -0.20984336 -0.18477541\n",
      "  -0.4210672  -0.08678117  0.01169808]\n",
      " [ 0.21820031  0.09440249  0.18887878 -0.32368832 -0.09529545 -0.03829725\n",
      "  -0.20062727 -0.05786074 -0.09468499]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.23843317  0.19531149 -0.1797869   0.24211639 -0.01193556 -0.12466818\n",
      "   0.15874424]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:5 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.6038479]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 5 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.02305996 -0.15161382  0.01255056  0.32329892 -0.01773434 -0.09661685\n",
      "   0.1669763   0.07454487  0.14768389]\n",
      " [ 0.13996271  0.23339525 -0.1571602  -0.35465474 -0.20726034 -0.09219453\n",
      "  -0.43746605 -0.19961731 -0.11014025]\n",
      " [ 0.23162414 -0.39395779  0.09987183 -0.29983014  0.250087   -0.29871537\n",
      "  -0.17716424  0.05126081  0.23079273]\n",
      " [ 0.14677174 -0.15803716  0.13336792  0.10804727  0.22885421  0.1403492\n",
      "   0.44441046 -0.24619998 -0.23639801]\n",
      " [-0.09172239  0.20020159 -0.03379549  0.01645304 -0.20984336 -0.19225956\n",
      "  -0.42855136 -0.08678117  0.01169808]\n",
      " [ 0.22017821  0.09440249  0.19085668 -0.32368832 -0.09529545 -0.03631935\n",
      "  -0.19864937 -0.05786074 -0.09468499]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.28581605  0.21967966 -0.16216074  0.26417711  0.02107571 -0.10911769\n",
      "   0.18442291]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:5 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.62016866]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 6 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.02835376 -0.15690762  0.01255056  0.31800512 -0.01773434 -0.09661685\n",
      "   0.1669763   0.07454487  0.14239009]\n",
      " [ 0.14162894  0.23506148 -0.1571602  -0.35298851 -0.20726034 -0.09219453\n",
      "  -0.43746605 -0.19961731 -0.10847402]\n",
      " [ 0.23579312 -0.3897888   0.09987183 -0.29566116  0.250087   -0.29871537\n",
      "  -0.17716424  0.05126081  0.23496171]\n",
      " [ 0.14930886 -0.15550004  0.13336792  0.1105844   0.22885421  0.1403492\n",
      "   0.44441046 -0.24619998 -0.23386089]\n",
      " [-0.09420577  0.19771822 -0.03379549  0.01396967 -0.20984336 -0.19225956\n",
      "  -0.42855136 -0.08678117  0.00921471]\n",
      " [ 0.22206846  0.09629273  0.19085668 -0.32179807 -0.09529545 -0.03631935\n",
      "  -0.19864937 -0.05786074 -0.09279474]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.21277275  0.17778641 -0.19701384  0.23186173 -0.01290057 -0.14813045\n",
      "   0.14979489]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:6 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.59317972]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 6 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.02650918 -0.15690762  0.01439514  0.31800512 -0.01773434 -0.09661685\n",
      "   0.16882088  0.07454487  0.14239009]\n",
      " [ 0.13634564  0.23506148 -0.16244349 -0.35298851 -0.20726034 -0.09219453\n",
      "  -0.44274935 -0.19961731 -0.10847402]\n",
      " [ 0.23772602 -0.3897888   0.10180473 -0.29566116  0.250087   -0.29871537\n",
      "  -0.17523135  0.05126081  0.23496171]\n",
      " [ 0.15714878 -0.15550004  0.14120784  0.1105844   0.22885421  0.1403492\n",
      "   0.45225038 -0.24619998 -0.23386089]\n",
      " [-0.10053283  0.19771822 -0.04012256  0.01396967 -0.20984336 -0.19225956\n",
      "  -0.43487842 -0.08678117  0.00921471]\n",
      " [ 0.22466801  0.09629273  0.19345623 -0.32179807 -0.09529545 -0.03631935\n",
      "  -0.19604982 -0.05786074 -0.09279474]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Theta two: \n",
      "[[ 0.26185918  0.20418124 -0.17793648  0.25834595  0.02019182 -0.13024603\n",
      "   0.17695761]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:6 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.61360458]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 6 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.02650553 -0.15690397  0.01439879  0.31800512 -0.01773434 -0.09661685\n",
      "   0.16882453  0.07454487  0.14239009]\n",
      " [ 0.14053952  0.23925536 -0.15824961 -0.35298851 -0.20726034 -0.09219453\n",
      "  -0.43855546 -0.19961731 -0.10847402]\n",
      " [ 0.24177491 -0.38573991  0.10585362 -0.29566116  0.250087   -0.29871537\n",
      "  -0.17118246  0.05126081  0.23496171]\n",
      " [ 0.14723094 -0.16541788  0.13129     0.1105844   0.22885421  0.1403492\n",
      "   0.44233254 -0.24619998 -0.23386089]\n",
      " [-0.09390163  0.20434942 -0.03349136  0.01396967 -0.20984336 -0.19225956\n",
      "  -0.42824722 -0.08678117  0.00921471]\n",
      " [ 0.21902269  0.09064741  0.18781091 -0.32179807 -0.09529545 -0.03631935\n",
      "  -0.20169514 -0.05786074 -0.09279474]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.1891182   0.1678144  -0.21007478  0.22605875 -0.02669237 -0.15982643\n",
      "   0.13484594]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:6 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.57722046]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 6 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.02570106 -0.15690397  0.01520325  0.31800512 -0.01773434 -0.09661685\n",
      "   0.16882453  0.07534933  0.14239009]\n",
      " [ 0.13776955  0.23925536 -0.16101959 -0.35298851 -0.20726034 -0.09219453\n",
      "  -0.43855546 -0.20238729 -0.10847402]\n",
      " [ 0.24671995 -0.38573991  0.11079866 -0.29566116  0.250087   -0.29871537\n",
      "  -0.17118246  0.05620585  0.23496171]\n",
      " [ 0.14764767 -0.16541788  0.13170673  0.1105844   0.22885421  0.1403492\n",
      "   0.44233254 -0.24578326 -0.23386089]\n",
      " [-0.09663234  0.20434942 -0.03622207  0.01396967 -0.20984336 -0.19225956\n",
      "  -0.42824722 -0.08951188  0.00921471]\n",
      " [ 0.22338901  0.09064741  0.19217723 -0.32179807 -0.09529545 -0.03631935\n",
      "  -0.20169514 -0.05349442 -0.09279474]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.24070512  0.19441284 -0.18707315  0.25692944 -0.0004821  -0.13678461\n",
      "   0.16509489]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:6 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.6056627]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 6 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.02081787 -0.15690397  0.01520325  0.32288831 -0.01773434 -0.09661685\n",
      "   0.16882453  0.07534933  0.14727329]\n",
      " [ 0.1340569   0.23925536 -0.16101959 -0.35670115 -0.20726034 -0.09219453\n",
      "  -0.43855546 -0.20238729 -0.11218667]\n",
      " [ 0.24889108 -0.38573991  0.11079866 -0.29349003  0.250087   -0.29871537\n",
      "  -0.17118246  0.05620585  0.23713284]\n",
      " [ 0.14793454 -0.16541788  0.13170673  0.11087127  0.22885421  0.1403492\n",
      "   0.44233254 -0.24578326 -0.23357402]\n",
      " [-0.09749586  0.20434942 -0.03622207  0.01310615 -0.20984336 -0.19225956\n",
      "  -0.42824722 -0.08951188  0.00835119]\n",
      " [ 0.22115847  0.09064741  0.19217723 -0.32402861 -0.09529545 -0.03631935\n",
      "  -0.20169514 -0.05349442 -0.09502528]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.28779598  0.22299671 -0.16730554  0.28265854  0.02335022 -0.11410347\n",
      "   0.18639616]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:6 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.63704358]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 6 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.02104072 -0.15690397  0.01520325  0.32288831 -0.0179572  -0.09683971\n",
      "   0.16882453  0.07534933  0.14705043]\n",
      " [ 0.13907061  0.23925536 -0.16101959 -0.35670115 -0.20224663 -0.08718082\n",
      "  -0.43855546 -0.20238729 -0.10717296]\n",
      " [ 0.24121095 -0.38573991  0.11079866 -0.29349003  0.24240687 -0.30639551\n",
      "  -0.17118246  0.05620585  0.22945271]\n",
      " [ 0.1428171  -0.16541788  0.13170673  0.11087127  0.22373678  0.13523176\n",
      "   0.44233254 -0.24578326 -0.23869145]\n",
      " [-0.08897544  0.20434942 -0.03622207  0.01310615 -0.20132294 -0.18373914\n",
      "  -0.42824722 -0.08951188  0.01687161]\n",
      " [ 0.22125939  0.09064741  0.19217723 -0.32402861 -0.09519452 -0.03621842\n",
      "  -0.20169514 -0.05349442 -0.09492435]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.21414767  0.1859497  -0.19905135  0.23790701 -0.01866023 -0.14206035\n",
      "   0.14967293]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:6 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.58129604]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 6 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.01847776 -0.154341    0.01520325  0.32288831 -0.01539423 -0.09427675\n",
      "   0.16882453  0.07534933  0.1496134 ]\n",
      " [ 0.13939377  0.23957852 -0.16101959 -0.35670115 -0.20192347 -0.08685766\n",
      "  -0.43855546 -0.20238729 -0.1068498 ]\n",
      " [ 0.24084074 -0.38611011  0.11079866 -0.29349003  0.24203666 -0.30676571\n",
      "  -0.17118246  0.05620585  0.22908251]\n",
      " [ 0.14109378 -0.1671412   0.13170673  0.11087127  0.22201346  0.13350845\n",
      "   0.44233254 -0.24578326 -0.24041477]\n",
      " [-0.084575    0.20874985 -0.03622207  0.01310615 -0.1969225  -0.1793387\n",
      "  -0.42824722 -0.08951188  0.02127205]\n",
      " [ 0.21974884  0.08913686  0.19217723 -0.32402861 -0.09670507 -0.03772898\n",
      "  -0.20169514 -0.05349442 -0.09643491]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.14340657  0.15315119 -0.23409872  0.20216623 -0.05575684 -0.17298343\n",
      "   0.11278999]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:6 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.53995378]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 6 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.01657821 -0.15244145  0.01520325  0.32288831 -0.01349469 -0.0923772\n",
      "   0.17072408  0.07534933  0.1496134 ]\n",
      " [ 0.14506071  0.24524546 -0.16101959 -0.35670115 -0.19625653 -0.08119073\n",
      "  -0.43288853 -0.20238729 -0.1068498 ]\n",
      " [ 0.24700492 -0.37994594  0.11079866 -0.29349003  0.24820084 -0.30060154\n",
      "  -0.16501828  0.05620585  0.22908251]\n",
      " [ 0.12990451 -0.17833047  0.13170673  0.11087127  0.21082418  0.12231917\n",
      "   0.43114327 -0.24578326 -0.24041477]\n",
      " [-0.074393    0.21893185 -0.03622207  0.01310615 -0.1867405  -0.1691567\n",
      "  -0.41806522 -0.08951188  0.02127205]\n",
      " [ 0.22020552  0.08959353  0.19217723 -0.32402861 -0.0962484  -0.0372723\n",
      "  -0.20123847 -0.05349442 -0.09643491]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.07634331  0.1215232  -0.2618481   0.17494913 -0.10162218 -0.19552922\n",
      "   0.07971509]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:6 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.50780918]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 6 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.00964474 -0.15244145  0.01520325  0.32982178 -0.01349469 -0.0923772\n",
      "   0.17765755  0.07534933  0.1496134 ]\n",
      " [ 0.13611167  0.24524546 -0.16101959 -0.36565019 -0.19625653 -0.08119073\n",
      "  -0.44183757 -0.20238729 -0.1068498 ]\n",
      " [ 0.24378869 -0.37994594  0.11079866 -0.29670626  0.24820084 -0.30060154\n",
      "  -0.16823451  0.05620585  0.22908251]\n",
      " [ 0.13915301 -0.17833047  0.13170673  0.12011977  0.21082418  0.12231917\n",
      "   0.44039178 -0.24578326 -0.24041477]\n",
      " [-0.08135637  0.21893185 -0.03622207  0.00614278 -0.1867405  -0.1691567\n",
      "  -0.42502859 -0.08951188  0.02127205]\n",
      " [ 0.21562199  0.08959353  0.19217723 -0.32861214 -0.0962484  -0.0372723\n",
      "  -0.205822   -0.05349442 -0.09643491]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.13785216  0.15947707 -0.24067531  0.20246329 -0.06090746 -0.17200792\n",
      "   0.10581456]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:6 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.54769843]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 6 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.00877525 -0.15244145  0.01520325  0.32982178 -0.01262519 -0.09150771\n",
      "   0.17852704  0.07534933  0.1496134 ]\n",
      " [ 0.12860077  0.24524546 -0.16101959 -0.36565019 -0.20376743 -0.08870163\n",
      "  -0.44934847 -0.20238729 -0.1068498 ]\n",
      " [ 0.24411292 -0.37994594  0.11079866 -0.29670626  0.24852508 -0.3002773\n",
      "  -0.16791027  0.05620585  0.22908251]\n",
      " [ 0.14960415 -0.17833047  0.13170673  0.12011977  0.22127532  0.13277031\n",
      "   0.45084291 -0.24578326 -0.24041477]\n",
      " [-0.09143991  0.21893185 -0.03622207  0.00614278 -0.19682404 -0.17924024\n",
      "  -0.43511213 -0.08951188  0.02127205]\n",
      " [ 0.2138958   0.08959353  0.19217723 -0.32861214 -0.09797459 -0.0389985\n",
      "  -0.20754819 -0.05349442 -0.09643491]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.19387533  0.1883587  -0.22060765  0.23079915 -0.02093213 -0.15537673\n",
      "   0.13209554]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:6 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.5757586]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 6 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.0152066  -0.15244145  0.01520325  0.32339043 -0.01262519 -0.09793905\n",
      "   0.17852704  0.07534933  0.14318205]\n",
      " [ 0.13586064  0.24524546 -0.16101959 -0.35839032 -0.20376743 -0.08144175\n",
      "  -0.44934847 -0.20238729 -0.09958993]\n",
      " [ 0.24628073 -0.37994594  0.11079866 -0.29453845  0.24852508 -0.29810949\n",
      "  -0.16791027  0.05620585  0.23125031]\n",
      " [ 0.14677352 -0.17833047  0.13170673  0.11728914  0.22127532  0.12993968\n",
      "   0.45084291 -0.24578326 -0.2432454 ]\n",
      " [-0.0872261   0.21893185 -0.03622207  0.01035659 -0.19682404 -0.17502643\n",
      "  -0.43511213 -0.08951188  0.02548586]\n",
      " [ 0.21822519  0.08959353  0.19217723 -0.32428274 -0.09797459 -0.0346691\n",
      "  -0.20754819 -0.05349442 -0.09210551]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.12355775  0.14661336 -0.248278    0.1978137  -0.05893396 -0.18628003\n",
      "   0.10131145]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:6 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.52575357]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 6 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.01176175 -0.14899661  0.01520325  0.32683528 -0.01262519 -0.09449421\n",
      "   0.18197189  0.07534933  0.14318205]\n",
      " [ 0.12881538  0.2382002  -0.16101959 -0.36543558 -0.20376743 -0.08848701\n",
      "  -0.45639373 -0.20238729 -0.09958993]\n",
      " [ 0.23538956 -0.3908371   0.11079866 -0.30542962  0.24852508 -0.30900066\n",
      "  -0.17880144  0.05620585  0.23125031]\n",
      " [ 0.15560725 -0.16949673  0.13170673  0.12612288  0.22127532  0.13877341\n",
      "   0.45967665 -0.24578326 -0.2432454 ]\n",
      " [-0.09377914  0.21237881 -0.03622207  0.00380355 -0.19682404 -0.18157947\n",
      "  -0.44166517 -0.08951188  0.02548586]\n",
      " [ 0.21446491  0.08583325  0.19217723 -0.32804302 -0.09797459 -0.03842938\n",
      "  -0.21130847 -0.05349442 -0.09210551]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.18268128  0.17965213 -0.22606854  0.21497366 -0.01986972 -0.16351323\n",
      "   0.12707086]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:6 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.56190948]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 6 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-7.20916858e-03 -1.44444026e-01  1.52032533e-02  3.31387862e-01\n",
      "  -1.26251933e-02 -9.44942058e-02  1.86524467e-01  7.53493336e-02\n",
      "   1.43182048e-01]\n",
      " [ 1.22990735e-01  2.32375552e-01 -1.61019587e-01 -3.71260224e-01\n",
      "  -2.03767433e-01 -8.84870125e-02 -4.62218374e-01 -2.02387290e-01\n",
      "  -9.95899253e-02]\n",
      " [ 2.27591691e-01 -3.98634978e-01  1.10798664e-01 -3.13227493e-01\n",
      "   2.48525075e-01 -3.09000660e-01 -1.86599314e-01  5.62058474e-02\n",
      "   2.31250310e-01]\n",
      " [ 1.62719277e-01 -1.62384711e-01  1.31706730e-01  1.33234903e-01\n",
      "   2.21275318e-01  1.38773415e-01  4.66788667e-01 -2.45783259e-01\n",
      "  -2.43245401e-01]\n",
      " [-9.79751030e-02  2.08182853e-01 -3.62220685e-02 -3.92412854e-04\n",
      "  -1.96824039e-01 -1.81579472e-01 -4.45861131e-01 -8.95118831e-02\n",
      "   2.54858602e-02]\n",
      " [ 2.11287974e-01  8.26563100e-02  1.92177227e-01 -3.31219962e-01\n",
      "  -9.79745915e-02 -3.84293771e-02 -2.14485409e-01 -5.34944244e-02\n",
      "  -9.21055082e-02]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.23660304  0.21125807 -0.20513521  0.23359388  0.01459727 -0.14081995\n",
      "   0.15082445]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:6 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.6055302]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 6 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-6.03401873e-03 -1.44444026e-01  1.63784031e-02  3.31387862e-01\n",
      "  -1.26251933e-02 -9.33190559e-02  1.87699617e-01  7.53493336e-02\n",
      "   1.43182048e-01]\n",
      " [ 1.16624372e-01  2.32375552e-01 -1.67385950e-01 -3.71260224e-01\n",
      "  -2.03767433e-01 -9.48533755e-02 -4.68584737e-01 -2.02387290e-01\n",
      "  -9.95899253e-02]\n",
      " [ 2.25751462e-01 -3.98634978e-01  1.08958435e-01 -3.13227493e-01\n",
      "   2.48525075e-01 -3.10840888e-01 -1.88439543e-01  5.62058474e-02\n",
      "   2.31250310e-01]\n",
      " [ 1.71432619e-01 -1.62384711e-01  1.40420072e-01  1.33234903e-01\n",
      "   2.21275318e-01  1.47486756e-01  4.75502009e-01 -2.45783259e-01\n",
      "  -2.43245401e-01]\n",
      " [-1.05760935e-01  2.08182853e-01 -4.40079007e-02 -3.92412854e-04\n",
      "  -1.96824039e-01 -1.89365304e-01 -4.53646964e-01 -8.95118831e-02\n",
      "   2.54858602e-02]\n",
      " [ 2.13051155e-01  8.26563100e-02  1.93940407e-01 -3.31219962e-01\n",
      "  -9.79745915e-02 -3.66661967e-02 -2.12722228e-01 -5.34944244e-02\n",
      "  -9.21055082e-02]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.28371524  0.23599128 -0.18831967  0.25530216  0.04809155 -0.12582457\n",
      "   0.1761504 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:6 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.6182402]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 7 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.01179277 -0.15020278  0.0163784   0.32562911 -0.01262519 -0.09331906\n",
      "   0.18769962  0.07534933  0.1374233 ]\n",
      " [ 0.11883864  0.23458982 -0.16738595 -0.36904596 -0.20376743 -0.09485338\n",
      "  -0.46858474 -0.20238729 -0.09737566]\n",
      " [ 0.23032534 -0.3940611   0.10895844 -0.30865362  0.24852508 -0.31084089\n",
      "  -0.18843954  0.05620585  0.23582419]\n",
      " [ 0.17326945 -0.16054788  0.14042007  0.13507173  0.22127532  0.14748676\n",
      "   0.47550201 -0.24578326 -0.24140857]\n",
      " [-0.10807733  0.20586646 -0.0440079  -0.00270881 -0.19682404 -0.1893653\n",
      "  -0.45364696 -0.08951188  0.02316946]\n",
      " [ 0.2153694   0.08497456  0.19394041 -0.32890172 -0.09797459 -0.0366662\n",
      "  -0.21272223 -0.05349442 -0.08978726]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.21075694  0.19365203 -0.22257907  0.22344656  0.01345235 -0.1646264\n",
      "   0.14199579]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:7 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.5945832]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 7 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.00946541 -0.15020278  0.01870576  0.32562911 -0.01262519 -0.09331906\n",
      "   0.19002698  0.07534933  0.1374233 ]\n",
      " [ 0.11292561  0.23458982 -0.17329898 -0.36904596 -0.20376743 -0.09485338\n",
      "  -0.47449777 -0.20238729 -0.09737566]\n",
      " [ 0.23215759 -0.3940611   0.11079069 -0.30865362  0.24852508 -0.31084089\n",
      "  -0.18660728  0.05620585  0.23582419]\n",
      " [ 0.18155214 -0.16054788  0.14870277  0.13507173  0.22127532  0.14748676\n",
      "   0.4837847  -0.24578326 -0.24140857]\n",
      " [-0.11483761  0.20586646 -0.05076818 -0.00270881 -0.19682404 -0.1893653\n",
      "  -0.46040724 -0.08951188  0.02316946]\n",
      " [ 0.21774785  0.08497456  0.19631885 -0.32890172 -0.09797459 -0.0366662\n",
      "  -0.21034378 -0.05349442 -0.08978726]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.25962062  0.2204256  -0.20432736  0.24971761  0.04705383 -0.14737589\n",
      "   0.16882142]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:7 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.61413249]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 7 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.01035744 -0.15109481  0.01781374  0.32562911 -0.01262519 -0.09331906\n",
      "   0.18913495  0.07534933  0.1374233 ]\n",
      " [ 0.11826691  0.23993112 -0.16795768 -0.36904596 -0.20376743 -0.09485338\n",
      "  -0.46915646 -0.20238729 -0.09737566]\n",
      " [ 0.23642159 -0.3897971   0.11505469 -0.30865362  0.24852508 -0.31084089\n",
      "  -0.18234329  0.05620585  0.23582419]\n",
      " [ 0.17084809 -0.17125193  0.13799871  0.13507173  0.22127532  0.14748676\n",
      "   0.47308065 -0.24578326 -0.24140857]\n",
      " [-0.10752206  0.213182   -0.04345263 -0.00270881 -0.19682404 -0.1893653\n",
      "  -0.45309169 -0.08951188  0.02316946]\n",
      " [ 0.21260389  0.0798306   0.19117489 -0.32890172 -0.09797459 -0.0366662\n",
      "  -0.21548774 -0.05349442 -0.08978726]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.18685397  0.18314989 -0.23528874  0.21763856 -0.00081184 -0.17622654\n",
      "   0.12722239]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:7 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.57607333]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 7 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.00928768 -0.15109481  0.01888349  0.32562911 -0.01262519 -0.09331906\n",
      "   0.18913495  0.07641909  0.1374233 ]\n",
      " [ 0.11505604  0.23993112 -0.17116855 -0.36904596 -0.20376743 -0.09485338\n",
      "  -0.46915646 -0.20559816 -0.09737566]\n",
      " [ 0.24148414 -0.3897971   0.12011724 -0.30865362  0.24852508 -0.31084089\n",
      "  -0.18234329  0.06126839  0.23582419]\n",
      " [ 0.17166339 -0.17125193  0.13881401  0.13507173  0.22127532  0.14748676\n",
      "   0.47308065 -0.24496796 -0.24140857]\n",
      " [-0.11058964  0.213182   -0.04652021 -0.00270881 -0.19682404 -0.1893653\n",
      "  -0.45309169 -0.09257946  0.02316946]\n",
      " [ 0.21700068  0.0798306   0.19557168 -0.32890172 -0.09797459 -0.0366662\n",
      "  -0.21548774 -0.04909763 -0.08978726]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.23861814  0.21010296 -0.21265165  0.2487246   0.02588608 -0.15344169\n",
      "   0.15759173]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:7 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.60469541]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 7 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.00419467 -0.15109481  0.01888349  0.33072212 -0.01262519 -0.09331906\n",
      "   0.18913495  0.07641909  0.14251631]\n",
      " [ 0.11103134  0.23993112 -0.17116855 -0.37307066 -0.20376743 -0.09485338\n",
      "  -0.46915646 -0.20559816 -0.10140036]\n",
      " [ 0.24346213 -0.3897971   0.12011724 -0.30667563  0.24852508 -0.31084089\n",
      "  -0.18234329  0.06126839  0.23780218]\n",
      " [ 0.17243418 -0.17125193  0.13881401  0.13584252  0.22127532  0.14748676\n",
      "   0.47308065 -0.24496796 -0.24063778]\n",
      " [-0.11165205  0.213182   -0.04652021 -0.00377122 -0.19682404 -0.1893653\n",
      "  -0.45309169 -0.09257946  0.02210705]\n",
      " [ 0.21464247  0.0798306   0.19557168 -0.33125992 -0.09797459 -0.0366662\n",
      "  -0.21548774 -0.04909763 -0.09214547]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.28586472  0.23899584 -0.19313638  0.27433527  0.05028071 -0.13088225\n",
      "   0.17884079]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:7 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.63591547]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 7 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.00479038 -0.15109481  0.01888349  0.33072212 -0.01322091 -0.09391477\n",
      "   0.18913495  0.07641909  0.14192059]\n",
      " [ 0.11624037  0.23993112 -0.17116855 -0.37307066 -0.1985584  -0.08964434\n",
      "  -0.46915646 -0.20559816 -0.09619132]\n",
      " [ 0.23608049 -0.3897971   0.12011724 -0.30667563  0.24114344 -0.31822253\n",
      "  -0.18234329  0.06126839  0.23042054]\n",
      " [ 0.16702578 -0.17125193  0.13881401  0.13584252  0.21586692  0.14207836\n",
      "   0.47308065 -0.24496796 -0.24604618]\n",
      " [-0.10337395  0.213182   -0.04652021 -0.00377122 -0.18854593 -0.1810872\n",
      "  -0.45309169 -0.09257946  0.03038516]\n",
      " [ 0.21486595  0.0798306   0.19557168 -0.33125992 -0.09775111 -0.03644271\n",
      "  -0.21548774 -0.04909763 -0.09192198]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.21224892  0.20159213 -0.22466244  0.2299279   0.00798262 -0.15909623\n",
      "   0.14225639]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:7 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57894779]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 7 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.00266186 -0.14896628  0.01888349  0.33072212 -0.01109238 -0.09178625\n",
      "   0.18913495  0.07641909  0.14404912]\n",
      " [ 0.11673815  0.2404289  -0.17116855 -0.37307066 -0.19806062 -0.08914656\n",
      "  -0.46915646 -0.20559816 -0.09569355]\n",
      " [ 0.23608711 -0.38979049  0.12011724 -0.30667563  0.24115005 -0.31821591\n",
      "  -0.18234329  0.06126839  0.23042715]\n",
      " [ 0.16513181 -0.1731459   0.13881401  0.13584252  0.21397295  0.14018439\n",
      "   0.47308065 -0.24496796 -0.24794015]\n",
      " [-0.09937919  0.21717676 -0.04652021 -0.00377122 -0.18455117 -0.17709244\n",
      "  -0.45309169 -0.09257946  0.03437992]\n",
      " [ 0.21365754  0.07862218  0.19557168 -0.33125992 -0.09895952 -0.03765113\n",
      "  -0.21548774 -0.04909763 -0.0931304 ]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.14168467  0.16844373 -0.25944672  0.19465239 -0.02919714 -0.19034846\n",
      "   0.1057649 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:7 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.54096707]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 7 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.00156532 -0.14786975  0.01888349  0.33072212 -0.00999585 -0.09068971\n",
      "   0.19023148  0.07641909  0.14404912]\n",
      " [ 0.12318124  0.24687199 -0.17116855 -0.37307066 -0.19161753 -0.08270348\n",
      "  -0.46271338 -0.20559816 -0.09569355]\n",
      " [ 0.24273624 -0.38314136  0.12011724 -0.30667563  0.24779918 -0.31156678\n",
      "  -0.17569416  0.06126839  0.23042715]\n",
      " [ 0.15344806 -0.18482965  0.13881401  0.13584252  0.2022892   0.12850064\n",
      "   0.4613969  -0.24496796 -0.24794015]\n",
      " [-0.08898987  0.22756607 -0.04652021 -0.00377122 -0.17416186 -0.16670312\n",
      "  -0.44270238 -0.09257946  0.03437992]\n",
      " [ 0.2146611   0.07962574  0.19557168 -0.33125992 -0.09795596 -0.03664757\n",
      "  -0.21448418 -0.04909763 -0.0931304 ]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.07451774  0.13595758 -0.2864146   0.1679088  -0.0758158  -0.21268089\n",
      "   0.07318559]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:7 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.51000483]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 7 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.00587161 -0.14786975  0.01888349  0.33815906 -0.00999585 -0.09068971\n",
      "   0.19766842  0.07641909  0.14404912]\n",
      " [ 0.11354938  0.24687199 -0.17116855 -0.38270251 -0.19161753 -0.08270348\n",
      "  -0.47234523 -0.20559816 -0.09569355]\n",
      " [ 0.23912051 -0.38314136  0.12011724 -0.31029135  0.24779918 -0.31156678\n",
      "  -0.17930988  0.06126839  0.23042715]\n",
      " [ 0.16346021 -0.18482965  0.13881401  0.14585467  0.2022892   0.12850064\n",
      "   0.47140905 -0.24496796 -0.24794015]\n",
      " [-0.09662528  0.22756607 -0.04652021 -0.01140663 -0.17416186 -0.16670312\n",
      "  -0.45033779 -0.09257946  0.03437992]\n",
      " [ 0.20972985  0.07962574  0.19557168 -0.33619118 -0.09795596 -0.03664757\n",
      "  -0.21941543 -0.04909763 -0.0931304 ]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.13574261  0.17434586 -0.26627015  0.1948708  -0.034224   -0.190074\n",
      "   0.09877619]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:7 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.54931921]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 7 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.00730231 -0.14786975  0.01888349  0.33815906 -0.00856515 -0.08925902\n",
      "   0.19909912  0.07641909  0.14404912]\n",
      " [ 0.10554838  0.24687199 -0.17116855 -0.38270251 -0.19961854 -0.09070448\n",
      "  -0.48034624 -0.20559816 -0.09569355]\n",
      " [ 0.23906533 -0.38314136  0.12011724 -0.31029135  0.247744   -0.31162197\n",
      "  -0.17936507  0.06126839  0.23042715]\n",
      " [ 0.17421885 -0.18482965  0.13881401  0.14585467  0.21304784  0.13925928\n",
      "   0.48216769 -0.24496796 -0.24794015]\n",
      " [-0.10685565  0.22756607 -0.04652021 -0.01140663 -0.18439223 -0.17693349\n",
      "  -0.46056816 -0.09257946  0.03437992]\n",
      " [ 0.20772792  0.07962574  0.19557168 -0.33619118 -0.09995789 -0.03864949\n",
      "  -0.22141736 -0.04909763 -0.0931304 ]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.1915296   0.20367258 -0.246923    0.22270911  0.00617992 -0.17380885\n",
      "   0.1246608 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:7 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57519051]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 7 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.00054488 -0.14786975  0.01888349  0.33140163 -0.00856515 -0.09601645\n",
      "   0.19909912  0.07641909  0.13729168]\n",
      " [ 0.11326988  0.24687199 -0.17116855 -0.37498101 -0.19961854 -0.08298298\n",
      "  -0.48034624 -0.20559816 -0.08797204]\n",
      " [ 0.2417276  -0.38314136  0.12011724 -0.30762909  0.247744   -0.3089597\n",
      "  -0.17936507  0.06126839  0.23308942]\n",
      " [ 0.17054624 -0.18482965  0.13881401  0.14218206  0.21304784  0.13558667\n",
      "   0.48216769 -0.24496796 -0.25161276]\n",
      " [-0.10235063  0.22756607 -0.04652021 -0.00690161 -0.18439223 -0.17242847\n",
      "  -0.46056816 -0.09257946  0.03888494]\n",
      " [ 0.21222338  0.07962574  0.19557168 -0.33169572 -0.09995789 -0.03415403\n",
      "  -0.22141736 -0.04909763 -0.08863494]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.12125674  0.16159684 -0.2740584   0.19024527 -0.03265654 -0.20438901\n",
      "   0.09407075]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:7 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.52751276]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 7 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.00468605 -0.14372857  0.01888349  0.3355428  -0.00856515 -0.09187527\n",
      "   0.2032403   0.07641909  0.13729168]\n",
      " [ 0.10543237  0.23903447 -0.17116855 -0.38281852 -0.19961854 -0.09082049\n",
      "  -0.48818375 -0.20559816 -0.08797204]\n",
      " [ 0.23056578 -0.39430317  0.12011724 -0.3187909   0.247744   -0.32012151\n",
      "  -0.19052688  0.06126839  0.23308942]\n",
      " [ 0.18012793 -0.17524797  0.13881401  0.15176375  0.21304784  0.14516836\n",
      "   0.49174938 -0.24496796 -0.25161276]\n",
      " [-0.10944663  0.22047007 -0.04652021 -0.01399761 -0.18439223 -0.17952447\n",
      "  -0.46766416 -0.09257946  0.03888494]\n",
      " [ 0.20796819  0.07537055  0.19557168 -0.33595091 -0.09995789 -0.03840922\n",
      "  -0.22567255 -0.04909763 -0.08863494]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.18013882  0.1952362  -0.25289888  0.20681659  0.0072791  -0.18236143\n",
      "   0.11919443]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:7 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.5633166]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 7 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.00984474 -0.13856988  0.01888349  0.34070149 -0.00856515 -0.09187527\n",
      "   0.20839898  0.07641909  0.13729168]\n",
      " [ 0.09883054  0.23243265 -0.17116855 -0.38942035 -0.19961854 -0.09082049\n",
      "  -0.49478557 -0.20559816 -0.08797204]\n",
      " [ 0.22247923 -0.40238973  0.12011724 -0.32687746  0.247744   -0.32012151\n",
      "  -0.19861344  0.06126839  0.23308942]\n",
      " [ 0.18797968 -0.16739622  0.13881401  0.1596155   0.21304784  0.14516836\n",
      "   0.49960113 -0.24496796 -0.25161276]\n",
      " [-0.11425631  0.2156604  -0.04652021 -0.01880729 -0.18439223 -0.17952447\n",
      "  -0.47247384 -0.09257946  0.03888494]\n",
      " [ 0.20430294  0.0717053   0.19557168 -0.33961616 -0.09995789 -0.03840922\n",
      "  -0.2293378  -0.04909763 -0.08863494]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.23384891  0.22738843 -0.23295497  0.22496057  0.04254774 -0.16042694\n",
      "   0.14233674]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:7 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.60738431]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 7 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.01153574 -0.13856988  0.02057449  0.34070149 -0.00856515 -0.09018428\n",
      "   0.21008998  0.07641909  0.13729168]\n",
      " [ 0.09190711  0.23243265 -0.17809198 -0.38942035 -0.19961854 -0.09774392\n",
      "  -0.501709   -0.20559816 -0.08797204]\n",
      " [ 0.22043373 -0.40238973  0.11807174 -0.32687746  0.247744   -0.32216701\n",
      "  -0.20065894  0.06126839  0.23308942]\n",
      " [ 0.19703883 -0.16739622  0.14787316  0.1596155   0.21304784  0.15422751\n",
      "   0.50866028 -0.24496796 -0.25161276]\n",
      " [-0.12235559  0.2156604  -0.05461949 -0.01880729 -0.18439223 -0.18762375\n",
      "  -0.48057312 -0.09257946  0.03888494]\n",
      " [ 0.20584254  0.0717053   0.19711128 -0.33961616 -0.09995789 -0.03686962\n",
      "  -0.2277982  -0.04909763 -0.08863494]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.28066217  0.252492   -0.21698221  0.2463111   0.07650753 -0.14604125\n",
      "   0.16728745]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:7 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.61589807]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 8 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.00533668 -0.14476894  0.02057449  0.33450244 -0.00856515 -0.09018428\n",
      "   0.21008998  0.07641909  0.13109263]\n",
      " [ 0.09467835  0.23520389 -0.17809198 -0.38664911 -0.19961854 -0.09774392\n",
      "  -0.501709   -0.20559816 -0.0852008 ]\n",
      " [ 0.2253615  -0.39746195  0.11807174 -0.32194968  0.247744   -0.32216701\n",
      "  -0.20065894  0.06126839  0.2380172 ]\n",
      " [ 0.19817337 -0.16626168  0.14787316  0.16075004  0.21304784  0.15422751\n",
      "   0.50866028 -0.24496796 -0.25047821]\n",
      " [-0.12441397  0.21360202 -0.05461949 -0.02086567 -0.18439223 -0.18762375\n",
      "  -0.48057312 -0.09257946  0.03682656]\n",
      " [ 0.20857173  0.07443449  0.19711128 -0.33688697 -0.09995789 -0.03686962\n",
      "  -0.2277982  -0.04909763 -0.08590575]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.20781139  0.20973952 -0.25062552  0.21487618  0.04121742 -0.18452943\n",
      "   0.13360159]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:8 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.59608555]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 8 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.00816598 -0.14476894  0.02340378  0.33450244 -0.00856515 -0.09018428\n",
      "   0.21291927  0.07641909  0.13109263]\n",
      " [ 0.08814117  0.23520389 -0.18462917 -0.38664911 -0.19961854 -0.09774392\n",
      "  -0.50824619 -0.20559816 -0.0852008 ]\n",
      " [ 0.22708828 -0.39746195  0.11979852 -0.32194968  0.247744   -0.32216701\n",
      "  -0.19893216  0.06126839  0.2380172 ]\n",
      " [ 0.20687502 -0.16626168  0.15657482  0.16075004  0.21304784  0.15422751\n",
      "   0.51736193 -0.24496796 -0.25047821]\n",
      " [-0.13161967  0.21360202 -0.0618252  -0.02086567 -0.18439223 -0.18762375\n",
      "  -0.48777882 -0.09257946  0.03682656]\n",
      " [ 0.21071711  0.07443449  0.19925667 -0.33688697 -0.09995789 -0.03686962\n",
      "  -0.22565282 -0.04909763 -0.08590575]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.25643614  0.23690752 -0.23322979  0.2409212   0.07533029 -0.16795675\n",
      "   0.16007068]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:8 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.61472256]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 8 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.0063557  -0.14657922  0.0215935   0.33450244 -0.00856515 -0.09018428\n",
      "   0.211109    0.07641909  0.13109263]\n",
      " [ 0.09464171  0.24170443 -0.17812862 -0.38664911 -0.19961854 -0.09774392\n",
      "  -0.50174565 -0.20559816 -0.0852008 ]\n",
      " [ 0.23155906 -0.39299117  0.1242693  -0.32194968  0.247744   -0.32216701\n",
      "  -0.19446138  0.06126839  0.2380172 ]\n",
      " [ 0.19539931 -0.17773739  0.14509911  0.16075004  0.21304784  0.15422751\n",
      "   0.50588622 -0.24496796 -0.25047821]\n",
      " [-0.12355832  0.22166337 -0.05376384 -0.02086567 -0.18439223 -0.18762375\n",
      "  -0.47971747 -0.09257946  0.03682656]\n",
      " [ 0.20608603  0.06980341  0.19462559 -0.33688697 -0.09995789 -0.03686962\n",
      "  -0.2302839  -0.04909763 -0.08590575]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.18364109  0.19869672 -0.26297782  0.20904099  0.02645527 -0.1959959\n",
      "   0.11899023]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:8 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.57454439]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 8 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.00770879 -0.14657922  0.0229466   0.33450244 -0.00856515 -0.09018428\n",
      "   0.211109    0.07777219  0.13109263]\n",
      " [ 0.09096105  0.24170443 -0.18180928 -0.38664911 -0.19961854 -0.09774392\n",
      "  -0.50174565 -0.20927882 -0.0852008 ]\n",
      " [ 0.23675215 -0.39299117  0.12946239 -0.32194968  0.247744   -0.32216701\n",
      "  -0.19446138  0.06646148  0.2380172 ]\n",
      " [ 0.19663838 -0.17773739  0.14633817  0.16075004  0.21304784  0.15422751\n",
      "   0.50588622 -0.2437289  -0.25047821]\n",
      " [-0.1270039   0.22166337 -0.05720942 -0.02086567 -0.18439223 -0.18762375\n",
      "  -0.47971747 -0.09602504  0.03682656]\n",
      " [ 0.21051858  0.06980341  0.19905814 -0.33688697 -0.09995789 -0.03686962\n",
      "  -0.2302839  -0.04466509 -0.08590575]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.23564094  0.2260522  -0.24071003  0.24038589  0.05369614 -0.17348354\n",
      "   0.1495146 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:8 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.60343107]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 8 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.01302029 -0.14657922  0.0229466   0.33981393 -0.00856515 -0.09018428\n",
      "   0.211109    0.07777219  0.13640412]\n",
      " [ 0.08660276  0.24170443 -0.18180928 -0.3910074  -0.19961854 -0.09774392\n",
      "  -0.50174565 -0.20927882 -0.08955909]\n",
      " [ 0.23855443 -0.39299117  0.12946239 -0.3201474   0.247744   -0.32216701\n",
      "  -0.19446138  0.06646148  0.23981948]\n",
      " [ 0.19790298 -0.17773739  0.14633817  0.16201464  0.21304784  0.15422751\n",
      "   0.50588622 -0.2437289  -0.24921361]\n",
      " [-0.12831709  0.22166337 -0.05720942 -0.02217886 -0.18439223 -0.18762375\n",
      "  -0.47971747 -0.09602504  0.03551337]\n",
      " [ 0.20802864  0.06980341  0.19905814 -0.33937691 -0.09995789 -0.03686962\n",
      "  -0.2302839  -0.04466509 -0.08839569]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.28309081  0.25528917 -0.22144954  0.26592013  0.07868809 -0.1510745\n",
      "   0.17073086]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:8 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.63437941]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 8 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.01208885 -0.14657922  0.0229466   0.33981393 -0.00949659 -0.09111572\n",
      "   0.211109    0.07777219  0.13547268]\n",
      " [ 0.09200365  0.24170443 -0.18180928 -0.3910074  -0.19421764 -0.09234303\n",
      "  -0.50174565 -0.20927882 -0.0841582 ]\n",
      " [ 0.23141982 -0.39299117  0.12946239 -0.3201474   0.24060939 -0.32930161\n",
      "  -0.19446138  0.06646148  0.23268487]\n",
      " [ 0.19223429 -0.17773739  0.14633817  0.16201464  0.20737915  0.14855882\n",
      "   0.50588622 -0.2437289  -0.25488231]\n",
      " [-0.12021358  0.22166337 -0.05720942 -0.02217886 -0.17628871 -0.17952024\n",
      "  -0.47971747 -0.09602504  0.04361688]\n",
      " [ 0.20834487  0.06980341  0.19905814 -0.33937691 -0.09964166 -0.03655339\n",
      "  -0.2302839  -0.04466509 -0.08807947]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.20952114  0.2175725  -0.25275193  0.22180506  0.03613977 -0.17946085\n",
      "   0.13426226]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:8 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57603667]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 8 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.01383649 -0.14483158  0.0229466   0.33981393 -0.00774895 -0.08936808\n",
      "   0.211109    0.07777219  0.13722032]\n",
      " [ 0.09265426  0.24235504 -0.18180928 -0.3910074  -0.19356704 -0.09169242\n",
      "  -0.50174565 -0.20927882 -0.0835076 ]\n",
      " [ 0.23172892 -0.39268207  0.12946239 -0.3201474   0.24091849 -0.32899252\n",
      "  -0.19446138  0.06646148  0.23299397]\n",
      " [ 0.19020908 -0.17976259  0.14633817  0.16201464  0.20535394  0.14653361\n",
      "   0.50588622 -0.2437289  -0.25690751]\n",
      " [-0.11654855  0.2253284  -0.05720942 -0.02217886 -0.17262369 -0.17585521\n",
      "  -0.47971747 -0.09602504  0.04728191]\n",
      " [ 0.20739819  0.06885673  0.19905814 -0.33937691 -0.10058834 -0.03750007\n",
      "  -0.2302839  -0.04466509 -0.08902614]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.13918176  0.18415333 -0.28727086  0.18694448 -0.00105964 -0.21093832\n",
      "   0.09814544]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:8 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.54210487]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 8 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.01412247 -0.1445456   0.0229466   0.33981393 -0.00746297 -0.0890821\n",
      "   0.21139498  0.07777219  0.13722032]\n",
      " [ 0.09988165  0.24958243 -0.18180928 -0.3910074  -0.18633964 -0.08446503\n",
      "  -0.49451825 -0.20927882 -0.0835076 ]\n",
      " [ 0.23883358 -0.38557741  0.12946239 -0.3201474   0.24802315 -0.32188786\n",
      "  -0.18735672  0.06646148  0.23299397]\n",
      " [ 0.17804505 -0.19192662  0.14633817  0.16201464  0.19318991  0.13436958\n",
      "   0.49372219 -0.2437289  -0.25690751]\n",
      " [-0.10588747  0.23598948 -0.05720942 -0.02217886 -0.1619626  -0.16519413\n",
      "  -0.46905639 -0.09602504  0.04728191]\n",
      " [ 0.20894439  0.07040293  0.19905814 -0.33937691 -0.09904214 -0.03595388\n",
      "  -0.2287377  -0.04466509 -0.08902614]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.07189918  0.15079804 -0.31343614  0.16064305 -0.04845182 -0.23297481\n",
      "   0.06605253]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:8 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.51254529]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 8 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.02207612 -0.1445456   0.0229466   0.34776758 -0.00746297 -0.0890821\n",
      "   0.21934863  0.07777219  0.13722032]\n",
      " [ 0.08959257  0.24958243 -0.18180928 -0.40129648 -0.18633964 -0.08446503\n",
      "  -0.50480734 -0.20927882 -0.0835076 ]\n",
      " [ 0.23481644 -0.38557741  0.12946239 -0.32416455  0.24802315 -0.32188786\n",
      "  -0.19137386  0.06646148  0.23299397]\n",
      " [ 0.18876453 -0.19192662  0.14633817  0.17273412  0.19318991  0.13436958\n",
      "   0.50444167 -0.2437289  -0.25690751]\n",
      " [-0.11421322  0.23598948 -0.05720942 -0.03050462 -0.1619626  -0.16519413\n",
      "  -0.47738214 -0.09602504  0.04728191]\n",
      " [ 0.20364922  0.07040293  0.19905814 -0.34467208 -0.09904214 -0.03595388\n",
      "  -0.23403287 -0.04466509 -0.08902614]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.13279266  0.18962891 -0.2943701   0.18702414 -0.00599969 -0.21135749\n",
      "   0.09108952]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:8 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.55108563]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 8 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.02407684 -0.1445456   0.0229466   0.34776758 -0.00546225 -0.08708137\n",
      "   0.22134935  0.07777219  0.13722032]\n",
      " [ 0.08110727  0.24958243 -0.18180928 -0.40129648 -0.19482494 -0.09295032\n",
      "  -0.51329263 -0.20927882 -0.0835076 ]\n",
      " [ 0.23439421 -0.38557741  0.12946239 -0.32416455  0.24760092 -0.32231008\n",
      "  -0.19179609  0.06646148  0.23299397]\n",
      " [ 0.19980145 -0.19192662  0.14633817  0.17273412  0.20422683  0.1454065\n",
      "   0.51547859 -0.2437289  -0.25690751]\n",
      " [-0.12461386  0.23598948 -0.05720942 -0.03050462 -0.17236324 -0.17559476\n",
      "  -0.48778277 -0.09602504  0.04728191]\n",
      " [ 0.20136903  0.07040293  0.19905814 -0.34467208 -0.10132234 -0.03823407\n",
      "  -0.23631306 -0.04466509 -0.08902614]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.18832118  0.2194009  -0.27577254  0.21436611  0.03482053 -0.19552008\n",
      "   0.11656317]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:8 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57449757]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 8 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.01698936 -0.1445456   0.0229466   0.3406801  -0.00546225 -0.09416885\n",
      "   0.22134935  0.07777219  0.13013284]\n",
      " [ 0.0893094   0.24958243 -0.18180928 -0.39309436 -0.19482494 -0.08474819\n",
      "  -0.51329263 -0.20927882 -0.07530547]\n",
      " [ 0.23751291 -0.38557741  0.12946239 -0.32104585  0.24760092 -0.31919138\n",
      "  -0.19179609  0.06646148  0.23611267]\n",
      " [ 0.1952963  -0.19192662  0.14633817  0.16822897  0.20422683  0.14090135\n",
      "   0.51547859 -0.2437289  -0.26141266]\n",
      " [-0.11973696  0.23598948 -0.05720942 -0.02562772 -0.17236324 -0.17071787\n",
      "  -0.48778277 -0.09602504  0.0521588 ]\n",
      " [ 0.20603276  0.07040293  0.19905814 -0.34000835 -0.10132234 -0.03357034\n",
      "  -0.23631306 -0.04466509 -0.08436241]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.11810318  0.1769922  -0.30233804  0.18239251 -0.00484495 -0.22568662\n",
      "   0.08617501]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:8 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.52963972]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 8 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.02183226 -0.13970269  0.0229466   0.345523   -0.00546225 -0.08932595\n",
      "   0.22619225  0.07777219  0.13013284]\n",
      " [ 0.08070406  0.24097709 -0.18180928 -0.4016997  -0.19482494 -0.09335354\n",
      "  -0.52189797 -0.20927882 -0.07530547]\n",
      " [ 0.22611956 -0.39697077  0.12946239 -0.3324392   0.24760092 -0.33058473\n",
      "  -0.20318944  0.06646148  0.23611267]\n",
      " [ 0.20556161 -0.18166131  0.14633817  0.17849429  0.20422683  0.15116666\n",
      "   0.52574391 -0.2437289  -0.26141266]\n",
      " [-0.12741858  0.22830786 -0.05720942 -0.03330934 -0.17236324 -0.17839949\n",
      "  -0.49546439 -0.09602504  0.0521588 ]\n",
      " [ 0.20128188  0.06565205  0.19905814 -0.34475922 -0.10132234 -0.03832121\n",
      "  -0.24106394 -0.04466509 -0.08436241]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.17669161  0.21122333 -0.28227243  0.19837967  0.03592833 -0.2044936\n",
      "   0.11062982]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:8 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.56503148]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 8 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.02759504 -0.13393992  0.0229466   0.35128577 -0.00546225 -0.08932595\n",
      "   0.23195503  0.07777219  0.13013284]\n",
      " [ 0.07334745  0.23362048 -0.18180928 -0.40905631 -0.19482494 -0.09335354\n",
      "  -0.52925459 -0.20927882 -0.07530547]\n",
      " [ 0.21776549 -0.40532484  0.12946239 -0.34079327  0.24760092 -0.33058473\n",
      "  -0.21154351  0.06646148  0.23611267]\n",
      " [ 0.21410788 -0.17311504  0.14633817  0.18704056  0.20422683  0.15116666\n",
      "   0.53429017 -0.2437289  -0.26141266]\n",
      " [-0.1328824   0.22284404 -0.05720942 -0.03877316 -0.17236324 -0.17839949\n",
      "  -0.50092821 -0.09602504  0.0521588 ]\n",
      " [ 0.19712714  0.06149731  0.19905814 -0.34891396 -0.10132234 -0.03832121\n",
      "  -0.24521868 -0.04466509 -0.08436241]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.23014291  0.24391164 -0.26335573  0.21603876  0.07197571 -0.18340003\n",
      "   0.13312996]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:8 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.60949425]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 8 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.0298185  -0.13393992  0.02517007  0.35128577 -0.00546225 -0.08710248\n",
      "   0.23417849  0.07777219  0.13013284]\n",
      " [ 0.06589476  0.23362048 -0.18926197 -0.40905631 -0.19482494 -0.10080622\n",
      "  -0.53670727 -0.20927882 -0.07530547]\n",
      " [ 0.21552248 -0.40532484  0.12721939 -0.34079327  0.24760092 -0.33282774\n",
      "  -0.21378651  0.06646148  0.23611267]\n",
      " [ 0.22345989 -0.17311504  0.15569017  0.18704056  0.20422683  0.16051866\n",
      "   0.54364218 -0.2437289  -0.26141266]\n",
      " [-0.14129162  0.22284404 -0.06561864 -0.03877316 -0.17236324 -0.18680871\n",
      "  -0.50933743 -0.09602504  0.0521588 ]\n",
      " [ 0.19843172  0.06149731  0.20036272 -0.34891396 -0.10132234 -0.03701663\n",
      "  -0.24391409 -0.04466509 -0.08436241]]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta two: \n",
      "[[ 0.27661524  0.26938512 -0.24825408  0.2370177   0.10636465 -0.16967326\n",
      "   0.15767347]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:8 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.61316141]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 9 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.02319774 -0.14056068  0.02517007  0.34466501 -0.00546225 -0.08710248\n",
      "   0.23417849  0.07777219  0.12351208]\n",
      " [ 0.06922669  0.23695241 -0.18926197 -0.40572438 -0.19482494 -0.10080622\n",
      "  -0.53670727 -0.20927882 -0.07197353]\n",
      " [ 0.22076172 -0.4000856   0.12721939 -0.33555403  0.24760092 -0.33282774\n",
      "  -0.21378651  0.06646148  0.2413519 ]\n",
      " [ 0.22389663 -0.17267829  0.15569017  0.1874773   0.20422683  0.16051866\n",
      "   0.54364218 -0.2437289  -0.26097591]\n",
      " [-0.14301369  0.22112197 -0.06561864 -0.04049523 -0.17236324 -0.18680871\n",
      "  -0.50933743 -0.09602504  0.05043673]\n",
      " [ 0.20155959  0.06462518  0.20036272 -0.34578609 -0.10132234 -0.03701663\n",
      "  -0.24391409 -0.04466509 -0.08123455]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.20389599  0.22624653 -0.28126277  0.20597336  0.07044181 -0.20775755\n",
      "   0.12445739]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:9 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.59776878]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 9 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.0265462  -0.14056068  0.02851853  0.34466501 -0.00546225 -0.08710248\n",
      "   0.23752695  0.07777219  0.12351208]\n",
      " [ 0.06208536  0.23695241 -0.1964033  -0.40572438 -0.19482494 -0.10080622\n",
      "  -0.5438486  -0.20927882 -0.07197353]\n",
      " [ 0.22237673 -0.4000856   0.1288344  -0.33555403  0.24760092 -0.33282774\n",
      "  -0.2121715   0.06646148  0.2413519 ]\n",
      " [ 0.23298054 -0.17267829  0.16477408  0.1874773   0.20422683  0.16051866\n",
      "   0.55272608 -0.2437289  -0.26097591]\n",
      " [-0.1506643   0.22112197 -0.07326925 -0.04049523 -0.17236324 -0.18680871\n",
      "  -0.51698804 -0.09602504  0.05043673]\n",
      " [ 0.2034579   0.06462518  0.20226103 -0.34578609 -0.10132234 -0.03701663\n",
      "  -0.24201579 -0.04466509 -0.08123455]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.25225248  0.25381797 -0.26475039  0.23177147  0.10505053 -0.19190434\n",
      "   0.15054185]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:9 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.61544189]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 9 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.02379406 -0.14331282  0.02576638  0.34466501 -0.00546225 -0.08710248\n",
      "   0.23477481  0.07777219  0.12351208]\n",
      " [ 0.06974007  0.24460712 -0.18874859 -0.40572438 -0.19482494 -0.10080622\n",
      "  -0.53619389 -0.20927882 -0.07197353]\n",
      " [ 0.22704962 -0.39541272  0.13350729 -0.33555403  0.24760092 -0.33282774\n",
      "  -0.20749861  0.06646148  0.2413519 ]\n",
      " [ 0.2207619  -0.18489693  0.15255544  0.1874773   0.20422683  0.16051866\n",
      "   0.54050744 -0.2437289  -0.26097591]\n",
      " [-0.1418117   0.22997457 -0.06441665 -0.04049523 -0.17236324 -0.18680871\n",
      "  -0.50813544 -0.09602504  0.05043673]\n",
      " [ 0.19935437  0.06052165  0.1981575  -0.34578609 -0.10132234 -0.03701663\n",
      "  -0.24611932 -0.04466509 -0.08123455]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.17942319  0.21464056 -0.29325954  0.20008297  0.05514745 -0.21906232\n",
      "   0.10998793]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:9 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.57266333]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 9 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.02545185 -0.14331282  0.02742418  0.34466501 -0.00546225 -0.08710248\n",
      "   0.23477481  0.07942998  0.12351208]\n",
      " [ 0.0655622   0.24460712 -0.19292646 -0.40572438 -0.19482494 -0.10080622\n",
      "  -0.53619389 -0.21345669 -0.07197353]\n",
      " [ 0.23238478 -0.39541272  0.13884244 -0.33555403  0.24760092 -0.33282774\n",
      "  -0.20749861  0.07179664  0.2413519 ]\n",
      " [ 0.22244882 -0.18489693  0.15424236  0.1874773   0.20422683  0.16051866\n",
      "   0.54050744 -0.24204198 -0.26097591]\n",
      " [-0.14567394  0.22997457 -0.06827889 -0.04049523 -0.17236324 -0.18680871\n",
      "  -0.50813544 -0.09988728  0.05043673]\n",
      " [ 0.20382622  0.06052165  0.20262935 -0.34578609 -0.10132234 -0.03701663\n",
      "  -0.24611932 -0.04019323 -0.08123455]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.23171211  0.2424473  -0.2713684   0.23172621  0.08298355 -0.19683918\n",
      "   0.14069762]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:9 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.6018935]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 9 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.03099348 -0.14331282  0.02742418  0.35020664 -0.00546225 -0.08710248\n",
      "   0.23477481  0.07942998  0.1290537 ]\n",
      " [ 0.06085074  0.24460712 -0.19292646 -0.41043584 -0.19482494 -0.10080622\n",
      "  -0.53619389 -0.21345669 -0.076685  ]\n",
      " [ 0.23402465 -0.39541272  0.13884244 -0.33391416  0.24760092 -0.33282774\n",
      "  -0.20749861  0.07179664  0.24299178]\n",
      " [ 0.22421511 -0.18489693  0.15424236  0.1892436   0.20422683  0.16051866\n",
      "   0.54050744 -0.24204198 -0.25920962]\n",
      " [-0.147285    0.22997457 -0.06827889 -0.0421063  -0.17236324 -0.18680871\n",
      "  -0.50813544 -0.09988728  0.04882567]\n",
      " [ 0.20119769  0.06052165  0.20262935 -0.34841463 -0.10132234 -0.03701663\n",
      "  -0.24611932 -0.04019323 -0.08386308]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.2794088   0.27206508 -0.25236603  0.25721965  0.10860473 -0.17460685\n",
      "   0.16189555]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:9 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.6323777]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 9 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.02975479 -0.14331282  0.02742418  0.35020664 -0.00670093 -0.08834117\n",
      "   0.23477481  0.07942998  0.12781502]\n",
      " [ 0.06643781  0.24460712 -0.19292646 -0.41043584 -0.18923787 -0.09521915\n",
      "  -0.53619389 -0.21345669 -0.07109792]\n",
      " [ 0.22709426 -0.39541272  0.13884244 -0.33391416  0.24067054 -0.33975812\n",
      "  -0.20749861  0.07179664  0.23606139]\n",
      " [ 0.2183172  -0.18489693  0.15424236  0.1892436   0.19832892  0.15462075\n",
      "   0.54050744 -0.24204198 -0.26510753]\n",
      " [-0.13930068  0.22997457 -0.06827889 -0.0421063  -0.16437891 -0.17882438\n",
      "  -0.50813544 -0.09988728  0.05680999]\n",
      " [ 0.20158363  0.06052165  0.20262935 -0.34841463 -0.10093639 -0.03663069\n",
      "  -0.24611932 -0.04019323 -0.08347714]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.20590243  0.23407227 -0.28344137  0.21335741  0.06584616 -0.20309409\n",
      "   0.12552834]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:9 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57248118]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 9 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.03116736 -0.14190025  0.02742418  0.35020664 -0.00528836 -0.0869286\n",
      "   0.23477481  0.07942998  0.12922759]\n",
      " [ 0.06721698  0.24538629 -0.19292646 -0.41043584 -0.1884587  -0.09443998\n",
      "  -0.53619389 -0.21345669 -0.07031876]\n",
      " [ 0.2276431  -0.39486388  0.13884244 -0.33391416  0.24121937 -0.33920928\n",
      "  -0.20749861  0.07179664  0.23661023]\n",
      " [ 0.21620119 -0.18701294  0.15424236  0.1892436   0.19621291  0.15250474\n",
      "   0.54050744 -0.24204198 -0.26722355]\n",
      " [-0.13590545  0.2333698  -0.06827889 -0.0421063  -0.16098368 -0.17542915\n",
      "  -0.50813544 -0.09988728  0.06020522]\n",
      " [ 0.20086479  0.0598028   0.20262935 -0.34841463 -0.10165524 -0.03734953\n",
      "  -0.24611932 -0.04019323 -0.08419598]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.13584605  0.20045819 -0.31769013  0.17887815  0.02869677 -0.23470533\n",
      "   0.0897811 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:9 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.54340571]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 9 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.03063114 -0.14243648  0.02742418  0.35020664 -0.00582459 -0.08746482\n",
      "   0.23423859  0.07942998  0.12922759]\n",
      " [ 0.07522821  0.25339752 -0.19292646 -0.41043584 -0.18044747 -0.08642875\n",
      "  -0.52818266 -0.21345669 -0.07031876]\n",
      " [ 0.2351809  -0.38732608  0.13884244 -0.33391416  0.24875717 -0.33167149\n",
      "  -0.19996082  0.07179664  0.23661023]\n",
      " [ 0.20357723 -0.1996369   0.15424236  0.1892436   0.18358895  0.13988078\n",
      "   0.52788349 -0.24204198 -0.26722355]\n",
      " [-0.12491998  0.24435527 -0.06827889 -0.0421063  -0.14999821 -0.16444369\n",
      "  -0.49714997 -0.09988728  0.06020522]\n",
      " [ 0.20295421  0.06189222  0.20262935 -0.34841463 -0.09956582 -0.03526011\n",
      "  -0.2440299  -0.04019323 -0.08419598]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.06843224  0.16621497 -0.34303886  0.15299292 -0.01948514 -0.25637597\n",
      "   0.05816902]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:9 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.5155351]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 9 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.03910688 -0.14243648  0.02742418  0.35868238 -0.00582459 -0.08746482\n",
      "   0.24271433  0.07942998  0.12922759]\n",
      " [ 0.06432988  0.25339752 -0.19292646 -0.42133418 -0.18044747 -0.08642875\n",
      "  -0.539081   -0.21345669 -0.07031876]\n",
      " [ 0.23076246 -0.38732608  0.13884244 -0.3383326   0.24875717 -0.33167149\n",
      "  -0.20437925  0.07179664  0.23661023]\n",
      " [ 0.21492352 -0.1996369   0.15424236  0.20058988  0.18358895  0.13988078\n",
      "   0.53922977 -0.24204198 -0.26722355]\n",
      " [-0.13393437  0.24435527 -0.06827889 -0.05112069 -0.14999821 -0.16444369\n",
      "  -0.50616436 -0.09988728  0.06020522]\n",
      " [ 0.19728109  0.06189222  0.20262935 -0.35408775 -0.09956582 -0.03526011\n",
      "  -0.24970302 -0.04019323 -0.08419598]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.1289319   0.20548517 -0.32509276  0.17875832  0.02378334 -0.23581806\n",
      "   0.0826012 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:9 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.55303264]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 9 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.04168762 -0.14243648  0.02742418  0.35868238 -0.00324385 -0.08488408\n",
      "   0.24529507  0.07942998  0.12922759]\n",
      " [ 0.05537619  0.25339752 -0.19292646 -0.42133418 -0.18940115 -0.09538244\n",
      "  -0.54803468 -0.21345669 -0.07031876]\n",
      " [ 0.22998236 -0.38732608  0.13884244 -0.3383326   0.24797707 -0.33245159\n",
      "  -0.20515936  0.07179664  0.23661023]\n",
      " [ 0.2262023  -0.1996369   0.15424236  0.20058988  0.19486773  0.15115957\n",
      "   0.55050856 -0.24204198 -0.26722355]\n",
      " [-0.14451771  0.24435527 -0.06827889 -0.05112069 -0.16058155 -0.17502702\n",
      "  -0.51674769 -0.09988728  0.06020522]\n",
      " [ 0.19471762  0.06189222  0.20262935 -0.35408775 -0.10212929 -0.03782358\n",
      "  -0.25226648 -0.04019323 -0.08419598]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.18417428  0.23570241 -0.30726889  0.20559899  0.06499658 -0.2204622\n",
      "   0.10764391]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:9 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57370118]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 9 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.03426123 -0.14243648  0.02742418  0.35125599 -0.00324385 -0.09231048\n",
      "   0.24529507  0.07942998  0.12180119]\n",
      " [ 0.06407168  0.25339752 -0.19292646 -0.41263868 -0.18940115 -0.08668694\n",
      "  -0.54803468 -0.21345669 -0.06162327]\n",
      " [ 0.23352653 -0.38732608  0.13884244 -0.33478843  0.24797707 -0.32890742\n",
      "  -0.20515936  0.07179664  0.2401544 ]\n",
      " [ 0.22088201 -0.1996369   0.15424236  0.1952696   0.19486773  0.14583928\n",
      "   0.55050856 -0.24204198 -0.27254383]\n",
      " [-0.13920179  0.24435527 -0.06827889 -0.04580477 -0.16058155 -0.1697111\n",
      "  -0.51674769 -0.09988728  0.06552114]\n",
      " [ 0.1995564   0.06189222  0.20262935 -0.34924897 -0.10212929 -0.03298481\n",
      "  -0.25226648 -0.04019323 -0.07935721]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.11401976  0.19295154 -0.33323518  0.17409058  0.02451301 -0.25013772\n",
      "   0.07746954]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:9 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.53221146]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 9 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.0398067  -0.136891    0.02742418  0.35680146 -0.00324385 -0.086765\n",
      "   0.25084054  0.07942998  0.12180119]\n",
      " [ 0.05474451  0.24407035 -0.19292646 -0.42196586 -0.18940115 -0.09601412\n",
      "  -0.55736185 -0.21345669 -0.06162327]\n",
      " [ 0.22194137 -0.39891124  0.13884244 -0.34637359  0.24797707 -0.34049258\n",
      "  -0.21674451  0.07179664  0.2401544 ]\n",
      " [ 0.23174626 -0.18877265  0.15424236  0.20613384  0.19486773  0.15670353\n",
      "   0.56137281 -0.24204198 -0.27254383]\n",
      " [-0.14748923  0.23606782 -0.06827889 -0.05409221 -0.16058155 -0.17799854\n",
      "  -0.52503514 -0.09988728  0.06552114]\n",
      " [ 0.19431026  0.05664609  0.20262935 -0.35449511 -0.10212929 -0.03823094\n",
      "  -0.25751262 -0.04019323 -0.07935721]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.17225065  0.22775858 -0.31429742  0.18949327  0.06606573 -0.2298637\n",
      "   0.10121631]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:9 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.56712883]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 9 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.0461652  -0.13053251  0.02742418  0.36315996 -0.00324385 -0.086765\n",
      "   0.25719904  0.07942998  0.12180119]\n",
      " [ 0.04667576  0.23600159 -0.19292646 -0.43003461 -0.18940115 -0.09601412\n",
      "  -0.56543061 -0.21345669 -0.06162327]\n",
      " [ 0.21334266 -0.40750994  0.13884244 -0.35497229  0.24797707 -0.34049258\n",
      "  -0.22534322  0.07179664  0.2401544 ]\n",
      " [ 0.24092118 -0.17959773  0.15424236  0.21530877  0.19486773  0.15670353\n",
      "   0.57054773 -0.24204198 -0.27254383]\n",
      " [-0.15362858  0.22992848 -0.06827889 -0.06023156 -0.16058155 -0.17799854\n",
      "  -0.53117448 -0.09988728  0.06552114]\n",
      " [ 0.18966724  0.05200307  0.20262935 -0.35913813 -0.10212929 -0.03823094\n",
      "  -0.26215564 -0.04019323 -0.07935721]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.22538422  0.26096373 -0.29643673  0.20665461  0.10284542 -0.20968547\n",
      "   0.12303854]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:9 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.61189358]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 9 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.04893513 -0.13053251  0.03019411  0.36315996 -0.00324385 -0.08399507\n",
      "   0.25996897  0.07942998  0.12180119]\n",
      " [ 0.03873704  0.23600159 -0.20086518 -0.43003461 -0.18940115 -0.10395283\n",
      "  -0.57336932 -0.21345669 -0.06162327]\n",
      " [ 0.21090909 -0.40750994  0.13640887 -0.35497229  0.24797707 -0.34292616\n",
      "  -0.2277768   0.07179664  0.2401544 ]\n",
      " [ 0.25050388 -0.17959773  0.16382505  0.21530877  0.19486773  0.16628622\n",
      "   0.58013042 -0.24204198 -0.27254383]\n",
      " [-0.16232953  0.22992848 -0.07697984 -0.06023156 -0.16058155 -0.18669949\n",
      "  -0.53987543 -0.09988728  0.06552114]\n",
      " [ 0.1907239   0.05200307  0.20368601 -0.35913813 -0.10212929 -0.03717429\n",
      "  -0.26109898 -0.04019323 -0.07935721]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.27146795  0.2868031  -0.28222535  0.22724434  0.13761265 -0.19665734\n",
      "   0.14713855]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:9 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.6100129]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 10 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.04190783 -0.13755981  0.03019411  0.35613266 -0.00324385 -0.08399507\n",
      "   0.25996897  0.07942998  0.11477389]\n",
      " [ 0.04262604  0.23989059 -0.20086518 -0.42614561 -0.18940115 -0.10395283\n",
      "  -0.57336932 -0.21345669 -0.05773427]\n",
      " [ 0.21642345 -0.40199558  0.13640887 -0.34945792  0.24797707 -0.34292616\n",
      "  -0.2277768   0.07179664  0.24566876]\n",
      " [ 0.25025589 -0.17984571  0.16382505  0.21506079  0.19486773  0.16628622\n",
      "   0.58013042 -0.24204198 -0.27279182]\n",
      " [-0.16364997  0.22860803 -0.07697984 -0.06155201 -0.16058155 -0.18669949\n",
      "  -0.53987543 -0.09988728  0.06420069]\n",
      " [ 0.19424133  0.05552051  0.20368601 -0.35562069 -0.10212929 -0.03717429\n",
      "  -0.26109898 -0.04019323 -0.07583977]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.19890778  0.2433036  -0.31458587  0.19656818  0.10108458 -0.23425904\n",
      "   0.11439841]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:10 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.59966958]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 10 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.04578909 -0.13755981  0.03407537  0.35613266 -0.00324385 -0.08399507\n",
      "   0.26385023  0.07942998  0.11477389]\n",
      " [ 0.03491566  0.23989059 -0.20857555 -0.42614561 -0.18940115 -0.10395283\n",
      "  -0.5810797  -0.21345669 -0.05773427]\n",
      " [ 0.21791991 -0.40199558  0.13790533 -0.34945792  0.24797707 -0.34292616\n",
      "  -0.22628034  0.07179664  0.24566876]\n",
      " [ 0.25967407 -0.17984571  0.17324322  0.21506079  0.19486773  0.16628622\n",
      "   0.5895486  -0.24204198 -0.27279182]\n",
      " [-0.17173175  0.22860803 -0.08506161 -0.06155201 -0.16058155 -0.18669949\n",
      "  -0.54795721 -0.09988728  0.06420069]\n",
      " [ 0.19587741  0.05552051  0.20532209 -0.35562069 -0.10212929 -0.03717429\n",
      "  -0.2594629  -0.04019323 -0.07583977]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.24696063  0.27128301 -0.29897629  0.22209498  0.13615927 -0.21916031\n",
      "   0.14006603]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:10 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.61631827]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 10 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.04207306 -0.14127583  0.03035934  0.35613266 -0.00324385 -0.08399507\n",
      "   0.2601342   0.07942998  0.11477389]\n",
      " [ 0.0436999   0.24867482 -0.19979132 -0.42614561 -0.18940115 -0.10395283\n",
      "  -0.57229546 -0.21345669 -0.05773427]\n",
      " [ 0.22279233 -0.39712316  0.14277775 -0.34945792  0.24797707 -0.34292616\n",
      "  -0.22140792  0.07179664  0.24566876]\n",
      " [ 0.24675554 -0.19276423  0.1603247   0.21506079  0.19486773  0.16628622\n",
      "   0.57663007 -0.24204198 -0.27279182]\n",
      " [-0.16206081  0.23827897 -0.07539067 -0.06155201 -0.16058155 -0.18669949\n",
      "  -0.53828627 -0.09988728  0.06420069]\n",
      " [ 0.19231859  0.05196169  0.20176327 -0.35562069 -0.10212929 -0.03717429\n",
      "  -0.26302172 -0.04019323 -0.07583977]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.17409022  0.2311054  -0.32623401  0.19059269  0.08522163 -0.24538059\n",
      "   0.10004888]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:10 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.57043061]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 10 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.04405974 -0.14127583  0.03234602  0.35613266 -0.00324385 -0.08399507\n",
      "   0.2601342   0.08141666  0.11477389]\n",
      " [ 0.0389998   0.24867482 -0.20449141 -0.42614561 -0.18940115 -0.10395283\n",
      "  -0.57229546 -0.21815679 -0.05773427]\n",
      " [ 0.2282804  -0.39712316  0.14826582 -0.34945792  0.24797707 -0.34292616\n",
      "  -0.22140792  0.07728471  0.24566876]\n",
      " [ 0.24891234 -0.19276423  0.1624815   0.21506079  0.19486773  0.16628622\n",
      "   0.57663007 -0.23988518 -0.27279182]\n",
      " [-0.16637549  0.23827897 -0.07970535 -0.06155201 -0.16058155 -0.18669949\n",
      "  -0.53828627 -0.10420196  0.06420069]\n",
      " [ 0.19683215  0.05196169  0.20627683 -0.35562069 -0.10212929 -0.03717429\n",
      "  -0.26302172 -0.03567967 -0.07583977]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.22672097  0.2594151  -0.30472645  0.22257278  0.11370361 -0.2234622\n",
      "   0.13097261]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:10 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.60007937]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 10 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.04984556 -0.14127583  0.03234602  0.36191848 -0.00324385 -0.08399507\n",
      "   0.2601342   0.08141666  0.12055972]\n",
      " [ 0.03391826  0.24867482 -0.20449141 -0.43122715 -0.18940115 -0.10395283\n",
      "  -0.57229546 -0.21815679 -0.06281581]\n",
      " [ 0.22976813 -0.39712316  0.14826582 -0.34797019  0.24797707 -0.34292616\n",
      "  -0.22140792  0.07728471  0.24715649]\n",
      " [ 0.25118508 -0.19276423  0.1624815   0.21733352  0.19486773  0.16628622\n",
      "   0.57663007 -0.23988518 -0.27051908]\n",
      " [-0.16832658  0.23827897 -0.07970535 -0.0635031  -0.16058155 -0.18669949\n",
      "  -0.53828627 -0.10420196  0.0622496 ]\n",
      " [ 0.19405574  0.05196169  0.20627683 -0.3583971  -0.10212929 -0.03717429\n",
      "  -0.26302172 -0.03567967 -0.07861618]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.27470827  0.28945371 -0.28598363  0.24805801  0.13998386 -0.20142837\n",
      "   0.15216431]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:10 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.62982811]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 10 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.04832167 -0.14127583  0.03234602  0.36191848 -0.00476774 -0.08551896\n",
      "   0.2601342   0.08141666  0.11903582]\n",
      " [ 0.03968229  0.24867482 -0.20449141 -0.43122715 -0.18363713 -0.09818881\n",
      "  -0.57229546 -0.21815679 -0.05705178]\n",
      " [ 0.22300657 -0.39712316  0.14826582 -0.34797019  0.2412155  -0.34968772\n",
      "  -0.22140792  0.07728471  0.24039493]\n",
      " [ 0.24509069 -0.19276423  0.1624815   0.21733352  0.18877334  0.16019183\n",
      "   0.57663007 -0.23988518 -0.27661347]\n",
      " [-0.16041849  0.23827897 -0.07970535 -0.0635031  -0.15267345 -0.1787914\n",
      "  -0.53828627 -0.10420196  0.07015769]\n",
      " [ 0.19449371  0.05196169  0.20627683 -0.3583971  -0.10169132 -0.03673632\n",
      "  -0.26302172 -0.03567967 -0.07817822]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.20128774  0.25121779 -0.31682958  0.20442055  0.09705983 -0.22995685\n",
      "   0.11589205]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:10 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.56817432]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 10 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.04943932 -0.14015819  0.03234602  0.36191848 -0.0036501  -0.08440132\n",
      "   0.2601342   0.08141666  0.12015347]\n",
      " [ 0.04056207  0.2495546  -0.20449141 -0.43122715 -0.18275735 -0.09730903\n",
      "  -0.57229546 -0.21815679 -0.05617201]\n",
      " [ 0.22374149 -0.39638824  0.14826582 -0.34797019  0.24195042 -0.3489528\n",
      "  -0.22140792  0.07728471  0.24112985]\n",
      " [ 0.24292655 -0.19492837  0.1624815   0.21733352  0.1866092   0.15802769\n",
      "   0.57663007 -0.23988518 -0.27877761]\n",
      " [-0.15724861  0.24144885 -0.07970535 -0.0635031  -0.14950357 -0.17562152\n",
      "  -0.53828627 -0.10420196  0.07332757]\n",
      " [ 0.19397368  0.05144166  0.20627683 -0.3583971  -0.10221134 -0.03725634\n",
      "  -0.26302172 -0.03567967 -0.07869824]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.13158631  0.21748549 -0.35080014  0.17030497  0.06003937 -0.26161987\n",
      "   0.08052124]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:10 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.54488142]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 10 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.04806732 -0.14153018  0.03234602  0.36191848 -0.00502209 -0.08577331\n",
      "   0.25876221  0.08141666  0.12015347]\n",
      " [ 0.04934592  0.25833845 -0.20449141 -0.43122715 -0.1739735  -0.08852518\n",
      "  -0.56351162 -0.21815679 -0.05617201]\n",
      " [ 0.2316948  -0.38843493  0.14826582 -0.34797019  0.24990373 -0.34099949\n",
      "  -0.21345461  0.07728471  0.24112985]\n",
      " [ 0.2298697  -0.20798522  0.1624815   0.21733352  0.17355235  0.14497084\n",
      "   0.56357322 -0.23988518 -0.27877761]\n",
      " [-0.14589917  0.25279829 -0.07970535 -0.0635031  -0.13815414 -0.16427208\n",
      "  -0.52693683 -0.10420196  0.07332757]\n",
      " [ 0.19661042  0.0540784   0.20627683 -0.3583971  -0.09957461 -0.0346196\n",
      "  -0.26038499 -0.03567967 -0.07869824]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.06402492  0.18233129 -0.37532683  0.14481458  0.01105919 -0.28286725\n",
      "   0.04938814]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:10 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.51904435]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 10 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.05706041 -0.14153018  0.03234602  0.37091157 -0.00502209 -0.08577331\n",
      "   0.2677553   0.08141666  0.12015347]\n",
      " [ 0.03790899  0.25833845 -0.20449141 -0.44266408 -0.1739735  -0.08852518\n",
      "  -0.57494854 -0.21815679 -0.05617201]\n",
      " [ 0.22687829 -0.38843493  0.14826582 -0.3527867   0.24990373 -0.34099949\n",
      "  -0.21827111  0.07728471  0.24112985]\n",
      " [ 0.24174042 -0.20798522  0.1624815   0.22920424  0.17355235  0.14497084\n",
      "   0.57544394 -0.23988518 -0.27877761]\n",
      " [-0.15557789  0.25279829 -0.07970535 -0.07318183 -0.13815414 -0.16427208\n",
      "  -0.53661555 -0.10420196  0.07332757]\n",
      " [ 0.19054858  0.0540784   0.20627683 -0.36445894 -0.09957461 -0.0346196\n",
      "  -0.26644683 -0.03567967 -0.07869824]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.12405716  0.22202597 -0.35852907  0.16992645  0.05507315 -0.2634285\n",
      "   0.07316074]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:10 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.55517033]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 10 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.06023031 -0.14153018  0.03234602  0.37091157 -0.0018522  -0.08260342\n",
      "   0.2709252   0.08141666  0.12015347]\n",
      " [ 0.02851328  0.25833845 -0.20449141 -0.44266408 -0.18336921 -0.09792089\n",
      "  -0.58434425 -0.21815679 -0.05617201]\n",
      " [ 0.22574751 -0.38843493  0.14826582 -0.3527867   0.24877295 -0.34213027\n",
      "  -0.21940189  0.07728471  0.24112985]\n",
      " [ 0.25321891 -0.20798522  0.1624815   0.22920424  0.18503085  0.15644934\n",
      "   0.58692243 -0.23988518 -0.27877761]\n",
      " [-0.16634572  0.25279829 -0.07970535 -0.07318183 -0.14892196 -0.17503991\n",
      "  -0.54738338 -0.10420196  0.07332757]\n",
      " [ 0.18769526  0.0540784   0.20627683 -0.36445894 -0.10242793 -0.03747293\n",
      "  -0.26930015 -0.03567967 -0.07869824]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.17898389  0.25268825 -0.34149496  0.19625775  0.09664668 -0.24859825\n",
      "   0.09774974]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:10 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57280354]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 10 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.05245351 -0.14153018  0.03234602  0.36313477 -0.0018522  -0.09038022\n",
      "   0.2709252   0.08141666  0.11237666]\n",
      " [ 0.03770691  0.25833845 -0.20449141 -0.43347045 -0.18336921 -0.08872726\n",
      "  -0.58434425 -0.21815679 -0.04697838]\n",
      " [ 0.22969141 -0.38843493  0.14826582 -0.3488428   0.24877295 -0.33818638\n",
      "  -0.21940189  0.07728471  0.24507374]\n",
      " [ 0.24711002 -0.20798522  0.1624815   0.22309534  0.18503085  0.15034044\n",
      "   0.58692243 -0.23988518 -0.28488651]\n",
      " [-0.16053772  0.25279829 -0.07970535 -0.06737382 -0.14892196 -0.1692319\n",
      "  -0.54738338 -0.10420196  0.07913557]\n",
      " [ 0.1927191   0.0540784   0.20627683 -0.3594351  -0.10242793 -0.03244908\n",
      "  -0.26930015 -0.03567967 -0.07367439]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.10890148  0.20958237 -0.36683971  0.16519471  0.05536369 -0.27771808\n",
      "   0.06780454]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:10 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.53527388]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 10 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.05869466 -0.13528903  0.03234602  0.36937593 -0.0018522  -0.08413906\n",
      "   0.27716635  0.08141666  0.11237666]\n",
      " [ 0.02772578  0.24835732 -0.20449141 -0.44345158 -0.18336921 -0.09870839\n",
      "  -0.59432538 -0.21815679 -0.04697838]\n",
      " [ 0.2179551  -0.40017124  0.14826582 -0.36057911  0.24877295 -0.34992269\n",
      "  -0.2311382   0.07728471  0.24507374]\n",
      " [ 0.25847135 -0.19662389  0.1624815   0.23445667  0.18503085  0.16170177\n",
      "   0.59828377 -0.23988518 -0.28488651]\n",
      " [-0.16942704  0.24390897 -0.07970535 -0.07626314 -0.14892196 -0.17812122\n",
      "  -0.5562727  -0.10420196  0.07913557]\n",
      " [ 0.18698092  0.04834022  0.20627683 -0.36517328 -0.10242793 -0.03818726\n",
      "  -0.27503833 -0.03567967 -0.07367439]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.16670312  0.24494155 -0.34904925  0.18001053  0.09761421 -0.25843263\n",
      "   0.09080155]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:10 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.56965304]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 10 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.06563197 -0.12835171  0.03234602  0.37631324 -0.0018522  -0.08413906\n",
      "   0.28410367  0.08141666  0.11237666]\n",
      " [ 0.01900786  0.2396394  -0.20449141 -0.4521695  -0.18336921 -0.09870839\n",
      "  -0.6030433  -0.21815679 -0.04697838]\n",
      " [ 0.20913669 -0.40898965  0.14826582 -0.36939751  0.24877295 -0.34992269\n",
      "  -0.23995661  0.07728471  0.24507374]\n",
      " [ 0.26819081 -0.18690442  0.1624815   0.24417614  0.18503085  0.16170177\n",
      "   0.60800323 -0.23988518 -0.28488651]\n",
      " [-0.17624181  0.2370942  -0.07970535 -0.08307792 -0.14892196 -0.17812122\n",
      "  -0.56308747 -0.10420196  0.07913557]\n",
      " [ 0.18185448  0.04321377  0.20627683 -0.37029973 -0.10242793 -0.03818726\n",
      "  -0.28016478 -0.03567967 -0.07367439]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.21945257  0.27863532 -0.33226023  0.19665978  0.13505766 -0.23923195\n",
      "   0.1119086 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:10 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.61458974]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 10 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.06895759 -0.12835171  0.03567164  0.37631324 -0.0018522  -0.08081345\n",
      "   0.28742928  0.08141666  0.11237666]\n",
      " [ 0.01064071  0.2396394  -0.21285856 -0.4521695  -0.18336921 -0.10707554\n",
      "  -0.61141045 -0.21815679 -0.04697838]\n",
      " [ 0.20651931 -0.40898965  0.14564844 -0.36939751  0.24877295 -0.35254007\n",
      "  -0.24257399  0.07728471  0.24507374]\n",
      " [ 0.27793609 -0.18690442  0.17222677  0.24417614  0.18503085  0.17144704\n",
      "   0.61774851 -0.23988518 -0.28488651]\n",
      " [-0.18520257  0.2370942  -0.08866611 -0.08307792 -0.14892196 -0.18708199\n",
      "  -0.57204823 -0.10420196  0.07913557]\n",
      " [ 0.18264979  0.04321377  0.20707214 -0.37029973 -0.10242793 -0.03739194\n",
      "  -0.27936946 -0.03567967 -0.07367439]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26509848  0.30483331 -0.3189462   0.21684171  0.17014074 -0.22693033\n",
      "   0.13552752]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:10 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.60642184]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 11 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.06153783 -0.13577147  0.03567164  0.36889348 -0.0018522  -0.08081345\n",
      "   0.28742928  0.08141666  0.10495691]\n",
      " [ 0.01507427  0.24407296 -0.21285856 -0.44773594 -0.18336921 -0.10707554\n",
      "  -0.61141045 -0.21815679 -0.04254482]\n",
      " [ 0.21227664 -0.40323232  0.14564844 -0.36364019  0.24877295 -0.35254007\n",
      "  -0.24257399  0.07728471  0.25083107]\n",
      " [ 0.27702624 -0.18781427  0.17222677  0.24326629  0.18503085  0.17144704\n",
      "   0.61774851 -0.23988518 -0.28579635]\n",
      " [-0.18606958  0.23622719 -0.08866611 -0.08394493 -0.14892196 -0.18708199\n",
      "  -0.57204823 -0.10420196  0.07826857]\n",
      " [ 0.18654932  0.0471133   0.20707214 -0.36640019 -0.10242793 -0.03739194\n",
      "  -0.27936946 -0.03567967 -0.06977486]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.1927298   0.2609992  -0.35065108  0.18651772  0.13304617 -0.26398201\n",
      "   0.1032737 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:11 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.6018026]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 11 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.06596014 -0.13577147  0.04009395  0.36889348 -0.0018522  -0.08081345\n",
      "   0.29185159  0.08141666  0.10495691]\n",
      " [ 0.00684466  0.24407296 -0.22108817 -0.44773594 -0.18336921 -0.10707554\n",
      "  -0.61964006 -0.21815679 -0.04254482]\n",
      " [ 0.21364796 -0.40323232  0.14701976 -0.36364019  0.24877295 -0.35254007\n",
      "  -0.24120267  0.07728471  0.25083107]\n",
      " [ 0.28672173 -0.18781427  0.18192226  0.24326629  0.18503085  0.17144704\n",
      "   0.62744399 -0.23988518 -0.28579635]\n",
      " [-0.19455556  0.23622719 -0.09715209 -0.08394493 -0.14892196 -0.18708199\n",
      "  -0.58053421 -0.10420196  0.07826857]\n",
      " [ 0.18790766  0.0471133   0.20843048 -0.36640019 -0.10242793 -0.03739194\n",
      "  -0.27801112 -0.03567967 -0.06977486]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.24044106  0.28938699 -0.33595305  0.21174772  0.1685446  -0.24966348\n",
      "   0.12849063]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:11 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.6173596]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 11 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.0612627  -0.14046891  0.0353965   0.36889348 -0.0018522  -0.08081345\n",
      "   0.28715414  0.08141666  0.10495691]\n",
      " [ 0.01671324  0.25394154 -0.21121959 -0.44773594 -0.18336921 -0.10707554\n",
      "  -0.60977148 -0.21815679 -0.04254482]\n",
      " [ 0.21871817 -0.39816211  0.15208997 -0.36364019  0.24877295 -0.35254007\n",
      "  -0.23613246  0.07728471  0.25083107]\n",
      " [ 0.27315908 -0.20137691  0.16835961  0.24326629  0.18503085  0.17144704\n",
      "   0.61388135 -0.23988518 -0.28579635]\n",
      " [-0.18405903  0.24672371 -0.08665556 -0.08394493 -0.14892196 -0.18708199\n",
      "  -0.57003769 -0.10420196  0.07826857]\n",
      " [ 0.18491229  0.04411794  0.20543511 -0.36640019 -0.10242793 -0.03739194\n",
      "  -0.28100649 -0.03567967 -0.06977486]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.16752264  0.24817636 -0.36196146  0.18042704  0.11657953 -0.27490405\n",
      "   0.08902237]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:11 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.5678417]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 11 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.06360471 -0.14046891  0.03773852  0.36889348 -0.0018522  -0.08081345\n",
      "   0.28715414  0.08375867  0.10495691]\n",
      " [ 0.01146928  0.25394154 -0.21646355 -0.44773594 -0.18336921 -0.10707554\n",
      "  -0.60977148 -0.22340075 -0.04254482]\n",
      " [ 0.22436977 -0.39816211  0.15774156 -0.36364019  0.24877295 -0.35254007\n",
      "  -0.23613246  0.0829363   0.25083107]\n",
      " [ 0.27580502 -0.20137691  0.17100555  0.24326629  0.18503085  0.17144704\n",
      "   0.61388135 -0.23723924 -0.28579635]\n",
      " [-0.18885841  0.24672371 -0.09145494 -0.08394493 -0.14892196 -0.18708199\n",
      "  -0.57003769 -0.10900134  0.07826857]\n",
      " [ 0.18946908  0.04411794  0.2099919  -0.36640019 -0.10242793 -0.03739194\n",
      "  -0.28100649 -0.03112289 -0.06977486]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.22054792  0.27704341 -0.34084289  0.21278231  0.14575607 -0.25330401\n",
      "   0.12018793]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:11 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.59797778]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 11 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.06965039 -0.14046891  0.03773852  0.37493917 -0.0018522  -0.08081345\n",
      "   0.28715414  0.08375867  0.11100259]\n",
      " [ 0.00600412  0.25394154 -0.21646355 -0.4532011  -0.18336921 -0.10707554\n",
      "  -0.60977148 -0.22340075 -0.04800998]\n",
      " [ 0.22571332 -0.39816211  0.15774156 -0.36229663  0.24877295 -0.35254007\n",
      "  -0.23613246  0.0829363   0.25217463]\n",
      " [ 0.27858517 -0.20137691  0.17100555  0.24604644  0.18503085  0.17144704\n",
      "   0.61388135 -0.23723924 -0.2830162 ]\n",
      " [-0.19118645  0.24672371 -0.09145494 -0.08627296 -0.14892196 -0.18708199\n",
      "  -0.57003769 -0.10900134  0.07594053]\n",
      " [ 0.18653357  0.04411794  0.2099919  -0.3693357  -0.10242793 -0.03739194\n",
      "  -0.28100649 -0.03112289 -0.07271037]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26887107  0.30754595 -0.3223577   0.23829022  0.17272307 -0.23148519\n",
      "   0.14138412]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:11 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.6266416]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 11 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.06785895 -0.14046891  0.03773852  0.37493917 -0.00364364 -0.08260489\n",
      "   0.28715414  0.08375867  0.10921115]\n",
      " [ 0.01193142  0.25394154 -0.21646355 -0.4532011  -0.17744192 -0.10114824\n",
      "  -0.60977148 -0.22340075 -0.04208268]\n",
      " [ 0.2190915  -0.39816211  0.15774156 -0.36229663  0.24215113 -0.35916189\n",
      "  -0.23613246  0.0829363   0.24555281]\n",
      " [ 0.2723293  -0.20137691  0.17100555  0.24604644  0.17877498  0.16519118\n",
      "   0.61388135 -0.23723924 -0.28927207]\n",
      " [-0.18332381  0.24672371 -0.09145494 -0.08627296 -0.14105933 -0.17921935\n",
      "  -0.57003769 -0.10900134  0.08380316]\n",
      " [ 0.18700991  0.04411794  0.2099919  -0.3693357  -0.10195159 -0.0369156\n",
      "  -0.28100649 -0.03112289 -0.07223403]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.19556593  0.26909908 -0.35297318  0.19486028  0.12968461 -0.26000555\n",
      "   0.10520795]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:11 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.56300183]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 11 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.06871804 -0.13960983  0.03773852  0.37493917 -0.00278455 -0.0817458\n",
      "   0.28715414  0.08375867  0.11007024]\n",
      " [ 0.01287954  0.25488966 -0.21646355 -0.4532011  -0.1764938  -0.10020012\n",
      "  -0.60977148 -0.22340075 -0.04113456]\n",
      " [ 0.21996582 -0.39728779  0.15774156 -0.36229663  0.24302544 -0.35828757\n",
      "  -0.23613246  0.0829363   0.24642712]\n",
      " [ 0.27016236 -0.20354385  0.17100555  0.24604644  0.17660805  0.16302424\n",
      "   0.61388135 -0.23723924 -0.29143901]\n",
      " [-0.18034943  0.2496981  -0.09145494 -0.08627296 -0.13808494 -0.17624497\n",
      "  -0.57003769 -0.10900134  0.08677754]\n",
      " [ 0.18666319  0.04377122  0.2099919  -0.3693357  -0.1022983  -0.03726232\n",
      "  -0.28100649 -0.03112289 -0.07258074]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.12630805  0.23532958 -0.38665353  0.16110603  0.09288303 -0.29164524\n",
      "   0.07023227]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:11 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.54653173]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 11 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.06649731 -0.14183056  0.03773852  0.37493917 -0.00500528 -0.08396654\n",
      "   0.28493341  0.08375867  0.11007024]\n",
      " [ 0.02241301  0.26442313 -0.21646355 -0.4532011  -0.16696032 -0.09066665\n",
      "  -0.600238   -0.22340075 -0.04113456]\n",
      " [ 0.22832006 -0.38893355  0.15774156 -0.36229663  0.25137968 -0.34993334\n",
      "  -0.22777822  0.0829363   0.24642712]\n",
      " [ 0.25670565 -0.21700057  0.17100555  0.24604644  0.16315133  0.14956752\n",
      "   0.60042463 -0.23723924 -0.29143901]\n",
      " [-0.16861001  0.26143752 -0.09145494 -0.08627296 -0.12634552 -0.16450555\n",
      "  -0.55829827 -0.10900134  0.08677754]\n",
      " [ 0.18985345  0.04696148  0.2099919  -0.3693357  -0.09910805 -0.03407206\n",
      "  -0.27781624 -0.03112289 -0.07258074]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.05858326  0.19924002 -0.41036249  0.13599257  0.04310542 -0.3124247\n",
      "   0.03957938]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:11 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.52312128]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 11 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.07599063 -0.14183056  0.03773852  0.38443248 -0.00500528 -0.08396654\n",
      "   0.29442673  0.08375867  0.11007024]\n",
      " [ 0.01052911  0.26442313 -0.21646355 -0.465085   -0.16696032 -0.09066665\n",
      "  -0.61212191 -0.22340075 -0.04113456]\n",
      " [ 0.22311278 -0.38893355  0.15774156 -0.36750391  0.25137968 -0.34993334\n",
      "  -0.2329855   0.0829363   0.24642712]\n",
      " [ 0.26898122 -0.21700057  0.17100555  0.25832202  0.16315133  0.14956752\n",
      "   0.61270021 -0.23723924 -0.29143901]\n",
      " [-0.17890474  0.26143752 -0.09145494 -0.09656769 -0.12634552 -0.16450555\n",
      "  -0.568593   -0.10900134  0.08677754]\n",
      " [ 0.18339664  0.04696148  0.2099919  -0.37579251 -0.09910805 -0.03407206\n",
      "  -0.28427305 -0.03112289 -0.07258074]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.11806563  0.23933163 -0.39472546  0.16041216  0.08776805 -0.29415094\n",
      "   0.06263635]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:11 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.55749802]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 11 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.07975598 -0.14183056  0.03773852  0.38443248 -0.00123993 -0.08020118\n",
      "   0.29819208  0.08375867  0.11007024]\n",
      " [ 0.00072781  0.26442313 -0.21646355 -0.465085   -0.17676162 -0.10046794\n",
      "  -0.6219232  -0.22340075 -0.04113456]\n",
      " [ 0.2216376  -0.38893355  0.15774156 -0.36750391  0.24990451 -0.35140851\n",
      "  -0.23446067  0.0829363   0.24642712]\n",
      " [ 0.28061336 -0.21700057  0.17100555  0.25832202  0.17478347  0.16119967\n",
      "   0.62433235 -0.23723924 -0.29143901]\n",
      " [-0.18984873  0.26143752 -0.09145494 -0.09656769 -0.13728951 -0.17544954\n",
      "  -0.57953699 -0.10900134  0.08677754]\n",
      " [ 0.18024621  0.04696148  0.2099919  -0.37579251 -0.10225848 -0.03722249\n",
      "  -0.28742348 -0.03112289 -0.07258074]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.17264692  0.27043753 -0.37848739  0.18622474  0.12966066 -0.27987964\n",
      "   0.08674772]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:11 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57180084]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 11 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.0716167  -0.14183056  0.03773852  0.3762932  -0.00123993 -0.08834046\n",
      "   0.29819208  0.08375867  0.10193096]\n",
      " [ 0.01041545  0.26442313 -0.21646355 -0.45539737 -0.17676162 -0.09078031\n",
      "  -0.6219232  -0.22340075 -0.03144693]\n",
      " [ 0.22595917 -0.38893355  0.15774156 -0.36318234  0.24990451 -0.34708694\n",
      "  -0.23446067  0.0829363   0.25074869]\n",
      " [ 0.27375161 -0.21700057  0.17100555  0.25146026  0.17478347  0.15433791\n",
      "   0.62433235 -0.23723924 -0.29830077]\n",
      " [-0.18351014  0.26143752 -0.09145494 -0.09022911 -0.13728951 -0.16911095\n",
      "  -0.57953699 -0.10900134  0.09311613]\n",
      " [ 0.18546726  0.04696148  0.2099919  -0.37057145 -0.10225848 -0.03200144\n",
      "  -0.28742348 -0.03112289 -0.06735968]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.10264573  0.22696247 -0.40319651  0.15559118  0.08760569 -0.30839196\n",
      "   0.0570497 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:11 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.53885043]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 11 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 7.85360888e-02 -1.34911168e-01  3.77385155e-02  3.83212591e-01\n",
      "  -1.23992730e-03 -8.14210718e-02  3.05111472e-01  8.37586744e-02\n",
      "   1.01930959e-01]\n",
      " [-1.31914389e-04  2.53875772e-01 -2.16463548e-01 -4.65944730e-01\n",
      "  -1.76761615e-01 -1.01327670e-01 -6.32470562e-01 -2.23400747e-01\n",
      "  -3.14469289e-02]\n",
      " [ 2.14113331e-01 -4.00779394e-01  1.57741558e-01 -3.75028181e-01\n",
      "   2.49904510e-01 -3.58932782e-01 -2.46306511e-01  8.29363017e-02\n",
      "   2.50748693e-01]\n",
      " [ 2.85496293e-01 -2.05255881e-01  1.71005548e-01  2.63204946e-01\n",
      "   1.74783472e-01  1.66082595e-01  6.36077034e-01 -2.37239245e-01\n",
      "  -2.98300766e-01]\n",
      " [-1.92972723e-01  2.51974937e-01 -9.14549394e-02 -9.96916879e-02\n",
      "  -1.37289513e-01 -1.78573532e-01 -5.88999566e-01 -1.09001340e-01\n",
      "   9.31161309e-02]\n",
      " [ 1.79244535e-01  4.07387465e-02  2.09991896e-01 -3.76794183e-01\n",
      "  -1.02258479e-01 -3.82241650e-02 -2.93646209e-01 -3.11228877e-02\n",
      "  -6.73596831e-02]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.15994141  0.26284078 -0.38655572  0.16981764  0.13045106 -0.29014672\n",
      "   0.07925538]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:11 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.57262677]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 11 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.08602504 -0.12742221  0.03773852  0.39070155 -0.00123993 -0.08142107\n",
      "   0.31260043  0.08375867  0.10193096]\n",
      " [-0.00941771  0.24458998 -0.21646355 -0.47523052 -0.17676162 -0.10132767\n",
      "  -0.64175636 -0.22340075 -0.03144693]\n",
      " [ 0.20510234 -0.40979038  0.15774156 -0.38403917  0.24990451 -0.35893278\n",
      "  -0.2553175   0.0829363   0.25074869]\n",
      " [ 0.29566239 -0.19508978  0.17100555  0.27337105  0.17478347  0.16608259\n",
      "   0.64624313 -0.23723924 -0.29830077]\n",
      " [-0.20044005  0.24450761 -0.09145494 -0.10715901 -0.13728951 -0.17857353\n",
      "  -0.59646689 -0.10900134  0.09311613]\n",
      " [ 0.17364435  0.03513856  0.2099919  -0.38239437 -0.10225848 -0.03822417\n",
      "  -0.2992464  -0.03112289 -0.06735968]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.21223594  0.29698552 -0.3708385   0.18594092  0.16846988 -0.27197013\n",
      "   0.09961114]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:11 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.61757444]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 11 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.08990873 -0.12742221  0.0416222   0.39070155 -0.00123993 -0.07753738\n",
      "   0.31648412  0.08375867  0.10193096]\n",
      " [-0.01814375  0.24458998 -0.22518959 -0.47523052 -0.17676162 -0.11005371\n",
      "  -0.65048239 -0.22340075 -0.03144693]\n",
      " [ 0.20230835 -0.40979038  0.15494757 -0.38403917  0.24990451 -0.36172677\n",
      "  -0.25811149  0.0829363   0.25074869]\n",
      " [ 0.30549988 -0.19508978  0.18084304  0.27337105  0.17478347  0.17592009\n",
      "   0.65608062 -0.23723924 -0.29830077]\n",
      " [-0.2096168   0.24450761 -0.10063169 -0.10715901 -0.13728951 -0.18775028\n",
      "  -0.60564364 -0.10900134  0.09311613]\n",
      " [ 0.17416528  0.03513856  0.21051283 -0.38239437 -0.10225848 -0.03770323\n",
      "  -0.29872547 -0.03112289 -0.06735968]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.25739586  0.32353123 -0.35841561  0.20569736  0.20379751 -0.26040981\n",
      "   0.12271221]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:11 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.60235459]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 12 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.08211155 -0.13521939  0.0416222   0.38290436 -0.00123993 -0.07753738\n",
      "   0.31648412  0.08375867  0.09413378]\n",
      " [-0.01318778  0.24954595 -0.22518959 -0.47027456 -0.17676162 -0.11005371\n",
      "  -0.65048239 -0.22340075 -0.02649096]\n",
      " [ 0.20827908 -0.40381965  0.15494757 -0.37806844  0.24990451 -0.36172677\n",
      "  -0.25811149  0.0829363   0.25671942]\n",
      " [ 0.30396108 -0.19662859  0.18084304  0.27183224  0.17478347  0.17592009\n",
      "   0.65608062 -0.23723924 -0.29983957]\n",
      " [-0.20999275  0.24413167 -0.10063169 -0.10753496 -0.13728951 -0.18775028\n",
      "  -0.60564364 -0.10900134  0.09274018]\n",
      " [ 0.17843966  0.03941294  0.21051283 -0.37811999 -0.10225848 -0.03770323\n",
      "  -0.29872547 -0.03112289 -0.0630853 ]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.18525681  0.27939254 -0.38946405  0.1757148   0.16618731 -0.29685531\n",
      "   0.09095838]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:12 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.60417045]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 12 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.08707611 -0.13521939  0.04658676  0.38290436 -0.00123993 -0.07753738\n",
      "   0.32144867  0.08375867  0.09413378]\n",
      " [-0.02187382  0.24954595 -0.23387562 -0.47027456 -0.17676162 -0.11005371\n",
      "  -0.65916843 -0.22340075 -0.02649096]\n",
      " [ 0.20951945 -0.40381965  0.15618794 -0.37806844  0.24990451 -0.36172677\n",
      "  -0.25687112  0.0829363   0.25671942]\n",
      " [ 0.31387088 -0.19662859  0.19075284  0.27183224  0.17478347  0.17592009\n",
      "   0.66599043 -0.23723924 -0.29983957]\n",
      " [-0.21884343  0.24413167 -0.10948237 -0.10753496 -0.13728951 -0.18775028\n",
      "  -0.61449432 -0.10900134  0.09274018]\n",
      " [ 0.17950513  0.03941294  0.21157829 -0.37811999 -0.10225848 -0.03770323\n",
      "  -0.29766    -0.03112289 -0.0630853 ]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.23258783  0.30818452 -0.37567438  0.20062297  0.20205675 -0.28333194\n",
      "   0.1156908 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:12 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.61856198]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 12 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.08138705 -0.14090846  0.0408977   0.38290436 -0.00123993 -0.07753738\n",
      "   0.31575961  0.08375867  0.09413378]\n",
      " [-0.01098555  0.26043421 -0.22298736 -0.47027456 -0.17676162 -0.11005371\n",
      "  -0.64828017 -0.22340075 -0.02649096]\n",
      " [ 0.2147855  -0.39855361  0.16145398 -0.37806844  0.24990451 -0.36172677\n",
      "  -0.25160507  0.0829363   0.25671942]\n",
      " [ 0.2997297  -0.21076977  0.17661166  0.27183224  0.17478347  0.17592009\n",
      "   0.65184925 -0.23723924 -0.29983957]\n",
      " [-0.20753437  0.25544072 -0.09817331 -0.10753496 -0.13728951 -0.18775028\n",
      "  -0.60318527 -0.10900134  0.09274018]\n",
      " [ 0.17709243  0.03700024  0.20916559 -0.37811999 -0.10225848 -0.03770323\n",
      "  -0.3000727  -0.03112289 -0.0630853 ]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.15961513  0.26591158 -0.40045109  0.16947936  0.1490849  -0.3075664\n",
      "   0.07678465]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:12 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.56489868]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 12 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.08411238 -0.14090846  0.04362303  0.38290436 -0.00123993 -0.07753738\n",
      "   0.31575961  0.086484    0.09413378]\n",
      " [-0.01679073  0.26043421 -0.22879254 -0.47027456 -0.17676162 -0.11005371\n",
      "  -0.64828017 -0.22920592 -0.02649096]\n",
      " [ 0.22061118 -0.39855361  0.16727967 -0.37806844  0.24990451 -0.36172677\n",
      "  -0.25160507  0.08876198  0.25671942]\n",
      " [ 0.30288073 -0.21076977  0.17976269  0.27183224  0.17478347  0.17592009\n",
      "   0.65184925 -0.23408822 -0.29983957]\n",
      " [-0.21284645  0.25544072 -0.10348539 -0.10753496 -0.13728951 -0.18775028\n",
      "  -0.60318527 -0.11431342  0.09274018]\n",
      " [ 0.18169322  0.03700024  0.21376639 -0.37811999 -0.10225848 -0.03770323\n",
      "  -0.3000727  -0.02652209 -0.0630853 ]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.21308651  0.29539192 -0.3797251   0.20224762  0.17900173 -0.28629637\n",
      "   0.10821845]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:12 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.59557952]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 12 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.09043425 -0.14090846  0.04362303  0.38922624 -0.00123993 -0.07753738\n",
      "   0.31575961  0.086484    0.10045565]\n",
      " [-0.02264911  0.26043421 -0.22879254 -0.47613294 -0.17676162 -0.11005371\n",
      "  -0.64828017 -0.22920592 -0.03234935]\n",
      " [ 0.22181685 -0.39855361  0.16727967 -0.37686276  0.24990451 -0.36172677\n",
      "  -0.25160507  0.08876198  0.2579251 ]\n",
      " [ 0.30616524 -0.21076977  0.17976269  0.27511675  0.17478347  0.17592009\n",
      "   0.65184925 -0.23408822 -0.29655506]\n",
      " [-0.21558268  0.25544072 -0.10348539 -0.11027119 -0.13728951 -0.18775028\n",
      "  -0.60318527 -0.11431342  0.09000395]\n",
      " [ 0.17858593  0.03700024  0.21376639 -0.38122728 -0.10225848 -0.03770323\n",
      "  -0.3000727  -0.02652209 -0.06619259]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26179179  0.32640349 -0.36149178  0.22780792  0.2066804  -0.26470365\n",
      "   0.12942881]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:12 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.62273176]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 12 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.08839039 -0.14090846  0.04362303  0.38922624 -0.00328379 -0.07958125\n",
      "   0.31575961  0.086484    0.09841179]\n",
      " [-0.01657717  0.26043421 -0.22879254 -0.47613294 -0.17068967 -0.10398176\n",
      "  -0.64828017 -0.22920592 -0.0262774 ]\n",
      " [ 0.21531106 -0.39855361  0.16727967 -0.37686276  0.24339871 -0.36823257\n",
      "  -0.25160507  0.08876198  0.2514193 ]\n",
      " [ 0.29978521 -0.21076977  0.17976269  0.27511675  0.16840344  0.16954005\n",
      "   0.65184925 -0.23408822 -0.30293509]\n",
      " [-0.20774642  0.25544072 -0.10348539 -0.11027119 -0.12945326 -0.17991402\n",
      "  -0.60318527 -0.11431342  0.09784021]\n",
      " [ 0.17909     0.03700024  0.21376639 -0.38122728 -0.10175441 -0.03719916\n",
      "  -0.3000727  -0.02652209 -0.06568852]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.18864045  0.28777967 -0.39187657  0.18457868  0.16358575 -0.29317516\n",
      "   0.09335727]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:12 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.55685248]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 12 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.08902521 -0.14027363  0.04362303  0.38922624 -0.00264897 -0.07894642\n",
      "   0.31575961  0.086484    0.09904661]\n",
      " [-0.01559732  0.26141406 -0.22879254 -0.47613294 -0.16970982 -0.10300192\n",
      "  -0.64828017 -0.22920592 -0.02529756]\n",
      " [ 0.21628346 -0.39758121  0.16727967 -0.37686276  0.24437111 -0.36726017\n",
      "  -0.25160507  0.08876198  0.2523917 ]\n",
      " [ 0.29766306 -0.21289191  0.17976269  0.27511675  0.16628129  0.16741791\n",
      "   0.65184925 -0.23408822 -0.30505724]\n",
      " [-0.20495112  0.25823602 -0.10348539 -0.11027119 -0.12665796 -0.17711872\n",
      "  -0.60318527 -0.11431342  0.10063551]\n",
      " [ 0.17889337  0.03680361  0.21376639 -0.38122728 -0.10195104 -0.0373958\n",
      "  -0.3000727  -0.02652209 -0.06588516]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.11993382  0.25406132 -0.42524951  0.15119828  0.12710484 -0.32472065\n",
      "   0.05880732]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:12 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.54835044]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 12 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.08594592 -0.14335293  0.04362303  0.38922624 -0.00572827 -0.08202572\n",
      "   0.31268032  0.086484    0.09904661]\n",
      " [-0.00534877  0.27166261 -0.22879254 -0.47613294 -0.15946127 -0.09275337\n",
      "  -0.63803162 -0.22920592 -0.02529756]\n",
      " [ 0.22502573 -0.38883894  0.16727967 -0.37686276  0.25311338 -0.3585179\n",
      "  -0.2428628   0.08876198  0.2523917 ]\n",
      " [ 0.28384393 -0.22671105  0.17976269  0.27511675  0.15246216  0.15359877\n",
      "   0.63803011 -0.23408822 -0.30505724]\n",
      " [-0.19280915  0.27037799 -0.10348539 -0.11027119 -0.11451599 -0.16497676\n",
      "  -0.5910433  -0.11431342  0.10063551]\n",
      " [ 0.18264409  0.04055433  0.21376639 -0.38122728 -0.09820032 -0.03364508\n",
      "  -0.29632198 -0.02652209 -0.06588516]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.05203097  0.2170134  -0.44815579  0.12644622  0.07654047 -0.34500066\n",
      "   0.02863798]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:12 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.5277968]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 12 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.09590824 -0.14335293  0.04362303  0.39918857 -0.00572827 -0.08202572\n",
      "   0.32264264  0.086484    0.09904661]\n",
      " [-0.01757102  0.27166261 -0.22879254 -0.48835519 -0.15946127 -0.09275337\n",
      "  -0.65025387 -0.22920592 -0.02529756]\n",
      " [ 0.2194398  -0.38883894  0.16727967 -0.3824487   0.25311338 -0.3585179\n",
      "  -0.24844873  0.08876198  0.2523917 ]\n",
      " [ 0.29639408 -0.22671105  0.17976269  0.2876669   0.15246216  0.15359877\n",
      "   0.65058026 -0.23408822 -0.30505724]\n",
      " [-0.20364788  0.27037799 -0.10348539 -0.12110992 -0.11451599 -0.16497676\n",
      "  -0.60188203 -0.11431342  0.10063551]\n",
      " [ 0.17579188  0.04055433  0.21376639 -0.38807949 -0.09820032 -0.03364508\n",
      "  -0.30317419 -0.02652209 -0.06588516]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.11087394  0.2574603  -0.43367444  0.1501356   0.12173122 -0.32792092\n",
      "   0.05092409]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:12 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.56001077]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 12 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.10027063 -0.14335293  0.04362303  0.39918857 -0.00136587 -0.07766333\n",
      "   0.32700503  0.086484    0.09904661]\n",
      " [-0.02773262  0.27166261 -0.22879254 -0.48835519 -0.16962287 -0.10291496\n",
      "  -0.66041546 -0.22920592 -0.02529756]\n",
      " [ 0.21762648 -0.38883894  0.16727967 -0.3824487   0.25130007 -0.36033121\n",
      "  -0.25026205  0.08876198  0.2523917 ]\n",
      " [ 0.308132   -0.22671105  0.17976269  0.2876669   0.16420007  0.16533669\n",
      "   0.66231818 -0.23408822 -0.30505724]\n",
      " [-0.21475052  0.27037799 -0.10348539 -0.12110992 -0.12561863 -0.1760794\n",
      "  -0.61298467 -0.11431342  0.10063551]\n",
      " [ 0.17233729  0.04055433  0.21376639 -0.38807949 -0.10165491 -0.03709966\n",
      "  -0.30662878 -0.02652209 -0.06588516]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.16508033  0.28900588 -0.41822834  0.17542002  0.1638943  -0.31423073\n",
      "   0.07453388]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:12 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.5706896]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 12 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.09175805 -0.14335293  0.04362303  0.39067599 -0.00136587 -0.08617591\n",
      "   0.32700503  0.086484    0.09053403]\n",
      " [-0.01756411  0.27166261 -0.22879254 -0.47818669 -0.16962287 -0.09274646\n",
      "  -0.66041546 -0.22920592 -0.01512906]\n",
      " [ 0.22230612 -0.38883894  0.16727967 -0.37776906  0.25130007 -0.35565158\n",
      "  -0.25026205  0.08876198  0.25707133]\n",
      " [ 0.30056141 -0.22671105  0.17976269  0.28009631  0.16420007  0.1577661\n",
      "   0.66231818 -0.23408822 -0.31262783]\n",
      " [-0.20785757  0.27037799 -0.10348539 -0.11421697 -0.12561863 -0.16918645\n",
      "  -0.61298467 -0.11431342  0.10752846]\n",
      " [ 0.17776876  0.04055433  0.21376639 -0.38264803 -0.10165491 -0.0316682\n",
      "  -0.30662878 -0.02652209 -0.06045369]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.09517     0.24514801 -0.44229672  0.14520273  0.12110334 -0.34209695\n",
      "   0.04510261]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:12 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.54294543]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 12 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.09932552 -0.13578546  0.04362303  0.39824346 -0.00136587 -0.07860844\n",
      "   0.3345725   0.086484    0.09053403]\n",
      " [-0.02857442  0.2606523  -0.22879254 -0.48919699 -0.16962287 -0.10375676\n",
      "  -0.67142577 -0.22920592 -0.01512906]\n",
      " [ 0.21039308 -0.40075197  0.16727967 -0.3896821   0.25130007 -0.36756461\n",
      "  -0.26217508  0.08876198  0.25707133]\n",
      " [ 0.31257046 -0.214702    0.17976269  0.29210537  0.16420007  0.16977515\n",
      "   0.67432723 -0.23408822 -0.31262783]\n",
      " [-0.21784136  0.2603942  -0.10348539 -0.12420076 -0.12561863 -0.17917024\n",
      "  -0.62296846 -0.11431342  0.10752846]\n",
      " [ 0.17107455  0.03386013  0.21376639 -0.38934223 -0.10165491 -0.0383624\n",
      "  -0.31332298 -0.02652209 -0.06045369]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.15188035  0.28150199 -0.42678997  0.15883872  0.16442284 -0.32492347\n",
      "   0.06647814]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:12 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.57605453]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 12 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.10732768 -0.1277833   0.04362303  0.40624561 -0.00136587 -0.07860844\n",
      "   0.34257466  0.086484    0.09053403]\n",
      " [-0.03833254  0.25089418 -0.22879254 -0.49895512 -0.16962287 -0.10375676\n",
      "  -0.68118389 -0.22920592 -0.01512906]\n",
      " [ 0.2012187  -0.40992635  0.16727967 -0.39885648  0.25130007 -0.36756461\n",
      "  -0.27134946  0.08876198  0.25707133]\n",
      " [ 0.32307733 -0.20419512  0.17976269  0.30261224  0.16420007  0.16977515\n",
      "   0.68483411 -0.23408822 -0.31262783]\n",
      " [-0.22591596  0.2523196  -0.10348539 -0.13227535 -0.12561863 -0.17917024\n",
      "  -0.63104306 -0.11431342  0.10752846]\n",
      " [ 0.16501608  0.02780165  0.21376639 -0.39540071 -0.10165491 -0.0383624\n",
      "  -0.31938145 -0.02652209 -0.06045369]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.20364742  0.31605027 -0.4121285   0.17442414  0.20291226 -0.30779947\n",
      "   0.08604993]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:12 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.62082932]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 12 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.11176329 -0.1277833   0.04805864  0.40624561 -0.00136587 -0.07417283\n",
      "   0.34701027  0.086484    0.09053403]\n",
      " [-0.04733965  0.25089418 -0.23779965 -0.49895512 -0.16962287 -0.11276388\n",
      "  -0.690191   -0.22920592 -0.01512906]\n",
      " [ 0.19825613 -0.40992635  0.1643171  -0.39885648  0.25130007 -0.37052718\n",
      "  -0.27431203  0.08876198  0.25707133]\n",
      " [ 0.33293817 -0.20419512  0.18962352  0.30261224  0.16420007  0.17963598\n",
      "   0.69469494 -0.23408822 -0.31262783]\n",
      " [-0.23525563  0.2523196  -0.11282506 -0.13227535 -0.12561863 -0.18850991\n",
      "  -0.64038272 -0.11431342  0.10752846]\n",
      " [ 0.16525078  0.02780165  0.21400109 -0.39540071 -0.10165491 -0.0381277\n",
      "  -0.31914675 -0.02652209 -0.06045369]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.24827586  0.34292847 -0.40057738  0.19373956  0.23840726 -0.29698193\n",
      "   0.10859888]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:12 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.59778032]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 13 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.10360634 -0.13594026  0.04805864  0.39808866 -0.00136587 -0.07417283\n",
      "   0.34701027  0.086484    0.08237708]\n",
      " [-0.04189298  0.25634086 -0.23779965 -0.49350844 -0.16962287 -0.11276388\n",
      "  -0.690191   -0.22920592 -0.00968238]\n",
      " [ 0.20441211 -0.40377038  0.1643171  -0.3927005   0.25130007 -0.37052718\n",
      "  -0.27431203  0.08876198  0.26322731]\n",
      " [ 0.33081258 -0.20632071  0.18962352  0.30048666  0.16420007  0.17963598\n",
      "   0.69469494 -0.23408822 -0.31475341]\n",
      " [-0.2351176   0.25245763 -0.11282506 -0.13213732 -0.12561863 -0.18850991\n",
      "  -0.64038272 -0.11431342  0.10766649]\n",
      " [ 0.16989174  0.03244261  0.21400109 -0.39075974 -0.10165491 -0.0381277\n",
      "  -0.31914675 -0.02652209 -0.05581273]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.17641101  0.29852122 -0.43097517  0.16409209  0.20034425 -0.33277633\n",
      "   0.07736102]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:13 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.60676897]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 13 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.10910583 -0.13594026  0.05355813  0.39808866 -0.00136587 -0.07417283\n",
      "   0.35250976  0.086484    0.08237708]\n",
      " [-0.05096251  0.25634086 -0.24686918 -0.49350844 -0.16962287 -0.11276388\n",
      "  -0.69926053 -0.22920592 -0.00968238]\n",
      " [ 0.20551698 -0.40377038  0.16542197 -0.3927005   0.25130007 -0.37052718\n",
      "  -0.27320716  0.08876198  0.26322731]\n",
      " [ 0.34087095 -0.20632071  0.19968189  0.30048666  0.16420007  0.17963598\n",
      "   0.70475331 -0.23408822 -0.31475341]\n",
      " [-0.24428255  0.25245763 -0.12199001 -0.13213732 -0.12561863 -0.18850991\n",
      "  -0.64954768 -0.11431342  0.10766649]\n",
      " [ 0.1706503   0.03244261  0.21475965 -0.39075974 -0.10165491 -0.0381277\n",
      "  -0.3183882  -0.02652209 -0.05581273]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.22332355  0.32770776 -0.41807874  0.18865487  0.23652346 -0.32005163\n",
      "   0.10157637]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:13 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.6199141]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 13 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.10242486 -0.14262122  0.04687716  0.39808866 -0.00136587 -0.07417283\n",
      "   0.34582879  0.086484    0.08237708]\n",
      " [-0.03913575  0.26816762 -0.23504242 -0.49350844 -0.16962287 -0.11276388\n",
      "  -0.68743377 -0.22920592 -0.00968238]\n",
      " [ 0.21097584 -0.39831152  0.17088082 -0.3927005   0.25130007 -0.37052718\n",
      "  -0.2677483   0.08876198  0.26322731]\n",
      " [ 0.32622287 -0.22096879  0.1850338   0.30048666  0.16420007  0.17963598\n",
      "   0.69010522 -0.23408822 -0.31475341]\n",
      " [-0.23219338  0.2645468  -0.10990084 -0.13213732 -0.12561863 -0.18850991\n",
      "  -0.63745851 -0.11431342  0.10766649]\n",
      " [ 0.16883876  0.03063107  0.21294811 -0.39075974 -0.10165491 -0.0381277\n",
      "  -0.32019974 -0.02652209 -0.05581273]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.15029128  0.28434936 -0.44165647  0.15768321  0.18257787 -0.3432698\n",
      "   0.06324571]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:13 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.56161648]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 13 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.10556215 -0.14262122  0.05001445  0.39808866 -0.00136587 -0.07417283\n",
      "   0.34582879  0.08962129  0.08237708]\n",
      " [-0.04551443  0.26816762 -0.24142111 -0.49350844 -0.16962287 -0.11276388\n",
      "  -0.68743377 -0.23558461 -0.00968238]\n",
      " [ 0.21698618 -0.39831152  0.17689116 -0.3927005   0.25130007 -0.37052718\n",
      "  -0.2677483   0.09477232  0.26322731]\n",
      " [ 0.32989136 -0.22096879  0.1887023   0.30048666  0.16420007  0.17963598\n",
      "   0.69010522 -0.23041972 -0.31475341]\n",
      " [-0.23804112  0.2645468  -0.11574858 -0.13213732 -0.12561863 -0.18850991\n",
      "  -0.63745851 -0.12016116  0.10766649]\n",
      " [ 0.17348368  0.03063107  0.21759303 -0.39075974 -0.10165491 -0.0381277\n",
      "  -0.32019974 -0.02187717 -0.05581273]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.20425704  0.31449868 -0.4213251   0.19090076  0.21327639 -0.32233981\n",
      "   0.09497184]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:13 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.59288197]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 13 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.11217626 -0.14262122  0.05001445  0.40470277 -0.00136587 -0.07417283\n",
      "   0.34582879  0.08962129  0.08899119]\n",
      " [-0.0517715   0.26816762 -0.24142111 -0.49976551 -0.16962287 -0.11276388\n",
      "  -0.68743377 -0.23558461 -0.01593945]\n",
      " [ 0.21805908 -0.39831152  0.17689116 -0.3916276   0.25130007 -0.37052718\n",
      "  -0.2677483   0.09477232  0.26430021]\n",
      " [ 0.33367333 -0.22096879  0.1887023   0.30426863  0.16420007  0.17963598\n",
      "   0.69010522 -0.23041972 -0.31097144]\n",
      " [-0.24121074  0.2645468  -0.11574858 -0.13530694 -0.12561863 -0.18850991\n",
      "  -0.63745851 -0.12016116  0.10449687]\n",
      " [ 0.17019099  0.03063107  0.21759303 -0.39405243 -0.10165491 -0.0381277\n",
      "  -0.32019974 -0.02187717 -0.05910542]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.25339068  0.34606474 -0.40333409  0.21654185  0.24168829 -0.30097914\n",
      "   0.11620489]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:13 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.61802132]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 13 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.10989417 -0.14262122  0.05001445  0.40470277 -0.00364796 -0.07645491\n",
      "   0.34582879  0.08962129  0.0867091 ]\n",
      " [-0.04557845  0.26816762 -0.24142111 -0.49976551 -0.16342981 -0.10657082\n",
      "  -0.68743377 -0.23558461 -0.00974639]\n",
      " [ 0.21165021 -0.39831152  0.17689116 -0.3916276   0.2448912  -0.37693605\n",
      "  -0.2677483   0.09477232  0.25789134]\n",
      " [ 0.32720832 -0.22096879  0.1887023   0.30426863  0.15773507  0.17317098\n",
      "   0.69010522 -0.23041972 -0.31743644]\n",
      " [-0.23339272  0.2645468  -0.11574858 -0.13530694 -0.11780062 -0.18069189\n",
      "  -0.63745851 -0.12016116  0.11231489]\n",
      " [ 0.17071423  0.03063107  0.21759303 -0.39405243 -0.10113167 -0.03760445\n",
      "  -0.32019974 -0.02187717 -0.05858218]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.18044223  0.30730242 -0.43348796  0.17351697  0.19860332 -0.3293678\n",
      "   0.08025398]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:13 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.54962563]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 13 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.11033832 -0.14217707  0.05001445  0.40470277 -0.00320381 -0.07601076\n",
      "   0.34582879  0.08962129  0.08715325]\n",
      " [-0.04460718  0.26913889 -0.24142111 -0.49976551 -0.16245855 -0.10559955\n",
      "  -0.68743377 -0.23558461 -0.00877512]\n",
      " [ 0.2126835  -0.39727822  0.17689116 -0.3916276   0.24592449 -0.37590275\n",
      "  -0.2677483   0.09477232  0.25892464]\n",
      " [ 0.32517975 -0.22299736  0.1887023   0.30426863  0.1557065   0.17114241\n",
      "   0.69010522 -0.23041972 -0.31946501]\n",
      " [-0.23077208  0.26716745 -0.11574858 -0.13530694 -0.11517997 -0.17807124\n",
      "  -0.63745851 -0.12016116  0.11493554]\n",
      " [ 0.17064575  0.03056258  0.21759303 -0.39405243 -0.10120015 -0.03767294\n",
      "  -0.32019974 -0.02187717 -0.05865066]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.11241581  0.27373342 -0.46652938  0.14053769  0.16255669 -0.36074985\n",
      "   0.04617228]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:13 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.55032859]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 13 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.10639619 -0.14611921  0.05001445  0.40470277 -0.00714594 -0.0799529\n",
      "   0.34188666  0.08962129  0.08715325]\n",
      " [-0.03368827  0.28005779 -0.24142111 -0.49976551 -0.15153964 -0.09468064\n",
      "  -0.67651486 -0.23558461 -0.00877512]\n",
      " [ 0.22180159 -0.38816014  0.17689116 -0.3916276   0.25504258 -0.36678467\n",
      "  -0.25863022  0.09477232  0.25892464]\n",
      " [ 0.31103802 -0.2371391   0.1887023   0.30426863  0.14156476  0.15700067\n",
      "   0.67596348 -0.23041972 -0.31946501]\n",
      " [-0.21822764  0.27971188 -0.11574858 -0.13530694 -0.10263554 -0.16552681\n",
      "  -0.62491408 -0.12016116  0.11493554]\n",
      " [ 0.17496334  0.03488017  0.21759303 -0.39405243 -0.09688256 -0.03335535\n",
      "  -0.31588215 -0.02187717 -0.05865066]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.04432172  0.23570792 -0.48865887  0.11613286  0.11122543 -0.38051214\n",
      "   0.01649083]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:13 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.53308667]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 13 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.11678128 -0.14611921  0.05001445  0.41508786 -0.00714594 -0.0799529\n",
      "   0.35227175  0.08962129  0.08715325]\n",
      " [-0.04612895  0.28005779 -0.24142111 -0.51220619 -0.15153964 -0.09468064\n",
      "  -0.68895554 -0.23558461 -0.00877512]\n",
      " [ 0.21585448 -0.38816014  0.17689116 -0.39757471  0.25504258 -0.36678467\n",
      "  -0.26457733  0.09477232  0.25892464]\n",
      " [ 0.32372911 -0.2371391   0.1887023   0.31695973  0.14156476  0.15700067\n",
      "   0.68865458 -0.23041972 -0.31946501]\n",
      " [-0.22951712  0.27971188 -0.11574858 -0.14659642 -0.10263554 -0.16552681\n",
      "  -0.63620355 -0.12016116  0.11493554]\n",
      " [ 0.16772229  0.03488017  0.21759303 -0.40129348 -0.09688256 -0.03335535\n",
      "  -0.32312319 -0.02187717 -0.05865066]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.10243031  0.27645326 -0.47531094  0.13905642  0.15680361 -0.36463689\n",
      "   0.03795372]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:13 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.56270331]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 13 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.12173588 -0.14611921  0.05001445  0.41508786 -0.00219134 -0.07499829\n",
      "   0.35722635  0.08962129  0.08715325]\n",
      " [-0.05659874  0.28005779 -0.24142111 -0.51220619 -0.16200942 -0.10515043\n",
      "  -0.69942533 -0.23558461 -0.00877512]\n",
      " [ 0.21370996 -0.38816014  0.17689116 -0.39757471  0.25289807 -0.36892918\n",
      "  -0.26672185  0.09477232  0.25892464]\n",
      " [ 0.33552527 -0.2371391   0.1887023   0.31695973  0.15336091  0.16879682\n",
      "   0.70045074 -0.23041972 -0.31946501]\n",
      " [-0.24075312  0.27971188 -0.11574858 -0.14659642 -0.11387154 -0.17676281\n",
      "  -0.64743955 -0.12016116  0.11493554]\n",
      " [ 0.16395759  0.03488017  0.21759303 -0.40129348 -0.10064726 -0.03712005\n",
      "  -0.3268879  -0.02187717 -0.05865066]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.15623274  0.30843043 -0.46064263  0.1638039   0.19918227 -0.35153894\n",
      "   0.06103883]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:13 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.56947038]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 13 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.11284199 -0.14611921  0.05001445  0.40619397 -0.00219134 -0.08389219\n",
      "   0.35722635  0.08962129  0.07825936]\n",
      " [-0.04597079  0.28005779 -0.24142111 -0.50157824 -0.16200942 -0.09452248\n",
      "  -0.69942533 -0.23558461  0.00185283]\n",
      " [ 0.21872954 -0.38816014  0.17689116 -0.39255514  0.25289807 -0.36390961\n",
      "  -0.26672185  0.09477232  0.26394421]\n",
      " [ 0.32729637 -0.2371391   0.1887023   0.30873083  0.15336091  0.16056793\n",
      "   0.70045074 -0.23041972 -0.32769391]\n",
      " [-0.23329639  0.27971188 -0.11574858 -0.13913968 -0.11387154 -0.16930608\n",
      "  -0.64743955 -0.12016116  0.12239227]\n",
      " [ 0.16961277  0.03488017  0.21759303 -0.3956383  -0.10064726 -0.03146487\n",
      "  -0.3268879  -0.02187717 -0.05299548]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.08642311  0.26417821 -0.48407438  0.13399122  0.15569881 -0.37873374\n",
      "   0.03189436]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:13 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.54754636]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 13 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.12101351 -0.13794768  0.05001445  0.41436549 -0.00219134 -0.07572066\n",
      "   0.36539788  0.08962129  0.07825936]\n",
      " [-0.0573312   0.26869738 -0.24142111 -0.51293865 -0.16200942 -0.10588289\n",
      "  -0.71078575 -0.23558461  0.00185283]\n",
      " [ 0.20679185 -0.40009783  0.17689116 -0.40449283  0.25289807 -0.3758473\n",
      "  -0.27865953  0.09477232  0.26394421]\n",
      " [ 0.33945244 -0.22498304  0.1887023   0.3208869   0.15336091  0.17272399\n",
      "   0.7126068  -0.23041972 -0.32769391]\n",
      " [-0.24372909  0.26927918 -0.11574858 -0.14957238 -0.11387154 -0.17973878\n",
      "  -0.65787225 -0.12016116  0.12239227]\n",
      " [ 0.16246672  0.02773412  0.21759303 -0.40278435 -0.10064726 -0.03861092\n",
      "  -0.33403394 -0.02187717 -0.05299548]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.1424684   0.30095317 -0.46966881  0.14703807  0.19935865 -0.36264267\n",
      "   0.05240586]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:13 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.57992541]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 13 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.12947911 -0.12948208  0.05001445  0.42283109 -0.00219134 -0.07572066\n",
      "   0.37386348  0.08962129  0.07825936]\n",
      " [-0.06745739  0.25857119 -0.24142111 -0.52306484 -0.16200942 -0.10588289\n",
      "  -0.72091193 -0.23558461  0.00185283]\n",
      " [ 0.197485   -0.40940467  0.17689116 -0.41379967  0.25289807 -0.3758473\n",
      "  -0.28796638  0.09477232  0.26394421]\n",
      " [ 0.3501926  -0.21424287  0.1887023   0.33162707  0.15336091  0.17272399\n",
      "   0.72334697 -0.23041972 -0.32769391]\n",
      " [-0.25234558  0.26066269 -0.11574858 -0.15818888 -0.11387154 -0.17973878\n",
      "  -0.66648875 -0.12016116  0.12239227]\n",
      " [ 0.1559718   0.0212392   0.21759303 -0.40927926 -0.10064726 -0.03861092\n",
      "  -0.34052886 -0.02187717 -0.05299548]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.19363599  0.33584781 -0.45603169  0.16207685  0.23820195 -0.34658056\n",
      "   0.07116643]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:13 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.62433035]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 13 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.13445071 -0.12948208  0.05498605  0.42283109 -0.00219134 -0.07074906\n",
      "   0.37883508  0.08962129  0.07825936]\n",
      " [-0.07666384  0.25857119 -0.25062756 -0.52306484 -0.16200942 -0.11508935\n",
      "  -0.73011839 -0.23558461  0.00185283]\n",
      " [ 0.19436304 -0.40940467  0.1737692  -0.41379967  0.25289807 -0.37896926\n",
      "  -0.29108834  0.09477232  0.26394421]\n",
      " [ 0.36001263 -0.21424287  0.19852232  0.33162707  0.15336091  0.18254402\n",
      "   0.73316699 -0.23041972 -0.32769391]\n",
      " [-0.26178916  0.26066269 -0.12519216 -0.15818888 -0.11387154 -0.18918236\n",
      "  -0.67593233 -0.12016116  0.12239227]\n",
      " [ 0.1559104   0.0212392   0.21753163 -0.40927926 -0.10064726 -0.03867232\n",
      "  -0.34059026 -0.02187717 -0.05299548]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.23769113  0.36303819 -0.4453212   0.18093869  0.27378414 -0.3364947\n",
      "   0.09313261]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:13 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.59267494]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 14 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.12595554 -0.13797725  0.05498605  0.41433592 -0.00219134 -0.07074906\n",
      "   0.37883508  0.08962129  0.06976419]\n",
      " [-0.07076681  0.26446823 -0.25062756 -0.5171678  -0.16200942 -0.11508935\n",
      "  -0.73011839 -0.23558461  0.00774987]\n",
      " [ 0.20067665 -0.40309107  0.1737692  -0.40748607  0.25289807 -0.37896926\n",
      "  -0.29108834  0.09477232  0.27025782]\n",
      " [ 0.35735017 -0.21690533  0.19852232  0.32896461  0.15336091  0.18254402\n",
      "   0.73316699 -0.23041972 -0.33035637]\n",
      " [-0.26112902  0.26132282 -0.12519216 -0.15752874 -0.11387154 -0.18918236\n",
      "  -0.67593233 -0.12016116  0.12305241]\n",
      " [ 0.16090756  0.02623636  0.21753163 -0.40428211 -0.10064726 -0.03867232\n",
      "  -0.34059026 -0.02187717 -0.04799832]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.1661519   0.31840614 -0.47507997  0.15162373  0.23534211 -0.37160403\n",
      "   0.06242813]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:14 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.6095909]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 14 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.13197305 -0.13797725  0.06100356  0.41433592 -0.00219134 -0.07074906\n",
      "   0.38485259  0.08962129  0.06976419]\n",
      " [-0.08014048  0.26446823 -0.26000123 -0.5171678  -0.16200942 -0.11508935\n",
      "  -0.73949206 -0.23558461  0.00774987]\n",
      " [ 0.2016431  -0.40309107  0.17473565 -0.40748607  0.25289807 -0.37896926\n",
      "  -0.29012189  0.09477232  0.27025782]\n",
      " [ 0.36749177 -0.21690533  0.20866393  0.32896461  0.15336091  0.18254402\n",
      "   0.7433086  -0.23041972 -0.33035637]\n",
      " [-0.27054932  0.26132282 -0.13461246 -0.15752874 -0.11387154 -0.18918236\n",
      "  -0.68535262 -0.12016116  0.12305241]\n",
      " [ 0.16134698  0.02623636  0.21797106 -0.40428211 -0.10064726 -0.03867232\n",
      "  -0.34015084 -0.02187717 -0.04799832]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.2126086   0.34797122 -0.46305072  0.17581964  0.27176344 -0.35967027\n",
      "   0.08609601]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:14 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.62140056]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 14 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.12431177 -0.14563853  0.05334228  0.41433592 -0.00219134 -0.07074906\n",
      "   0.37719131  0.08962129  0.06976419]\n",
      " [-0.06746846  0.27714025 -0.24732921 -0.5171678  -0.16200942 -0.11508935\n",
      "  -0.72682004 -0.23558461  0.00774987]\n",
      " [ 0.20729001 -0.39744415  0.18038257 -0.40748607  0.25289807 -0.37896926\n",
      "  -0.28447497  0.09477232  0.27025782]\n",
      " [ 0.3524105  -0.23198661  0.19358265  0.32896461  0.15336091  0.18254402\n",
      "   0.72822733 -0.23041972 -0.33035637]\n",
      " [-0.25772948  0.27414267 -0.12179261 -0.15752874 -0.11387154 -0.18918236\n",
      "  -0.67253278 -0.12016116  0.12305241]\n",
      " [ 0.16015308  0.02504245  0.21677715 -0.40428211 -0.10064726 -0.03867232\n",
      "  -0.34134474 -0.02187717 -0.04799832]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.13951266  0.30351253 -0.48547589  0.14501354  0.21688745 -0.38187799\n",
      "   0.04835329]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:14 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.55802611]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 14 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.1278893  -0.14563853  0.05691982  0.41433592 -0.00219134 -0.07074906\n",
      "   0.37719131  0.09319882  0.06976419]\n",
      " [-0.07442735  0.27714025 -0.2542881  -0.5171678  -0.16200942 -0.11508935\n",
      "  -0.72682004 -0.2425435   0.00774987]\n",
      " [ 0.2134955  -0.39744415  0.18658806 -0.40748607  0.25289807 -0.37896926\n",
      "  -0.28447497  0.10097781  0.27025782]\n",
      " [ 0.3566051  -0.23198661  0.19777725  0.32896461  0.15336091  0.18254402\n",
      "   0.72822733 -0.22622512 -0.33035637]\n",
      " [-0.26413008  0.27414267 -0.12819322 -0.15752874 -0.11387154 -0.18918236\n",
      "  -0.67253278 -0.12656176  0.12305241]\n",
      " [ 0.16484159  0.02504245  0.22146566 -0.40428211 -0.10064726 -0.03867232\n",
      "  -0.34134474 -0.01718865 -0.04799832]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.19401533  0.33438415 -0.46554013  0.17871391  0.24840335 -0.36129696\n",
      "   0.08039226]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:14 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.58989218]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 14 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.13481044 -0.14563853  0.05691982  0.42125706 -0.00219134 -0.07074906\n",
      "   0.37719131  0.09319882  0.07668533]\n",
      " [-0.08108438  0.27714025 -0.2542881  -0.52382483 -0.16200942 -0.11508935\n",
      "  -0.72682004 -0.2425435   0.00109284]\n",
      " [ 0.21443997 -0.39744415  0.18658806 -0.4065416   0.25289807 -0.37896926\n",
      "  -0.28447497  0.10097781  0.27120228]\n",
      " [ 0.36087423 -0.23198661  0.19777725  0.33323374  0.15336091  0.18254402\n",
      "   0.72822733 -0.22622512 -0.32608724]\n",
      " [-0.26775195  0.27414267 -0.12819322 -0.16115061 -0.11387154 -0.18918236\n",
      "  -0.67253278 -0.12656176  0.11943054]\n",
      " [ 0.16134951  0.02504245  0.22146566 -0.40777419 -0.10064726 -0.03867232\n",
      "  -0.34134474 -0.01718865 -0.05149041]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.24362185  0.36654874 -0.4477786   0.20446255  0.27756609 -0.34016964\n",
      "   0.10165515]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:14 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.61244721]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 14 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.1323048  -0.14563853  0.05691982  0.42125706 -0.00469698 -0.0732547\n",
      "   0.37719131  0.09319882  0.07417969]\n",
      " [-0.07479823  0.27714025 -0.2542881  -0.52382483 -0.15572327 -0.1088032\n",
      "  -0.72682004 -0.2425435   0.00737898]\n",
      " [ 0.20811293 -0.39744415  0.18658806 -0.4065416   0.24657103 -0.3852963\n",
      "  -0.28447497  0.10097781  0.26487525]\n",
      " [ 0.3543646  -0.23198661  0.19777725  0.33323374  0.14685128  0.17603438\n",
      "   0.72822733 -0.22622512 -0.33259687]\n",
      " [-0.259954    0.27414267 -0.12819322 -0.16115061 -0.10607359 -0.18138441\n",
      "  -0.67253278 -0.12656176  0.12722849]\n",
      " [ 0.16188472  0.02504245  0.22146566 -0.40777419 -0.10011205 -0.03813711\n",
      "  -0.34134474 -0.01718865 -0.0509552 ]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.17093795  0.32769313 -0.47769996  0.16165634  0.2345644  -0.36844613\n",
      "   0.06584849]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:14 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.5412377]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 14 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.13259213 -0.1453512   0.05691982  0.42125706 -0.00440965 -0.07296737\n",
      "   0.37719131  0.09319882  0.07446702]\n",
      " [-0.07387826  0.27806021 -0.2542881  -0.52382483 -0.15480331 -0.10788324\n",
      "  -0.72682004 -0.2425435   0.00829895]\n",
      " [ 0.20917323 -0.39638385  0.18658806 -0.4065416   0.24763133 -0.384236\n",
      "  -0.28447497  0.10097781  0.26593555]\n",
      " [ 0.35247794 -0.23387326  0.19777725  0.33323374  0.14496463  0.17414773\n",
      "   0.72822733 -0.22622512 -0.33448353]\n",
      " [-0.25751374  0.27658292 -0.12819322 -0.16115061 -0.10363333 -0.17894416\n",
      "  -0.67253278 -0.12656176  0.12966874]\n",
      " [ 0.16192297  0.02508071  0.22146566 -0.40777419 -0.1000738  -0.03809886\n",
      "  -0.34134474 -0.01718865 -0.05091694]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.10374344  0.29438321 -0.51037678  0.12912009  0.19907649 -0.39959445\n",
      "   0.03228949]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:14 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.55245659]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 14 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.12779048 -0.15015285  0.05691982  0.42125706 -0.00921131 -0.07776902\n",
      "   0.37238965  0.09319882  0.07446702]\n",
      " [-0.06234153  0.28959694 -0.2542881  -0.52382483 -0.14326658 -0.09634651\n",
      "  -0.71528331 -0.2425435   0.00829895]\n",
      " [ 0.21865486 -0.38690222  0.18658806 -0.4065416   0.25711295 -0.37475437\n",
      "  -0.27499335  0.10097781  0.26593555]\n",
      " [ 0.33805365 -0.24829756  0.19777725  0.33323374  0.13054033  0.15972343\n",
      "   0.71380303 -0.22622512 -0.33448353]\n",
      " [-0.24457805  0.28951862 -0.12819322 -0.16115061 -0.09069764 -0.16600846\n",
      "  -0.65959708 -0.12656176  0.12966874]\n",
      " [ 0.16681209  0.02996983  0.22146566 -0.40777419 -0.09518468 -0.03320974\n",
      "  -0.33645562 -0.01718865 -0.05091694]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.03544646  0.25536686 -0.53176558  0.10504887  0.14700611 -0.41883376\n",
      "   0.00310014]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:14 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.53899342]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 14 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.13853723 -0.15015285  0.05691982  0.43200381 -0.00921131 -0.07776902\n",
      "   0.3831364   0.09319882  0.07446702]\n",
      " [-0.07487626  0.28959694 -0.2542881  -0.53635956 -0.14326658 -0.09634651\n",
      "  -0.72781804 -0.2425435   0.00829895]\n",
      " [ 0.21236967 -0.38690222  0.18658806 -0.41282679  0.25711295 -0.37475437\n",
      "  -0.28127854  0.10097781  0.26593555]\n",
      " [ 0.35075586 -0.24829756  0.19777725  0.34593596  0.13054033  0.15972343\n",
      "   0.72650525 -0.22622512 -0.33448353]\n",
      " [-0.25620829  0.28951862 -0.12819322 -0.17278085 -0.09069764 -0.16600846\n",
      "  -0.67122732 -0.12656176  0.12966874]\n",
      " [ 0.15919674  0.02996983  0.22146566 -0.41538955 -0.09518468 -0.03320974\n",
      "  -0.34407098 -0.01718865 -0.05091694]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.09272181  0.29633791 -0.51951318  0.12717449  0.19281535 -0.40415449\n",
      "   0.02369218]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:14 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.56557252]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 14 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.14407149 -0.15015285  0.05691982  0.43200381 -0.00367705 -0.07223476\n",
      "   0.38867066  0.09319882  0.07446702]\n",
      " [-0.08559777  0.28959694 -0.2542881  -0.53635956 -0.15398809 -0.10706801\n",
      "  -0.73853955 -0.2425435   0.00829895]\n",
      " [ 0.20990218 -0.38690222  0.18658806 -0.41282679  0.25464546 -0.37722186\n",
      "  -0.28374603  0.10097781  0.26593555]\n",
      " [ 0.3625649  -0.24829756  0.19777725  0.34593596  0.14234937  0.17153247\n",
      "   0.73831428 -0.22622512 -0.33448353]\n",
      " [-0.26754634  0.28951862 -0.12819322 -0.17278085 -0.10203568 -0.17734651\n",
      "  -0.68256537 -0.12656176  0.12966874]\n",
      " [ 0.15511788  0.02996983  0.22146566 -0.41538955 -0.09926353 -0.03728859\n",
      "  -0.34814983 -0.01718865 -0.05091694]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.14609128  0.32873284 -0.50559949  0.1513774   0.23534975 -0.39164958\n",
      "   0.04623102]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:14 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.56815024]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 14 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.13479226 -0.15015285  0.05691982  0.42272459 -0.00367705 -0.08151399\n",
      "   0.38867066  0.09319882  0.0651878 ]\n",
      " [-0.07453863  0.28959694 -0.2542881  -0.52530042 -0.15398809 -0.09600888\n",
      "  -0.73853955 -0.2425435   0.01935809]\n",
      " [ 0.21524431 -0.38690222  0.18658806 -0.40748465  0.25464546 -0.37187973\n",
      "  -0.28374603  0.10097781  0.27127768]\n",
      " [ 0.35373243 -0.24829756  0.19777725  0.33710348  0.14234937  0.1627\n",
      "   0.73831428 -0.22622512 -0.343316  ]\n",
      " [-0.25952976  0.28951862 -0.12819322 -0.16476427 -0.10203568 -0.16932993\n",
      "  -0.68256537 -0.12656176  0.13768532]\n",
      " [ 0.16100937  0.02996983  0.22146566 -0.40949806 -0.09926353 -0.03139711\n",
      "  -0.34814983 -0.01718865 -0.04502546]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.07639187  0.28407798 -0.52840744  0.12195817  0.19122288 -0.4181607\n",
      "   0.01739276]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:14 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.55262674]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 14 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.14351005 -0.14143507  0.05691982  0.43144237 -0.00367705 -0.0727962\n",
      "   0.39738845  0.09319882  0.0651878 ]\n",
      " [-0.0861336   0.27800198 -0.2542881  -0.53689539 -0.15398809 -0.10760384\n",
      "  -0.75013451 -0.2425435   0.01935809]\n",
      " [ 0.203324   -0.39882254  0.18658806 -0.41940497  0.25464546 -0.38380004\n",
      "  -0.29566634  0.10097781  0.27127768]\n",
      " [ 0.36592586 -0.23610412  0.19777725  0.34929692  0.14234937  0.17489343\n",
      "   0.75050772 -0.22622512 -0.343316  ]\n",
      " [-0.27032379  0.27872458 -0.12819322 -0.17555831 -0.10203568 -0.18012396\n",
      "  -0.69335941 -0.12656176  0.13768532]\n",
      " [ 0.15343823  0.02239869  0.22146566 -0.4170692  -0.09926353 -0.03896825\n",
      "  -0.35572097 -0.01718865 -0.04502546]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.13169401  0.3212078  -0.51505518  0.13442039  0.23508155 -0.40314229\n",
      "   0.03701321]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:14 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.58421616]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 14 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.15237901 -0.13256611  0.05691982  0.44031133 -0.00367705 -0.0727962\n",
      "   0.40625741  0.09319882  0.0651878 ]\n",
      " [-0.09652099  0.26761458 -0.2542881  -0.54728278 -0.15398809 -0.10760384\n",
      "  -0.7605219  -0.2425435   0.01935809]\n",
      " [ 0.19391685 -0.40822968  0.18658806 -0.42881211  0.25464546 -0.38380004\n",
      "  -0.30507349  0.10097781  0.27127768]\n",
      " [ 0.37679605 -0.22523394  0.19777725  0.3601671   0.14234937  0.17489343\n",
      "   0.7613779  -0.22622512 -0.343316  ]\n",
      " [-0.27940085  0.26964752 -0.12819322 -0.18463537 -0.10203568 -0.18012396\n",
      "  -0.70243647 -0.12656176  0.13768532]\n",
      " [ 0.14653536  0.01549581  0.22146566 -0.42397207 -0.09926353 -0.03896825\n",
      "  -0.36262384 -0.01718865 -0.04502546]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.18219255  0.35638237 -0.50239785  0.14890758  0.27415495 -0.38813254\n",
      "   0.05494226]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:14 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.62805175]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 14 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.15786025 -0.13256611  0.06240106  0.44031133 -0.00367705 -0.06731496\n",
      "   0.41173865  0.09319882  0.0651878 ]\n",
      " [-0.10584557  0.26761458 -0.26361269 -0.54728278 -0.15398809 -0.11692842\n",
      "  -0.76984649 -0.2425435   0.01935809]\n",
      " [ 0.19064603 -0.40822968  0.18331724 -0.42881211  0.25464546 -0.38707086\n",
      "  -0.30834431  0.10097781  0.27127768]\n",
      " [ 0.38651824 -0.22523394  0.20749944  0.3601671   0.14234937  0.18461562\n",
      "   0.77110009 -0.22622512 -0.343316  ]\n",
      " [-0.28888699  0.26964752 -0.13767935 -0.18463537 -0.10203568 -0.1896101\n",
      "  -0.71192261 -0.12656176  0.13768532]\n",
      " [ 0.14617063  0.01549581  0.22110093 -0.42397207 -0.09926353 -0.03933298\n",
      "  -0.36298857 -0.01718865 -0.04502546]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.22563662  0.38385875 -0.49248722  0.1673067   0.30974374 -0.37875615\n",
      "   0.0762995 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:14 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.58702395]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 15 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.14905312 -0.14137324  0.06240106  0.4315042  -0.00367705 -0.06731496\n",
      "   0.41173865  0.09319882  0.05638066]\n",
      " [-0.09954563  0.27391452 -0.26361269 -0.54098284 -0.15398809 -0.11692842\n",
      "  -0.76984649 -0.2425435   0.02565803]\n",
      " [ 0.19708958 -0.40178613  0.18331724 -0.42236856  0.25464546 -0.38707086\n",
      "  -0.30834431  0.10097781  0.27772124]\n",
      " [ 0.38337443 -0.22837774  0.20749944  0.35702329  0.14234937  0.18461562\n",
      "   0.77110009 -0.22622512 -0.34645981]\n",
      " [-0.28771085  0.27082366 -0.13767935 -0.18345923 -0.10203568 -0.1896101\n",
      "  -0.71192261 -0.12656176  0.13886147]\n",
      " [ 0.15151059  0.02083578  0.22110093 -0.41863211 -0.09926353 -0.03933298\n",
      "  -0.36298857 -0.01718865 -0.0396855 ]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.15448144  0.33905461 -0.52162315  0.13832485  0.27100569 -0.41315674\n",
      "   0.04614633]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:15 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.61262876]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 15 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.15556165 -0.14137324  0.06890959  0.4315042  -0.00367705 -0.06731496\n",
      "   0.41824718  0.09319882  0.05638066]\n",
      " [-0.10914165  0.27391452 -0.27320871 -0.54098284 -0.15398809 -0.11692842\n",
      "  -0.77944251 -0.2425435   0.02565803]\n",
      " [ 0.19791659 -0.40178613  0.18414424 -0.42236856  0.25464546 -0.38707086\n",
      "  -0.30751731  0.10097781  0.27772124]\n",
      " [ 0.39353708 -0.22837774  0.21766209  0.35702329  0.14234937  0.18461562\n",
      "   0.78126274 -0.22622512 -0.34645981]\n",
      " [-0.29732207  0.27082366 -0.14729057 -0.18345923 -0.10203568 -0.1896101\n",
      "  -0.72153382 -0.12656176  0.13886147]\n",
      " [ 0.15162117  0.02083578  0.22121151 -0.41863211 -0.09926353 -0.03933298\n",
      "  -0.362878   -0.01718865 -0.0396855 ]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.20044589  0.36897491 -0.51042588  0.1621348   0.30759709 -0.40199592\n",
      "   0.06923913]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:15 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.6230047]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 15 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.14694463 -0.14999026  0.06029257  0.4315042  -0.00367705 -0.06731496\n",
      "   0.40963017  0.09319882  0.05638066]\n",
      " [-0.09572444  0.28733174 -0.25979149 -0.54098284 -0.15398809 -0.11692842\n",
      "  -0.76602529 -0.2425435   0.02565803]\n",
      " [ 0.20374464 -0.39595808  0.18997229 -0.42236856  0.25464546 -0.38707086\n",
      "  -0.30168926  0.10097781  0.27772124]\n",
      " [ 0.37809478 -0.24382004  0.20221979  0.35702329  0.14234937  0.18461562\n",
      "   0.76582044 -0.22622512 -0.34645981]\n",
      " [-0.28383443  0.28431129 -0.13380294 -0.18345923 -0.10203568 -0.1896101\n",
      "  -0.70804619 -0.12656176  0.13886147]\n",
      " [ 0.15105812  0.02027273  0.22064847 -0.41863211 -0.09926353 -0.03933298\n",
      "  -0.36344104 -0.01718865 -0.0396855 ]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.12728339  0.3234111  -0.53175625  0.13148617  0.25184184 -0.42321397\n",
      "   0.03209475]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:15 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.55417577]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 15 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.15098921 -0.14999026  0.06433715  0.4315042  -0.00367705 -0.06731496\n",
      "   0.40963017  0.0972434   0.05638066]\n",
      " [-0.10326425  0.28733174 -0.2673313  -0.54098284 -0.15398809 -0.11692842\n",
      "  -0.76602529 -0.25008331  0.02565803]\n",
      " [ 0.21015553 -0.39595808  0.19638318 -0.42236856  0.25464546 -0.38707086\n",
      "  -0.30168926  0.1073887   0.27772124]\n",
      " [ 0.38282037 -0.24382004  0.20694538  0.35702329  0.14234937  0.18461562\n",
      "   0.76582044 -0.22149954 -0.34645981]\n",
      " [-0.29079873  0.28431129 -0.14076724 -0.18345923 -0.10203568 -0.1896101\n",
      "  -0.70804619 -0.13352606  0.13886147]\n",
      " [ 0.15578907  0.02027273  0.22537941 -0.41863211 -0.09926353 -0.03933298\n",
      "  -0.36344104 -0.01245771 -0.0396855 ]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.18235716  0.35505369 -0.51221675  0.16569865  0.28420368 -0.40299026\n",
      "   0.06446231]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:15 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.58662846]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 15 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.15823004 -0.14999026  0.06433715  0.43874502 -0.00367705 -0.06731496\n",
      "   0.40963017  0.0972434   0.06362148]\n",
      " [-0.11031859  0.28733174 -0.2673313  -0.54803718 -0.15398809 -0.11692842\n",
      "  -0.76602529 -0.25008331  0.01860369]\n",
      " [ 0.21097547 -0.39595808  0.19638318 -0.42154862  0.25464546 -0.38707086\n",
      "  -0.30168926  0.1073887   0.27854118]\n",
      " [ 0.38756362 -0.24382004  0.20694538  0.36176655  0.14234937  0.18461562\n",
      "   0.76582044 -0.22149954 -0.34171656]\n",
      " [-0.29488528  0.28431129 -0.14076724 -0.18754577 -0.10203568 -0.1896101\n",
      "  -0.70804619 -0.13352606  0.13477492]\n",
      " [ 0.15208379  0.02027273  0.22537941 -0.42233739 -0.09926353 -0.03933298\n",
      "  -0.36344104 -0.01245771 -0.04339078]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.23247754  0.38785742 -0.49466949  0.19157937  0.31413041 -0.38209366\n",
      "   0.08576043]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:15 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.60596452]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 15 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.15551707 -0.14999026  0.06433715  0.43874502 -0.00639001 -0.07002792\n",
      "   0.40963017  0.0972434   0.06090852]\n",
      " [-0.10397106  0.28733174 -0.2673313  -0.54803718 -0.14764055 -0.11058089\n",
      "  -0.76602529 -0.25008331  0.02495122]\n",
      " [ 0.20471874 -0.39595808  0.19638318 -0.42154862  0.24838874 -0.39332759\n",
      "  -0.30168926  0.1073887   0.27228445]\n",
      " [ 0.38104995 -0.24382004  0.20694538  0.36176655  0.1358357   0.17810195\n",
      "   0.76582044 -0.22149954 -0.34823023]\n",
      " [-0.28711795  0.28431129 -0.14076724 -0.18754577 -0.09426836 -0.18184277\n",
      "  -0.70804619 -0.13352606  0.14254225]\n",
      " [ 0.1526245   0.02027273  0.22537941 -0.42233739 -0.09872282 -0.03879227\n",
      "  -0.36344104 -0.01245771 -0.04285007]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.160134    0.34896239 -0.52435368  0.14901705  0.27129303 -0.4102312\n",
      "   0.05012946]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:15 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.53162768]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 15 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.15568225 -0.14982508  0.06433715  0.43874502 -0.00622483 -0.06986275\n",
      "   0.40963017  0.0972434   0.0610737 ]\n",
      " [-0.10314572  0.28815707 -0.2673313  -0.54803718 -0.14681522 -0.10975556\n",
      "  -0.76602529 -0.25008331  0.02577656]\n",
      " [ 0.20577491 -0.39490192  0.19638318 -0.42154862  0.2494449  -0.39227143\n",
      "  -0.30168926  0.1073887   0.27334062]\n",
      " [ 0.37935116 -0.24551883  0.20694538  0.36176655  0.13413691  0.17640317\n",
      "   0.76582044 -0.22149954 -0.34992901]\n",
      " [-0.28487182  0.28655743 -0.14076724 -0.18754577 -0.09202223 -0.17959664\n",
      "  -0.70804619 -0.13352606  0.14478838]\n",
      " [ 0.15274807  0.0203963   0.22537941 -0.42233739 -0.09859925 -0.0386687\n",
      "  -0.36344104 -0.01245771 -0.0427265 ]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.09394644  0.31603378 -0.55662179  0.11698015  0.23649746 -0.44107188\n",
      "   0.01715924]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:15 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.55472547]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 15 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.15003342 -0.15547391  0.06433715  0.43874502 -0.01187366 -0.07551158\n",
      "   0.40398134  0.0972434   0.0610737 ]\n",
      " [-0.09104863  0.30025416 -0.2673313  -0.54803718 -0.13471813 -0.09765847\n",
      "  -0.7539282  -0.25008331  0.02577656]\n",
      " [ 0.21560723 -0.38506959  0.19638318 -0.42154862  0.25927723 -0.3824391\n",
      "  -0.29185693  0.1073887   0.27334062]\n",
      " [ 0.36468266 -0.26018733  0.20694538  0.36176655  0.11946841  0.16173467\n",
      "   0.75115195 -0.22149954 -0.34992901]\n",
      " [-0.27156508  0.29986416 -0.14076724 -0.18754577 -0.07871549 -0.16628991\n",
      "  -0.69473945 -0.13352606  0.14478838]\n",
      " [ 0.15821059  0.02585882  0.22537941 -0.42233739 -0.09313673 -0.03320618\n",
      "  -0.35797852 -0.01245771 -0.0427265 ]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.02543642  0.27602087 -0.57731481  0.09322876  0.18372193 -0.45979518\n",
      "  -0.01153503]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:15 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.54550845]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 15 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.16106724 -0.15547391  0.06433715  0.44977884 -0.01187366 -0.07551158\n",
      "   0.41501516  0.0972434   0.0610737 ]\n",
      " [-0.10355538  0.30025416 -0.2673313  -0.56054393 -0.13471813 -0.09765847\n",
      "  -0.76643495 -0.25008331  0.02577656]\n",
      " [ 0.20901269 -0.38506959  0.19638318 -0.42814316  0.25927723 -0.3824391\n",
      "  -0.29845148  0.1073887   0.27334062]\n",
      " [ 0.37727595 -0.26018733  0.20694538  0.37435984  0.11946841  0.16173467\n",
      "   0.76374523 -0.22149954 -0.34992901]\n",
      " [-0.28341548  0.29986416 -0.14076724 -0.19939617 -0.07871549 -0.16628991\n",
      "  -0.70658985 -0.13352606  0.14478838]\n",
      " [ 0.15024408  0.02585882  0.22537941 -0.4303039  -0.09313673 -0.03320618\n",
      "  -0.36594503 -0.01245771 -0.0427265 ]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.08177723  0.31712897 -0.5661071   0.11452886  0.22959518 -0.44628541\n",
      "   0.00814497]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:15 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.568619]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 15 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.16716004 -0.15547391  0.06433715  0.44977884 -0.00578086 -0.06941877\n",
      "   0.42110796  0.0972434   0.0610737 ]\n",
      " [-0.1144704   0.30025416 -0.2673313  -0.56054393 -0.14563315 -0.10857348\n",
      "  -0.77734997 -0.25008331  0.02577656]\n",
      " [ 0.20623221 -0.38506959  0.19638318 -0.42814316  0.25649675 -0.38521958\n",
      "  -0.30123196  0.1073887   0.27334062]\n",
      " [ 0.38905607 -0.26018733  0.20694538  0.37435984  0.13124853  0.17351479\n",
      "   0.77552536 -0.22149954 -0.34992901]\n",
      " [-0.29482025  0.29986416 -0.14076724 -0.19939617 -0.09012026 -0.17769468\n",
      "  -0.71799462 -0.13352606  0.14478838]\n",
      " [ 0.14584973  0.02585882  0.22537941 -0.4303039  -0.09753108 -0.03760052\n",
      "  -0.37033938 -0.01245771 -0.0427265 ]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.13468426  0.34992072 -0.55291755  0.13818091  0.27222168 -0.43436516\n",
      "   0.03011795]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:15 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.56674407]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 15 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.15749625 -0.15547391  0.06433715  0.44011505 -0.00578086 -0.07908256\n",
      "   0.42110796  0.0972434   0.05140991]\n",
      " [-0.10301328  0.30025416 -0.2673313  -0.54908681 -0.14563315 -0.09711637\n",
      "  -0.77734997 -0.25008331  0.03723367]\n",
      " [ 0.21187976 -0.38506959  0.19638318 -0.42249561  0.25649675 -0.37957202\n",
      "  -0.30123196  0.1073887   0.27898817]\n",
      " [ 0.37967653 -0.26018733  0.20694538  0.36498029  0.13124853  0.16413525\n",
      "   0.77552536 -0.22149954 -0.35930856]\n",
      " [-0.28625944  0.29986416 -0.14076724 -0.19083535 -0.09012026 -0.16913386\n",
      "  -0.71799462 -0.13352606  0.1533492 ]\n",
      " [ 0.15198867  0.02585882  0.22537941 -0.42416497 -0.09753108 -0.03146159\n",
      "  -0.37033938 -0.01245771 -0.03658757]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.06510361  0.3048589  -0.57512224  0.1091436   0.2275038  -0.46019268\n",
      "   0.00160362]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:15 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.55814942]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 15 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.16669011 -0.14628005  0.06433715  0.44930891 -0.00578086 -0.0698887\n",
      "   0.43030182  0.0972434   0.05140991]\n",
      " [-0.11473104  0.2885364  -0.2673313  -0.56080457 -0.14563315 -0.10883413\n",
      "  -0.78906773 -0.25008331  0.03723367]\n",
      " [ 0.20001757 -0.39693178  0.19638318 -0.4343578   0.25649675 -0.39143422\n",
      "  -0.31309415  0.1073887   0.27898817]\n",
      " [ 0.39180995 -0.24805391  0.20694538  0.37711371  0.13124853  0.17626867\n",
      "   0.78765878 -0.22149954 -0.35930856]\n",
      " [-0.2973181   0.2888055  -0.14076724 -0.20189402 -0.09012026 -0.18019253\n",
      "  -0.72905329 -0.13352606  0.1533492 ]\n",
      " [ 0.14402643  0.01789658  0.22537941 -0.43212721 -0.09753108 -0.03942383\n",
      "  -0.37830161 -0.01245771 -0.03658757]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.11958791  0.34226654 -0.56276371  0.12102926  0.27141733 -0.44621869\n",
      "   0.02031427]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:15 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.58889458]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 15 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.17589408 -0.13707608  0.06433715  0.45851288 -0.00578086 -0.0698887\n",
      "   0.43950579  0.0972434   0.05140991]\n",
      " [-0.12527603  0.27799142 -0.2673313  -0.57134956 -0.14563315 -0.10883413\n",
      "  -0.79961271 -0.25008331  0.03723367]\n",
      " [ 0.19054292 -0.40640643  0.19638318 -0.44383245  0.25649675 -0.39143422\n",
      "  -0.3225688   0.1073887   0.27898817]\n",
      " [ 0.40271567 -0.23714819  0.20694538  0.38801943  0.13124853  0.17626867\n",
      "   0.7985645  -0.22149954 -0.35930856]\n",
      " [-0.30676385  0.27935975 -0.14076724 -0.21133976 -0.09012026 -0.18019253\n",
      "  -0.73849904 -0.13352606  0.1533492 ]\n",
      " [ 0.13675056  0.01062072  0.22537941 -0.43940307 -0.09753108 -0.03942383\n",
      "  -0.38557748 -0.01245771 -0.03658757]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.16935176  0.37764634 -0.55103138  0.13496409  0.31059476 -0.43223458\n",
      "   0.03739951]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:15 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.63196906]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 15 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.18184833 -0.13707608  0.0702914   0.45851288 -0.00578086 -0.06393446\n",
      "   0.44546004  0.0972434   0.05140991]\n",
      " [-0.13464189  0.27799142 -0.27669717 -0.57134956 -0.14563315 -0.11819999\n",
      "  -0.80897858 -0.25008331  0.03723367]\n",
      " [ 0.18713523 -0.40640643  0.19297548 -0.44383245  0.25649675 -0.39484191\n",
      "  -0.3259765   0.1073887   0.27898817]\n",
      " [ 0.41229151 -0.23714819  0.21652122  0.38801943  0.13124853  0.18584451\n",
      "   0.80814034 -0.22149954 -0.35930856]\n",
      " [-0.31623228  0.27935975 -0.15023567 -0.21133976 -0.09012026 -0.18966096\n",
      "  -0.74796747 -0.13352606  0.1533492 ]\n",
      " [ 0.13607852  0.01062072  0.22470737 -0.43940307 -0.09753108 -0.04009587\n",
      "  -0.38624952 -0.01245771 -0.03658757]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.21215084  0.40537613 -0.5418729   0.15289486  0.34611116 -0.42353649\n",
      "   0.05812656]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:15 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.58082439]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 16 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.17276044 -0.14616397  0.0702914   0.44942499 -0.00578086 -0.06393446\n",
      "   0.44546004  0.0972434   0.04232202]\n",
      " [-0.12799164  0.28464167 -0.27669717 -0.5646993  -0.14563315 -0.11819999\n",
      "  -0.80897858 -0.25008331  0.04388393]\n",
      " [ 0.19368062 -0.39986104  0.19297548 -0.43728706  0.25649675 -0.39484191\n",
      "  -0.3259765   0.1073887   0.28553357]\n",
      " [ 0.40872526 -0.24071444  0.21652122  0.38445319  0.13124853  0.18584451\n",
      "   0.80814034 -0.22149954 -0.3628748 ]\n",
      " [-0.3145592   0.28103283 -0.15023567 -0.20966669 -0.09012026 -0.18966096\n",
      "  -0.74796747 -0.13352606  0.15502227]\n",
      " [ 0.14174425  0.01628645  0.22470737 -0.43373734 -0.09753108 -0.04009587\n",
      "  -0.38624952 -0.01245771 -0.03092183]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.14144494  0.36046203 -0.57040506  0.12424944  0.3071672  -0.45721385\n",
      "   0.02854229]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:16 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.61587683]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 16 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.17972306 -0.14616397  0.07725402  0.44942499 -0.00578086 -0.06393446\n",
      "   0.45242266  0.0972434   0.04232202]\n",
      " [-0.13772953  0.28464167 -0.28643506 -0.5646993  -0.14563315 -0.11819999\n",
      "  -0.81871647 -0.25008331  0.04388393]\n",
      " [ 0.19436925 -0.39986104  0.19366412 -0.43728706  0.25649675 -0.39484191\n",
      "  -0.32528786  0.1073887   0.28553357]\n",
      " [ 0.41885188 -0.24071444  0.22664784  0.38445319  0.13124853  0.18584451\n",
      "   0.81826695 -0.22149954 -0.3628748 ]\n",
      " [-0.3242946   0.28103283 -0.15997107 -0.20966669 -0.09012026 -0.18966096\n",
      "  -0.75770287 -0.13352606  0.15502227]\n",
      " [ 0.14151939  0.01628645  0.2244825  -0.43373734 -0.09753108 -0.04009587\n",
      "  -0.38647439 -0.01245771 -0.03092183]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.18688144  0.39070619 -0.5599977   0.14765675  0.34385393 -0.44679923\n",
      "   0.05103566]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:16 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.62471094]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 16 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.17018792 -0.1556991   0.06771889  0.44942499 -0.00578086 -0.06393446\n",
      "   0.44288752  0.0972434   0.04232202]\n",
      " [-0.12366875  0.29870245 -0.27237428 -0.5646993  -0.14563315 -0.11819999\n",
      "  -0.80465569 -0.25008331  0.04388393]\n",
      " [ 0.20036898 -0.39386131  0.19966385 -0.43728706  0.25649675 -0.39484191\n",
      "  -0.31928813  0.1073887   0.28553357]\n",
      " [ 0.40311626 -0.25645006  0.21091221  0.38445319  0.13124853  0.18584451\n",
      "   0.80253133 -0.22149954 -0.3628748 ]\n",
      " [-0.31021121  0.29511622 -0.14588768 -0.20966669 -0.09012026 -0.18966096\n",
      "  -0.74361947 -0.13352606  0.15502227]\n",
      " [ 0.14159602  0.01636309  0.22455914 -0.43373734 -0.09753108 -0.04009587\n",
      "  -0.38639775 -0.01245771 -0.03092183]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.11365058  0.34404342 -0.58029955  0.11715534  0.28727575 -0.46706154\n",
      "   0.01449687]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:16 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.55013]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 16 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.17472368 -0.1556991   0.07225464  0.44942499 -0.00578086 -0.06393446\n",
      "   0.44288752  0.10177916  0.04232202]\n",
      " [-0.13178409  0.29870245 -0.28048962 -0.5646993  -0.14563315 -0.11819999\n",
      "  -0.80465569 -0.25819865  0.04388393]\n",
      " [ 0.20699508 -0.39386131  0.20628995 -0.43728706  0.25649675 -0.39484191\n",
      "  -0.31928813  0.1140148   0.28553357]\n",
      " [ 0.40837399 -0.25645006  0.21616994  0.38445319  0.13124853  0.18584451\n",
      "   0.80253133 -0.21624181 -0.3628748 ]\n",
      " [-0.31774326  0.29511622 -0.15341973 -0.20966669 -0.09012026 -0.18966096\n",
      "  -0.74361947 -0.14105811  0.15502227]\n",
      " [ 0.14636768  0.01636309  0.22933079 -0.43373734 -0.09753108 -0.04009587\n",
      "  -0.38639775 -0.00768606 -0.03092183]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.16931906  0.37649882 -0.56115722  0.15190371  0.32050379 -0.44720343\n",
      "   0.04720288]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:16 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.58312087]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 16 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.18229394 -0.1556991   0.07225464  0.45699524 -0.00578086 -0.06393446\n",
      "   0.44288752  0.10177916  0.04989227]\n",
      " [-0.13922959  0.29870245 -0.28048962 -0.5721448  -0.14563315 -0.11819999\n",
      "  -0.80465569 -0.25819865  0.03643843]\n",
      " [ 0.20769432 -0.39386131  0.20628995 -0.43658782  0.25649675 -0.39484191\n",
      "  -0.31928813  0.1140148   0.2862328 ]\n",
      " [ 0.41357624 -0.25645006  0.21616994  0.38965544  0.13124853  0.18584451\n",
      "   0.80253133 -0.21624181 -0.35767255]\n",
      " [-0.32230063  0.29511622 -0.15341973 -0.21422406 -0.09012026 -0.18966096\n",
      "  -0.74361947 -0.14105811  0.15046491]\n",
      " [ 0.14243616  0.01636309  0.22933079 -0.43766886 -0.09753108 -0.04009587\n",
      "  -0.38639775 -0.00768606 -0.03485335]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.21998883  0.40997696 -0.54380769  0.17793818  0.35120269 -0.42653198\n",
      "   0.06853952]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:16 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.59854936]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 16 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.17939222 -0.1556991   0.07225464  0.45699524 -0.00868257 -0.06683617\n",
      "   0.44288752  0.10177916  0.04699055]\n",
      " [-0.13285502  0.29870245 -0.28048962 -0.5721448  -0.13925858 -0.11182543\n",
      "  -0.80465569 -0.25819865  0.042813  ]\n",
      " [ 0.20149954 -0.39386131  0.20628995 -0.43658782  0.25030197 -0.40103669\n",
      "  -0.31928813  0.1140148   0.28003803]\n",
      " [ 0.40709843 -0.25645006  0.21616994  0.38965544  0.12477072  0.17936669\n",
      "   0.80253133 -0.21624181 -0.36415036]\n",
      " [-0.31458176  0.29511622 -0.15341973 -0.21422406 -0.08240139 -0.18194209\n",
      "  -0.74361947 -0.14105811  0.15818377]\n",
      " [ 0.14297617  0.01636309  0.22933079 -0.43766886 -0.09699107 -0.03955586\n",
      "  -0.38639775 -0.00768606 -0.03431334]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.14807671  0.3711064  -0.57324542  0.13565598  0.30861756 -0.4545041\n",
      "   0.03312355]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:16 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.52076166]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 16 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.17947077 -0.15562056  0.07225464  0.45699524 -0.00860402 -0.06675762\n",
      "   0.44288752  0.10177916  0.0470691 ]\n",
      " [-0.13216613  0.29939134 -0.28048962 -0.5721448  -0.13856969 -0.11113653\n",
      "  -0.80465569 -0.25819865  0.04350189]\n",
      " [ 0.20252296 -0.39283789  0.20628995 -0.43658782  0.25132538 -0.40001327\n",
      "  -0.31928813  0.1140148   0.28106144]\n",
      " [ 0.40562901 -0.25791948  0.21616994  0.38965544  0.1233013   0.17789727\n",
      "   0.80253133 -0.21624181 -0.36561978]\n",
      " [-0.31254892  0.29714906 -0.15341973 -0.21422406 -0.08036855 -0.17990925\n",
      "  -0.74361947 -0.14105811  0.16021662]\n",
      " [ 0.14316333  0.01655025  0.22933079 -0.43766886 -0.09680391 -0.0393687\n",
      "  -0.38639775 -0.00768606 -0.03412618]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.08309374  0.33869347 -0.6050478   0.10418858  0.27465464 -0.48495739\n",
      "   0.00081923]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:16 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.55712725]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 16 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.17299686 -0.16209446  0.07225464  0.45699524 -0.01507793 -0.07323153\n",
      "   0.43641362  0.10177916  0.0470691 ]\n",
      " [-0.11956809  0.31198938 -0.28048962 -0.5721448  -0.12597165 -0.09853849\n",
      "  -0.79205765 -0.25819865  0.04350189]\n",
      " [ 0.21269226 -0.38266859  0.20628995 -0.43658782  0.26149469 -0.38984396\n",
      "  -0.30911882  0.1140148   0.28106144]\n",
      " [ 0.39075151 -0.27279698  0.21616994  0.38965544  0.10842381  0.16301978\n",
      "   0.78765383 -0.21624181 -0.36561978]\n",
      " [-0.29889793  0.31080005 -0.15341973 -0.21422406 -0.06671757 -0.16625827\n",
      "  -0.72996849 -0.14105811  0.16021662]\n",
      " [ 0.14919747  0.02258439  0.22933079 -0.43766886 -0.09076977 -0.03333456\n",
      "  -0.38036361 -0.00768606 -0.03412618]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.01436193  0.29768711 -0.62509694  0.08074254  0.2212123  -0.50318261\n",
      "  -0.02737924]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:16 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.55261408]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 16 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.18423228 -0.16209446  0.07225464  0.46823065 -0.01507793 -0.07323153\n",
      "   0.44764903  0.10177916  0.0470691 ]\n",
      " [-0.13193314  0.31198938 -0.28048962 -0.58450985 -0.12597165 -0.09853849\n",
      "  -0.8044227  -0.25819865  0.04350189]\n",
      " [ 0.20582241 -0.38266859  0.20628995 -0.44345768  0.26149469 -0.38984396\n",
      "  -0.31598868  0.1140148   0.28106144]\n",
      " [ 0.40312986 -0.27279698  0.21616994  0.40203378  0.10842381  0.16301978\n",
      "   0.80003218 -0.21624181 -0.36561978]\n",
      " [-0.31084411  0.31080005 -0.15341973 -0.22617023 -0.06671757 -0.16625827\n",
      "  -0.74191467 -0.14105811  0.16021662]\n",
      " [ 0.14091184  0.02258439  0.22933079 -0.44595449 -0.09076977 -0.03333456\n",
      "  -0.38864924 -0.00768606 -0.03412618]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.06966593  0.33882847 -0.61487352  0.10119474  0.26697675 -0.49080013\n",
      "  -0.00864458]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:16 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.57184739]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 16 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.1908537  -0.16209446  0.07225464  0.46823065 -0.00845651 -0.0666101\n",
      "   0.45427045  0.10177916  0.0470691 ]\n",
      " [-0.14298411  0.31198938 -0.28048962 -0.58450985 -0.13702261 -0.10958946\n",
      "  -0.81547367 -0.25819865  0.04350189]\n",
      " [ 0.20274105 -0.38266859  0.20628995 -0.44345768  0.25841333 -0.39292532\n",
      "  -0.31907004  0.1140148   0.28106144]\n",
      " [ 0.41484369 -0.27279698  0.21616994  0.40203378  0.12013764  0.17473361\n",
      "   0.81174601 -0.21624181 -0.36561978]\n",
      " [-0.3222783   0.31080005 -0.15341973 -0.22617023 -0.07815176 -0.17769246\n",
      "  -0.75334886 -0.14105811  0.16021662]\n",
      " [ 0.13620403  0.02258439  0.22933079 -0.44595449 -0.09547758 -0.03804237\n",
      "  -0.39335705 -0.00768606 -0.03412618]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.12207994  0.37198782 -0.60237226  0.12429109  0.30962898 -0.47944853\n",
      "   0.01274537]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:16 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.56527478]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 16 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.18081118 -0.16209446  0.07225464  0.45818813 -0.00845651 -0.07665262\n",
      "   0.45427045  0.10177916  0.03702659]\n",
      " [-0.13116509  0.31198938 -0.28048962 -0.57269083 -0.13702261 -0.09777044\n",
      "  -0.81547367 -0.25819865  0.05532091]\n",
      " [ 0.20867679 -0.38266859  0.20628995 -0.43752194  0.25841333 -0.38698958\n",
      "  -0.31907004  0.1140148   0.28699717]\n",
      " [ 0.4049731  -0.27279698  0.21616994  0.39216319  0.12013764  0.16486302\n",
      "   0.81174601 -0.21624181 -0.37549038]\n",
      " [-0.31319834  0.31080005 -0.15341973 -0.21709027 -0.07815176 -0.1686125\n",
      "  -0.75334886 -0.14105811  0.16929658]\n",
      " [ 0.14259952  0.02258439  0.22933079 -0.439559   -0.09547758 -0.03164688\n",
      "  -0.39335705 -0.00768606 -0.02773069]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.05262485  0.32651903 -0.62400059  0.09562303  0.26437344 -0.50460362\n",
      "  -0.01543007]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:16 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.56406991]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 16 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.19040121 -0.15250444  0.07225464  0.46777816 -0.00845651 -0.0670626\n",
      "   0.46386047  0.10177916  0.03702659]\n",
      " [-0.14290306  0.3002514  -0.28048962 -0.58442881 -0.13702261 -0.10950842\n",
      "  -0.82721164 -0.25819865  0.05532091]\n",
      " [ 0.19691141 -0.39443396  0.20628995 -0.44928732  0.25841333 -0.39875496\n",
      "  -0.33083541  0.1140148   0.28699717]\n",
      " [ 0.41696405 -0.26080603  0.21616994  0.40415414  0.12013764  0.17685397\n",
      "   0.82373696 -0.21624181 -0.37549038]\n",
      " [-0.32442226  0.29957613 -0.15341973 -0.22831419 -0.07815176 -0.17983641\n",
      "  -0.76457277 -0.14105811  0.16929658]\n",
      " [ 0.13428704  0.01427191  0.22933079 -0.44787148 -0.09547758 -0.03995936\n",
      "  -0.40166953 -0.00768606 -0.02773069]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.10622138  0.3641178  -0.61256812  0.10694381  0.30820017 -0.49163035\n",
      "   0.00236129]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:16 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.59392275]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 16 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.19986631 -0.14303933  0.07225464  0.47724326 -0.00845651 -0.0670626\n",
      "   0.47332558  0.10177916  0.03702659]\n",
      " [-0.15351     0.28964447 -0.28048962 -0.59503574 -0.13702261 -0.10950842\n",
      "  -0.83781858 -0.25819865  0.05532091]\n",
      " [ 0.18740203 -0.40394334  0.20628995 -0.4587967   0.25841333 -0.39875496\n",
      "  -0.34034479  0.1140148   0.28699717]\n",
      " [ 0.42782264 -0.24994743  0.21616994  0.41501273  0.12013764  0.17685397\n",
      "   0.83459556 -0.21624181 -0.37549038]\n",
      " [-0.33414022  0.28985817 -0.15341973 -0.23803215 -0.07815176 -0.17983641\n",
      "  -0.77429074 -0.14105811  0.16929658]\n",
      " [ 0.12667902  0.00666389  0.22933079 -0.4554795  -0.09547758 -0.03995936\n",
      "  -0.40927755 -0.00768606 -0.02773069]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.15518993  0.39962131 -0.60169915  0.12032982  0.34735741 -0.47863071\n",
      "   0.018599  ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:16 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.63606124]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 16 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.20624754 -0.14303933  0.07863588  0.47724326 -0.00845651 -0.06068136\n",
      "   0.47970681  0.10177916  0.03702659]\n",
      " [-0.16284758  0.28964447 -0.28982721 -0.59503574 -0.13702261 -0.118846\n",
      "  -0.84715616 -0.25819865  0.05532091]\n",
      " [ 0.18387095 -0.40394334  0.20275887 -0.4587967   0.25841333 -0.40228605\n",
      "  -0.34387587  0.1140148   0.28699717]\n",
      " [ 0.43721254 -0.24994743  0.22555985  0.41501273  0.12013764  0.18624387\n",
      "   0.84398546 -0.21624181 -0.37549038]\n",
      " [-0.34353468  0.28985817 -0.16281419 -0.23803215 -0.07815176 -0.18923087\n",
      "  -0.7836852  -0.14105811  0.16929658]\n",
      " [ 0.12569938  0.00666389  0.22835116 -0.4554795  -0.09547758 -0.040939\n",
      "  -0.41025719 -0.00768606 -0.02773069]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.19731354  0.42756518 -0.5932409   0.1377899   0.38272545 -0.47057313\n",
      "   0.03867974]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:16 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57408582]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 17 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.19691487 -0.15237201  0.07863588  0.46791058 -0.00845651 -0.06068136\n",
      "   0.47970681  0.10177916  0.02769391]\n",
      " [-0.15590266  0.2965894  -0.28982721 -0.58809082 -0.13702261 -0.118846\n",
      "  -0.84715616 -0.25819865  0.06226583]\n",
      " [ 0.19048949 -0.3973248   0.20275887 -0.45217815  0.25841333 -0.40228605\n",
      "  -0.34387587  0.1140148   0.29361572]\n",
      " [ 0.43328399 -0.25387599  0.22555985  0.41108418  0.12013764  0.18624387\n",
      "   0.84398546 -0.21624181 -0.37941893]\n",
      " [-0.34139483  0.29199802 -0.16281419 -0.2358923  -0.07815176 -0.18923087\n",
      "  -0.7836852  -0.14105811  0.17143643]\n",
      " [ 0.13166983  0.01263433  0.22835116 -0.44950905 -0.09547758 -0.040939\n",
      "  -0.41025719 -0.00768606 -0.02176024]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.12712831  0.38261246 -0.62118955  0.10948653  0.34367052 -0.50352054\n",
      "   0.00968082]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:17 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.61933218]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 17 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.20428555 -0.15237201  0.08600656  0.46791058 -0.00845651 -0.06068136\n",
      "   0.48707749  0.10177916  0.02769391]\n",
      " [-0.16570633  0.2965894  -0.29963088 -0.58809082 -0.13702261 -0.118846\n",
      "  -0.85695983 -0.25819865  0.06226583]\n",
      " [ 0.19104303 -0.3973248   0.2033124  -0.45217815  0.25841333 -0.40228605\n",
      "  -0.34332234  0.1140148   0.29361572]\n",
      " [ 0.44332385 -0.25387599  0.23559971  0.41108418  0.12013764  0.18624387\n",
      "   0.85402532 -0.21624181 -0.37941893]\n",
      " [-0.35118839  0.29199802 -0.17260775 -0.2358923  -0.07815176 -0.18923087\n",
      "  -0.79347876 -0.14105811  0.17143643]\n",
      " [ 0.13110662  0.01263433  0.22778795 -0.44950905 -0.09547758 -0.040939\n",
      "  -0.4108204  -0.00768606 -0.02176024]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.1720014   0.41314067 -0.61152547  0.13247684  0.38037653 -0.49381855\n",
      "   0.03155392]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:17 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.62650617]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 17 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.19388194 -0.16277562  0.07560295  0.46791058 -0.00845651 -0.06068136\n",
      "   0.47667388  0.10177916  0.02769391]\n",
      " [-0.15110064  0.31119508 -0.28502519 -0.58809082 -0.13702261 -0.118846\n",
      "  -0.84235415 -0.25819865  0.06226583]\n",
      " [ 0.19720232 -0.3911655   0.2094717  -0.45217815  0.25841333 -0.40228605\n",
      "  -0.33716305  0.1140148   0.29361572]\n",
      " [ 0.42735613 -0.2698437   0.21963199  0.41108418  0.12013764  0.18624387\n",
      "   0.8380576  -0.21624181 -0.37941893]\n",
      " [-0.33658587  0.30660054 -0.15800523 -0.2358923  -0.07815176 -0.18923087\n",
      "  -0.77887624 -0.14105811  0.17143643]\n",
      " [ 0.13182636  0.01335407  0.22850769 -0.44950905 -0.09547758 -0.040939\n",
      "  -0.41010066 -0.00768606 -0.02176024]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.09870137  0.36539638 -0.6308706   0.10210996  0.32303451 -0.51316982\n",
      "  -0.00437617]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:17 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.54596728]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 17 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.19892917 -0.16277562  0.08065018  0.46791058 -0.00845651 -0.06068136\n",
      "   0.47667388  0.1068264   0.02769391]\n",
      " [-0.15978005  0.31119508 -0.2937046  -0.58809082 -0.13702261 -0.118846\n",
      "  -0.84235415 -0.26687806  0.06226583]\n",
      " [ 0.20405278 -0.3911655   0.21632215 -0.45217815  0.25841333 -0.40228605\n",
      "  -0.33716305  0.12086526  0.29361572]\n",
      " [ 0.4331435  -0.2698437   0.22541936  0.41108418  0.12013764  0.18624387\n",
      "   0.8380576  -0.21045444 -0.37941893]\n",
      " [-0.34468276  0.30660054 -0.16610213 -0.2358923  -0.07815176 -0.18923087\n",
      "  -0.77887624 -0.14915501  0.17143643]\n",
      " [ 0.13663652  0.01335407  0.23331785 -0.44950905 -0.09547758 -0.040939\n",
      "  -0.41010066 -0.0028759  -0.02176024]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.15497577  0.39869757 -0.61212719  0.13741139  0.35713972 -0.49368589\n",
      "   0.02867151]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:17 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.57941062]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 17 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.2068351  -0.16277562  0.08065018  0.47581651 -0.00845651 -0.06068136\n",
      "   0.47667388  0.1068264   0.03559984]\n",
      " [-0.16760751  0.31119508 -0.2937046  -0.59591828 -0.13702261 -0.118846\n",
      "  -0.84235415 -0.26687806  0.05443837]\n",
      " [ 0.2046353  -0.3911655   0.21632215 -0.45159563  0.25841333 -0.40228605\n",
      "  -0.33716305  0.12086526  0.29419824]\n",
      " [ 0.43878817 -0.2698437   0.22541936  0.41672885  0.12013764  0.18624387\n",
      "   0.8380576  -0.21045444 -0.37377426]\n",
      " [-0.34971122  0.30660054 -0.16610213 -0.24092075 -0.07815176 -0.18923087\n",
      "  -0.77887624 -0.14915501  0.16640798]\n",
      " [ 0.13246706  0.01335407  0.23331785 -0.45367851 -0.09547758 -0.040939\n",
      "  -0.41010066 -0.0028759  -0.02592971]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.20622332  0.43287838 -0.59495867  0.16361788  0.38861362 -0.47323204\n",
      "   0.05004757]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:17 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.59020031]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 17 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.20376597 -0.16277562  0.08065018  0.47581651 -0.01152564 -0.06375049\n",
      "   0.47667388  0.1068264   0.03253071]\n",
      " [-0.16124179  0.31119508 -0.2937046  -0.59591828 -0.13065689 -0.11248028\n",
      "  -0.84235415 -0.26687806  0.0608041 ]\n",
      " [ 0.19849694 -0.3911655   0.21632215 -0.45159563  0.25227498 -0.4084244\n",
      "  -0.33716305  0.12086526  0.28805988]\n",
      " [ 0.43238454 -0.2698437   0.22541936  0.41672885  0.11373401  0.17984024\n",
      "   0.8380576  -0.21045444 -0.38017789]\n",
      " [-0.34206432  0.30660054 -0.16610213 -0.24092075 -0.07050486 -0.18158397\n",
      "  -0.77887624 -0.14915501  0.17405488]\n",
      " [ 0.13300009  0.01335407  0.23331785 -0.45367851 -0.09494455 -0.04040597\n",
      "  -0.41010066 -0.0028759  -0.02539667]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.13484924  0.39410683 -0.6241345   0.12166277  0.34637472 -0.50101067\n",
      "   0.01489364]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:17 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.50863583]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 17 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.20379386 -0.16274773  0.08065018  0.47581651 -0.01149775 -0.0637226\n",
      "   0.47667388  0.1068264   0.0325586 ]\n",
      " [-0.1607274   0.31170947 -0.2937046  -0.59591828 -0.1301425  -0.11196589\n",
      "  -0.84235415 -0.26687806  0.06131848]\n",
      " [ 0.19946151 -0.39020094  0.21632215 -0.45159563  0.25323954 -0.40745984\n",
      "  -0.33716305  0.12086526  0.28902444]\n",
      " [ 0.43117963 -0.27104862  0.22541936  0.41672885  0.11252909  0.17863533\n",
      "   0.8380576  -0.21045444 -0.3813828 ]\n",
      " [-0.34026662  0.30839824 -0.16610213 -0.24092075 -0.06870716 -0.17978627\n",
      "  -0.77887624 -0.14915501  0.17585258]\n",
      " [ 0.13322878  0.01358277  0.23331785 -0.45367851 -0.09471585 -0.04017727\n",
      "  -0.41010066 -0.0028759  -0.02516798]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.07128873  0.36235446 -0.65540029  0.09084767  0.31338839 -0.53098937\n",
      "  -0.01665791]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:17 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.55965425]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 17 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.19652664 -0.17001495  0.08065018  0.47581651 -0.01876497 -0.07098982\n",
      "   0.46940666  0.1068264   0.0325586 ]\n",
      " [-0.14768712  0.32474975 -0.2937046  -0.59591828 -0.11710222 -0.09892561\n",
      "  -0.82931386 -0.26687806  0.06131848]\n",
      " [ 0.20995301 -0.37970944  0.21632215 -0.45159563  0.26373104 -0.39696834\n",
      "  -0.32667154  0.12086526  0.28902444]\n",
      " [ 0.4161243  -0.28610395  0.22541936  0.41672885  0.09747376  0.16358\n",
      "   0.82300227 -0.21045444 -0.3813828 ]\n",
      " [-0.32630225  0.32236261 -0.16610213 -0.24092075 -0.05474279 -0.1658219\n",
      "  -0.76491187 -0.14915501  0.17585258]\n",
      " [ 0.13982848  0.02018246  0.23331785 -0.45367851 -0.08811616 -0.03357758\n",
      "  -0.40350097 -0.0028759  -0.02516798]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.00232775  0.32036726 -0.6748624   0.06769142  0.25932039 -0.54874372\n",
      "  -0.04436293]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:17 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.56028498]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 17 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.2078709  -0.17001495  0.08065018  0.48716077 -0.01876497 -0.07098982\n",
      "   0.48075092  0.1068264   0.0325586 ]\n",
      " [-0.15980949  0.32474975 -0.2937046  -0.60804065 -0.11710222 -0.09892561\n",
      "  -0.84143624 -0.26687806  0.06131848]\n",
      " [ 0.20284669 -0.37970944  0.21632215 -0.45870196  0.26373104 -0.39696834\n",
      "  -0.33377786  0.12086526  0.28902444]\n",
      " [ 0.42819809 -0.28610395  0.22541936  0.42880264  0.09747376  0.16358\n",
      "   0.83507606 -0.21045444 -0.3813828 ]\n",
      " [-0.33822281  0.32236261 -0.16610213 -0.25284131 -0.05474279 -0.1658219\n",
      "  -0.77683243 -0.14915501  0.17585258]\n",
      " [ 0.1312644   0.02018246  0.23331785 -0.46224259 -0.08811616 -0.03357758\n",
      "  -0.41206504 -0.0028759  -0.02516798]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.0564931   0.36142464 -0.66555675  0.08727902  0.304802   -0.53743365\n",
      "  -0.02659796]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:17 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.57526542]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 17 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.2149826  -0.17001495  0.08065018  0.48716077 -0.01165327 -0.06387812\n",
      "   0.48786262  0.1068264   0.0325586 ]\n",
      " [-0.17094147  0.32474975 -0.2937046  -0.60804065 -0.1282342  -0.11005759\n",
      "  -0.85256821 -0.26687806  0.06131848]\n",
      " [ 0.19947894 -0.37970944  0.21632215 -0.45870196  0.26036329 -0.40033609\n",
      "  -0.33714561  0.12086526  0.28902444]\n",
      " [ 0.43981301 -0.28610395  0.22541936  0.42880264  0.10908868  0.17519492\n",
      "   0.84669098 -0.21045444 -0.3813828 ]\n",
      " [-0.34964898  0.32236261 -0.16610213 -0.25284131 -0.06616896 -0.17724807\n",
      "  -0.7882586  -0.14915501  0.17585258]\n",
      " [ 0.12624904  0.02018246  0.23331785 -0.46224259 -0.09313151 -0.03859294\n",
      "  -0.4170804  -0.0028759  -0.02516798]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.10838189  0.39491351 -0.65370449  0.10981635  0.34741177 -0.52662878\n",
      "  -0.00580544]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:17 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.56377245]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 17 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.20457213 -0.17001495  0.08065018  0.47675031 -0.01165327 -0.07428859\n",
      "   0.48786262  0.1068264   0.02214813]\n",
      " [-0.15879752  0.32474975 -0.2937046  -0.5958967  -0.1282342  -0.09791364\n",
      "  -0.85256821 -0.26687806  0.07346243]\n",
      " [ 0.2056853  -0.37970944  0.21632215 -0.45249559  0.26036329 -0.39412972\n",
      "  -0.33714561  0.12086526  0.29523081]\n",
      " [ 0.42950505 -0.28610395  0.22541936  0.41849469  0.10908868  0.16488696\n",
      "   0.84669098 -0.21045444 -0.39169076]\n",
      " [-0.34008192  0.32236261 -0.16610213 -0.24327424 -0.06616896 -0.16768101\n",
      "  -0.7882586  -0.14915501  0.18541964]\n",
      " [ 0.13290772  0.02018246  0.23331785 -0.45558391 -0.09313151 -0.03193426\n",
      "  -0.4170804  -0.0028759  -0.0185093 ]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.03905674  0.34904192 -0.67478815  0.08150314  0.30167065 -0.55113211\n",
      "  -0.03363056]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:17 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.57033902]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 17 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.21447223 -0.16011485  0.08065018  0.4866504  -0.01165327 -0.06438849\n",
      "   0.49776271  0.1068264   0.02214813]\n",
      " [-0.17046601  0.31308126 -0.2937046  -0.60756519 -0.1282342  -0.10958213\n",
      "  -0.86423671 -0.26687806  0.07346243]\n",
      " [ 0.19405266 -0.39134209  0.21632215 -0.46412824  0.26036329 -0.40576237\n",
      "  -0.34877826  0.12086526  0.29523081]\n",
      " [ 0.44128697 -0.27432203  0.22541936  0.4302766   0.10908868  0.17666888\n",
      "   0.8584729  -0.21045444 -0.39169076]\n",
      " [-0.351375    0.31106953 -0.16610213 -0.25456733 -0.06616896 -0.17897409\n",
      "  -0.79955168 -0.14915501  0.18541964]\n",
      " [ 0.12429181  0.01156655  0.23331785 -0.46419982 -0.09313151 -0.04055017\n",
      "  -0.4256963  -0.0028759  -0.0185093 ]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.09170148  0.38673743 -0.66420969  0.0922742   0.34527514 -0.53910402\n",
      "  -0.01675831]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:17 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.59925968]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 17 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.22412233 -0.15046475  0.08065018  0.49630051 -0.01165327 -0.06438849\n",
      "   0.50741282  0.1068264   0.02214813]\n",
      " [-0.18105048  0.30249679 -0.2937046  -0.61814966 -0.1282342  -0.10958213\n",
      "  -0.87482117 -0.26687806  0.07346243]\n",
      " [ 0.18454066 -0.40085408  0.21632215 -0.47364023  0.26036329 -0.40576237\n",
      "  -0.35829025  0.12086526  0.29523081]\n",
      " [ 0.45202904 -0.26357996  0.22541936  0.44101867  0.10908868  0.17666888\n",
      "   0.86921496 -0.21045444 -0.39169076]\n",
      " [-0.36126978  0.30117474 -0.16610213 -0.26446211 -0.06616896 -0.17897409\n",
      "  -0.80944646 -0.14915501  0.18541964]\n",
      " [ 0.11639737  0.00367211  0.23331785 -0.47209426 -0.09313151 -0.04055017\n",
      "  -0.43359075 -0.0028759  -0.0185093 ]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.13981987  0.42227829 -0.6541388   0.10511899  0.38429321 -0.5270366\n",
      "  -0.00136321]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:17 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.64031138]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 17 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.23087675 -0.15046475  0.0874046   0.49630051 -0.01165327 -0.05763408\n",
      "   0.51416724  0.1068264   0.02214813]\n",
      " [-0.19029931  0.30249679 -0.30295343 -0.61814966 -0.1282342  -0.11883096\n",
      "  -0.88407    -0.26687806  0.07346243]\n",
      " [ 0.18090114 -0.40085408  0.21268263 -0.47364023  0.26036329 -0.40940189\n",
      "  -0.36192978  0.12086526  0.29523081]\n",
      " [ 0.46120206 -0.26357996  0.23459238  0.44101867  0.10908868  0.1858419\n",
      "   0.87838799 -0.21045444 -0.39169076]\n",
      " [-0.3705401   0.30117474 -0.17537245 -0.26446211 -0.06616896 -0.18824442\n",
      "  -0.81871679 -0.14915501  0.18541964]\n",
      " [ 0.1151139   0.00367211  0.23203438 -0.47209426 -0.09313151 -0.04183363\n",
      "  -0.43487422 -0.0028759  -0.0185093 ]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.1812403   0.45039027 -0.64632707  0.12210914  0.41944083 -0.51957753\n",
      "   0.01806022]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:17 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.56683028]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 18 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.22133935 -0.16000215  0.0874046   0.48676311 -0.01165327 -0.05763408\n",
      "   0.51416724  0.1068264   0.01261073]\n",
      " [-0.18311644  0.30967966 -0.30295343 -0.61096679 -0.1282342  -0.11883096\n",
      "  -0.88407    -0.26687806  0.0806453 ]\n",
      " [ 0.18756361 -0.39419161  0.21268263 -0.46697776  0.26036329 -0.40940189\n",
      "  -0.36192978  0.12086526  0.30189328]\n",
      " [ 0.45697071 -0.26781131  0.23459238  0.43678732  0.10908868  0.1858419\n",
      "   0.87838799 -0.21045444 -0.39592211]\n",
      " [-0.36797242  0.30374243 -0.17537245 -0.26189442 -0.06616896 -0.18824442\n",
      "  -0.81871679 -0.14915501  0.18798732]\n",
      " [ 0.1213639   0.0099221   0.23203438 -0.46584427 -0.09313151 -0.04183363\n",
      "  -0.43487422 -0.0028759  -0.01225931]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.11165233  0.4054785  -0.67371216  0.09415525  0.38037235 -0.55179439\n",
      "  -0.01033876]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:18 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.62299462]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 18 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.22906444 -0.16000215  0.09512969  0.48676311 -0.01165327 -0.05763408\n",
      "   0.52189233  0.1068264   0.01261073]\n",
      " [-0.19291643  0.30967966 -0.31275342 -0.61096679 -0.1282342  -0.11883096\n",
      "  -0.89386999 -0.26687806  0.0806453 ]\n",
      " [ 0.18798756 -0.39419161  0.21310658 -0.46697776  0.26036329 -0.40940189\n",
      "  -0.36150583  0.12086526  0.30189328]\n",
      " [ 0.46687996 -0.26781131  0.24450163  0.43678732  0.10908868  0.1858419\n",
      "   0.88829724 -0.21045444 -0.39592211]\n",
      " [-0.37776145  0.30374243 -0.18516148 -0.26189442 -0.06616896 -0.18824442\n",
      "  -0.82850581 -0.14915501  0.18798732]\n",
      " [ 0.12046357  0.0099221   0.23113406 -0.46584427 -0.09313151 -0.04183363\n",
      "  -0.43577454 -0.0028759  -0.01225931]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.1559264   0.43624251 -0.66474236  0.11671633  0.41702139 -0.54276667\n",
      "   0.01089695]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:18 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.6283803]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 18 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.21785209 -0.1712145   0.08391735  0.48676311 -0.01165327 -0.05763408\n",
      "   0.51067998  0.1068264   0.01261073]\n",
      " [-0.17785798  0.32473811 -0.29769498 -0.61096679 -0.1282342  -0.11883096\n",
      "  -0.87881154 -0.26687806  0.0806453 ]\n",
      " [ 0.19429154 -0.38788762  0.21941056 -0.46697776  0.26036329 -0.40940189\n",
      "  -0.35520184  0.12086526  0.30189328]\n",
      " [ 0.45073381 -0.28395747  0.22835548  0.43678732  0.10908868  0.1858419\n",
      "   0.87215108 -0.21045444 -0.39592211]\n",
      " [-0.36271681  0.31878706 -0.17011685 -0.26189442 -0.06616896 -0.18824442\n",
      "  -0.81346118 -0.14915501  0.18798732]\n",
      " [ 0.1218237   0.01128223  0.23249419 -0.46584427 -0.09313151 -0.04183363\n",
      "  -0.43441441 -0.0028759  -0.01225931]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.08255718  0.38744499 -0.68320532  0.08646866  0.35897538 -0.56125969\n",
      "  -0.02442627]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:18 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.54177638]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 18 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.22342623 -0.1712145   0.08949148  0.48676311 -0.01165327 -0.05763408\n",
      "   0.51067998  0.11240053  0.01261073]\n",
      " [-0.18708414  0.32473811 -0.30692114 -0.61096679 -0.1282342  -0.11883096\n",
      "  -0.87881154 -0.27610422  0.0806453 ]\n",
      " [ 0.20137465 -0.38788762  0.22649367 -0.46697776  0.26036329 -0.40940189\n",
      "  -0.35520184  0.12794837  0.30189328]\n",
      " [ 0.45704469 -0.28395747  0.23466636  0.43678732  0.10908868  0.1858419\n",
      "   0.87215108 -0.20414356 -0.39592211]\n",
      " [-0.37136874  0.31878706 -0.17876878 -0.26189442 -0.06616896 -0.18824442\n",
      "  -0.81346118 -0.15780694  0.18798732]\n",
      " [ 0.12666985  0.01128223  0.23734034 -0.46584427 -0.09313151 -0.04183363\n",
      "  -0.43441441  0.00197026 -0.01225931]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.13943528  0.42161457 -0.66486369  0.12233301  0.39395886 -0.54215902\n",
      "   0.0089593 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:18 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.5755486]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 18 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.23167019 -0.1712145   0.08949148  0.49500708 -0.01165327 -0.05763408\n",
      "   0.51067998  0.11240053  0.0208547 ]\n",
      " [-0.19528184  0.32473811 -0.30692114 -0.61916448 -0.1282342  -0.11883096\n",
      "  -0.87881154 -0.27610422  0.0724476 ]\n",
      " [ 0.20184486 -0.38788762  0.22649367 -0.46650755  0.26036329 -0.40940189\n",
      "  -0.35520184  0.12794837  0.30236349]\n",
      " [ 0.46311422 -0.28395747  0.23466636  0.44285686  0.10908868  0.1858419\n",
      "   0.87215108 -0.20414356 -0.38985257]\n",
      " [-0.37686321  0.31878706 -0.17876878 -0.26738889 -0.06616896 -0.18824442\n",
      "  -0.81346118 -0.15780694  0.18249285]\n",
      " [ 0.12225262  0.01128223  0.23734034 -0.4702615  -0.09313151 -0.04183363\n",
      "  -0.43441441  0.00197026 -0.01667654]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.1912804   0.45651792 -0.64786032  0.14872589  0.42620487 -0.5219143\n",
      "   0.03037314]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:18 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.58093874]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 18 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.22845785 -0.1712145   0.08949148  0.49500708 -0.01486561 -0.06084642\n",
      "   0.51067998  0.11240053  0.01764235]\n",
      " [-0.18896121  0.32473811 -0.30692114 -0.61916448 -0.12191357 -0.11251034\n",
      "  -0.87881154 -0.27610422  0.07876823]\n",
      " [ 0.19575988 -0.38788762  0.22649367 -0.46650755  0.25427831 -0.41548688\n",
      "  -0.35520184  0.12794837  0.29627851]\n",
      " [ 0.4568208  -0.28395747  0.23466636  0.44285686  0.10279526  0.17954849\n",
      "   0.87215108 -0.20414356 -0.39614599]\n",
      " [-0.36931579  0.31878706 -0.17876878 -0.26738889 -0.05862154 -0.18069699\n",
      "  -0.81346118 -0.15780694  0.19004028]\n",
      " [ 0.12277207  0.01128223  0.23734034 -0.4702615  -0.09261207 -0.04131419\n",
      "  -0.43441441  0.00197026 -0.01615709]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.12056595  0.41793034 -0.6767518   0.10715494  0.38441106 -0.54946819\n",
      "  -0.00446456]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:18 with online instance: 5-------------\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.49527803]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 18 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.22847064 -0.17120171  0.08949148  0.49500708 -0.01485283 -0.06083363\n",
      "   0.51067998  0.11240053  0.01765514]\n",
      " [-0.1886536   0.32504572 -0.30692114 -0.61916448 -0.12160596 -0.11220272\n",
      "  -0.87881154 -0.27610422  0.07907584]\n",
      " [ 0.19664219 -0.38700531  0.22649367 -0.46650755  0.25516062 -0.41460457\n",
      "  -0.35520184  0.12794837  0.29716082]\n",
      " [ 0.45590757 -0.2848707   0.23466636  0.44285686  0.10188203  0.17863525\n",
      "   0.87215108 -0.20414356 -0.39705922]\n",
      " [-0.36777501  0.32032784 -0.17876878 -0.26738889 -0.05708076 -0.17915621\n",
      "  -0.81346118 -0.15780694  0.19158106]\n",
      " [ 0.12302012  0.01153028  0.23734034 -0.4702615  -0.09236402 -0.04106614\n",
      "  -0.43441441  0.00197026 -0.01590904]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.05866172  0.38699101 -0.70739628  0.07708561  0.35254518 -0.57887697\n",
      "  -0.03516862]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:18 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.56229734]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 18 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.22045073 -0.17922162  0.08949148  0.49500708 -0.02287274 -0.06885355\n",
      "   0.50266007  0.11240053  0.01765514]\n",
      " [-0.17522703  0.33847229 -0.30692114 -0.61916448 -0.10817939 -0.09877616\n",
      "  -0.86538498 -0.27610422  0.07907584]\n",
      " [ 0.20743992 -0.37620758  0.22649367 -0.46650755  0.26595835 -0.40380683\n",
      "  -0.34440411  0.12794837  0.29716082]\n",
      " [ 0.44070118 -0.30007709  0.23466636  0.44285686  0.08667564  0.16342886\n",
      "   0.85694469 -0.20414356 -0.39705922]\n",
      " [-0.35352992  0.33457293 -0.17876878 -0.26738889 -0.04283567 -0.16491113\n",
      "  -0.7992161  -0.15780694  0.19158106]\n",
      " [ 0.13017459  0.01868475  0.23734034 -0.4702615  -0.08520955 -0.03391166\n",
      "  -0.42725994  0.00197026 -0.01590904]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.01053432  0.34404523 -0.72633114  0.0542023   0.29789432 -0.59619502\n",
      "  -0.06238619]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:18 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.56848881]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 18 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.23180807 -0.17922162  0.08949148  0.50636442 -0.02287274 -0.06885355\n",
      "   0.51401741  0.11240053  0.01765514]\n",
      " [-0.1870212   0.33847229 -0.30692114 -0.63095866 -0.10817939 -0.09877616\n",
      "  -0.87717915 -0.27610422  0.07907584]\n",
      " [ 0.20014    -0.37620758  0.22649367 -0.47380748  0.26595835 -0.40380683\n",
      "  -0.35170404  0.12794837  0.29716082]\n",
      " [ 0.45239792 -0.30007709  0.23466636  0.4545536   0.08667564  0.16342886\n",
      "   0.86864143 -0.20414356 -0.39705922]\n",
      " [-0.36531218  0.33457293 -0.17876878 -0.27917115 -0.04283567 -0.16491113\n",
      "  -0.81099835 -0.15780694  0.19158106]\n",
      " [ 0.12138058  0.01868475  0.23734034 -0.47905551 -0.08520955 -0.03391166\n",
      "  -0.43605395  0.00197026 -0.01590904]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.04239253  0.38489058 -0.71787386  0.07291458  0.34292188 -0.58589331\n",
      "  -0.04560556]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:18 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.578882]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 18 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.23936427 -0.17922162  0.08949148  0.50636442 -0.01531653 -0.06129734\n",
      "   0.52157362  0.11240053  0.01765514]\n",
      " [-0.19818332  0.33847229 -0.30692114 -0.63095866 -0.1193415  -0.10993827\n",
      "  -0.88834126 -0.27610422  0.07907584]\n",
      " [ 0.19650283 -0.37620758  0.22649367 -0.47380748  0.26232119 -0.40744399\n",
      "  -0.3553412   0.12794837  0.29716082]\n",
      " [ 0.46388604 -0.30007709  0.23466636  0.4545536   0.09816376  0.17491699\n",
      "   0.88012956 -0.20414356 -0.39705922]\n",
      " [-0.37669433  0.33457293 -0.17876878 -0.27917115 -0.05421782 -0.17629328\n",
      "  -0.8223805  -0.15780694  0.19158106]\n",
      " [ 0.11606784  0.01868475  0.23734034 -0.47905551 -0.09052229 -0.03922441\n",
      "  -0.44136669  0.00197026 -0.01590904]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.0937221   0.41866203 -0.70662963  0.09489122  0.38542003 -0.57560901\n",
      "  -0.02542171]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:18 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.56227241]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 18 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.22860101 -0.17922162  0.08949148  0.49560116 -0.01531653 -0.0720606\n",
      "   0.52157362  0.11240053  0.00689188]\n",
      " [-0.18575062  0.33847229 -0.30692114 -0.61852596 -0.1193415  -0.09750557\n",
      "  -0.88834126 -0.27610422  0.09150854]\n",
      " [ 0.20296188 -0.37620758  0.22649367 -0.46734843  0.26232119 -0.40098495\n",
      "  -0.3553412   0.12794837  0.30361986]\n",
      " [ 0.45319075 -0.30007709  0.23466636  0.4438583   0.09816376  0.16422169\n",
      "   0.88012956 -0.20414356 -0.40775452]\n",
      " [-0.36667661  0.33457293 -0.17876878 -0.26915343 -0.05421782 -0.16627556\n",
      "  -0.8223805  -0.15780694  0.20159878]\n",
      " [ 0.12299356  0.01868475  0.23734034 -0.47212979 -0.09052229 -0.03229869\n",
      "  -0.44136669  0.00197026 -0.00898333]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.02452825  0.37239555 -0.72720362  0.06691627  0.33924239 -0.59948891\n",
      "  -0.05288935]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:18 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.57690478]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 18 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.23872298 -0.16909966  0.08949148  0.50572312 -0.01531653 -0.06193864\n",
      "   0.53169558  0.11240053  0.00689188]\n",
      " [-0.19727478  0.32694813 -0.30692114 -0.63005012 -0.1193415  -0.10902973\n",
      "  -0.89986542 -0.27610422  0.09150854]\n",
      " [ 0.19149447 -0.38767499  0.22649367 -0.47881584  0.26232119 -0.41245236\n",
      "  -0.36680861  0.12794837  0.30361986]\n",
      " [ 0.46471252 -0.28855532  0.23466636  0.45538008  0.09816376  0.17574347\n",
      "   0.89165133 -0.20414356 -0.40775452]\n",
      " [-0.37795086  0.32329868 -0.17876878 -0.28042768 -0.05421782 -0.17754981\n",
      "  -0.83365475 -0.15780694  0.20159878]\n",
      " [ 0.11412569  0.00981688  0.23734034 -0.48099766 -0.09052229 -0.04116656\n",
      "  -0.45023456  0.00197026 -0.00898333]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.07616399  0.41008833 -0.71740586  0.07715591  0.38249841 -0.58834244\n",
      "  -0.03692639]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:18 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.60486303]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 18 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.24848301 -0.15933962  0.08949148  0.51548316 -0.01531653 -0.06193864\n",
      "   0.54145562  0.11240053  0.00689188]\n",
      " [-0.20776531  0.3164576  -0.30692114 -0.64054065 -0.1193415  -0.10902973\n",
      "  -0.91035596 -0.27610422  0.09150854]\n",
      " [ 0.18201071 -0.39715874  0.22649367 -0.4882996   0.26232119 -0.41245236\n",
      "  -0.37629236  0.12794837  0.30361986]\n",
      " [ 0.47528211 -0.27798573  0.23466636  0.46594966  0.09816376  0.17574347\n",
      "   0.90222091 -0.20414356 -0.40775452]\n",
      " [-0.38793285  0.31331669 -0.17876878 -0.29040966 -0.05421782 -0.17754981\n",
      "  -0.84363674 -0.15780694  0.20159878]\n",
      " [ 0.10599412  0.00168531  0.23734034 -0.48912923 -0.09052229 -0.04116656\n",
      "  -0.45836613  0.00197026 -0.00898333]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.1233836   0.44557751 -0.70806688  0.08947087  0.42126607 -0.57714743\n",
      "  -0.02236063]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:18 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.64470629]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 18 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.25555117 -0.15933962  0.09655963  0.51548316 -0.01531653 -0.05487048\n",
      "   0.54852377  0.11240053  0.00689188]\n",
      " [-0.21687477  0.3164576  -0.31603059 -0.64054065 -0.1193415  -0.11813919\n",
      "  -0.91946541 -0.27610422  0.09150854]\n",
      " [ 0.17827906 -0.39715874  0.22276202 -0.4882996   0.26232119 -0.41618401\n",
      "  -0.38002401  0.12794837  0.30361986]\n",
      " [ 0.48421518 -0.27798573  0.24359943  0.46594966  0.09816376  0.18467654\n",
      "   0.91115399 -0.20414356 -0.40775452]\n",
      " [-0.39703629  0.31331669 -0.18787222 -0.29040966 -0.05421782 -0.18665325\n",
      "  -0.85274018 -0.15780694  0.20159878]\n",
      " [ 0.10441478  0.00168531  0.23576099 -0.48912923 -0.09052229 -0.0427459\n",
      "  -0.45994547  0.00197026 -0.00898333]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.1640754   0.47380556 -0.70084821  0.10599455  0.45612556 -0.5702428\n",
      "  -0.0036005 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:18 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.5590913]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 19 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.24585221 -0.16903858  0.09655963  0.5057842  -0.01531653 -0.05487048\n",
      "   0.54852377  0.11240053 -0.00280707]\n",
      " [-0.20951008  0.32382229 -0.31603059 -0.63317596 -0.1193415  -0.11813919\n",
      "  -0.91946541 -0.27610422  0.09887324]\n",
      " [ 0.18495586 -0.39048195  0.22276202 -0.4816228   0.26232119 -0.41618401\n",
      "  -0.38002401  0.12794837  0.31029666]\n",
      " [ 0.4797385  -0.28246242  0.24359943  0.46147298  0.09816376  0.18467654\n",
      "   0.91115399 -0.20414356 -0.4122312 ]\n",
      " [-0.39408601  0.31626696 -0.18787222 -0.28745939 -0.05421782 -0.18665325\n",
      "  -0.85274018 -0.15780694  0.20454905]\n",
      " [ 0.11091525  0.00818579  0.23576099 -0.48262876 -0.09052229 -0.0427459\n",
      "  -0.45994547  0.00197026 -0.00248285]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.0951651   0.42902096 -0.7276882   0.07839891  0.41714138 -0.60173302\n",
      "  -0.03138741]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:19 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.62686578]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 19 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.25387245 -0.16903858  0.10457987  0.5057842  -0.01531653 -0.05487048\n",
      "   0.55654401  0.11240053 -0.00280707]\n",
      " [-0.21924491  0.32382229 -0.32576543 -0.63317596 -0.1193415  -0.11813919\n",
      "  -0.92920025 -0.27610422  0.09887324]\n",
      " [ 0.18525793 -0.39048195  0.2230641  -0.4816228   0.26232119 -0.41618401\n",
      "  -0.37972194  0.12794837  0.31029666]\n",
      " [ 0.48948017 -0.28246242  0.2533411   0.46147298  0.09816376  0.18467654\n",
      "   0.92089566 -0.20414356 -0.4122312 ]\n",
      " [-0.40381306  0.31626696 -0.19759927 -0.28745939 -0.05421782 -0.18665325\n",
      "  -0.86246723 -0.15780694  0.20454905]\n",
      " [ 0.10968346  0.00818579  0.23452921 -0.48262876 -0.09052229 -0.0427459\n",
      "  -0.46117725  0.00197026 -0.00248285]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.13880409  0.45996464 -0.71936313  0.10052052  0.45365779 -0.59333828\n",
      "  -0.01080233]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:19 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.63032583]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 19 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.24191858 -0.18099244  0.092626    0.5057842  -0.01531653 -0.05487048\n",
      "   0.54459014  0.11240053 -0.00280707]\n",
      " [-0.20381706  0.33925015 -0.31033757 -0.63317596 -0.1193415  -0.11813919\n",
      "  -0.91377239 -0.27610422  0.09887324]\n",
      " [ 0.19168902 -0.38405086  0.22949518 -0.4816228   0.26232119 -0.41618401\n",
      "  -0.37329085  0.12794837  0.31029666]\n",
      " [ 0.47320129 -0.2987413   0.23706222  0.46147298  0.09816376  0.18467654\n",
      "   0.90461678 -0.20414356 -0.4122312 ]\n",
      " [-0.38840026  0.33167977 -0.18218646 -0.28745939 -0.05421782 -0.18665325\n",
      "  -0.84705442 -0.15780694  0.20454905]\n",
      " [ 0.11167468  0.01017701  0.23652043 -0.48262876 -0.09052229 -0.0427459\n",
      "  -0.45918603  0.00197026 -0.00248285]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.06536635  0.41015202 -0.7370189   0.07037403  0.3949669  -0.6110313\n",
      "  -0.04552605]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:19 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.53765188]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 19 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.24802931 -0.18099244  0.09873673  0.5057842  -0.01531653 -0.05487048\n",
      "   0.54459014  0.11851126 -0.00280707]\n",
      " [-0.21356713  0.33925015 -0.32008764 -0.63317596 -0.1193415  -0.11813919\n",
      "  -0.91377239 -0.28585429  0.09887324]\n",
      " [ 0.19901204 -0.38405086  0.2368182  -0.4816228   0.26232119 -0.41618401\n",
      "  -0.37329085  0.13527138  0.31029666]\n",
      " [ 0.48002598 -0.2987413   0.24388691  0.46147298  0.09816376  0.18467654\n",
      "   0.90461678 -0.19731887 -0.4122312 ]\n",
      " [-0.3975908   0.33167977 -0.191377   -0.28745939 -0.05421782 -0.18665325\n",
      "  -0.84705442 -0.16699748  0.20454905]\n",
      " [ 0.11655421  0.01017701  0.24139995 -0.48262876 -0.09052229 -0.0427459\n",
      "  -0.45918603  0.00684978 -0.00248285]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.12283214  0.44520113 -0.71908307  0.10680356  0.43081961 -0.59232347\n",
      "  -0.0118133 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:19 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.57159305]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 19 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.25660964 -0.18099244  0.09873673  0.51436454 -0.01531653 -0.05487048\n",
      "   0.54459014  0.11851126  0.00577326]\n",
      " [-0.22212123  0.33925015 -0.32008764 -0.64173006 -0.1193415  -0.11813919\n",
      "  -0.91377239 -0.28585429  0.09031914]\n",
      " [ 0.19937502 -0.38405086  0.2368182  -0.48125982  0.26232119 -0.41618401\n",
      "  -0.37329085  0.13527138  0.31065964]\n",
      " [ 0.48650217 -0.2987413   0.24388691  0.46794917  0.09816376  0.18467654\n",
      "   0.90461678 -0.19731887 -0.40575501]\n",
      " [-0.40354159  0.33167977 -0.191377   -0.29341018 -0.05421782 -0.18665325\n",
      "  -0.84705442 -0.16699748  0.19859826]\n",
      " [ 0.1118817   0.01017701  0.24139995 -0.48730126 -0.09052229 -0.0427459\n",
      "  -0.45918603  0.00684978 -0.00715536]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.17528509  0.48083756 -0.70223069  0.13339307  0.4638289  -0.57227942\n",
      "   0.00963417]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:19 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57080771]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 19 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.25328089 -0.18099244  0.09873673  0.51436454 -0.01864528 -0.05819923\n",
      "   0.54459014  0.11851126  0.00244451]\n",
      " [-0.21588132  0.33925015 -0.32008764 -0.64173006 -0.1131016  -0.11189928\n",
      "  -0.91377239 -0.28585429  0.09655904]\n",
      " [ 0.19334248 -0.38405086  0.2368182  -0.48125982  0.25628866 -0.42221654\n",
      "  -0.37329085  0.13527138  0.30462711]\n",
      " [ 0.48035216 -0.2987413   0.24388691  0.46794917  0.09201375  0.17852653\n",
      "   0.90461678 -0.19731887 -0.41190502]\n",
      " [-0.39612348  0.33167977 -0.191377   -0.29341018 -0.04679972 -0.17923514\n",
      "  -0.84705442 -0.16699748  0.20601637]\n",
      " [ 0.11238051  0.01017701  0.24139995 -0.48730126 -0.09002348 -0.04224709\n",
      "  -0.45918603  0.00684978 -0.00665655]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.10536507  0.44252827 -0.73080801  0.09227216  0.42258247 -0.59957315\n",
      "  -0.02482697]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:19 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.48074749]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 19 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.25331257 -0.18096077  0.09873673  0.51436454 -0.01861361 -0.05816756\n",
      "   0.54459014  0.11851126  0.00247619]\n",
      " [-0.21580522  0.33932625 -0.32008764 -0.64173006 -0.1130255  -0.11182318\n",
      "  -0.91377239 -0.28585429  0.09663514]\n",
      " [ 0.19412215 -0.38327119  0.2368182  -0.48125982  0.25706833 -0.42143688\n",
      "  -0.37329085  0.13527138  0.30540677]\n",
      " [ 0.47974867 -0.29934479  0.24388691  0.46794917  0.09141026  0.17792304\n",
      "   0.90461678 -0.19731887 -0.41250851]\n",
      " [-0.39485871  0.33294454 -0.191377   -0.29341018 -0.04553495 -0.17797038\n",
      "  -0.84705442 -0.16699748  0.20728114]\n",
      " [ 0.11262606  0.01042255  0.24139995 -0.48730126 -0.08977793 -0.04200154\n",
      "  -0.45918603  0.00684978 -0.006411  ]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.04536073  0.41255777 -0.76073408  0.06305001  0.39197665 -0.62830905\n",
      "  -0.05458358]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:19 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.56504358]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 19 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.24458805 -0.18968529  0.09873673  0.51436454 -0.02733813 -0.06689208\n",
      "   0.53586562  0.11851126  0.00247619]\n",
      " [-0.20204427  0.3530872  -0.32008764 -0.64173006 -0.09926455 -0.09806223\n",
      "  -0.90001144 -0.28585429  0.09663514]\n",
      " [ 0.20520892 -0.37218442  0.2368182  -0.48125982  0.2681551  -0.4103501\n",
      "  -0.36220408  0.13527138  0.30540677]\n",
      " [ 0.46441368 -0.31467978  0.24388691  0.46794917  0.07607527  0.16258805\n",
      "   0.88928179 -0.19731887 -0.41250851]\n",
      " [-0.38036555  0.34743771 -0.191377   -0.29341018 -0.03104178 -0.16347721\n",
      "  -0.83256125 -0.16699748  0.20728114]\n",
      " [ 0.12031967  0.01811617  0.24139995 -0.48730126 -0.08208432 -0.03430793\n",
      "  -0.45149242  0.00684978 -0.006411  ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.02407446  0.36868516 -0.77920265  0.04042139  0.33678675 -0.64523074\n",
      "  -0.08132373]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:19 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.57718625]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 19 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.25586411 -0.18968529  0.09873673  0.5256406  -0.02733813 -0.06689208\n",
      "   0.54714168  0.11851126  0.00247619]\n",
      " [-0.21344128  0.3530872  -0.32008764 -0.65312706 -0.09926455 -0.09806223\n",
      "  -0.91140845 -0.28585429  0.09663514]\n",
      " [ 0.19776126 -0.37218442  0.2368182  -0.48870748  0.2681551  -0.4103501\n",
      "  -0.36965174  0.13527138  0.30540677]\n",
      " [ 0.47567749 -0.31467978  0.24388691  0.47921298  0.07607527  0.16258805\n",
      "   0.9005456  -0.19731887 -0.41250851]\n",
      " [-0.39190987  0.34743771 -0.191377   -0.30495451 -0.03104178 -0.16347721\n",
      "  -0.84410558 -0.16699748  0.20728114]\n",
      " [ 0.11135076  0.01811617  0.24139995 -0.49627017 -0.08208432 -0.03430793\n",
      "  -0.46046133  0.00684978 -0.006411  ]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.02751775  0.40918307 -0.77152404  0.05825378  0.38119543 -0.63586767\n",
      "  -0.06553199]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:19 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.58270452]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 19 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.26381317 -0.18968529  0.09873673  0.5256406  -0.01938906 -0.05894301\n",
      "   0.55509075  0.11851126  0.00247619]\n",
      " [-0.22458765  0.3530872  -0.32008764 -0.65312706 -0.11041092 -0.1092086\n",
      "  -0.92255482 -0.28585429  0.09663514]\n",
      " [ 0.19387416 -0.37218442  0.2368182  -0.48870748  0.26426799 -0.41423721\n",
      "  -0.37353885  0.13527138  0.30540677]\n",
      " [ 0.48701547 -0.31467978  0.24388691  0.47921298  0.08741326  0.17392604\n",
      "   0.91188359 -0.19731887 -0.41250851]\n",
      " [-0.40321463  0.34743771 -0.191377   -0.30495451 -0.04234654 -0.17478197\n",
      "  -0.85541034 -0.16699748  0.20728114]\n",
      " [ 0.10575511  0.01811617  0.24139995 -0.49627017 -0.08767997 -0.03990358\n",
      "  -0.46605697  0.00684978 -0.006411  ]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.07825253  0.44318199 -0.76084659  0.07966984  0.42351274 -0.62607507\n",
      "  -0.04596453]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:19 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.56081261]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 19 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 2.52715824e-01 -1.89685287e-01  9.87367306e-02  5.14543248e-01\n",
      "  -1.93890639e-02 -7.00403611e-02  5.55090746e-01  1.18511257e-01\n",
      "  -8.62116228e-03]\n",
      " [-2.11900336e-01  3.53087197e-01 -3.20087644e-01 -6.40439751e-01\n",
      "  -1.10410920e-01 -9.65212932e-02 -9.22554815e-01 -2.85854293e-01\n",
      "   1.09322449e-01]\n",
      " [ 2.00567490e-01 -3.72184420e-01  2.36818197e-01 -4.82014143e-01\n",
      "   2.64267991e-01 -4.07543876e-01 -3.73538845e-01  1.35271381e-01\n",
      "   3.12100107e-01]\n",
      " [ 4.75978388e-01 -3.14679775e-01  2.43886910e-01  4.68175891e-01\n",
      "   8.74132609e-02  1.62888954e-01  9.11883587e-01 -1.97318870e-01\n",
      "  -4.23545598e-01]\n",
      " [-3.92784782e-01  3.47437706e-01 -1.91377002e-01 -2.94524655e-01\n",
      "  -4.23465422e-02 -1.64352118e-01 -8.55410342e-01 -1.66997479e-01\n",
      "   2.17710987e-01]\n",
      " [ 1.12948812e-01  1.81161653e-02  2.41399952e-01 -4.89076473e-01\n",
      "  -8.76799690e-02 -3.27098816e-02 -4.66056975e-01  6.84977719e-03\n",
      "   7.82693781e-04]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.00918794  0.3965316  -0.78094786  0.05201414  0.37694328 -0.64936561\n",
      "  -0.07307215]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:19 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.58371337]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 19 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 2.62973386e-01 -1.79427725e-01  9.87367306e-02  5.24800810e-01\n",
      "  -1.93890639e-02 -5.97827991e-02  5.65348308e-01  1.18511257e-01\n",
      "  -8.62116228e-03]\n",
      " [-2.23220580e-01  3.41766952e-01 -3.20087644e-01 -6.51759995e-01\n",
      "  -1.10410920e-01 -1.07841538e-01 -9.33875059e-01 -2.85854293e-01\n",
      "   1.09322449e-01]\n",
      " [ 1.89293919e-01 -3.83457991e-01  2.36818197e-01 -4.93287714e-01\n",
      "   2.64267991e-01 -4.18817447e-01 -3.84812417e-01  1.35271381e-01\n",
      "   3.12100107e-01]\n",
      " [ 4.87203062e-01 -3.03455102e-01  2.43886910e-01  4.79400564e-01\n",
      "   8.74132609e-02  1.74113628e-01  9.23108261e-01 -1.97318870e-01\n",
      "  -4.23545598e-01]\n",
      " [-4.03963551e-01  3.36258937e-01 -1.91377002e-01 -3.05703424e-01\n",
      "  -4.23465422e-02 -1.75530888e-01 -8.66589111e-01 -1.66997479e-01\n",
      "   2.17710987e-01]\n",
      " [ 1.03883406e-01  9.05075997e-03  2.41399952e-01 -4.98141878e-01\n",
      "  -8.76799690e-02 -4.17752870e-02 -4.75122380e-01  6.84977719e-03\n",
      "   7.82693781e-04]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.05976512  0.43412014 -0.77185875  0.06174343  0.41973605 -0.63903271\n",
      "  -0.0579995 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:19 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.61069]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 19 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 2.72772372e-01 -1.69628739e-01  9.87367306e-02  5.34599796e-01\n",
      "  -1.93890639e-02 -5.97827991e-02  5.75147294e-01  1.18511257e-01\n",
      "  -8.62116228e-03]\n",
      " [-2.33559112e-01  3.31428420e-01 -3.20087644e-01 -6.62098527e-01\n",
      "  -1.10410920e-01 -1.07841538e-01 -9.44213592e-01 -2.85854293e-01\n",
      "   1.09322449e-01]\n",
      " [ 1.79867437e-01 -3.92884473e-01  2.36818197e-01 -5.02714196e-01\n",
      "   2.64267991e-01 -4.18817447e-01 -3.94238899e-01  1.35271381e-01\n",
      "   3.12100107e-01]\n",
      " [ 4.97556915e-01 -2.93101249e-01  2.43886910e-01  4.89754417e-01\n",
      "   8.74132609e-02  1.74113628e-01  9.33462113e-01 -1.97318870e-01\n",
      "  -4.23545598e-01]\n",
      " [-4.13952354e-01  3.26270134e-01 -1.91377002e-01 -3.15692227e-01\n",
      "  -4.23465422e-02 -1.75530888e-01 -8.76577914e-01 -1.66997479e-01\n",
      "   2.17710987e-01]\n",
      " [ 9.55660186e-02  7.33372296e-04  2.41399952e-01 -5.06459266e-01\n",
      "  -8.76799690e-02 -4.17752870e-02 -4.83439768e-01  6.84977719e-03\n",
      "   7.82693781e-04]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.1060439   0.46946844 -0.76318678  0.07354326  0.45815155 -0.62864606\n",
      "  -0.04424219]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:19 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.64923522]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 19 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 2.80091645e-01 -1.69628739e-01  1.06056004e-01  5.34599796e-01\n",
      "  -1.93890639e-02 -5.24635259e-02  5.82466567e-01  1.18511257e-01\n",
      "  -8.62116228e-03]\n",
      " [-2.42488410e-01  3.31428420e-01 -3.29016941e-01 -6.62098527e-01\n",
      "  -1.10410920e-01 -1.16770835e-01 -9.53142889e-01 -2.85854293e-01\n",
      "   1.09322449e-01]\n",
      " [ 1.76061179e-01 -3.92884473e-01  2.33011940e-01 -5.02714196e-01\n",
      "   2.64267991e-01 -4.22623704e-01 -3.98045156e-01  1.35271381e-01\n",
      "   3.12100107e-01]\n",
      " [ 5.06233843e-01 -2.93101249e-01  2.52563839e-01  4.89754417e-01\n",
      "   8.74132609e-02  1.82790557e-01  9.42139042e-01 -1.97318870e-01\n",
      "  -4.23545598e-01]\n",
      " [-4.22854073e-01  3.26270134e-01 -2.00278721e-01 -3.15692227e-01\n",
      "  -4.23465422e-02 -1.84432607e-01 -8.85479632e-01 -1.66997479e-01\n",
      "   2.17710987e-01]\n",
      " [ 9.37029237e-02  7.33372296e-04  2.39536857e-01 -5.06459266e-01\n",
      "  -8.76799690e-02 -4.36383819e-02 -4.85302863e-01  6.84977719e-03\n",
      "   7.82693781e-04]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.14598353  0.49775546 -0.75650942  0.08960637  0.49265987 -0.62225148\n",
      "  -0.0261465 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:19 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.55091214]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 20 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.27027623 -0.17944416  0.106056    0.52478438 -0.01938906 -0.05246353\n",
      "   0.58246657  0.11851126 -0.01843658]\n",
      " [-0.23499606  0.33892077 -0.32901694 -0.65460618 -0.11041092 -0.11677084\n",
      "  -0.95314289 -0.28585429  0.1168148 ]\n",
      " [ 0.18272259 -0.38622306  0.23301194 -0.49605278  0.26426799 -0.4226237\n",
      "  -0.39804516  0.13527138  0.31876152]\n",
      " [ 0.50156621 -0.29776888  0.25256384  0.48508679  0.08741326  0.18279056\n",
      "   0.94213904 -0.19731887 -0.42821323]\n",
      " [-0.41957034  0.32955386 -0.20027872 -0.3124085  -0.04234654 -0.18443261\n",
      "  -0.88547963 -0.16699748  0.22099472]\n",
      " [ 0.10042135  0.0074518   0.23953686 -0.49974084 -0.08767997 -0.04363838\n",
      "  -0.48530286  0.00684978  0.00750112]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.07783351  0.45318885 -0.78282052  0.06237861  0.45385629 -0.653022\n",
      "  -0.0533121 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:20 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.63094752]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 20 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.278529   -0.17944416  0.11430878  0.52478438 -0.01938906 -0.05246353\n",
      "   0.59071934  0.11851126 -0.01843658]\n",
      " [-0.24461288  0.33892077 -0.33863376 -0.65460618 -0.11041092 -0.11677084\n",
      "  -0.96275971 -0.28585429  0.1168148 ]\n",
      " [ 0.18291259 -0.38622306  0.23320194 -0.49605278  0.26426799 -0.4226237\n",
      "  -0.39785515  0.13527138  0.31876152]\n",
      " [ 0.51110988 -0.29776888  0.26210751  0.48508679  0.08741326  0.18279056\n",
      "   0.95168271 -0.19731887 -0.42821323]\n",
      " [-0.42918452  0.32955386 -0.2098929  -0.3124085  -0.04234654 -0.18443261\n",
      "  -0.89509381 -0.16699748  0.22099472]\n",
      " [ 0.0988683   0.0074518   0.2379838  -0.49974084 -0.08767997 -0.04363838\n",
      "  -0.48685592  0.00684978  0.00750112]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.12080095  0.48424921 -0.77509151  0.08405234  0.49016579 -0.64521774\n",
      "  -0.03338691]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:20 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.63233684]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 20 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.26590537 -0.19206779  0.10168514  0.52478438 -0.01938906 -0.05246353\n",
      "   0.57809571  0.11851126 -0.01843658]\n",
      " [-0.22888895  0.3546447  -0.32290983 -0.65460618 -0.11041092 -0.11677084\n",
      "  -0.94703578 -0.28585429  0.1168148 ]\n",
      " [ 0.18945058 -0.37968508  0.23973992 -0.49605278  0.26426799 -0.4226237\n",
      "  -0.39131717  0.13527138  0.31876152]\n",
      " [ 0.49473626 -0.31414251  0.24573388  0.48508679  0.08741326  0.18279056\n",
      "   0.93530909 -0.19731887 -0.42821323]\n",
      " [-0.41347185  0.34526653 -0.19418023 -0.3124085  -0.04234654 -0.18443261\n",
      "  -0.87938114 -0.16699748  0.22099472]\n",
      " [ 0.10147461  0.01005811  0.24059011 -0.49974084 -0.08767997 -0.04363838\n",
      "  -0.48424961  0.00684978  0.00750112]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.04729592  0.43346799 -0.79201363  0.05398635  0.43088736 -0.66217203\n",
      "  -0.06752428]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:20 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.5336895]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 20 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.2725561  -0.19206779  0.10833588  0.52478438 -0.01938906 -0.05246353\n",
      "   0.57809571  0.12516199 -0.01843658]\n",
      " [-0.23913507  0.3546447  -0.33315595 -0.65460618 -0.11041092 -0.11677084\n",
      "  -0.94703578 -0.29610041  0.1168148 ]\n",
      " [ 0.1970196  -0.37968508  0.24730895 -0.49605278  0.26426799 -0.4226237\n",
      "  -0.39131717  0.14284041  0.31876152]\n",
      " [ 0.50206155 -0.31414251  0.25305918  0.48508679  0.08741326  0.18279056\n",
      "   0.93530909 -0.18999357 -0.42821323]\n",
      " [-0.42317847  0.34526653 -0.20388685 -0.3124085  -0.04234654 -0.18443261\n",
      "  -0.87938114 -0.1767041   0.22099472]\n",
      " [ 0.10638501  0.01005811  0.24550051 -0.49974084 -0.08767997 -0.04363838\n",
      "  -0.48424961  0.01176018  0.00750112]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.1053201   0.46939585 -0.77448862  0.09097585  0.46759028 -0.6438669\n",
      "  -0.03350155]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:20 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.56760689]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 20 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.28146716 -0.19206779  0.10833588  0.53369543 -0.01938906 -0.05246353\n",
      "   0.57809571  0.12516199 -0.00952552]\n",
      " [-0.24803002  0.3546447  -0.33315595 -0.66350112 -0.11041092 -0.11677084\n",
      "  -0.94703578 -0.29610041  0.10791985]\n",
      " [ 0.19728128 -0.37968508  0.24730895 -0.49579111  0.26426799 -0.4226237\n",
      "  -0.39131717  0.14284041  0.3190232 ]\n",
      " [ 0.50892575 -0.31414251  0.25305918  0.49195098  0.08741326  0.18279056\n",
      "   0.93530909 -0.18999357 -0.42134903]\n",
      " [-0.42957198  0.34526653 -0.20388685 -0.31880201 -0.04234654 -0.18443261\n",
      "  -0.87938114 -0.1767041   0.21460121]\n",
      " [ 0.10145243  0.01005811  0.24550051 -0.50467341 -0.08767997 -0.04363838\n",
      "  -0.48424961  0.01176018  0.00256855]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.15838107  0.50576616 -0.75777533  0.11776803  0.501348   -0.62401585\n",
      "  -0.01202695]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:20 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.55986988]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 20 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.27805091 -0.19206779  0.10833588  0.53369543 -0.02280531 -0.05587977\n",
      "   0.57809571  0.12516199 -0.01294177]\n",
      " [-0.24190494  0.3546447  -0.33315595 -0.66350112 -0.10428585 -0.11064576\n",
      "  -0.94703578 -0.29610041  0.11404493]\n",
      " [ 0.191302   -0.37968508  0.24730895 -0.49579111  0.25828872 -0.42860298\n",
      "  -0.39131717  0.14284041  0.31304392]\n",
      " [ 0.50294911 -0.31414251  0.25305918  0.49195098  0.08143662  0.17681391\n",
      "   0.93530909 -0.18999357 -0.42732567]\n",
      " [-0.42231385  0.34526653 -0.20388685 -0.31880201 -0.03508842 -0.17717448\n",
      "  -0.87938114 -0.1767041   0.22185933]\n",
      " [ 0.10192309  0.01005811  0.24550051 -0.50467341 -0.08720931 -0.04316772\n",
      "  -0.48424961  0.01176018  0.00303921]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.08940074  0.46783691 -0.78600178  0.07717008  0.46075286 -0.65100952\n",
      "  -0.0460464 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:20 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.46513298]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 20 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.27813255 -0.19198615  0.10833588  0.53369543 -0.02272367 -0.05579814\n",
      "   0.57809571  0.12516199 -0.01286013]\n",
      " [-0.24207639  0.35447325 -0.33315595 -0.66350112 -0.10445729 -0.11081721\n",
      "  -0.94703578 -0.29610041  0.11387348]\n",
      " [ 0.19196202 -0.37902506  0.24730895 -0.49579111  0.25894874 -0.42794296\n",
      "  -0.39131717  0.14284041  0.31370394]\n",
      " [ 0.50266373 -0.31442789  0.25305918  0.49195098  0.08115124  0.17652853\n",
      "   0.93530909 -0.18999357 -0.42761106]\n",
      " [-0.4213393   0.34624108 -0.20388685 -0.31880201 -0.03411387 -0.17619993\n",
      "  -0.87938114 -0.1767041   0.22283388]\n",
      " [ 0.10214521  0.01028023  0.24550051 -0.50467341 -0.0869872  -0.04294561\n",
      "  -0.48424961  0.01176018  0.00326133]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.03154185  0.4389891  -0.81510267  0.04890089  0.43153801 -0.67896367\n",
      "  -0.07475372]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:20 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.56787365]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 20 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.26875721 -0.20136149  0.10833588  0.53369543 -0.03209901 -0.06517347\n",
      "   0.56872037  0.12516199 -0.01286013]\n",
      " [-0.22802824  0.3685214  -0.33315595 -0.66350112 -0.09040914 -0.09676906\n",
      "  -0.93298763 -0.29610041  0.11387348]\n",
      " [ 0.20331939 -0.36766769  0.24730895 -0.49579111  0.2703061  -0.41658559\n",
      "  -0.37995981  0.14284041  0.31370394]\n",
      " [ 0.48721873 -0.32987288  0.25305918  0.49195098  0.06570624  0.16108354\n",
      "   0.91986409 -0.18999357 -0.42761106]\n",
      " [-0.40662937  0.36095101 -0.20388685 -0.31880201 -0.01940394 -0.16149\n",
      "  -0.86467121 -0.1767041   0.22283388]\n",
      " [ 0.11035751  0.01849253  0.24550051 -0.50467341 -0.07877489 -0.0347333\n",
      "  -0.4760373   0.01176018  0.00326133]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.03813431  0.39423035 -0.83316564  0.02650741  0.37585369 -0.69553249\n",
      "  -0.10103054]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:20 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.58633043]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 20 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.27986317 -0.20136149  0.10833588  0.5448014  -0.03209901 -0.06517347\n",
      "   0.57982633  0.12516199 -0.01286013]\n",
      " [-0.23897547  0.3685214  -0.33315595 -0.67444836 -0.09040914 -0.09676906\n",
      "  -0.94393486 -0.29610041  0.11387348]\n",
      " [ 0.19577173 -0.36766769  0.24730895 -0.50333877  0.2703061  -0.41658559\n",
      "  -0.38750747  0.14284041  0.31370394]\n",
      " [ 0.49800903 -0.32987288  0.25305918  0.50274128  0.06570624  0.16108354\n",
      "   0.93065439 -0.18999357 -0.42761106]\n",
      " [-0.41785182  0.36095101 -0.20388685 -0.33002446 -0.01940394 -0.16149\n",
      "  -0.87589366 -0.1767041   0.22283388]\n",
      " [ 0.10127343  0.01849253  0.24550051 -0.51375749 -0.07877489 -0.0347333\n",
      "  -0.48512138  0.01176018  0.00326133]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.01203286  0.43424216 -0.82619779  0.04346154  0.4194882  -0.6870357\n",
      "  -0.08622215]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:20 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.58673607]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 20 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.28814947 -0.20136149  0.10833588  0.5448014  -0.02381271 -0.05688718\n",
      "   0.58811263  0.12516199 -0.01286013]\n",
      " [-0.25006571  0.3685214  -0.33315595 -0.67444836 -0.10149938 -0.1078593\n",
      "  -0.9550251  -0.29610041  0.11387348]\n",
      " [ 0.19165646 -0.36766769  0.24730895 -0.50333877  0.26619084 -0.42070086\n",
      "  -0.39162273  0.14284041  0.31370394]\n",
      " [ 0.50917774 -0.32987288  0.25305918  0.50274128  0.07687495  0.17225225\n",
      "   0.9418231  -0.18999357 -0.42761106]\n",
      " [-0.4290493   0.36095101 -0.20388685 -0.33002446 -0.03060142 -0.17268748\n",
      "  -0.88709114 -0.1767041   0.22283388]\n",
      " [ 0.09541353  0.01849253  0.24550051 -0.51375749 -0.08463479 -0.04059321\n",
      "  -0.49098129  0.01176018  0.00326133]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.06213633  0.46840663 -0.81604655  0.06431919  0.46155641 -0.67770451\n",
      "  -0.06727492]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:20 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.55943062]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 20 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.27673927 -0.20136149  0.10833588  0.5333912  -0.02381271 -0.06829737\n",
      "   0.58811263  0.12516199 -0.02427033]\n",
      " [-0.23715506  0.3685214  -0.33315595 -0.6615377  -0.10149938 -0.09494865\n",
      "  -0.9550251  -0.29610041  0.12678413]\n",
      " [ 0.19856528 -0.36766769  0.24730895 -0.49642996  0.26619084 -0.41379204\n",
      "  -0.39162273  0.14284041  0.32061275]\n",
      " [ 0.49783958 -0.32987288  0.25305918  0.49140312  0.07687495  0.16091409\n",
      "   0.9418231  -0.18999357 -0.43894921]\n",
      " [-0.41824597  0.36095101 -0.20388685 -0.31922113 -0.03060142 -0.16188415\n",
      "  -0.88709114 -0.1767041   0.23363721]\n",
      " [ 0.10287322  0.01849253  0.24550051 -0.5062978  -0.08463479 -0.03313351\n",
      "  -0.49098129  0.01176018  0.01072102]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.00680455  0.42138558 -0.83571287  0.03696119  0.4146348  -0.70044368\n",
      "  -0.09402481]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:20 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.59070951]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 20 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.28705171 -0.19104905  0.10833588  0.54370364 -0.02381271 -0.05798493\n",
      "   0.59842507  0.12516199 -0.02427033]\n",
      " [-0.24822642  0.35745004 -0.33315595 -0.67260906 -0.10149938 -0.10602\n",
      "  -0.96609646 -0.29610041  0.12678413]\n",
      " [ 0.18750983 -0.37872314  0.24730895 -0.5074854   0.26619084 -0.42484749\n",
      "  -0.40267818  0.14284041  0.32061275]\n",
      " [ 0.50874258 -0.31896988  0.25305918  0.50230612  0.07687495  0.17181709\n",
      "   0.9527261  -0.18999357 -0.43894921]\n",
      " [-0.42926573  0.34993125 -0.20388685 -0.33024089 -0.03060142 -0.17290391\n",
      "  -0.8981109  -0.1767041   0.23363721]\n",
      " [ 0.09366574  0.00928505  0.24550051 -0.51550528 -0.08463479 -0.042341\n",
      "  -0.50018877  0.01176018  0.01072102]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.0426729   0.45876963 -0.82726336  0.04620351  0.45686249 -0.69085485\n",
      "  -0.07981514]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:20 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.61669759]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 20 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 2.96825194e-01 -1.81275572e-01  1.08335876e-01  5.53477121e-01\n",
      "  -2.38127131e-02 -5.79849301e-02  6.08198549e-01  1.25161991e-01\n",
      "  -2.42703287e-02]\n",
      " [-2.58367732e-01  3.47308728e-01 -3.33155953e-01 -6.82750375e-01\n",
      "  -1.01499384e-01 -1.06020001e-01 -9.76237774e-01 -2.96100415e-01\n",
      "   1.26784133e-01]\n",
      " [ 1.78167362e-01 -3.88065605e-01  2.47308949e-01 -5.16827870e-01\n",
      "   2.66190838e-01 -4.24847488e-01 -4.12020647e-01  1.42840407e-01\n",
      "   3.20612755e-01]\n",
      " [ 5.18848894e-01 -3.08863572e-01  2.53059182e-01  5.12412434e-01\n",
      "   7.68749505e-02  1.71817092e-01  9.62832410e-01 -1.89993571e-01\n",
      "  -4.38949214e-01]\n",
      " [-4.39192233e-01  3.40004752e-01 -2.03886846e-01 -3.40167389e-01\n",
      "  -3.06014158e-02 -1.72903910e-01 -9.08037398e-01 -1.76704098e-01\n",
      "   2.33637211e-01]\n",
      " [ 8.52141716e-02  8.33484106e-04  2.45500509e-01 -5.23956846e-01\n",
      "  -8.46347931e-02 -4.23410000e-02 -5.08640337e-01  1.17601751e-02\n",
      "   1.07210176e-02]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.08797573  0.49389013 -0.81919631  0.05750575  0.49483476 -0.68121104\n",
      "  -0.06683865]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:20 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.65388819]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 20 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 3.04332339e-01 -1.81275572e-01  1.15843022e-01  5.53477121e-01\n",
      "  -2.38127131e-02 -5.04777849e-02  6.15705694e-01  1.25161991e-01\n",
      "  -2.42703287e-02]\n",
      " [-2.67085335e-01  3.47308728e-01 -3.41873555e-01 -6.82750375e-01\n",
      "  -1.01499384e-01 -1.14737604e-01 -9.84955377e-01 -2.96100415e-01\n",
      "   1.26784133e-01]\n",
      " [ 1.74304984e-01 -3.88065605e-01  2.43446571e-01 -5.16827870e-01\n",
      "   2.66190838e-01 -4.28709866e-01 -4.15883025e-01  1.42840407e-01\n",
      "   3.20612755e-01]\n",
      " [ 5.27259320e-01 -3.08863572e-01  2.61469609e-01  5.12412434e-01\n",
      "   7.68749505e-02  1.80227518e-01  9.71242836e-01 -1.89993571e-01\n",
      "  -4.38949214e-01]\n",
      " [-4.47865232e-01  3.40004752e-01 -2.12559845e-01 -3.40167389e-01\n",
      "  -3.06014158e-02 -1.81576909e-01 -9.16710397e-01 -1.76704098e-01\n",
      "   2.33637211e-01]\n",
      " [ 8.30833945e-02  8.33484106e-04  2.43369732e-01 -5.23956846e-01\n",
      "  -8.46347931e-02 -4.44717771e-02 -5.10771114e-01  1.17601751e-02\n",
      "   1.07210176e-02]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.12714147  0.52217543 -0.81301123  0.07311637  0.52893371 -0.67528315\n",
      "  -0.04940383]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:20 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.54234341]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 21 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.29444623 -0.19116168  0.11584302  0.54359101 -0.02381271 -0.05047778\n",
      "   0.61570569  0.12516199 -0.03415644]\n",
      " [-0.25951656  0.3548775  -0.34187356 -0.6751816  -0.10149938 -0.1147376\n",
      "  -0.98495538 -0.29610041  0.13435291]\n",
      " [ 0.18092157 -0.38144902  0.24344657 -0.51021128  0.26619084 -0.42870987\n",
      "  -0.41588303  0.14284041  0.32722934]\n",
      " [ 0.52245142 -0.31367148  0.26146961  0.50760453  0.07687495  0.18022752\n",
      "   0.97124284 -0.18999357 -0.44375712]\n",
      " [-0.44429889  0.3435711  -0.21255984 -0.33660104 -0.03060142 -0.18157691\n",
      "  -0.9167104  -0.1767041   0.23720356]\n",
      " [ 0.0899845   0.00773459  0.24336973 -0.51705574 -0.08463479 -0.04447178\n",
      "  -0.51077111  0.01176018  0.01762212]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.05983475  0.47791988 -0.83880706  0.04626651  0.49040389 -0.70534278\n",
      "  -0.07594213]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:21 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.63524026]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 21 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.30286798 -0.19116168  0.12426477  0.54359101 -0.02381271 -0.05047778\n",
      "   0.62412744  0.12516199 -0.03415644]\n",
      " [-0.26897111  0.3548775  -0.3513281  -0.6751816  -0.10149938 -0.1147376\n",
      "  -0.99440992 -0.29610041  0.13435291]\n",
      " [ 0.18101121 -0.38144902  0.24353621 -0.51021128  0.26619084 -0.42870987\n",
      "  -0.41579338  0.14284041  0.32722934]\n",
      " [ 0.53177269 -0.31367148  0.27079088  0.50760453  0.07687495  0.18022752\n",
      "   0.98056411 -0.18999357 -0.44375712]\n",
      " [-0.45375651  0.3435711  -0.22201747 -0.33660104 -0.03060142 -0.18157691\n",
      "  -0.92616802 -0.1767041   0.23720356]\n",
      " [ 0.0881248   0.00773459  0.24151003 -0.51705574 -0.08463479 -0.04447178\n",
      "  -0.51263082  0.01176018  0.01762212]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.102094    0.50902855 -0.83162727  0.06748578  0.52643425 -0.69808652\n",
      "  -0.05668198]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:21 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.63440764]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 21 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.28964794 -0.20438172  0.11104473  0.54359101 -0.02381271 -0.05047778\n",
      "   0.6109074   0.12516199 -0.03415644]\n",
      " [-0.25301415  0.37083446 -0.33537114 -0.6751816  -0.10149938 -0.1147376\n",
      "  -0.97845296 -0.29610041  0.13435291]\n",
      " [ 0.18763342 -0.37482681  0.25015842 -0.51021128  0.26619084 -0.42870987\n",
      "  -0.40917117  0.14284041  0.32722934]\n",
      " [ 0.51533511 -0.33010906  0.2543533   0.50760453  0.07687495  0.18022752\n",
      "   0.96412653 -0.18999357 -0.44375712]\n",
      " [-0.43780506  0.35952255 -0.20606602 -0.33660104 -0.03060142 -0.18157691\n",
      "  -0.91021657 -0.1767041   0.23720356]\n",
      " [ 0.09132369  0.01093348  0.24470892 -0.51705574 -0.08463479 -0.04447178\n",
      "  -0.50943193  0.01176018  0.01762212]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.02852346  0.45733192 -0.84788657  0.0374771   0.46662327 -0.71436433\n",
      "  -0.09025195]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:21 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.52998154]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 21 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.29683552 -0.20438172  0.11823231  0.54359101 -0.02381271 -0.05047778\n",
      "   0.6109074   0.13234957 -0.03415644]\n",
      " [-0.26372404  0.37083446 -0.34608103 -0.6751816  -0.10149938 -0.1147376\n",
      "  -0.97845296 -0.3068103   0.13435291]\n",
      " [ 0.19545331 -0.37482681  0.25797832 -0.51021128  0.26619084 -0.42870987\n",
      "  -0.40917117  0.1506603   0.32722934]\n",
      " [ 0.52314444 -0.33010906  0.26216263  0.50760453  0.07687495  0.18022752\n",
      "   0.96412653 -0.18218424 -0.44375712]\n",
      " [-0.44799977  0.35952255 -0.21626073 -0.33660104 -0.03060142 -0.18157691\n",
      "  -0.91021657 -0.18689881  0.23720356]\n",
      " [ 0.09626285  0.01093348  0.24964809 -0.51705574 -0.08463479 -0.04447178\n",
      "  -0.50943193  0.01669934  0.01762212]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.08706452  0.4941259  -0.830778    0.07501438  0.50414792 -0.69647175\n",
      "  -0.05594208]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:21 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.56365479]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 21 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.30606796 -0.20438172  0.11823231  0.55282345 -0.02381271 -0.05047778\n",
      "   0.6109074   0.13234957 -0.024924  ]\n",
      " [-0.27294286  0.37083446 -0.34608103 -0.68440043 -0.10149938 -0.1147376\n",
      "  -0.97845296 -0.3068103   0.12513408]\n",
      " [ 0.19562061 -0.37482681  0.25797832 -0.51004399  0.26619084 -0.42870987\n",
      "  -0.40917117  0.1506603   0.32739663]\n",
      " [ 0.53037764 -0.33010906  0.26216263  0.51483774  0.07687495  0.18022752\n",
      "   0.96412653 -0.18218424 -0.43652391]\n",
      " [-0.45481925  0.35952255 -0.21626073 -0.34342052 -0.03060142 -0.18157691\n",
      "  -0.91021657 -0.18689881  0.23038408]\n",
      " [ 0.09106837  0.01093348  0.24964809 -0.52225023 -0.08463479 -0.04447178\n",
      "  -0.50943193  0.01669934  0.01242764]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.14072365  0.53122121 -0.81419451  0.10201124  0.53863328 -0.67680737\n",
      "  -0.03444887]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:21 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.5482044]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 21 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Theta One: \n",
      "[[ 0.3025945  -0.20438172  0.11823231  0.55282345 -0.02728617 -0.05395125\n",
      "   0.6109074   0.13234957 -0.02839746]\n",
      " [-0.26696449  0.37083446 -0.34608103 -0.68440043 -0.09552102 -0.10875924\n",
      "  -0.97845296 -0.3068103   0.13111245]\n",
      " [ 0.18969673 -0.37482681  0.25797832 -0.51004399  0.26026696 -0.43463374\n",
      "  -0.40917117  0.1506603   0.32147276]\n",
      " [ 0.52460088 -0.33010906  0.26216263  0.51483774  0.07109818  0.17445075\n",
      "   0.96412653 -0.18218424 -0.44230068]\n",
      " [-0.44775121  0.35952255 -0.21626073 -0.34342052 -0.02353338 -0.17450887\n",
      "  -0.91021657 -0.18689881  0.23745212]\n",
      " [ 0.09150296  0.01093348  0.24964809 -0.52225023 -0.0842002  -0.04403718\n",
      "  -0.50943193  0.01669934  0.01286223]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.07283503  0.49377861 -0.84202753  0.06201391  0.49879288 -0.70345693\n",
      "  -0.06795854]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:21 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.44854928]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 21 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.3027529  -0.20422332  0.11823231  0.55282345 -0.02712777 -0.05379284\n",
      "   0.6109074   0.13234957 -0.02823906]\n",
      " [-0.26739019  0.37040876 -0.34608103 -0.68440043 -0.09594672 -0.10918494\n",
      "  -0.97845296 -0.3068103   0.13068675]\n",
      " [ 0.19022389 -0.37429965  0.25797832 -0.51004399  0.26079412 -0.43410658\n",
      "  -0.40917117  0.1506603   0.32199992]\n",
      " [ 0.52463222 -0.33007771  0.26216263  0.51483774  0.07112952  0.17448209\n",
      "   0.96412653 -0.18218424 -0.44226934]\n",
      " [-0.44707456  0.3601992  -0.21626073 -0.34342052 -0.02285672 -0.17383222\n",
      "  -0.91021657 -0.18689881  0.23812877]\n",
      " [ 0.0916824   0.01111292  0.24964809 -0.52225023 -0.08402076 -0.04385775\n",
      "  -0.50943193  0.01669934  0.01304167]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.01736006  0.46619954 -0.87019078  0.03480371  0.47108674 -0.73051749\n",
      "  -0.09551659]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:21 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57075963]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 21 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.29278436 -0.21419186  0.11823231  0.55282345 -0.03709631 -0.06376138\n",
      "   0.60093887  0.13234957 -0.02823906]\n",
      " [-0.25309725  0.38470171 -0.34608103 -0.68440043 -0.08165377 -0.09489199\n",
      "  -0.96416002 -0.3068103   0.13068675]\n",
      " [ 0.20183215 -0.3626914   0.25797832 -0.51004399  0.27240238 -0.42249833\n",
      "  -0.39756292  0.1506603   0.32199992]\n",
      " [ 0.50909253 -0.3456174   0.26216263  0.51483774  0.05558984  0.1589424\n",
      "   0.94858684 -0.18218424 -0.44226934]\n",
      " [-0.43217712  0.37509664 -0.21626073 -0.34342052 -0.00795929 -0.15893478\n",
      "  -0.89531913 -0.18689881  0.23812877]\n",
      " [ 0.10038841  0.01981893  0.24964809 -0.52225023 -0.07531475 -0.03515173\n",
      "  -0.50072591  0.01669934  0.01304167]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.05255602  0.42060356 -0.88790744  0.01262466  0.41495363 -0.74677881\n",
      "  -0.12134809]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:21 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.5958666]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 21 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.3036404  -0.21419186  0.11823231  0.56367949 -0.03709631 -0.06376138\n",
      "   0.61179491  0.13234957 -0.02823906]\n",
      " [-0.26355742  0.38470171 -0.34608103 -0.6948606  -0.08165377 -0.09489199\n",
      "  -0.97462019 -0.3068103   0.13068675]\n",
      " [ 0.19423276 -0.3626914   0.25797832 -0.51764338  0.27240238 -0.42249833\n",
      "  -0.40516231  0.1506603   0.32199992]\n",
      " [ 0.51938234 -0.3456174   0.26216263  0.52512755  0.05558984  0.1589424\n",
      "   0.95887665 -0.18218424 -0.44226934]\n",
      " [-0.44301055  0.37509664 -0.21626073 -0.35425395 -0.00795929 -0.15893478\n",
      "  -0.90615256 -0.18689881  0.23812877]\n",
      " [ 0.09125139  0.01981893  0.24964809 -0.53138725 -0.07531475 -0.03515173\n",
      "  -0.50986293  0.01669934  0.01304167]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.00389642  0.45999187 -0.88158554  0.02870827  0.45767098 -0.7390759\n",
      "  -0.10750779]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:21 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.59097296]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 21 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.31220643 -0.21419186  0.11823231  0.56367949 -0.02853028 -0.05519535\n",
      "   0.62036093  0.13234957 -0.02823906]\n",
      " [-0.27455683  0.38470171 -0.34608103 -0.6948606  -0.09265318 -0.1058914\n",
      "  -0.9856196  -0.3068103   0.13068675]\n",
      " [ 0.18991315 -0.3626914   0.25797832 -0.51764338  0.26808277 -0.42681793\n",
      "  -0.40948191  0.1506603   0.32199992]\n",
      " [ 0.53036653 -0.3456174   0.26216263  0.52512755  0.06657402  0.16992659\n",
      "   0.96986084 -0.18218424 -0.44226934]\n",
      " [-0.45407486  0.37509664 -0.21626073 -0.35425395 -0.01902359 -0.16999909\n",
      "  -0.91721687 -0.18689881  0.23812877]\n",
      " [ 0.0851496   0.01981893  0.24964809 -0.53138725 -0.08141654 -0.04125352\n",
      "  -0.51596472  0.01669934  0.01304167]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.04553939  0.49425505 -0.87192132  0.04901212  0.49942401 -0.73017533\n",
      "  -0.08918036]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:21 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.55816074]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 21 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.30050615 -0.21419186  0.11823231  0.5519792  -0.02853028 -0.06689563\n",
      "   0.62036093  0.13234957 -0.03993934]\n",
      " [-0.26145081  0.38470171 -0.34608103 -0.68175458 -0.09265318 -0.09278538\n",
      "  -0.9856196  -0.3068103   0.14379277]\n",
      " [ 0.19701825 -0.3626914   0.25797832 -0.51053828  0.26808277 -0.41971284\n",
      "  -0.40948191  0.1506603   0.32910501]\n",
      " [ 0.5187632  -0.3456174   0.26216263  0.51352422  0.06657402  0.15832326\n",
      "   0.96986084 -0.18218424 -0.45387267]\n",
      " [-0.44293538  0.37509664 -0.21626073 -0.34311447 -0.01902359 -0.15885961\n",
      "  -0.91721687 -0.18689881  0.24926825]\n",
      " [ 0.09287052  0.01981893  0.24964809 -0.52366634 -0.08141654 -0.03353261\n",
      "  -0.51596472  0.01669934  0.02076258]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.02328666  0.44687812 -0.89119039  0.02192783  0.45218463 -0.75240344\n",
      "  -0.11557953]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:21 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.59783662]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 21 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.31080112 -0.20389688  0.11823231  0.56227418 -0.02853028 -0.05660066\n",
      "   0.63065591  0.13234957 -0.03993934]\n",
      " [-0.27224153  0.37391098 -0.34608103 -0.6925453  -0.09265318 -0.10357611\n",
      "  -0.99641032 -0.3068103   0.14379277]\n",
      " [ 0.18620064 -0.373509    0.25797832 -0.52135589  0.26808277 -0.43053044\n",
      "  -0.42029952  0.1506603   0.32910501]\n",
      " [ 0.52933045 -0.33505015  0.26216263  0.52409147  0.06657402  0.16889051\n",
      "   0.98042809 -0.18218424 -0.45387267]\n",
      " [-0.45374616  0.36428586 -0.21626073 -0.35392525 -0.01902359 -0.16967038\n",
      "  -0.92802765 -0.18689881  0.24926825]\n",
      " [ 0.08357547  0.01052388  0.24964809 -0.53296138 -0.08141654 -0.04282766\n",
      "  -0.52525977  0.01669934  0.02076258]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.025059    0.48396192 -0.88331555  0.0307084   0.49375937 -0.74349004\n",
      "  -0.10219826]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:21 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.62284262]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 21 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.32049288 -0.19420513  0.11823231  0.57196593 -0.02853028 -0.05660066\n",
      "   0.64034766  0.13234957 -0.03993934]\n",
      " [-0.28215214  0.36400038 -0.34608103 -0.70245591 -0.09265318 -0.10357611\n",
      "  -1.00632093 -0.3068103   0.14379277]\n",
      " [ 0.17696623 -0.38274342  0.25797832 -0.5305903   0.26808277 -0.43053044\n",
      "  -0.42953393  0.1506603   0.32910501]\n",
      " [ 0.53916735 -0.32521325  0.26216263  0.53392837  0.06657402  0.16889051\n",
      "   0.99026499 -0.18218424 -0.45387267]\n",
      " [-0.46355334  0.35447868 -0.21626073 -0.36373243 -0.01902359 -0.16967038\n",
      "  -0.93783483 -0.18689881  0.24926825]\n",
      " [ 0.07504004  0.00198845  0.24964809 -0.54149681 -0.08141654 -0.04282766\n",
      "  -0.5337952   0.01669934  0.02076258]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.06935797  0.51877236 -0.87579509  0.04153295  0.53120873 -0.73452431\n",
      "  -0.08996937]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:21 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.6586543]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 21 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.3281264  -0.19420513  0.12586583  0.57196593 -0.02853028 -0.04896714\n",
      "   0.64798118  0.13234957 -0.03993934]\n",
      " [-0.29063486  0.36400038 -0.35456375 -0.70245591 -0.09265318 -0.11205883\n",
      "  -1.01480365 -0.3068103   0.14379277]\n",
      " [ 0.17306688 -0.38274342  0.25407897 -0.5305903   0.26808277 -0.43442979\n",
      "  -0.43343328  0.1506603   0.32910501]\n",
      " [ 0.54730578 -0.32521325  0.27030106  0.53392837  0.06657402  0.17702894\n",
      "   0.99840342 -0.18218424 -0.45387267]\n",
      " [-0.47197795  0.35447868 -0.22468535 -0.36373243 -0.01902359 -0.178095\n",
      "  -0.94625944 -0.18689881  0.24926825]\n",
      " [ 0.07266117  0.00198845  0.24726921 -0.54149681 -0.08141654 -0.04520653\n",
      "  -0.53617407  0.01669934  0.02076258]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.10773014  0.54699333 -0.87005651  0.05670118  0.56484523 -0.72902167\n",
      "  -0.0731874 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:21 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.53344039]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 22 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.31821483 -0.2041167   0.12586583  0.56205436 -0.02853028 -0.04896714\n",
      "   0.64798118  0.13234957 -0.04985091]\n",
      " [-0.28303731  0.37159793 -0.35456375 -0.69485836 -0.09265318 -0.11205883\n",
      "  -1.01480365 -0.3068103   0.15139032]\n",
      " [ 0.17960985 -0.37620045  0.25407897 -0.52404734  0.26808277 -0.43442979\n",
      "  -0.43343328  0.1506603   0.33564798]\n",
      " [ 0.54240421 -0.33011481  0.27030106  0.5290268   0.06657402  0.17702894\n",
      "   0.99840342 -0.18218424 -0.45877423]\n",
      " [-0.46817974  0.3582769  -0.22468535 -0.35993421 -0.01902359 -0.178095\n",
      "  -0.94625944 -0.18689881  0.25306647]\n",
      " [ 0.07970774  0.00903502  0.24726921 -0.53445024 -0.08141654 -0.04520653\n",
      "  -0.53617407  0.01669934  0.02780915]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.04134836  0.50314184 -0.8953481   0.03023917  0.52667785 -0.75838019\n",
      "  -0.09909569]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:22 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.63974146]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 22 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.32674328 -0.2041167   0.13439429  0.56205436 -0.02853028 -0.04896714\n",
      "   0.65650964  0.13234957 -0.04985091]\n",
      " [-0.29229359  0.37159793 -0.36382003 -0.69485836 -0.09265318 -0.11205883\n",
      "  -1.02405993 -0.3068103   0.15139032]\n",
      " [ 0.1796125  -0.37620045  0.25408162 -0.52404734  0.26808277 -0.43442979\n",
      "  -0.43343063  0.1506603   0.33564798]\n",
      " [ 0.55148411 -0.33011481  0.27938095  0.5290268   0.06657402  0.17702894\n",
      "   1.00748331 -0.18218424 -0.45877423]\n",
      " [-0.47744451  0.3582769  -0.23395012 -0.35993421 -0.01902359 -0.178095\n",
      "  -0.95552421 -0.18689881  0.25306647]\n",
      " [ 0.07756012  0.00903502  0.2451216  -0.53445024 -0.08141654 -0.04520653\n",
      "  -0.53832168  0.01669934  0.02780915]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.08286317  0.53422687 -0.88867318  0.05099923  0.56235966 -0.75163054\n",
      "  -0.08050159]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:22 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.6365315]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 22 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.31299917 -0.21786081  0.12065018  0.56205436 -0.02853028 -0.04896714\n",
      "   0.64276553  0.13234957 -0.04985091]\n",
      " [-0.27615667  0.38773485 -0.34768311 -0.69485836 -0.09265318 -0.11205883\n",
      "  -1.00792301 -0.3068103   0.15139032]\n",
      " [ 0.18629404 -0.36951891  0.26076316 -0.52404734  0.26808277 -0.43442979\n",
      "  -0.42674909  0.1506603   0.33564798]\n",
      " [ 0.53500694 -0.34659198  0.26290378  0.5290268   0.06657402  0.17702894\n",
      "   0.99100614 -0.18218424 -0.45877423]\n",
      " [-0.46130738  0.37441403 -0.21781299 -0.35993421 -0.01902359 -0.178095\n",
      "  -0.93938708 -0.18689881  0.25306647]\n",
      " [ 0.0813231   0.01279799  0.24888457 -0.53445024 -0.08141654 -0.04520653\n",
      "  -0.53455871  0.01669934  0.02780915]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.00922947  0.48167288 -0.90433692  0.02102242  0.50206844 -0.76729347\n",
      "  -0.11352864]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:22 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.52661285]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 22 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.32071394 -0.21786081  0.12836494  0.56205436 -0.02853028 -0.04896714\n",
      "   0.64276553  0.14006434 -0.04985091]\n",
      " [-0.28729432  0.38773485 -0.35882076 -0.69485836 -0.09265318 -0.11205883\n",
      "  -1.00792301 -0.31794796  0.15139032]\n",
      " [ 0.19436834 -0.36951891  0.26883746 -0.52404734  0.26808277 -0.43442979\n",
      "  -0.42674909  0.1587346   0.33564798]\n",
      " [ 0.54328053 -0.34659198  0.27117738  0.5290268   0.06657402  0.17702894\n",
      "   0.99100614 -0.17391064 -0.45877423]\n",
      " [-0.47195756  0.37441403 -0.22846317 -0.35993421 -0.01902359 -0.178095\n",
      "  -0.93938708 -0.197549    0.25306647]\n",
      " [ 0.08628955  0.01279799  0.25385103 -0.53445024 -0.08141654 -0.04520653\n",
      "  -0.53455871  0.02166579  0.02780915]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.06823523  0.51930908 -0.88765052  0.05908904  0.54037769 -0.74982285\n",
      "  -0.07895909]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:22 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.55980024]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 22 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.33025514 -0.21786081  0.12836494  0.57159557 -0.02853028 -0.04896714\n",
      "   0.64276553  0.14006434 -0.04030971]\n",
      " [-0.29681891  0.38773485 -0.35882076 -0.70438295 -0.09265318 -0.11205883\n",
      "  -1.00792301 -0.31794796  0.14186573]\n",
      " [ 0.19444927 -0.36951891  0.26883746 -0.5239664   0.26808277 -0.43442979\n",
      "  -0.42674909  0.1587346   0.33572892]\n",
      " [ 0.55086345 -0.34659198  0.27117738  0.53660972  0.06657402  0.17702894\n",
      "   0.99100614 -0.17391064 -0.45119131]\n",
      " [-0.47918377  0.37441403 -0.22846317 -0.36716042 -0.01902359 -0.178095\n",
      "  -0.93938708 -0.197549    0.24584026]\n",
      " [ 0.08083435  0.01279799  0.25385103 -0.53990544 -0.08141654 -0.04520653\n",
      "  -0.53455871  0.02166579  0.02235395]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.12247311  0.55711133 -0.8711902   0.08628891  0.57556432 -0.73034045\n",
      "  -0.05745743]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:22 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.53590297]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 22 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.32675526 -0.21786081  0.12836494  0.57159557 -0.03203015 -0.05246702\n",
      "   0.64276553  0.14006434 -0.04380959]\n",
      " [-0.29101635  0.38773485 -0.35882076 -0.70438295 -0.08685062 -0.10625627\n",
      "  -1.00792301 -0.31794796  0.14766829]\n",
      " [ 0.18858388 -0.36951891  0.26883746 -0.5239664   0.26221737 -0.44029518\n",
      "  -0.42674909  0.1587346   0.32986352]\n",
      " [ 0.5453095  -0.34659198  0.27117738  0.53660972  0.06102008  0.17147499\n",
      "   0.99100614 -0.17391064 -0.45674526]\n",
      " [-0.47233423  0.37441403 -0.22846317 -0.36716042 -0.01217406 -0.17124547\n",
      "  -0.93938708 -0.197549    0.25268979]\n",
      " [ 0.08122473  0.01279799  0.25385103 -0.53990544 -0.08102615 -0.04481615\n",
      "  -0.53455871  0.02166579  0.02274433]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.05583063  0.52026383 -0.89858296  0.04697201  0.53657941 -0.75659865\n",
      "  -0.09038825]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:22 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.4311325]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 22 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.32701182 -0.21760425  0.12836494  0.57159557 -0.0317736  -0.05221046\n",
      "   0.64276553  0.14006434 -0.04355303]\n",
      " [-0.29169369  0.38705752 -0.35882076 -0.70438295 -0.08752796 -0.10693361\n",
      "  -1.00792301 -0.31794796  0.14699095]\n",
      " [ 0.18896907 -0.36913372  0.26883746 -0.5239664   0.26260257 -0.43990999\n",
      "  -0.42674909  0.1587346   0.33024871]\n",
      " [ 0.54564692 -0.34625456  0.27117738  0.53660972  0.06135749  0.17181241\n",
      "   0.99100614 -0.17391064 -0.45640784]\n",
      " [-0.47195564  0.37479262 -0.22846317 -0.36716042 -0.01179546 -0.17086687\n",
      "  -0.93938708 -0.197549    0.25306839]\n",
      " [ 0.08134468  0.01291794  0.25385103 -0.53990544 -0.08090621 -0.0446962\n",
      "  -0.53455871  0.02166579  0.02286428]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.00296144  0.49408581 -0.92569519  0.02092267  0.51048227 -0.78265459\n",
      "  -0.11670289]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:22 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57366365]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 22 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.31650971 -0.22810637  0.12836494  0.57159557 -0.04227571 -0.06271257\n",
      "   0.63226342  0.14006434 -0.04355303]\n",
      " [-0.27719388  0.40155733 -0.35882076 -0.70438295 -0.07302814 -0.09243379\n",
      "  -0.99342319 -0.31794796  0.14699095]\n",
      " [ 0.20080729 -0.35729549  0.26883746 -0.5239664   0.27444079 -0.42807177\n",
      "  -0.41491086  0.1587346   0.33024871]\n",
      " [ 0.53002528 -0.36187621  0.27117738  0.53660972  0.04573585  0.15619076\n",
      "   0.97538449 -0.17391064 -0.45640784]\n",
      " [-0.45689757  0.38985069 -0.22846317 -0.36716042  0.00326261 -0.1558088\n",
      "  -0.92432901 -0.197549    0.25306839]\n",
      " [ 0.09051533  0.02208859  0.25385103 -0.53990544 -0.07173556 -0.03552555\n",
      "  -0.52538806  0.02166579  0.02286428]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.06719007  0.44770882 -0.9431227  -0.00106358  0.45394736 -0.79865441\n",
      "  -0.14211063]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:22 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.60573215]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 22 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.3270475  -0.22810637  0.12836494  0.58213336 -0.04227571 -0.06271257\n",
      "   0.64280122  0.14006434 -0.04355303]\n",
      " [-0.28714343  0.40155733 -0.35882076 -0.7143325  -0.07302814 -0.09243379\n",
      "  -1.00337274 -0.31794796  0.14699095]\n",
      " [ 0.19320366 -0.35729549  0.26883746 -0.53157004  0.27444079 -0.42807177\n",
      "  -0.4225145   0.1587346   0.33024871]\n",
      " [ 0.53979941 -0.36187621  0.27117738  0.54638385  0.04573585  0.15619076\n",
      "   0.98515862 -0.17391064 -0.45640784]\n",
      " [-0.46729149  0.38985069 -0.22846317 -0.37755435  0.00326261 -0.1558088\n",
      "  -0.93472294 -0.197549    0.25306839]\n",
      " [ 0.08138774  0.02208859  0.25385103 -0.54903304 -0.07173556 -0.03552555\n",
      "  -0.53451566  0.02166579  0.02286428]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.0201104   0.48634205 -0.93738591  0.01416323  0.4956194  -0.79167481\n",
      "  -0.12921414]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:22 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.59540312]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 22 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.33583596 -0.22810637  0.12836494  0.58213336 -0.03348726 -0.05392412\n",
      "   0.65158967  0.14006434 -0.04355303]\n",
      " [-0.29802296  0.40155733 -0.35882076 -0.7143325  -0.08390768 -0.10331333\n",
      "  -1.01425228 -0.31794796  0.14699095]\n",
      " [ 0.1887051  -0.35729549  0.26883746 -0.53157004  0.26994223 -0.43257033\n",
      "  -0.42701306  0.1587346   0.33024871]\n",
      " [ 0.55058744 -0.36187621  0.27117738  0.54638385  0.05652388  0.16697879\n",
      "   0.99594666 -0.17391064 -0.45640784]\n",
      " [-0.47820102  0.38985069 -0.22846317 -0.37755435 -0.00764692 -0.16671833\n",
      "  -0.94563247 -0.197549    0.25306839]\n",
      " [ 0.07506949  0.02208859  0.25385103 -0.54903304 -0.07805381 -0.0418438\n",
      "  -0.54083391  0.02166579  0.02286428]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.02862294  0.52063449 -0.92817125  0.03392058  0.53699466 -0.78317432\n",
      "  -0.11150164]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:22 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.55703152]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 22 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.32386891 -0.22810637  0.12836494  0.57016632 -0.03348726 -0.06589117\n",
      "   0.65158967  0.14006434 -0.05552008]\n",
      " [-0.28474615  0.40155733 -0.35882076 -0.70105569 -0.08390768 -0.09003652\n",
      "  -1.01425228 -0.31794796  0.16026776]\n",
      " [ 0.19598694 -0.35729549  0.26883746 -0.52428819  0.26994223 -0.42528848\n",
      "  -0.42701306  0.1587346   0.33753056]\n",
      " [ 0.53875026 -0.36187621  0.27117738  0.53454667  0.05652388  0.15514161\n",
      "   0.99594666 -0.17391064 -0.46824502]\n",
      " [-0.46676041  0.38985069 -0.22846317 -0.36611374 -0.00764692 -0.15527772\n",
      "  -0.94563247 -0.197549    0.264509  ]\n",
      " [ 0.08304428  0.02208859  0.25385103 -0.54105824 -0.07805381 -0.03386901\n",
      "  -0.54083391  0.02166579  0.03083907]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.04010011  0.47291727 -0.94707999  0.00708383  0.4894668  -0.80493257\n",
      "  -0.13756145]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:22 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.60503711]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 22 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.33408434 -0.21789094  0.12836494  0.58038175 -0.03348726 -0.05567574\n",
      "   0.6618051   0.14006434 -0.05552008]\n",
      " [-0.29523603  0.39106745 -0.35882076 -0.71154557 -0.08390768 -0.1005264\n",
      "  -1.02474216 -0.31794796  0.16026776]\n",
      " [ 0.1854222  -0.36786023  0.26883746 -0.53485293  0.26994223 -0.43585322\n",
      "  -0.4375778   0.1587346   0.33753056]\n",
      " [ 0.54897634 -0.35165013  0.27117738  0.54477275  0.05652388  0.16536769\n",
      "   1.00617274 -0.17391064 -0.46824502]\n",
      " [-0.47732527  0.37928584 -0.22846317 -0.3766786  -0.00764692 -0.16584258\n",
      "  -0.95619732 -0.197549    0.264509  ]\n",
      " [ 0.07371338  0.0127577   0.25385103 -0.55038914 -0.07805381 -0.0431999\n",
      "  -0.5501648   0.02166579  0.03083907]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.00709148  0.50961258 -0.93971957  0.01542924  0.53031533 -0.79662854\n",
      "  -0.12496805]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:22 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.62908173]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 22 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.34364734 -0.20832794  0.12836494  0.58994475 -0.03348726 -0.05567574\n",
      "   0.6713681   0.14006434 -0.05552008]\n",
      " [-0.3048927   0.38141078 -0.35882076 -0.72120224 -0.08390768 -0.1005264\n",
      "  -1.03439882 -0.31794796  0.16026776]\n",
      " [ 0.17631686 -0.37696558  0.26883746 -0.54395828  0.26994223 -0.43585322\n",
      "  -0.44668314  0.1587346   0.33753056]\n",
      " [ 0.55853036 -0.3420961   0.27117738  0.55432678  0.05652388  0.16536769\n",
      "   1.01572676 -0.17391064 -0.46824502]\n",
      " [-0.48696807  0.36964304 -0.22846317 -0.3863214  -0.00764692 -0.16584258\n",
      "  -0.96584012 -0.197549    0.264509  ]\n",
      " [ 0.06514155  0.00418587  0.25385103 -0.55896097 -0.07805381 -0.0431999\n",
      "  -0.55873663  0.02166579  0.03083907]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.05036613  0.5440374  -0.93269164  0.02579788  0.56717384 -0.78827844\n",
      "  -0.11344914]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:22 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.66352052]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 22 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.3513495  -0.20832794  0.1360671   0.58994475 -0.03348726 -0.04797358\n",
      "   0.67907026  0.14006434 -0.05552008]\n",
      " [-0.31312468  0.38141078 -0.36705274 -0.72120224 -0.08390768 -0.10875838\n",
      "  -1.0426308  -0.31794796  0.16026776]\n",
      " [ 0.17240002 -0.37696558  0.26492062 -0.54395828  0.26994223 -0.43977006\n",
      "  -0.45059998  0.1587346   0.33753056]\n",
      " [ 0.5663953  -0.3420961   0.27904232  0.55432678  0.05652388  0.17323263\n",
      "   1.0235917  -0.17391064 -0.46824502]\n",
      " [-0.49513122  0.36964304 -0.23662632 -0.3863214  -0.00764692 -0.17400573\n",
      "  -0.97400328 -0.197549    0.264509  ]\n",
      " [ 0.06253709  0.00418587  0.25124656 -0.55896097 -0.07805381 -0.04580437\n",
      "  -0.5613411   0.02166579  0.03083907]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.08792751  0.57213141 -0.92735729  0.04053565  0.60030022 -0.78316212\n",
      "  -0.0973078 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:22 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.52426027]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 23 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.34145612 -0.21822132  0.1360671   0.58005137 -0.03348726 -0.04797358\n",
      "   0.67907026  0.14006434 -0.06541346]\n",
      " [-0.30554202  0.38899344 -0.36705274 -0.71361958 -0.08390768 -0.10875838\n",
      "  -1.0426308  -0.31794796  0.16785042]\n",
      " [ 0.17884165 -0.37052394  0.26492062 -0.53751664  0.26994223 -0.43977006\n",
      "  -0.45059998  0.1587346   0.34397219]\n",
      " [ 0.56144251 -0.3470489   0.27904232  0.54937398  0.05652388  0.17323263\n",
      "   1.0235917  -0.17391064 -0.47319782]\n",
      " [-0.49115038  0.37362388 -0.23662632 -0.38234055 -0.00764692 -0.17400573\n",
      "  -0.97400328 -0.197549    0.26848984]\n",
      " [ 0.06969091  0.01133969  0.25124656 -0.55180715 -0.07805381 -0.04580437\n",
      "  -0.5613411   0.02166579  0.03799289]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.02254925  0.52877461 -0.9521533   0.01447104  0.56257839 -0.8118297\n",
      "  -0.12258657]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:23 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.64444444]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 23 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.35003224 -0.21822132  0.14464322  0.58005137 -0.03348726 -0.04797358\n",
      "   0.68764638  0.14006434 -0.06541346]\n",
      " [-0.31457168  0.38899344 -0.3760824  -0.71361958 -0.08390768 -0.10875838\n",
      "  -1.05166046 -0.31794796  0.16785042]\n",
      " [ 0.17877202 -0.37052394  0.26485099 -0.53751664  0.26994223 -0.43977006\n",
      "  -0.45066961  0.1587346   0.34397219]\n",
      " [ 0.57026685 -0.3470489   0.28786666  0.54937398  0.05652388  0.17323263\n",
      "   1.03241605 -0.17391064 -0.47319782]\n",
      " [-0.50019316  0.37362388 -0.2456691  -0.38234055 -0.00764692 -0.17400573\n",
      "  -0.98304605 -0.197549    0.26848984]\n",
      " [ 0.06727772  0.01133969  0.24883337 -0.55180715 -0.07805381 -0.04580437\n",
      "  -0.56375429  0.02166579  0.03799289]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.06328451  0.55976245 -0.94594169  0.03476904  0.59784581 -0.80554699\n",
      "  -0.10465544]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:23 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.63869962]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 23 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.33583323 -0.23242033  0.13044421  0.58005137 -0.03348726 -0.04797358\n",
      "   0.67344737  0.14006434 -0.06541346]\n",
      " [-0.29829864  0.40526648 -0.35980936 -0.71361958 -0.08390768 -0.10875838\n",
      "  -1.03538742 -0.31794796  0.16785042]\n",
      " [ 0.18548604 -0.36380992  0.27156501 -0.53751664  0.26994223 -0.43977006\n",
      "  -0.44395559  0.1587346   0.34397219]\n",
      " [ 0.55376887 -0.36354688  0.27136868  0.54937398  0.05652388  0.17323263\n",
      "   1.01591807 -0.17391064 -0.47319782]\n",
      " [-0.48391537  0.38990167 -0.22939131 -0.38234055 -0.00764692 -0.17400573\n",
      "  -0.96676826 -0.197549    0.26848984]\n",
      " [ 0.07157104  0.01563301  0.25312669 -0.55180715 -0.07805381 -0.04580437\n",
      "  -0.55946097  0.02166579  0.03799289]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.01040942  0.50641232 -0.96107312  0.00479676  0.537124   -0.82065482\n",
      "  -0.13716902]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:23 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.52365762]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 23 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.34405931 -0.23242033  0.1386703   0.58005137 -0.03348726 -0.04797358\n",
      "   0.67344737  0.14829042 -0.06541346]\n",
      " [-0.30982512  0.40526648 -0.37133584 -0.71361958 -0.08390768 -0.10875838\n",
      "  -1.03538742 -0.32947444  0.16785042]\n",
      " [ 0.1938169  -0.36380992  0.27989587 -0.53751664  0.26994223 -0.43977006\n",
      "  -0.44395559  0.16706545  0.34397219]\n",
      " [ 0.562484   -0.36354688  0.28008381  0.54937398  0.05652388  0.17323263\n",
      "   1.01591807 -0.16519551 -0.47319782]\n",
      " [-0.49498462  0.38990167 -0.24046056 -0.38234055 -0.00764692 -0.17400573\n",
      "  -0.96676826 -0.20861825  0.26848984]\n",
      " [ 0.0765641   0.01563301  0.25811976 -0.55180715 -0.07805381 -0.04580437\n",
      "  -0.55946097  0.02665886  0.03799289]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.04900007  0.54485655 -0.94481422  0.04336893  0.57617308 -0.8036147\n",
      "  -0.10237077]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:23 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.55610298]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 23 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.35389392 -0.23242033  0.1386703   0.58988597 -0.03348726 -0.04797358\n",
      "   0.67344737  0.14829042 -0.05557885]\n",
      " [-0.31963649  0.40526648 -0.37133584 -0.72343094 -0.08390768 -0.10875838\n",
      "  -1.03538742 -0.32947444  0.15803906]\n",
      " [ 0.19382063 -0.36380992  0.27989587 -0.53751291  0.26994223 -0.43977006\n",
      "  -0.44395559  0.16706545  0.34397592]\n",
      " [ 0.5703971  -0.36354688  0.28008381  0.55728708  0.05652388  0.17323263\n",
      "   1.01591807 -0.16519551 -0.46528472]\n",
      " [-0.50259647  0.38990167 -0.24046056 -0.3899524  -0.00764692 -0.17400573\n",
      "  -0.96676826 -0.20861825  0.26087799]\n",
      " [ 0.07085241  0.01563301  0.25811976 -0.55751884 -0.07805381 -0.04580437\n",
      "  -0.55946097  0.02665886  0.0322812 ]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.10378861  0.58333938 -0.92847295  0.07076693  0.61202947 -0.78431133\n",
      "  -0.08087195]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:23 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.52306551]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 23 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.35039807 -0.23242033  0.1386703   0.58988597 -0.03698311 -0.05146943\n",
      "   0.67344737  0.14829042 -0.05907471]\n",
      " [-0.31403566  0.40526648 -0.37133584 -0.72343094 -0.07830686 -0.10315756\n",
      "  -1.03538742 -0.32947444  0.16363988]\n",
      " [ 0.18801736 -0.36380992  0.27989587 -0.53751291  0.26413896 -0.44557333\n",
      "  -0.44395559  0.16706545  0.33817265]\n",
      " [ 0.56508536 -0.36354688  0.28008381  0.55728708  0.05121214  0.16792089\n",
      "   1.01591807 -0.16519551 -0.47059646]\n",
      " [-0.49599129  0.38990167 -0.24046056 -0.3899524  -0.00104174 -0.16740055\n",
      "  -0.96676826 -0.20861825  0.26748318]\n",
      " [ 0.07119041  0.01563301  0.25811976 -0.55751884 -0.0777158  -0.04546636\n",
      "  -0.55946097  0.02665886  0.0326192 ]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.03854456  0.54719404 -0.95537627  0.03220978  0.57399587 -0.81012908\n",
      "  -0.11315595]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:23 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.41303428]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 23 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 3.50767854e-01 -2.32050545e-01  1.38670297e-01  5.89885971e-01\n",
      "  -3.66133241e-02 -5.10996426e-02  6.73447370e-01  1.48290420e-01\n",
      "  -5.87049193e-02]\n",
      " [-3.14953243e-01  4.04348901e-01 -3.71335843e-01 -7.23430944e-01\n",
      "  -7.92244347e-02 -1.04075134e-01 -1.03538742e+00 -3.29474443e-01\n",
      "   1.62722299e-01]\n",
      " [ 1.88255836e-01 -3.63571444e-01  2.79895869e-01 -5.37512912e-01\n",
      "   2.64377435e-01 -4.45334856e-01 -4.43955592e-01  1.67065454e-01\n",
      "   3.38411127e-01]\n",
      " [ 5.65709872e-01 -3.62922367e-01  2.80083813e-01  5.57287081e-01\n",
      "   5.18366516e-02  1.68545405e-01  1.01591807e+00 -1.65195514e-01\n",
      "  -4.69971951e-01]\n",
      " [-4.95903061e-01  3.89989898e-01 -2.40460564e-01 -3.89952399e-01\n",
      "  -9.53515473e-04 -1.67312323e-01 -9.66768264e-01 -2.08618250e-01\n",
      "   2.67571402e-01]\n",
      " [ 7.12372172e-02  1.56798138e-02  2.58119755e-01 -5.57518844e-01\n",
      "  -7.76689956e-02 -4.54195555e-02 -5.59460967e-01  2.66588606e-02\n",
      "   3.26660084e-02]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.01152283  0.52253019 -0.98132837  0.00741458  0.54958694 -0.83507455\n",
      "  -0.13814284]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:23 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57653758]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 23 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.33979221 -0.24302619  0.1386703   0.58988597 -0.04758897 -0.06207529\n",
      "   0.66247173  0.14829042 -0.05870492]\n",
      " [-0.30028057  0.41902158 -0.37133584 -0.72343094 -0.06455176 -0.08940246\n",
      "  -1.02071475 -0.32947444  0.1627223 ]\n",
      " [ 0.20030195 -0.35152533  0.27989587 -0.53751291  0.27642355 -0.43328874\n",
      "  -0.43190948  0.16706545  0.33841113]\n",
      " [ 0.55001712 -0.37861512  0.28008381  0.55728708  0.0361439   0.15285266\n",
      "   1.00022532 -0.16519551 -0.46997195]\n",
      " [-0.4807089   0.40518406 -0.24046056 -0.3899524   0.01424065 -0.15211816\n",
      "  -0.9515741  -0.20861825  0.2675714 ]\n",
      " [ 0.08083996  0.02528256  0.25811976 -0.55751884 -0.06806625 -0.03581681\n",
      "  -0.54985823  0.02665886  0.03266601]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.08190135  0.47543471 -0.99852127 -0.01440111  0.49269905 -0.85085842\n",
      "  -0.16315139]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:23 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.6158576]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 23 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.34995642 -0.24302619  0.1386703   0.60005018 -0.04758897 -0.06207529\n",
      "   0.67263593  0.14829042 -0.05870492]\n",
      " [-0.30970784  0.41902158 -0.37133584 -0.73285821 -0.06455176 -0.08940246\n",
      "  -1.03014201 -0.32947444  0.1627223 ]\n",
      " [ 0.19273944 -0.35152533  0.27989587 -0.54507542  0.27642355 -0.43328874\n",
      "  -0.43947199  0.16706545  0.33841113]\n",
      " [ 0.55927033 -0.37861512  0.28008381  0.56654029  0.0361439   0.15285266\n",
      "   1.00947852 -0.16519551 -0.46997195]\n",
      " [-0.49062844  0.40518406 -0.24046056 -0.39987194  0.01424065 -0.15211816\n",
      "  -0.96149364 -0.20861825  0.2675714 ]\n",
      " [ 0.07178193  0.02528256  0.25811976 -0.56657687 -0.06806625 -0.03581681\n",
      "  -0.55891625  0.02665886  0.03266601]]\n",
      "\n",
      "Theta two: \n",
      "[[-3.64617177e-02  5.13191333e-01 -9.93313123e-01 -1.17720629e-05\n",
      "   5.33214519e-01 -8.44534772e-01 -1.51166412e-01]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:23 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.60000538]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 23 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.3589121  -0.24302619  0.1386703   0.60005018 -0.03863328 -0.0531196\n",
      "   0.68159162  0.14829042 -0.05870492]\n",
      " [-0.32044394  0.41902158 -0.37133584 -0.73285821 -0.07528786 -0.10013856\n",
      "  -1.04087812 -0.32947444  0.1627223 ]\n",
      " [ 0.18808834 -0.35152533  0.27989587 -0.54507542  0.27177245 -0.43793984\n",
      "  -0.44412309  0.16706545  0.33841113]\n",
      " [ 0.56985394 -0.37861512  0.28008381  0.56654029  0.04672752  0.16343627\n",
      "   1.02006214 -0.16519551 -0.46997195]\n",
      " [-0.50136593  0.40518406 -0.24046056 -0.39987194  0.00350315 -0.16285566\n",
      "  -0.97223114 -0.20861825  0.2675714 ]\n",
      " [ 0.06527482  0.02528256  0.25811976 -0.56657687 -0.07457336 -0.04232392\n",
      "  -0.56542336  0.02665886  0.03266601]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.01153742  0.54744347 -0.98451241  0.0192094   0.57415428 -0.83640449\n",
      "  -0.13405945]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:23 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.556064]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 23 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.34670139 -0.24302619  0.1386703   0.58783947 -0.03863328 -0.06533032\n",
      "   0.68159162  0.14829042 -0.07091563]\n",
      " [-0.30701767  0.41902158 -0.37133584 -0.71943194 -0.07528786 -0.08671229\n",
      "  -1.04087812 -0.32947444  0.17614857]\n",
      " [ 0.19552713 -0.35152533  0.27989587 -0.53763664  0.27177245 -0.43050105\n",
      "  -0.44412309  0.16706545  0.34584991]\n",
      " [ 0.5578101  -0.37861512  0.28008381  0.55449644  0.04672752  0.15139243\n",
      "   1.02006214 -0.16519551 -0.48201579]\n",
      " [-0.48965636  0.40518406 -0.24046056 -0.38816237  0.00350315 -0.15114608\n",
      "  -0.97223114 -0.20861825  0.27928097]\n",
      " [ 0.07349391  0.02528256  0.25811976 -0.55835778 -0.07457336 -0.03410484\n",
      "  -0.56542336  0.02665886  0.04088509]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.05709668  0.49940191 -1.00309648 -0.00740778  0.52636264 -0.8577339\n",
      "  -0.15979507]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:23 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.61225297]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 23 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.35678639 -0.23294118  0.1386703   0.59792447 -0.03863328 -0.05524531\n",
      "   0.69167663  0.14829042 -0.07091563]\n",
      " [-0.3171962   0.40884304 -0.37133584 -0.72961047 -0.07528786 -0.09689082\n",
      "  -1.05105665 -0.32947444  0.17614857]\n",
      " [ 0.18522558 -0.36182688  0.27989587 -0.54793819  0.27177245 -0.4408026\n",
      "  -0.45442463  0.16706545  0.34584991]\n",
      " [ 0.56769659 -0.36872862  0.28008381  0.56438294  0.04672752  0.16127892\n",
      "   1.02994863 -0.16519551 -0.48201579]\n",
      " [-0.49995021  0.39489021 -0.24046056 -0.39845622  0.00350315 -0.16143993\n",
      "  -0.98252499 -0.20861825  0.27928097]\n",
      " [ 0.06417446  0.01596311  0.25811976 -0.56767723 -0.07457336 -0.04342428\n",
      "  -0.57474281  0.02665886  0.04088509]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.10712441e-02  5.35630402e-01 -9.96195114e-01  5.29988382e-04\n",
      "   5.66426533e-01 -8.49976789e-01 -1.47944461e-01]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:23 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.63537166]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 23 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.36618302 -0.22354456  0.1386703   0.60732109 -0.03863328 -0.05524531\n",
      "   0.70107325  0.14829042 -0.07091563]\n",
      " [-0.32658444  0.3994548  -0.37133584 -0.73899872 -0.07528786 -0.09689082\n",
      "  -1.06044489 -0.32947444  0.17614857]\n",
      " [ 0.17626711 -0.37078534  0.27989587 -0.55689665  0.27177245 -0.4408026\n",
      "  -0.4633831   0.16706545  0.34584991]\n",
      " [ 0.57696124 -0.35946398  0.28008381  0.57364758  0.04672752  0.16127892\n",
      "   1.03921328 -0.16519551 -0.48201579]\n",
      " [-0.50939474  0.38544568 -0.24046056 -0.40790074  0.00350315 -0.16143993\n",
      "  -0.99196951 -0.20861825  0.27928097]\n",
      " [ 0.05560959  0.00739824  0.25811976 -0.5762421  -0.07457336 -0.04342428\n",
      "  -0.58330768  0.02665886  0.04088509]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.0311663   0.56960244 -0.98961008  0.01046589  0.6026381  -0.84218326\n",
      "  -0.13709475]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:23 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.66847093]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 23 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.37390141 -0.22354456  0.14638868  0.60732109 -0.03863328 -0.04752692\n",
      "   0.70879164  0.14829042 -0.07091563]\n",
      " [-0.33455606  0.3994548  -0.37930746 -0.73899872 -0.07528786 -0.10486244\n",
      "  -1.06841651 -0.32947444  0.17614857]\n",
      " [ 0.1723522  -0.37078534  0.27598095 -0.55689665  0.27177245 -0.44471751\n",
      "  -0.46729802  0.16706545  0.34584991]\n",
      " [ 0.58455447 -0.35946398  0.28767705  0.57364758  0.04672752  0.16887215\n",
      "   1.04680651 -0.16519551 -0.48201579]\n",
      " [-0.51728908  0.38544568 -0.24835491 -0.40790074  0.00350315 -0.16933428\n",
      "  -0.99986386 -0.20861825  0.27928097]\n",
      " [ 0.05280422  0.00739824  0.25531439 -0.5762421  -0.07457336 -0.04622965\n",
      "  -0.58611305  0.02665886  0.04088509]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.06790263  0.59750865 -0.98464124  0.02478684  0.63521247 -0.83741711\n",
      "  -0.12157799]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:23 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.51485957]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 24 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.36406743 -0.23337854  0.14638868  0.59748711 -0.03863328 -0.04752692\n",
      "   0.70879164  0.14829042 -0.08074961]\n",
      " [-0.3270278   0.40698305 -0.37930746 -0.73147046 -0.07528786 -0.10486244\n",
      "  -1.06841651 -0.32947444  0.18367682]\n",
      " [ 0.17866626 -0.36447129  0.27598095 -0.5505826   0.27177245 -0.44471751\n",
      "  -0.46729802  0.16706545  0.35216397]\n",
      " [ 0.57958869 -0.36442976  0.28767705  0.5686818   0.04672752  0.16887215\n",
      "   1.04680651 -0.16519551 -0.48698157]\n",
      " [-0.51317232  0.38956245 -0.24835491 -0.40378398  0.00350315 -0.16933428\n",
      "  -0.99986386 -0.20861825  0.28339774]\n",
      " [ 0.060027    0.01462102  0.25531439 -0.56901932 -0.07457336 -0.04622965\n",
      "  -0.58611305  0.02665886  0.04810787]]\n",
      "\n",
      "Theta two: \n",
      "[[ 3.60202715e-03  5.54732913e-01 -1.00894829e+00 -8.71465079e-04\n",
      "   5.98012943e-01 -8.65403937e-01 -1.46230646e-01]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:24 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.64933793]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 24 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.37263691 -0.23337854  0.15495817  0.59748711 -0.03863328 -0.04752692\n",
      "   0.71736113  0.14829042 -0.08074961]\n",
      " [-0.33580936  0.40698305 -0.38808902 -0.73147046 -0.07528786 -0.10486244\n",
      "  -1.07719807 -0.32947444  0.18367682]\n",
      " [ 0.17854    -0.36447129  0.2758547  -0.5505826   0.27177245 -0.44471751\n",
      "  -0.46742427  0.16706545  0.35216397]\n",
      " [ 0.58814756 -0.36442976  0.29623592  0.5686818   0.04672752  0.16887215\n",
      "   1.05536538 -0.16519551 -0.48698157]\n",
      " [-0.52197065  0.38956245 -0.25715325 -0.40378398  0.00350315 -0.16933428\n",
      "  -1.0086622  -0.20861825  0.28339774]\n",
      " [ 0.05737346  0.01462102  0.25266085 -0.56901932 -0.07457336 -0.04622965\n",
      "  -0.58876658  0.02665886  0.04810787]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.04352459  0.58555038 -1.00316142  0.01896355  0.63280449 -0.85955064\n",
      "  -0.12895545]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:24 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.64090055]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 24 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.35804742 -0.24796803  0.14036868  0.59748711 -0.03863328 -0.04752692\n",
      "   0.70277163  0.14829042 -0.08074961]\n",
      " [-0.31943583  0.42335659 -0.37171548 -0.73147046 -0.07528786 -0.10486244\n",
      "  -1.06082453 -0.32947444  0.18367682]\n",
      " [ 0.18525804 -0.35775325  0.28257273 -0.5505826   0.27177245 -0.44471751\n",
      "  -0.46070623  0.16706545  0.35216397]\n",
      " [ 0.57164278 -0.38093454  0.27973114  0.5686818   0.04672752  0.16887215\n",
      "   1.0388606  -0.16519551 -0.48698157]\n",
      " [-0.50558951  0.4059436  -0.2407721  -0.40378398  0.00350315 -0.16933428\n",
      "  -0.99228105 -0.20861825  0.28339774]\n",
      " [ 0.06215912  0.01940667  0.2574465  -0.56901932 -0.07457336 -0.04622965\n",
      "  -0.58398093  0.02665886  0.04810787]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.0302261   0.53146688 -1.01781951 -0.01103305  0.57169906 -0.87416052\n",
      "  -0.1609893 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:24 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.52117703]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 24 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.36676335 -0.24796803  0.14908461  0.59748711 -0.03863328 -0.04752692\n",
      "   0.70277163  0.15700635 -0.08074961]\n",
      " [-0.33131007  0.42335659 -0.38358973 -0.73147046 -0.07528786 -0.10486244\n",
      "  -1.06082453 -0.34134869  0.18367682]\n",
      " [ 0.19384617 -0.35775325  0.29116086 -0.5505826   0.27177245 -0.44471751\n",
      "  -0.46070623  0.17565358  0.35216397]\n",
      " [ 0.58077411 -0.38093454  0.28886246  0.5686818   0.04672752  0.16887215\n",
      "   1.0388606  -0.15606419 -0.48698157]\n",
      " [-0.51703858  0.4059436  -0.25222117 -0.40378398  0.00350315 -0.16933428\n",
      "  -0.99228105 -0.22006732  0.28339774]\n",
      " [ 0.06717907  0.01940667  0.26246646 -0.56901932 -0.07457336 -0.04622965\n",
      "  -0.58398093  0.03167882  0.04810787]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.02951941  0.57067594 -1.0019926   0.02801654  0.61143677 -0.85755817\n",
      "  -0.12599568]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:24 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.55261673]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 24 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.37687383 -0.24796803  0.14908461  0.60759759 -0.03863328 -0.04752692\n",
      "   0.70277163  0.15700635 -0.07063914]\n",
      " [-0.34138857  0.42335659 -0.38358973 -0.74154896 -0.07528786 -0.10486244\n",
      "  -1.06082453 -0.34134869  0.17359833]\n",
      " [ 0.19378295 -0.35775325  0.29116086 -0.55064581  0.27177245 -0.44471751\n",
      "  -0.46070623  0.17565358  0.35210075]\n",
      " [ 0.58899766 -0.38093454  0.28886246  0.57690535  0.04672752  0.16887215\n",
      "   1.0388606  -0.15606419 -0.47875802]\n",
      " [-0.52501366  0.4059436  -0.25222117 -0.41175906  0.00350315 -0.16933428\n",
      "  -0.99228105 -0.22006732  0.27542266]\n",
      " [ 0.06121792  0.01940667  0.26246646 -0.57498047 -0.07457336 -0.04622965\n",
      "  -0.58398093  0.03167882  0.04214672]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.08482302  0.60980585 -0.98576853  0.05560513  0.64792698 -0.83843258\n",
      "  -0.10451168]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:24 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.50979562]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 24 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.37341124 -0.24796803  0.14908461  0.60759759 -0.04209587 -0.05098951\n",
      "   0.70277163  0.15700635 -0.07410173]\n",
      " [-0.33601199  0.42335659 -0.38358973 -0.74154896 -0.06991128 -0.09948586\n",
      "  -1.06082453 -0.34134869  0.1789749 ]\n",
      " [ 0.18804571 -0.35775325  0.29116086 -0.55064581  0.26603522 -0.45045475\n",
      "  -0.46070623  0.17565358  0.34636352]\n",
      " [ 0.583944   -0.38093454  0.28886246  0.57690535  0.04167386  0.16381849\n",
      "   1.0388606  -0.15606419 -0.48381168]\n",
      " [-0.51867547  0.4059436  -0.25222117 -0.41175906  0.00984134 -0.16299609\n",
      "  -0.99228105 -0.22006732  0.28176085]\n",
      " [ 0.06149561  0.01940667  0.26246646 -0.57498047 -0.07429568 -0.04595197\n",
      "  -0.58398093  0.03167882  0.04242441]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.02112303  0.57446525 -1.01213281  0.01788398  0.61093342 -0.86376051\n",
      "  -0.13608398]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:24 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.39441576]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 24 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.3739026  -0.24747667  0.14908461  0.60759759 -0.04160452 -0.05049816\n",
      "   0.70277163  0.15700635 -0.07361037]\n",
      " [-0.33715066  0.42221792 -0.38358973 -0.74154896 -0.07104995 -0.10062453\n",
      "  -1.06082453 -0.34134869  0.17783624]\n",
      " [ 0.18813713 -0.35766183  0.29116086 -0.55064581  0.26612664 -0.45036333\n",
      "  -0.46070623  0.17565358  0.34645494]\n",
      " [ 0.58482958 -0.38004896  0.28886246  0.57690535  0.04255944  0.16470408\n",
      "   1.0388606  -0.15606419 -0.4829261 ]\n",
      " [-0.51886239  0.40575668 -0.25222117 -0.41175906  0.00965442 -0.16318302\n",
      "  -0.99228105 -0.22006732  0.28157393]\n",
      " [ 0.06145935  0.01937042  0.26246646 -0.57498047 -0.07433194 -0.04598823\n",
      "  -0.58398093  0.03167882  0.04238815]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.02598046  0.551405   -1.03682501 -0.00557634  0.58826809 -0.88749918\n",
      "  -0.15967198]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:24 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57932388]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 24 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.36251259 -0.25886668  0.14908461  0.60759759 -0.05299453 -0.06188817\n",
      "   0.69138163  0.15700635 -0.07361037]\n",
      " [-0.32233588  0.4370327  -0.38358973 -0.74154896 -0.05623517 -0.08580974\n",
      "  -1.04600975 -0.34134869  0.17783624]\n",
      " [ 0.20036803 -0.34543093  0.29116086 -0.55064581  0.27835753 -0.43813243\n",
      "  -0.44847534  0.17565358  0.34645494]\n",
      " [ 0.56907535 -0.39580319  0.28886246  0.57690535  0.02680521  0.14894984\n",
      "   1.02310637 -0.15606419 -0.4829261 ]\n",
      " [-0.50355459  0.42106449 -0.25222117 -0.41175906  0.02496223 -0.14787521\n",
      "  -0.97697324 -0.22006732  0.28157393]\n",
      " [ 0.07145887  0.02936994  0.26246646 -0.57498047 -0.06433242 -0.0359887\n",
      "  -0.57398141  0.03167882  0.04238815]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.09657331  0.5036588  -1.05383495 -0.02724406  0.53107817 -0.90311155\n",
      "  -0.18430835]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:24 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.62616827]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 24 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.37226128 -0.25886668  0.14908461  0.61734629 -0.05299453 -0.06188817\n",
      "   0.70113032  0.15700635 -0.07361037]\n",
      " [-0.33123922  0.4370327  -0.38358973 -0.7504523  -0.05623517 -0.08580974\n",
      "  -1.05491309 -0.34134869  0.17783624]\n",
      " [ 0.19288871 -0.34543093  0.29116086 -0.55812513  0.27835753 -0.43813243\n",
      "  -0.45595466  0.17565358  0.34645494]\n",
      " [ 0.57781064 -0.39580319  0.28886246  0.58564065  0.02680521  0.14894984\n",
      "   1.03184166 -0.15606419 -0.4829261 ]\n",
      " [-0.51297882  0.42106449 -0.25222117 -0.42118329  0.02496223 -0.14787521\n",
      "  -0.98639748 -0.22006732  0.28157393]\n",
      " [ 0.06252622  0.02936994  0.26246646 -0.58391313 -0.06433242 -0.0359887\n",
      "  -0.58291406  0.03167882  0.04238815]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.05281976  0.54043096 -1.04910349 -0.01366767  0.57034431 -0.89738053\n",
      "  -0.17319587]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:24 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.60474985]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 24 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.3813327  -0.25886668  0.14908461  0.61734629 -0.04392311 -0.05281675\n",
      "   0.71020173  0.15700635 -0.07361037]\n",
      " [-0.34181355  0.4370327  -0.38358973 -0.7504523  -0.0668095  -0.09638408\n",
      "  -1.06548743 -0.34134869  0.17783624]\n",
      " [ 0.18811191 -0.34543093  0.29116086 -0.55812513  0.27358073 -0.44290923\n",
      "  -0.46073146  0.17565358  0.34645494]\n",
      " [ 0.58818475 -0.39580319  0.28886246  0.58564065  0.03717931  0.15932395\n",
      "   1.04221577 -0.15606419 -0.4829261 ]\n",
      " [-0.52353131  0.42106449 -0.25222117 -0.42118329  0.01440974 -0.15842769\n",
      "  -0.99694997 -0.22006732  0.28157393]\n",
      " [ 0.05585905  0.02936994  0.26246646 -0.58391313 -0.07099959 -0.04265587\n",
      "  -0.58958123  0.03167882  0.04238815]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.00558193  0.57457555 -1.04068299  0.00503082  0.61079705 -0.88959152\n",
      "  -0.15668057]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:24 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.5552708]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 24 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.36890058 -0.25886668  0.14908461  0.60491417 -0.04392311 -0.06524886\n",
      "   0.71020173  0.15700635 -0.08604248]\n",
      " [-0.32825616  0.4370327  -0.38358973 -0.73689491 -0.0668095  -0.08282669\n",
      "  -1.06548743 -0.34134869  0.19139363]\n",
      " [ 0.19568761 -0.34543093  0.29116086 -0.55054943  0.27358073 -0.43533353\n",
      "  -0.46073146  0.17565358  0.35403064]\n",
      " [ 0.57595779 -0.39580319  0.28886246  0.57341368  0.03717931  0.14709698\n",
      "   1.04221577 -0.15606419 -0.49515307]\n",
      " [-0.51158188  0.42106449 -0.25222117 -0.40923386  0.01440974 -0.14647826\n",
      "  -0.99694997 -0.22006732  0.29352336]\n",
      " [ 0.06431095  0.02936994  0.26246646 -0.57546122 -0.07099959 -0.03420397\n",
      "  -0.58958123  0.03167882  0.05084005]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.07414265  0.52622558 -1.05897641 -0.02139609  0.56276246 -0.91053207\n",
      "  -0.18211031]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:24 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.6194268]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 24 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.37881558 -0.24895169  0.14908461  0.61482917 -0.04392311 -0.05533387\n",
      "   0.72011673  0.15700635 -0.08604248]\n",
      " [-0.33812083  0.42716804 -0.38358973 -0.74675958 -0.0668095  -0.09269135\n",
      "  -1.07535209 -0.34134869  0.19139363]\n",
      " [ 0.18565508 -0.35546347  0.29116086 -0.56058196  0.27358073 -0.44536607\n",
      "  -0.470764    0.17565358  0.35403064]\n",
      " [ 0.58551182 -0.38624915  0.28886246  0.58296772  0.03717931  0.15665102\n",
      "   1.0517698  -0.15606419 -0.49515307]\n",
      " [-0.52158997  0.41105639 -0.25222117 -0.41924196  0.01440974 -0.15648636\n",
      "  -1.00695806 -0.22006732  0.29352336]\n",
      " [ 0.05504461  0.02010359  0.26246646 -0.58472757 -0.07099959 -0.04347031\n",
      "  -0.59884757  0.03167882  0.05084005]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.02928501  0.56192075 -1.05248352 -0.01383797  0.60199801 -0.90326366\n",
      "  -0.17095429]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:24 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.64166981]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 24 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.38801729 -0.23974997  0.14908461  0.62403088 -0.04392311 -0.05533387\n",
      "   0.72931844  0.15700635 -0.08604248]\n",
      " [-0.34723339  0.41805548 -0.38358973 -0.75587214 -0.0668095  -0.09269135\n",
      "  -1.08446465 -0.34134869  0.19139363]\n",
      " [ 0.17685796 -0.36426058  0.29116086 -0.56937908  0.27358073 -0.44536607\n",
      "  -0.47956111  0.17565358  0.35403064]\n",
      " [ 0.59448622 -0.37727475  0.28886246  0.59194212  0.03717931  0.15665102\n",
      "   1.06074421 -0.15606419 -0.49515307]\n",
      " [-0.5308123   0.40183406 -0.25222117 -0.42846428  0.01440974 -0.15648636\n",
      "  -1.01618039 -0.22006732  0.29352336]\n",
      " [ 0.04652504  0.01158402  0.26246646 -0.59324714 -0.07099959 -0.04347031\n",
      "  -0.60736714  0.03167882  0.05084005]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.01191036  0.5953824  -1.04629614 -0.00431072  0.63751821 -0.89597164\n",
      "  -0.16073107]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:24 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.67348659]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 24 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.39570582 -0.23974997  0.15677314  0.62403088 -0.04392311 -0.04764534\n",
      "   0.73700697  0.15700635 -0.08604248]\n",
      " [-0.35494027  0.41805548 -0.39129661 -0.75587214 -0.0668095  -0.10039823\n",
      "  -1.09217153 -0.34134869  0.19139363]\n",
      " [ 0.17296394 -0.36426058  0.28726684 -0.56937908  0.27358073 -0.44926009\n",
      "  -0.48345513  0.17565358  0.35403064]\n",
      " [ 0.60181218 -0.37727475  0.29618842  0.59194212  0.03717931  0.16397697\n",
      "   1.06807016 -0.15606419 -0.49515307]\n",
      " [-0.53843534  0.40183406 -0.25984421 -0.42846428  0.01440974 -0.16410939\n",
      "  -1.02380343 -0.22006732  0.29352336]\n",
      " [ 0.04354484  0.01158402  0.25948626 -0.59324714 -0.07099959 -0.04645051\n",
      "  -0.61034734  0.03167882  0.05084005]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.0478109   0.62304345 -1.0416575   0.00960862  0.66950471 -0.89152238\n",
      "  -0.14581938]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:24 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.5052919]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 25 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.38596938 -0.24948642  0.15677314  0.61429444 -0.04392311 -0.04764534\n",
      "   0.73700697  0.15700635 -0.09577893]\n",
      " [-0.34750176  0.42549399 -0.39129661 -0.74843363 -0.0668095  -0.10039823\n",
      "  -1.09217153 -0.34134869  0.19883213]\n",
      " [ 0.17912603 -0.3580985   0.28726684 -0.56321699  0.27358073 -0.44926009\n",
      "  -0.48345513  0.17565358  0.36019273]\n",
      " [ 0.59686762 -0.38221931  0.29618842  0.58699756  0.03717931  0.16397697\n",
      "   1.06807016 -0.15606419 -0.50009763]\n",
      " [-0.53422614  0.40604326 -0.25984421 -0.42425508  0.01440974 -0.16410939\n",
      "  -1.02380343 -0.22006732  0.29773257]\n",
      " [ 0.05079908  0.01883827  0.25948626 -0.5859929  -0.07099959 -0.04645051\n",
      "  -0.61034734  0.03167882  0.05809429]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.01534351  0.58092933 -1.06548063 -0.01563526  0.63289741 -0.91883846\n",
      "  -0.16985184]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:25 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.65440599]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 25 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.39448374 -0.24948642  0.1652875   0.61429444 -0.04392311 -0.04764534\n",
      "   0.74552133  0.15700635 -0.09577893]\n",
      " [-0.35601984  0.42549399 -0.39981469 -0.74843363 -0.0668095  -0.10039823\n",
      "  -1.10068961 -0.34134869  0.19883213]\n",
      " [ 0.17895934 -0.3580985   0.28710016 -0.56321699  0.27358073 -0.44926009\n",
      "  -0.48362182  0.17565358  0.36019273]\n",
      " [ 0.60515478 -0.38221931  0.30447558  0.58699756  0.03717931  0.16397697\n",
      "   1.07635733 -0.15606419 -0.50009763]\n",
      " [-0.54276365  0.40604326 -0.26838173 -0.42425508  0.01440974 -0.16410939\n",
      "  -1.03234095 -0.22006732  0.29773257]\n",
      " [ 0.04793252  0.01883827  0.2566197  -0.5859929  -0.07099959 -0.04645051\n",
      "  -0.61321391  0.03167882  0.05809429]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.02373605  0.61150545 -1.06008285  0.00373782  0.6671567  -0.91337947\n",
      "  -0.15322183]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:25 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.64312008]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 25 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.37956244 -0.26440772  0.1503662   0.61429444 -0.04392311 -0.04764534\n",
      "   0.73060003  0.15700635 -0.09577893]\n",
      " [-0.33957422  0.4419396  -0.38336907 -0.74843363 -0.0668095  -0.10039823\n",
      "  -1.08424399 -0.34134869  0.19883213]\n",
      " [ 0.1856517  -0.35140614  0.29379252 -0.56321699  0.27358073 -0.44926009\n",
      "  -0.47692946  0.17565358  0.36019273]\n",
      " [ 0.58865322 -0.39872087  0.28797403  0.58699756  0.03717931  0.16397697\n",
      "   1.05985577 -0.15606419 -0.50009763]\n",
      " [-0.52630938  0.42249754 -0.25192745 -0.42425508  0.01440974 -0.16410939\n",
      "  -1.01588667 -0.22006732  0.29773257]\n",
      " [ 0.05316926  0.02407501  0.26185644 -0.5859929  -0.07099959 -0.04645051\n",
      "  -0.60797716  0.03167882  0.05809429]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.05006733  0.55675159 -1.07432235 -0.02631299  0.60571211 -0.92754541\n",
      "  -0.18481315]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:25 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.5192179]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 25 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.38874185 -0.26440772  0.15954562  0.61429444 -0.04392311 -0.04764534\n",
      "   0.73060003  0.16618576 -0.09577893]\n",
      " [-0.35175386  0.4419396  -0.39554871 -0.74843363 -0.0668095  -0.10039823\n",
      "  -1.08424399 -0.35352832  0.19883213]\n",
      " [ 0.1944963  -0.35140614  0.30263711 -0.56321699  0.27358073 -0.44926009\n",
      "  -0.47692946  0.18449818  0.36019273]\n",
      " [ 0.59817317 -0.39872087  0.29749397  0.58699756  0.03717931  0.16397697\n",
      "   1.05985577 -0.14654424 -0.50009763]\n",
      " [-0.53809708  0.42249754 -0.26371515 -0.42425508  0.01440974 -0.16410939\n",
      "  -1.01588667 -0.23185502  0.29773257]\n",
      " [ 0.05821738  0.02407501  0.26690456 -0.5859929  -0.07099959 -0.04645051\n",
      "  -0.60797716  0.03672693  0.05809429]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.00994165  0.59667465 -1.05893064  0.01318242  0.64608201 -0.91138649\n",
      "  -0.1496588 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:25 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.54938766]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 25 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.3991091  -0.26440772  0.15954562  0.62466169 -0.04392311 -0.04764534\n",
      "   0.73060003  0.16618576 -0.08541168]\n",
      " [-0.36207946  0.4419396  -0.39554871 -0.75875922 -0.0668095  -0.10039823\n",
      "  -1.08424399 -0.35352832  0.18850654]\n",
      " [ 0.19437738 -0.35140614  0.30263711 -0.56333591  0.27358073 -0.44926009\n",
      "  -0.47692946  0.18449818  0.36007382]\n",
      " [ 0.60668734 -0.39872087  0.29749397  0.59551172  0.03717931  0.16397697\n",
      "   1.05985577 -0.14654424 -0.49158346]\n",
      " [-0.54641216  0.42249754 -0.26371515 -0.43257017  0.01440974 -0.16410939\n",
      "  -1.01588667 -0.23185502  0.28941748]\n",
      " [ 0.05201635  0.02407501  0.26690456 -0.59219393 -0.07099959 -0.04645051\n",
      "  -0.60797716  0.03672693  0.05189326]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.06571864  0.63641233 -1.0428238   0.040952    0.68316645 -0.8924389\n",
      "  -0.12820186]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:25 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.49619614]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 25 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.39570708 -0.26440772  0.15954562  0.62466169 -0.04732513 -0.05104736\n",
      "   0.73060003  0.16618576 -0.0888137 ]\n",
      " [-0.35694606  0.4419396  -0.39554871 -0.75875922 -0.0616761  -0.09526484\n",
      "  -1.08424399 -0.35352832  0.19363994]\n",
      " [ 0.18871015 -0.35140614  0.30263711 -0.56333591  0.26791349 -0.45492732\n",
      "  -0.47692946  0.18449818  0.35440658]\n",
      " [ 0.60190425 -0.39872087  0.29749397  0.59551172  0.03239623  0.15919389\n",
      "   1.05985577 -0.14654424 -0.49636655]\n",
      " [-0.54036     0.42249754 -0.26371515 -0.43257017  0.0204619  -0.15805723\n",
      "  -1.01588667 -0.23185502  0.29546964]\n",
      " [ 0.05222629  0.02407501  0.26690456 -0.59219393 -0.07078965 -0.04624057\n",
      "  -0.60797716  0.03672693  0.0521032 ]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.00369771  0.6019718  -1.06860093  0.00413782  0.64729277 -0.91722898\n",
      "  -0.15900237]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:25 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.37544139]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 25 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.3963216  -0.26379319  0.15954562  0.62466169 -0.04671061 -0.05043284\n",
      "   0.73060003  0.16618576 -0.08819918]\n",
      " [-0.35828027  0.44060539 -0.39554871 -0.75875922 -0.06301032 -0.09659905\n",
      "  -1.08424399 -0.35352832  0.19230572]\n",
      " [ 0.18865846 -0.35145782  0.30263711 -0.56333591  0.26786181 -0.45497901\n",
      "  -0.47692946  0.18449818  0.35435489]\n",
      " [ 0.60301934 -0.39760578  0.29749397  0.59551172  0.03351132  0.16030898\n",
      "   1.05985577 -0.14654424 -0.49525145]\n",
      " [-0.54080013  0.42205741 -0.26371515 -0.43257017  0.02002177 -0.15849736\n",
      "  -1.01588667 -0.23185502  0.29502951]\n",
      " [ 0.05210113  0.02394985  0.26690456 -0.59219393 -0.07091481 -0.04636574\n",
      "  -0.60797716  0.03672693  0.05197804]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.04032     0.58057778 -1.0919473  -0.01792273  0.62640093 -0.93967808\n",
      "  -0.1811364 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:25 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.58195752]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 25 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.38457456 -0.27554024  0.15954562  0.62466169 -0.05845766 -0.06217988\n",
      "   0.71885299  0.16618576 -0.08819918]\n",
      " [-0.34335153  0.45553413 -0.39554871 -0.75875922 -0.04808158 -0.08167031\n",
      "  -1.06931526 -0.35352832  0.19230572]\n",
      " [ 0.20105018 -0.3390661   0.30263711 -0.56333591  0.28025353 -0.44258729\n",
      "  -0.46453774  0.18449818  0.35435489]\n",
      " [ 0.58721257 -0.41341256  0.29749397  0.59551172  0.01770454  0.14450221\n",
      "   1.04404899 -0.14654424 -0.49525145]\n",
      " [-0.5253994   0.43745814 -0.26371515 -0.43257017  0.0354225  -0.14309663\n",
      "  -1.00048594 -0.23185502  0.29502951]\n",
      " [ 0.06246012  0.03430884  0.26690456 -0.59219393 -0.06055582 -0.03600675\n",
      "  -0.59761817  0.03672693  0.05197804]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.11111018  0.53225275 -1.10882295 -0.03946505  0.56896224 -0.95516175\n",
      "  -0.20542934]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:25 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.63658686]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 25 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.39387883 -0.27554024  0.15954562  0.63396596 -0.05845766 -0.06217988\n",
      "   0.72815726  0.16618576 -0.08819918]\n",
      " [-0.35173751  0.45553413 -0.39554871 -0.7671452  -0.04808158 -0.08167031\n",
      "  -1.07770123 -0.35352832  0.19230572]\n",
      " [ 0.19369185 -0.3390661   0.30263711 -0.57069424  0.28025353 -0.44258729\n",
      "  -0.47189607  0.18449818  0.35435489]\n",
      " [ 0.59543968 -0.41341256  0.29749397  0.60373883  0.01770454  0.14450221\n",
      "   1.0522761  -0.14654424 -0.49525145]\n",
      " [-0.53431944  0.43745814 -0.26371515 -0.44149021  0.0354225  -0.14309663\n",
      "  -1.00940598 -0.23185502  0.29502951]\n",
      " [ 0.05370258  0.03430884  0.26690456 -0.60095146 -0.06055582 -0.03600675\n",
      "  -0.60637571  0.03672693  0.05197804]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.06907345  0.56794898 -1.10452066 -0.02667256  0.60690578 -0.94996457\n",
      "  -0.19514505]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:25 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.6095993]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 25 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.40301937 -0.27554024  0.15954562  0.63396596 -0.04931712 -0.05303935\n",
      "   0.7372978   0.16618576 -0.08819918]\n",
      " [-0.36213662  0.45553413 -0.39554871 -0.7671452  -0.05848069 -0.09206943\n",
      "  -1.08810035 -0.35352832  0.19230572]\n",
      " [ 0.18881593 -0.3390661   0.30263711 -0.57069424  0.27537761 -0.4474632\n",
      "  -0.47677198  0.18449818  0.35435489]\n",
      " [ 0.60560214 -0.41341256  0.29749397  0.60373883  0.02786701  0.15466467\n",
      "   1.06243856 -0.14654424 -0.49525145]\n",
      " [-0.54467804  0.43745814 -0.26371515 -0.44149021  0.0250639  -0.15345523\n",
      "  -1.01976458 -0.23185502  0.29502951]\n",
      " [ 0.04690435  0.03430884  0.26690456 -0.60095146 -0.06735405 -0.04280497\n",
      "  -0.61317393  0.03672693  0.05197804]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.02261811  0.60192333 -1.09644845 -0.00848005  0.64682729 -0.94248898\n",
      "  -0.17920336]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:25 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.55465604]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 25 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.39038689 -0.27554024  0.15954562  0.62133349 -0.04931712 -0.06567183\n",
      "   0.7372978   0.16618576 -0.10083165]\n",
      " [-0.34846385  0.45553413 -0.39554871 -0.75347243 -0.05848069 -0.07839666\n",
      "  -1.08810035 -0.35352832  0.20597849]\n",
      " [ 0.1965084  -0.3390661   0.30263711 -0.56300177  0.27537761 -0.43977073\n",
      "  -0.47677198  0.18449818  0.36204736]\n",
      " [ 0.59321246 -0.41341256  0.29749397  0.59134915  0.02786701  0.14227499\n",
      "   1.06243856 -0.14654424 -0.50764113]\n",
      " [-0.5325148   0.43745814 -0.26371515 -0.42932697  0.0250639  -0.14129199\n",
      "  -1.01976458 -0.23185502  0.30719276]\n",
      " [ 0.05557609  0.03430884  0.26690456 -0.59227973 -0.06735405 -0.03413324\n",
      "  -0.61317393  0.03672693  0.06064977]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.09112166  0.55328069 -1.11448332 -0.03474682  0.59856747 -0.96307896\n",
      "  -0.20434789]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:25 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.62650311]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 25 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.40010301 -0.26582413  0.15954562  0.6310496  -0.04931712 -0.05595571\n",
      "   0.74701391  0.16618576 -0.10083165]\n",
      " [-0.3580185   0.44597948 -0.39554871 -0.76302708 -0.05848069 -0.0879513\n",
      "  -1.09765499 -0.35352832  0.20597849]\n",
      " [ 0.18674645 -0.34882805  0.30263711 -0.57276372  0.27537761 -0.44953269\n",
      "  -0.48653394  0.18449818  0.36204736]\n",
      " [ 0.60244548 -0.40417954  0.29749397  0.60058217  0.02786701  0.151508\n",
      "   1.07167158 -0.14654424 -0.50764113]\n",
      " [-0.54223109  0.42774184 -0.26371515 -0.43904326  0.0250639  -0.15100828\n",
      "  -1.02948087 -0.23185502  0.30719276]\n",
      " [ 0.04639807  0.02513082  0.26690456 -0.60145775 -0.06735405 -0.04331126\n",
      "  -0.62235195  0.03672693  0.06064977]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.04742309  0.58838898 -1.10835293 -0.02754027  0.63694528 -0.95624554\n",
      "  -0.19383648]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:25 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.64793499]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 25 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.40908961 -0.25683752  0.15954562  0.64003621 -0.04931712 -0.05595571\n",
      "   0.75600052  0.16618576 -0.10083165]\n",
      " [-0.36685396  0.43714402 -0.39554871 -0.77186254 -0.05848069 -0.0879513\n",
      "  -1.10649046 -0.35352832  0.20597849]\n",
      " [ 0.17812186 -0.35745264  0.30263711 -0.58138831  0.27537761 -0.44953269\n",
      "  -0.49515853  0.18449818  0.36204736]\n",
      " [ 0.61113325 -0.39549176  0.29749397  0.60926995  0.02786701  0.151508\n",
      "   1.08035936 -0.14654424 -0.50764113]\n",
      " [-0.55121587  0.41875706 -0.26371515 -0.44802804  0.0250639  -0.15100828\n",
      "  -1.03846566 -0.23185502  0.30719276]\n",
      " [ 0.03795656  0.0166893   0.26690456 -0.60989926 -0.06735405 -0.04331126\n",
      "  -0.63079347  0.03672693  0.06064977]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.00726739  0.62129289 -1.10252215 -0.01839706  0.67174093 -0.94940424\n",
      "  -0.1841962 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:25 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.67854591]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 25 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.41670905 -0.25683752  0.16716505  0.64003621 -0.04931712 -0.04833627\n",
      "   0.76361996  0.16618576 -0.10083165]\n",
      " [-0.37429604  0.43714402 -0.40299079 -0.77186254 -0.05848069 -0.09539338\n",
      "  -1.11393254 -0.35352832  0.20597849]\n",
      " [ 0.1742669  -0.35745264  0.29878215 -0.58138831  0.27537761 -0.45338764\n",
      "  -0.49901348  0.18449818  0.36204736]\n",
      " [ 0.61819848 -0.39549176  0.3045592   0.60926995  0.02786701  0.15857323\n",
      "   1.08742458 -0.14654424 -0.50764113]\n",
      " [-0.55856909  0.41875706 -0.27106837 -0.44802804  0.0250639  -0.1583615\n",
      "  -1.04581888 -0.23185502  0.30719276]\n",
      " [ 0.03482817  0.0166893   0.26377618 -0.60989926 -0.06735405 -0.04643964\n",
      "  -0.63392185  0.03672693  0.06064977]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.02779061  0.64865624 -1.09818166 -0.0048628   0.70311003 -0.94524142\n",
      "  -0.16986716]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:25 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.49560627]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 26 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.4071048  -0.26644177  0.16716505  0.63043196 -0.04931712 -0.04833627\n",
      "   0.76361996  0.16618576 -0.1104359 ]\n",
      " [-0.36697853  0.44446152 -0.40299079 -0.76454503 -0.05848069 -0.09539338\n",
      "  -1.11393254 -0.35352832  0.213296  ]\n",
      " [ 0.18025479 -0.35146476  0.29878215 -0.57540043  0.27537761 -0.45338764\n",
      "  -0.49901348  0.18449818  0.36803525]\n",
      " [ 0.61330547 -0.40038478  0.3045592   0.60437694  0.02786701  0.15857323\n",
      "   1.08742458 -0.14654424 -0.51253415]\n",
      " [-0.55430732  0.42301884 -0.27106837 -0.44376627  0.0250639  -0.1583615\n",
      "  -1.04581888 -0.23185502  0.31145453]\n",
      " [ 0.04207794  0.02393907  0.26377618 -0.60264949 -0.06735405 -0.04643964\n",
      "  -0.63392185  0.03672693  0.06789954]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.03415539  0.60727737 -1.12152461 -0.0296849   0.66715778 -0.97189647\n",
      "  -0.19328732]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:26 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.65962863]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 26 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.41552194 -0.26644177  0.17558219  0.63043196 -0.04931712 -0.04833627\n",
      "   0.7720371   0.16618576 -0.1104359 ]\n",
      " [-0.37522302  0.44446152 -0.41123528 -0.76454503 -0.05848069 -0.09539338\n",
      "  -1.12217703 -0.35352832  0.213296  ]\n",
      " [ 0.18006398 -0.35146476  0.29859135 -0.57540043  0.27537761 -0.45338764\n",
      "  -0.49920429  0.18449818  0.36803525]\n",
      " [ 0.62131791 -0.40038478  0.31257164  0.60437694  0.02786701  0.15857323\n",
      "   1.09543702 -0.14654424 -0.51253415]\n",
      " [-0.562573    0.42301884 -0.27933405 -0.44376627  0.0250639  -0.1583615\n",
      "  -1.05408456 -0.23185502  0.31145453]\n",
      " [ 0.03902692  0.02393907  0.26072515 -0.60264949 -0.06735405 -0.04643964\n",
      "  -0.63697288  0.03672693  0.06789954]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.00405448  0.63754504 -1.11648315 -0.01077079  0.7008341  -0.96679914\n",
      "  -0.17728843]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:26 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.64534151]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 26 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.40032124 -0.28164247  0.1603815   0.63043196 -0.04931712 -0.04833627\n",
      "   0.7568364   0.16618576 -0.1104359 ]\n",
      " [-0.35872759  0.46095696 -0.39473985 -0.76454503 -0.05848069 -0.09539338\n",
      "  -1.10568159 -0.35352832  0.213296  ]\n",
      " [ 0.18670017 -0.34482857  0.30522753 -0.57540043  0.27537761 -0.45338764\n",
      "  -0.49256811  0.18449818  0.36803525]\n",
      " [ 0.6048263  -0.41687638  0.29608004  0.60437694  0.02786701  0.15857323\n",
      "   1.07894542 -0.14654424 -0.51253415]\n",
      " [-0.54606955  0.43952229 -0.2628306  -0.44376627  0.0250639  -0.1583615\n",
      "  -1.03758111 -0.23185502  0.31145453]\n",
      " [ 0.04467137  0.02958353  0.26636961 -0.60264949 -0.06735405 -0.04643964\n",
      "  -0.63132842  0.03672693  0.06789954]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.06979706  0.58218289 -1.13035467 -0.04090624  0.63909239 -0.9805718\n",
      "  -0.20847696]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:26 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.5178123]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 26 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.40993373 -0.28164247  0.16999399  0.63043196 -0.04931712 -0.04833627\n",
      "   0.7568364   0.17579825 -0.1104359 ]\n",
      " [-0.37116975  0.46095696 -0.407182   -0.76454503 -0.05848069 -0.09539338\n",
      "  -1.10568159 -0.36597048  0.213296  ]\n",
      " [ 0.1957988  -0.34482857  0.31432616 -0.57540043  0.27537761 -0.45338764\n",
      "  -0.49256811  0.1935968   0.36803525]\n",
      " [ 0.61470549 -0.41687638  0.30595922  0.60437694  0.02786701  0.15857323\n",
      "   1.07894542 -0.13666506 -0.51253415]\n",
      " [-0.55815361  0.43952229 -0.27491466 -0.44376627  0.0250639  -0.1583615\n",
      "  -1.03758111 -0.24393908  0.31145453]\n",
      " [ 0.04974991  0.02958353  0.27144814 -0.60264949 -0.06735405 -0.04643964\n",
      "  -0.63132842  0.04180547  0.06789954]]\n",
      "\n",
      "Theta two: \n",
      "[[-9.60009311e-03  6.22762995e-01 -1.11539985e+00 -9.99230840e-04\n",
      "   6.80034061e-01 -9.64860135e-01 -1.73196951e-01]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:26 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.54645327]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 26 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.42053765 -0.28164247  0.16999399  0.64103587 -0.04931712 -0.04833627\n",
      "   0.7568364   0.17579825 -0.09983199]\n",
      " [-0.38172225  0.46095696 -0.407182   -0.77509753 -0.05848069 -0.09539338\n",
      "  -1.10568159 -0.36597048  0.20274351]\n",
      " [ 0.19563628 -0.34482857  0.31432616 -0.57556294  0.27537761 -0.45338764\n",
      "  -0.49256811  0.1935968   0.36787274]\n",
      " [ 0.62349043 -0.41687638  0.30595922  0.61316188  0.02786701  0.15857323\n",
      "   1.07894542 -0.13666506 -0.5037492 ]\n",
      " [-0.56678506  0.43952229 -0.27491466 -0.45239772  0.0250639  -0.1583615\n",
      "  -1.03758111 -0.24393908  0.30282308]\n",
      " [ 0.04332072  0.02958353  0.27144814 -0.60907868 -0.06735405 -0.04643964\n",
      "  -0.63132842  0.04180547  0.06147036]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.04660389  0.66306468 -1.09941177  0.02694024  0.71767039 -0.94609196\n",
      "  -0.15177918]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:26 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.48236537]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 26 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.41722098 -0.28164247  0.16999399  0.64103587 -0.05263378 -0.05165294\n",
      "   0.7568364   0.17579825 -0.10314866]\n",
      " [-0.37684737  0.46095696 -0.407182   -0.77509753 -0.05360581 -0.0905185\n",
      "  -1.10568159 -0.36597048  0.20761839]\n",
      " [ 0.19004292 -0.34482857  0.31432616 -0.57556294  0.26978426 -0.458981\n",
      "  -0.49256811  0.1935968   0.36227938]\n",
      " [ 0.61898719 -0.41687638  0.30595922  0.61316188  0.02336376  0.15406999\n",
      "   1.07894542 -0.13666506 -0.50825244]\n",
      " [-0.56103415  0.43952229 -0.27491466 -0.45239772  0.03081482 -0.15261058\n",
      "  -1.03758111 -0.24393908  0.30857399]\n",
      " [ 0.04345625  0.02958353  0.27144814 -0.60907868 -0.06721851 -0.04630411\n",
      "  -0.63132842  0.04180547  0.06160589]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.01361678  0.62961011 -1.12455671 -0.00890302  0.68298612 -0.97029892\n",
      "  -0.18175398]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:26 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.35627344]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 26 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.41795395 -0.2809095   0.16999399  0.64103587 -0.05190081 -0.05091997\n",
      "   0.7568364   0.17579825 -0.10241569]\n",
      " [-0.37834675  0.45945758 -0.407182   -0.77509753 -0.0551052  -0.09201788\n",
      "  -1.10568159 -0.36597048  0.20611901]\n",
      " [ 0.18985606 -0.34501544  0.31432616 -0.57556294  0.26959739 -0.45916786\n",
      "  -0.49256811  0.1935968   0.36209252]\n",
      " [ 0.62029633 -0.41556724  0.30595922  0.61316188  0.0246729   0.15537913\n",
      "   1.07894542 -0.13666506 -0.5069433 ]\n",
      " [-0.56170004  0.4388564  -0.27491466 -0.45239772  0.03014892 -0.15327648\n",
      "  -1.03758111 -0.24393908  0.3079081 ]\n",
      " [ 0.04324053  0.02936781  0.27144814 -0.60907868 -0.06743423 -0.04651983\n",
      "  -0.63132842  0.04180547  0.06139017]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.05447113  0.60991654 -1.14648871 -0.02951707  0.6638717  -0.99139246\n",
      "  -0.20239689]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:26 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.58436847]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 26 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.40590471 -0.29295874  0.16999399  0.64103587 -0.06395006 -0.06296921\n",
      "   0.74478716  0.17579825 -0.10241569]\n",
      " [-0.36333026  0.47447407 -0.407182   -0.77509753 -0.04008871 -0.07700139\n",
      "  -1.0906651  -0.36597048  0.20611901]\n",
      " [ 0.20238399 -0.3324875   0.31432616 -0.57556294  0.28212533 -0.44663993\n",
      "  -0.48004017  0.1935968   0.36209252]\n",
      " [ 0.60444576 -0.4314178   0.30595922  0.61316188  0.00882234  0.13952856\n",
      "   1.06309485 -0.13666506 -0.5069433 ]\n",
      " [-0.54622581  0.45433063 -0.27491466 -0.45239772  0.04562316 -0.13780224\n",
      "  -1.02210687 -0.24393908  0.3079081 ]\n",
      " [ 0.05392044  0.04004772  0.27144814 -0.60907868 -0.05675432 -0.03583992\n",
      "  -0.62064852  0.04180547  0.06139017]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.12543741  0.56108756 -1.16327571 -0.05095628  0.60623984 -1.00678833\n",
      "  -0.22637627]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:26 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.64703635]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 26 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.41474763 -0.29295874  0.16999399  0.64987879 -0.06395006 -0.06296921\n",
      "   0.75363008  0.17579825 -0.10241569]\n",
      " [-0.37121192  0.47447407 -0.407182   -0.78297919 -0.04008871 -0.07700139\n",
      "  -1.09854677 -0.36597048  0.20611901]\n",
      " [ 0.19517945 -0.3324875   0.31432616 -0.58276749  0.28212533 -0.44663993\n",
      "  -0.48724472  0.1935968   0.36209252]\n",
      " [ 0.61217969 -0.4314178   0.30595922  0.62089581  0.00882234  0.13952856\n",
      "   1.07082878 -0.13666506 -0.5069433 ]\n",
      " [-0.55464273  0.45433063 -0.27491466 -0.46081465  0.04562316 -0.13780224\n",
      "  -1.0305238  -0.24393908  0.3079081 ]\n",
      " [ 0.04538051  0.04004772  0.27144814 -0.61761861 -0.05675432 -0.03583992\n",
      "  -0.62918845  0.04180547  0.06139017]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.08513243  0.59563452 -1.15935935 -0.03891484  0.64280718 -1.00207089\n",
      "  -0.21687208]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:26 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.61451112]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 26 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.42391635 -0.29295874  0.16999399  0.64987879 -0.05478133 -0.05380049\n",
      "   0.7627988   0.17579825 -0.10241569]\n",
      " [-0.38142686  0.47447407 -0.407182   -0.78297919 -0.05030365 -0.08721633\n",
      "  -1.10876171 -0.36597048  0.20611901]\n",
      " [ 0.19023018 -0.3324875   0.31432616 -0.58276749  0.27717607 -0.45158919\n",
      "  -0.49219398  0.1935968   0.36209252]\n",
      " [ 0.62213112 -0.4314178   0.30595922  0.62089581  0.01877376  0.14947999\n",
      "   1.08078021 -0.13666506 -0.5069433 ]\n",
      " [-0.56480237  0.45433063 -0.27491466 -0.46081465  0.03546352 -0.14796188\n",
      "  -1.04068344 -0.24393908  0.3079081 ]\n",
      " [ 0.0384795   0.04004772  0.27144814 -0.61761861 -0.06365534 -0.04274094\n",
      "  -0.63608946  0.04180547  0.06139017]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.03947374  0.62938229 -1.15160524 -0.02120852  0.68216148 -0.99488205\n",
      "  -0.20148214]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:26 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.554216]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 26 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.41110308 -0.29295874  0.16999399  0.63706552 -0.05478133 -0.06661376\n",
      "   0.7627988   0.17579825 -0.11522896]\n",
      " [-0.36765223  0.47447407 -0.407182   -0.76920455 -0.05030365 -0.0734417\n",
      "  -1.10876171 -0.36597048  0.21989364]\n",
      " [ 0.19801921 -0.3324875   0.31432616 -0.57497846  0.27717607 -0.44380017\n",
      "  -0.49219398  0.1935968   0.36988154]\n",
      " [ 0.60959649 -0.4314178   0.30595922  0.60836119  0.01877376  0.13694536\n",
      "   1.08078021 -0.13666506 -0.51947793]\n",
      " [-0.55244852  0.45433063 -0.27491466 -0.4484608   0.03546352 -0.13560803\n",
      "  -1.04068344 -0.24393908  0.32026194]\n",
      " [ 0.04735697  0.04004772  0.27144814 -0.60874114 -0.06365534 -0.03386346\n",
      "  -0.63608946  0.04180547  0.07026765]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.10793622  0.58046253 -1.16941161 -0.04734557  0.63369184 -1.01515762\n",
      "  -0.22636374]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:26 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.63342971]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 26 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.42060113 -0.28346069  0.16999399  0.64656356 -0.05478133 -0.05711571\n",
      "   0.77229685  0.17579825 -0.11522896]\n",
      " [-0.37690566  0.46522063 -0.407182   -0.77845799 -0.05030365 -0.08269514\n",
      "  -1.11801514 -0.36597048  0.21989364]\n",
      " [ 0.1885256  -0.34198111  0.31432616 -0.58447207  0.27717607 -0.45329378\n",
      "  -0.50168759  0.1935968   0.36988154]\n",
      " [ 0.61852316 -0.42249114  0.30595922  0.61728785  0.01877376  0.14587203\n",
      "   1.08970688 -0.13666506 -0.51947793]\n",
      " [-0.56187402  0.44490513 -0.27491466 -0.4578863   0.03546352 -0.14503353\n",
      "  -1.05010893 -0.24393908  0.32026194]\n",
      " [ 0.03829572  0.03098646  0.27144814 -0.61780239 -0.06365534 -0.04292472\n",
      "  -0.64515072  0.04180547  0.07026765]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.06537805  0.61494371 -1.163602   -0.0404628   0.67119599 -1.00870998\n",
      "  -0.21644648]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:26 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.65412845]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 26 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.42935978 -0.27470204  0.16999399  0.65532222 -0.05478133 -0.05711571\n",
      "   0.7810555   0.17579825 -0.11522896]\n",
      " [-0.38546723  0.45665906 -0.407182   -0.78701956 -0.05030365 -0.08269514\n",
      "  -1.12657671 -0.36597048  0.21989364]\n",
      " [ 0.18008153 -0.35042518  0.31432616 -0.59291614  0.27717607 -0.45329378\n",
      "  -0.51013166  0.1935968   0.36988154]\n",
      " [ 0.62693141 -0.41408289  0.30595922  0.6256961   0.01877376  0.14587203\n",
      "   1.09811512 -0.13666506 -0.51947793]\n",
      " [-0.57061307  0.43616608 -0.27491466 -0.46662535  0.03546352 -0.14503353\n",
      "  -1.05884798 -0.24393908  0.32026194]\n",
      " [ 0.02995923  0.02264998  0.27144814 -0.62613887 -0.06365534 -0.04292472\n",
      "  -0.6534872   0.04180547  0.07026765]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.02625229  0.64725293 -1.15809072 -0.03167896  0.70524454 -1.00227296\n",
      "  -0.20734571]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:26 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.68362547]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 26 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.43687782 -0.27470204  0.17751203  0.65532222 -0.05478133 -0.04959767\n",
      "   0.78857354  0.17579825 -0.11522896]\n",
      " [-0.3926479   0.45665906 -0.41436268 -0.78701956 -0.05030365 -0.08987582\n",
      "  -1.13375738 -0.36597048  0.21989364]\n",
      " [ 0.17628268 -0.35042518  0.31052731 -0.59291614  0.27717607 -0.45709263\n",
      "  -0.51393051  0.1935968   0.36988154]\n",
      " [ 0.63374413 -0.41408289  0.31277194  0.6256961   0.01877376  0.15268475\n",
      "   1.10492784 -0.13666506 -0.51947793]\n",
      " [-0.57770118  0.43616608 -0.28200277 -0.46662535  0.03546352 -0.15212164\n",
      "  -1.06593609 -0.24393908  0.32026194]\n",
      " [ 0.0267091   0.02264998  0.26819801 -0.62613887 -0.06365534 -0.04617485\n",
      "  -0.65673734  0.04180547  0.07026765]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.00796072  0.67427194 -1.15401932 -0.01851212  0.73597308 -0.99836884\n",
      "  -0.19357452]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:26 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.48584602]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 27 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.42743671 -0.28414315  0.17751203  0.64588111 -0.05478133 -0.04959767\n",
      "   0.78857354  0.17579825 -0.12467007]\n",
      " [-0.38547872  0.46382825 -0.41436268 -0.77985037 -0.05030365 -0.08987582\n",
      "  -1.13375738 -0.36597048  0.22706283]\n",
      " [ 0.18207655 -0.34463131  0.31052731 -0.58712227  0.27717607 -0.45709263\n",
      "  -0.51393051  0.1935968   0.37567541]\n",
      " [ 0.62892933 -0.41889769  0.31277194  0.6208813   0.01877376  0.15268475\n",
      "   1.10492784 -0.13666506 -0.52429272]\n",
      " [-0.57342288  0.44044439 -0.28200277 -0.46234705  0.03546352 -0.15212164\n",
      "  -1.06593609 -0.24393908  0.32454025]\n",
      " [ 0.03392064  0.02986152  0.26819801 -0.61892733 -0.06365534 -0.04617485\n",
      "  -0.65673734  0.04180547  0.07747919]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.05272137  0.6336944  -1.17688488 -0.04290576  0.70073157 -1.02437223\n",
      "  -0.21639169]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:27 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.66498261]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 27 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.43572113 -0.28414315  0.18579644  0.64588111 -0.05478133 -0.04959767\n",
      "   0.79685796  0.17579825 -0.12467007]\n",
      " [-0.39344402  0.46382825 -0.42232798 -0.77985037 -0.05030365 -0.08987582\n",
      "  -1.14172269 -0.36597048  0.22706283]\n",
      " [ 0.1818776  -0.34463131  0.31032836 -0.58712227  0.27717607 -0.45709263\n",
      "  -0.51412946  0.1935968   0.37567541]\n",
      " [ 0.63666675 -0.41889769  0.32050936  0.6208813   0.01877376  0.15268475\n",
      "   1.11266526 -0.13666506 -0.52429272]\n",
      " [-0.58141035  0.44044439 -0.28999024 -0.46234705  0.03546352 -0.15212164\n",
      "  -1.07392357 -0.24393908  0.32454025]\n",
      " [ 0.03071412  0.02986152  0.26499149 -0.61892733 -0.06365534 -0.04617485\n",
      "  -0.65994386  0.04180547  0.07747919]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.01540366  0.66359163 -1.17216968 -0.02444586  0.73378038 -1.01960639\n",
      "  -0.20100698]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:27 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.64754619]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 27 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.42028709 -0.29957718  0.17036241  0.64588111 -0.05478133 -0.04959767\n",
      "   0.78142392  0.17579825 -0.12467007]\n",
      " [-0.37691585  0.48035642 -0.40579982 -0.77985037 -0.05030365 -0.08987582\n",
      "  -1.12519452 -0.36597048  0.22706283]\n",
      " [ 0.18842676 -0.33808216  0.31687752 -0.58712227  0.27717607 -0.45709263\n",
      "  -0.5075803   0.1935968   0.37567541]\n",
      " [ 0.62018917 -0.43537526  0.30403178  0.6208813   0.01877376  0.15268475\n",
      "   1.09618768 -0.13666506 -0.52429272]\n",
      " [-0.56487624  0.4569785  -0.27345613 -0.46234705  0.03546352 -0.15212164\n",
      "  -1.05738945 -0.24393908  0.32454025]\n",
      " [ 0.03672179  0.0358692   0.27099916 -0.61892733 -0.06365534 -0.04617485\n",
      "  -0.65393618  0.04180547  0.07747919]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.08929843  0.60768143 -1.18571992 -0.05469644  0.67178129 -1.03303299\n",
      "  -0.23183411]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:27 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.51697788]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 27 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.43029908 -0.29957718  0.1803744   0.64588111 -0.05478133 -0.04959767\n",
      "   0.78142392  0.18581024 -0.12467007]\n",
      " [-0.38957786  0.48035642 -0.41846183 -0.77985037 -0.05030365 -0.08987582\n",
      "  -1.12519452 -0.3786325   0.22706283]\n",
      " [ 0.19777528 -0.33808216  0.32622605 -0.58712227  0.27717607 -0.45709263\n",
      "  -0.5075803   0.20294533  0.37567541]\n",
      " [ 0.63039685 -0.43537526  0.31423946  0.6208813   0.01877376  0.15268475\n",
      "   1.09618768 -0.12645738 -0.52429272]\n",
      " [-0.5772141   0.4569785  -0.28579399 -0.46234705  0.03546352 -0.15212164\n",
      "  -1.05738945 -0.25627695  0.32454025]\n",
      " [ 0.04183389  0.0358692   0.27611126 -0.61892733 -0.06365534 -0.04617485\n",
      "  -0.65393618  0.04691756  0.07747919]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.02899028  0.64885694 -1.17120199 -0.01441397  0.71323146 -1.01777042\n",
      "  -0.19646322]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:27 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.54384204]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 27 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.44111905 -0.29957718  0.1803744   0.65670108 -0.05478133 -0.04959767\n",
      "   0.78142392  0.18581024 -0.1138501 ]\n",
      " [-0.40033715  0.48035642 -0.41846183 -0.79060965 -0.05030365 -0.08987582\n",
      "  -1.12519452 -0.3786325   0.21630354]\n",
      " [ 0.1975819  -0.33808216  0.32622605 -0.58731565  0.27717607 -0.45709263\n",
      "  -0.5075803   0.20294533  0.37548203]\n",
      " [ 0.63943288 -0.43537526  0.31423946  0.62991734  0.01877376  0.15268475\n",
      "   1.09618768 -0.12645738 -0.51525669]\n",
      " [-0.58613825  0.4569785  -0.28579399 -0.4712712   0.03546352 -0.15212164\n",
      "  -1.05738945 -0.25627695  0.3156161 ]\n",
      " [ 0.03518998  0.0358692   0.27611126 -0.62557124 -0.06365534 -0.04617485\n",
      "  -0.65393618  0.04691756  0.07083527]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.02759107  0.68967578 -1.15533529  0.01368332  0.75137541 -0.99918399\n",
      "  -0.17509633]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:27 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.46839392]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 27 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Theta One: \n",
      "[[ 0.43790961 -0.29957718  0.1803744   0.65670108 -0.05799078 -0.05280712\n",
      "   0.78142392  0.18581024 -0.11705954]\n",
      " [-0.39573257  0.48035642 -0.41846183 -0.79060965 -0.04569907 -0.08527124\n",
      "  -1.12519452 -0.3786325   0.22090812]\n",
      " [ 0.19206617 -0.33808216  0.32622605 -0.58731565  0.27166033 -0.46260836\n",
      "  -0.5075803   0.20294533  0.3699663 ]\n",
      " [ 0.63521569 -0.43537526  0.31423946  0.62991734  0.01455657  0.14846756\n",
      "   1.09618768 -0.12645738 -0.51947388]\n",
      " [-0.58069999  0.4569785  -0.28579399 -0.4712712   0.04090178 -0.14668338\n",
      "  -1.05738945 -0.25627695  0.32105436]\n",
      " [ 0.03524545  0.0358692   0.27611126 -0.62557124 -0.06359987 -0.04611938\n",
      "  -0.65393618  0.04691756  0.07089075]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.03072422  0.65728205 -1.17980726 -0.02113325  0.71793887 -1.02276648\n",
      "  -0.2041985 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:27 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.33706726]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 27 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.43875078 -0.29873601  0.1803744   0.65670108 -0.0571496  -0.05196594\n",
      "   0.78142392  0.18581024 -0.11621837]\n",
      " [-0.39736358  0.47872541 -0.41846183 -0.79060965 -0.04733008 -0.08690225\n",
      "  -1.12519452 -0.3786325   0.21927711]\n",
      " [ 0.19175554 -0.33839278  0.32622605 -0.58731565  0.27134971 -0.46291899\n",
      "  -0.5075803   0.20294533  0.36965567]\n",
      " [ 0.63668114 -0.43390982  0.31423946  0.62991734  0.01602202  0.14993301\n",
      "   1.09618768 -0.12645738 -0.51800843]\n",
      " [-0.58156013  0.45611836 -0.28579399 -0.4712712   0.04004165 -0.14754351\n",
      "  -1.05738945 -0.25627695  0.32019422]\n",
      " [ 0.03494156  0.03556531  0.27611126 -0.62557124 -0.06390376 -0.04642327\n",
      "  -0.65393618  0.04691756  0.07058686]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.06838356  0.63929469 -1.20027623 -0.0402736   0.70058065 -1.04245748\n",
      "  -0.22333211]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:27 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.58648466]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 27 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 4.26451303e-01 -3.11035487e-01  1.80374397e-01  6.56701077e-01\n",
      "  -6.94490806e-02 -6.42654215e-02  7.69124443e-01  1.85810244e-01\n",
      "  -1.16218368e-01]\n",
      " [-3.82284136e-01  4.93804852e-01 -4.18461829e-01 -7.90609650e-01\n",
      "  -3.22506350e-02 -7.18228055e-02 -1.11011507e+00 -3.78632496e-01\n",
      "   2.19277111e-01]\n",
      " [ 2.04394682e-01 -3.25753645e-01  3.26226045e-01 -5.87315652e-01\n",
      "   2.83988845e-01 -4.50279852e-01 -4.94941162e-01  2.02945328e-01\n",
      "   3.69655669e-01]\n",
      " [ 6.20795752e-01 -4.49795206e-01  3.14239458e-01  6.29917336e-01\n",
      "   1.36632432e-04  1.34047622e-01  1.08030230e+00 -1.26457380e-01\n",
      "  -5.18008432e-01]\n",
      " [-5.66030899e-01  4.71647591e-01 -2.85793993e-01 -4.71271196e-01\n",
      "   5.55708728e-02 -1.32014287e-01 -1.04186023e+00 -2.56276948e-01\n",
      "   3.20194223e-01]\n",
      " [ 4.59033396e-02  4.65270890e-02  2.76111256e-01 -6.25571244e-01\n",
      "  -5.29419732e-02 -3.54614900e-02 -6.42974397e-01  4.69175645e-02\n",
      "   7.05868569e-02]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.1395008   0.59003857 -1.21701725 -0.06163135  0.64281331 -1.05780433\n",
      "  -0.24702823]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:27 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.65744296]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 27 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 4.34826400e-01 -3.11035487e-01  1.80374397e-01  6.65076174e-01\n",
      "  -6.94490806e-02 -6.42654215e-02  7.77499541e-01  1.85810244e-01\n",
      "  -1.16218368e-01]\n",
      " [-3.89679479e-01  4.93804852e-01 -4.18461829e-01 -7.98004994e-01\n",
      "  -3.22506350e-02 -7.18228055e-02 -1.11751042e+00 -3.78632496e-01\n",
      "   2.19277111e-01]\n",
      " [ 1.97371320e-01 -3.25753645e-01  3.26226045e-01 -5.94339015e-01\n",
      "   2.83988845e-01 -4.50279852e-01 -5.01964524e-01  2.02945328e-01\n",
      "   3.69655669e-01]\n",
      " [ 6.28055519e-01 -4.49795206e-01  3.14239458e-01  6.37177103e-01\n",
      "   1.36632432e-04  1.34047622e-01  1.08756206e+00 -1.26457380e-01\n",
      "  -5.18008432e-01]\n",
      " [-5.73953751e-01  4.71647591e-01 -2.85793993e-01 -4.79194049e-01\n",
      "   5.55708728e-02 -1.32014287e-01 -1.04978308e+00 -2.56276948e-01\n",
      "   3.20194223e-01]\n",
      " [ 3.76156173e-02  4.65270890e-02  2.76111256e-01 -6.33858967e-01\n",
      "  -5.29419732e-02 -3.54614900e-02 -6.51262120e-01  4.69175645e-02\n",
      "   7.05868569e-02]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.10092687  0.62338165 -1.21344762 -0.05030523  0.67797011 -1.05351728\n",
      "  -0.23825375]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:27 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.6194397]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 27 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.44398845 -0.31103549  0.1803744   0.66507617 -0.06028703 -0.05510337\n",
      "   0.78666159  0.18581024 -0.11621837]\n",
      " [-0.39970533  0.49380485 -0.41846183 -0.79800499 -0.04227649 -0.08184866\n",
      "  -1.12753627 -0.3786325   0.21927711]\n",
      " [ 0.1923731  -0.32575364  0.32622605 -0.59433901  0.27899063 -0.45527807\n",
      "  -0.50696274  0.20294533  0.36965567]\n",
      " [ 0.637799   -0.44979521  0.31423946  0.6371771   0.00988012  0.14379111\n",
      "   1.09730555 -0.12645738 -0.51800843]\n",
      " [-0.58391283  0.47164759 -0.28579399 -0.47919405  0.04561179 -0.14197337\n",
      "  -1.05974216 -0.25627695  0.32019422]\n",
      " [ 0.03063852  0.04652709  0.27611126 -0.63385897 -0.05991908 -0.04243859\n",
      "  -0.65823922  0.04691756  0.07058686]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.05607134  0.65685425 -1.20598307 -0.03306245  0.71672994 -1.04658971\n",
      "  -0.22339052]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:27 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.55394039]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 27 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.43101238 -0.31103549  0.1803744   0.6521001  -0.06028703 -0.06807944\n",
      "   0.78666159  0.18581024 -0.12919444]\n",
      " [-0.3858405   0.49380485 -0.41846183 -0.78414016 -0.04227649 -0.06798383\n",
      "  -1.12753627 -0.3786325   0.23314194]\n",
      " [ 0.20023851 -0.32575364  0.32622605 -0.5864736   0.27899063 -0.44741266\n",
      "  -0.50696274  0.20294533  0.37752108]\n",
      " [ 0.62513501 -0.44979521  0.31423946  0.6245131   0.00988012  0.13112711\n",
      "   1.09730555 -0.12645738 -0.53067243]\n",
      " [-0.57138899  0.47164759 -0.28579399 -0.4666702   0.04561179 -0.12944952\n",
      "  -1.05974216 -0.25627695  0.33271807]\n",
      " [ 0.03970689  0.04652709  0.27611126 -0.6247906  -0.05991908 -0.03337022\n",
      "  -0.65823922  0.04691756  0.07965523]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.12450803  0.60767267 -1.22358879 -0.0591      0.66806425 -1.0665846\n",
      "  -0.24803233]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:27 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.64015914]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 27 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.44028156 -0.30176631  0.1803744   0.66136928 -0.06028703 -0.05881026\n",
      "   0.79593077  0.18581024 -0.12919444]\n",
      " [-0.39480527  0.48484009 -0.41846183 -0.79310493 -0.04227649 -0.07694859\n",
      "  -1.13650103 -0.3786325   0.23314194]\n",
      " [ 0.19100767 -0.33498449  0.32622605 -0.59570445  0.27899063 -0.4566435\n",
      "  -0.51619359  0.20294533  0.37752108]\n",
      " [ 0.63377232 -0.44115789  0.31423946  0.63315042  0.00988012  0.13976443\n",
      "   1.10594287 -0.12645738 -0.53067243]\n",
      " [-0.58053025  0.46250632 -0.28579399 -0.47581147  0.04561179 -0.13859079\n",
      "  -1.06888342 -0.25627695  0.33271807]\n",
      " [ 0.03078413  0.03760433  0.27611126 -0.63371336 -0.05991908 -0.04229298\n",
      "  -0.66716198  0.04691756  0.07965523]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.08306238  0.64149966 -1.21806219 -0.0525139   0.70469121 -1.06047792\n",
      "  -0.2386593 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:27 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.66021479]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 27 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.44880563 -0.29324224  0.1803744   0.66989335 -0.06028703 -0.05881026\n",
      "   0.80445484  0.18581024 -0.12919444]\n",
      " [-0.40309965  0.4765457  -0.41846183 -0.80139931 -0.04227649 -0.07694859\n",
      "  -1.14479542 -0.3786325   0.23314194]\n",
      " [ 0.18274915 -0.34324301  0.32622605 -0.60396297  0.27899063 -0.4566435\n",
      "  -0.5244521   0.20294533  0.37752108]\n",
      " [ 0.64191074 -0.43301947  0.31423946  0.64128884  0.00988012  0.13976443\n",
      "   1.11408129 -0.12645738 -0.53067243]\n",
      " [-0.58902117  0.4540154  -0.28579399 -0.48430239  0.04561179 -0.13859079\n",
      "  -1.07737435 -0.25627695  0.33271807]\n",
      " [ 0.02257398  0.02939419  0.27611126 -0.6419235  -0.05991908 -0.04229298\n",
      "  -0.67537212  0.04691756  0.07965523]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.04495017  0.6731874  -1.21283693 -0.04406499  0.73797989 -1.05440294\n",
      "  -0.2300556 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:27 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.68870095]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 27 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.45619663 -0.29324224  0.1877654   0.66989335 -0.06028703 -0.05141927\n",
      "   0.81184584  0.18581024 -0.12919444]\n",
      " [-0.41002504  0.4765457  -0.42538722 -0.80139931 -0.04227649 -0.08387398\n",
      "  -1.15172081 -0.3786325   0.23314194]\n",
      " [ 0.17902206 -0.34324301  0.32249896 -0.60396297  0.27899063 -0.46037059\n",
      "  -0.52817919  0.20294533  0.37752108]\n",
      " [ 0.64848046 -0.43301947  0.32080918  0.64128884  0.00988012  0.14633414\n",
      "   1.120651   -0.12645738 -0.53067243]\n",
      " [-0.59585139  0.4540154  -0.2926242  -0.48430239  0.04561179 -0.145421\n",
      "  -1.08420456 -0.25627695  0.33271807]\n",
      " [ 0.01922766  0.02939419  0.27276493 -0.6419235  -0.05991908 -0.0456393\n",
      "  -0.67871845  0.04691756  0.07965523]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.01158016  0.69982195 -1.20900837 -0.03124709  0.76805105 -1.05073237\n",
      "  -0.21681569]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:27 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.4760482]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 28 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.44694586 -0.30249301  0.1877654   0.66064258 -0.06028703 -0.05141927\n",
      "   0.81184584  0.18581024 -0.13844521]\n",
      " [-0.40302777  0.48354298 -0.42538722 -0.79440204 -0.04227649 -0.08387398\n",
      "  -1.15172081 -0.3786325   0.24013921]\n",
      " [ 0.1846047  -0.33766037  0.32249896 -0.59838033  0.27899063 -0.46037059\n",
      "  -0.52817919  0.20294533  0.38310372]\n",
      " [ 0.6437671  -0.43773283  0.32080918  0.63657548  0.00988012  0.14633414\n",
      "   1.120651   -0.12645738 -0.53538579]\n",
      " [-0.59158874  0.45827805 -0.2926242  -0.48003975  0.04561179 -0.145421\n",
      "  -1.08420456 -0.25627695  0.33698071]\n",
      " [ 0.02636986  0.03653638  0.27276493 -0.6347813  -0.05991908 -0.0456393\n",
      "  -0.67871845  0.04691756  0.08679742]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.07094963  0.66010394 -1.23139854 -0.0552061   0.73356899 -1.07609303\n",
      "  -0.23904   ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:28 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.67044255]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 28 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.4550685  -0.30249301  0.19588803  0.66064258 -0.06028703 -0.05141927\n",
      "   0.81996848  0.18581024 -0.13844521]\n",
      " [-0.41071206  0.48354298 -0.43307151 -0.79440204 -0.04227649 -0.08387398\n",
      "  -1.1594051  -0.3786325   0.24013921]\n",
      " [ 0.18441289 -0.33766037  0.32230715 -0.59838033  0.27899063 -0.46037059\n",
      "  -0.528371    0.20294533  0.38310372]\n",
      " [ 0.65123153 -0.43773283  0.3282736   0.63657548  0.00988012  0.14633414\n",
      "   1.12811543 -0.12645738 -0.53538579]\n",
      " [-0.59929554  0.45827805 -0.300331   -0.48003975  0.04561179 -0.145421\n",
      "  -1.09191136 -0.25627695  0.33698071]\n",
      " [ 0.02303643  0.03653638  0.26943151 -0.6347813  -0.05991908 -0.0456393\n",
      "  -0.68205187  0.04691756  0.08679742]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.03454188  0.68957485 -1.22698212 -0.03719405  0.76595219 -1.07163093\n",
      "  -0.22425018]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:28 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.64971431]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 28 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.43944103 -0.31812047  0.18026057  0.66064258 -0.06028703 -0.05141927\n",
      "   0.80434102  0.18581024 -0.13844521]\n",
      " [-0.39416394  0.50009109 -0.41652339 -0.79440204 -0.04227649 -0.08387398\n",
      "  -1.14285698 -0.3786325   0.24013921]\n",
      " [ 0.19084429 -0.33122897  0.32873855 -0.59838033  0.27899063 -0.46037059\n",
      "  -0.5219396   0.20294533  0.38310372]\n",
      " [ 0.63476991 -0.45419445  0.31181198  0.63657548  0.00988012  0.14633414\n",
      "   1.11165381 -0.12645738 -0.53538579]\n",
      " [-0.58274469  0.4748289  -0.28378015 -0.48003975  0.04561179 -0.145421\n",
      "  -1.0753605  -0.25627695  0.33698071]\n",
      " [ 0.0293627   0.04286264  0.27575777 -0.6347813  -0.05991908 -0.0456393\n",
      "  -0.67572561  0.04691756  0.08679742]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.1084747   0.63317435 -1.24025407 -0.06758977  0.70373322 -1.08475528\n",
      "  -0.25475809]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:28 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.51671892]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 28 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.44981665 -0.31812047  0.19063619  0.66064258 -0.06028703 -0.05141927\n",
      "   0.80434102  0.19618586 -0.13844521]\n",
      " [-0.40700401  0.50009109 -0.42936346 -0.79440204 -0.04227649 -0.08387398\n",
      "  -1.14285698 -0.39147256  0.24013921]\n",
      " [ 0.20043675 -0.33122897  0.33833101 -0.59838033  0.27899063 -0.46037059\n",
      "  -0.5219396   0.21253779  0.38310372]\n",
      " [ 0.64527442 -0.45419445  0.3223165   0.63657548  0.00988012  0.14633414\n",
      "   1.11165381 -0.11595287 -0.53538579]\n",
      " [-0.59529424  0.4748289  -0.2963297  -0.48003975  0.04561179 -0.145421\n",
      "  -1.0753605  -0.2688265   0.33698071]\n",
      " [ 0.03451222  0.04286264  0.28090729 -0.6347813  -0.05991908 -0.0456393\n",
      "  -0.67572561  0.05206709  0.08679742]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.04813211  0.67488027 -1.22617128 -0.02696934  0.74562684 -1.06994166\n",
      "  -0.21933024]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:28 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.54157353]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 28 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.46083201 -0.31812047  0.19063619  0.67165794 -0.06028703 -0.05141927\n",
      "   0.80434102  0.19618586 -0.12742985]\n",
      " [-0.41795025  0.50009109 -0.42936346 -0.80534829 -0.04227649 -0.08387398\n",
      "  -1.14285698 -0.39147256  0.22919296]\n",
      " [ 0.20022564 -0.33122897  0.33833101 -0.59859144  0.27899063 -0.46037059\n",
      "  -0.5219396   0.21253779  0.3828926 ]\n",
      " [ 0.65454213 -0.45419445  0.3223165   0.64584319  0.00988012  0.14633414\n",
      "   1.11165381 -0.11595287 -0.52611809]\n",
      " [-0.60448768  0.4748289  -0.2963297  -0.48923319  0.04561179 -0.145421\n",
      "  -1.0753605  -0.2688265   0.32778727]\n",
      " [ 0.02766827  0.04286264  0.28090729 -0.64162525 -0.05991908 -0.0456393\n",
      "  -0.67572561  0.05206709  0.07995348]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.00877503  0.71616759 -1.21042936  0.00127311  0.78423309 -1.05153992\n",
      "  -0.19802534]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:28 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.45436264]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 28 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.45774846 -0.31812047  0.19063619  0.67165794 -0.06337058 -0.05450282\n",
      "   0.80434102  0.19618586 -0.1305134 ]\n",
      " [-0.41362432  0.50009109 -0.42936346 -0.80534829 -0.03795055 -0.07954805\n",
      "  -1.14285698 -0.39147256  0.2335189 ]\n",
      " [ 0.19479118 -0.33122897  0.33833101 -0.59859144  0.27355617 -0.46580505\n",
      "  -0.5219396   0.21253779  0.37745815]\n",
      " [ 0.65061436 -0.45419445  0.3223165   0.64584319  0.00595235  0.14240638\n",
      "   1.11165381 -0.11595287 -0.53004585]\n",
      " [-0.59936975  0.4748289  -0.2963297  -0.48923319  0.05072972 -0.14030307\n",
      "  -1.0753605  -0.2688265   0.3329052 ]\n",
      " [ 0.02763922  0.04286264  0.28090729 -0.64162525 -0.05994813 -0.04566836\n",
      "  -0.67572561  0.05206709  0.07992442]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.04754713  0.68489763 -1.23419266 -0.03246979  0.75209098 -1.0744613\n",
      "  -0.22621548]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:28 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.31796774]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 28 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.45868314 -0.31718579  0.19063619  0.67165794 -0.0624359  -0.05356814\n",
      "   0.80434102  0.19618586 -0.12957872]\n",
      " [-0.41535188  0.49836353 -0.42936346 -0.80534829 -0.03967812 -0.08127561\n",
      "  -1.14285698 -0.39147256  0.23179133]\n",
      " [ 0.19437105 -0.3316491   0.33833101 -0.59859144  0.27313604 -0.46622518\n",
      "  -0.5219396   0.21253779  0.37703801]\n",
      " [ 0.65219764 -0.45261116  0.3223165   0.64584319  0.00753563  0.14398966\n",
      "   1.11165381 -0.11595287 -0.52846257]\n",
      " [-0.60039007  0.47380858 -0.2963297  -0.48923319  0.04970941 -0.14132338\n",
      "  -1.0753605  -0.2688265   0.33188489]\n",
      " [ 0.02725317  0.0424766   0.28090729 -0.64162525 -0.06033417 -0.0460544\n",
      "  -0.67572561  0.05206709  0.07953838]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.08202505  0.66859519 -1.25317101 -0.05012906  0.73644438 -1.09272297\n",
      "  -0.24384061]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:28 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.5882348]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 28 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.44618234 -0.32968659  0.19063619  0.67165794 -0.0749367  -0.06606894\n",
      "   0.79184022  0.19618586 -0.12957872]\n",
      " [-0.40023337  0.51348205 -0.42936346 -0.80534829 -0.0245596  -0.06615709\n",
      "  -1.12773846 -0.39147256  0.23179133]\n",
      " [ 0.20709623 -0.31892393  0.33833101 -0.59859144  0.28586122 -0.4535\n",
      "  -0.50921442  0.21253779  0.37703801]\n",
      " [ 0.63628695 -0.46852185  0.3223165   0.64584319 -0.00837506  0.12807897\n",
      "   1.09574312 -0.11595287 -0.52846257]\n",
      " [-0.58482385  0.4893748  -0.2963297  -0.48923319  0.06527562 -0.12575717\n",
      "  -1.05979429 -0.2688265   0.33188489]\n",
      " [ 0.03845793  0.05368136  0.28090729 -0.64162525 -0.04912941 -0.03484964\n",
      "  -0.66452085  0.05206709  0.07953838]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.15326458  0.61898968 -1.26990579 -0.07142606  0.67860106 -1.10805739\n",
      "  -0.26728356]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:28 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.66773888]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 28 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.4540919  -0.32968659  0.19063619  0.67956751 -0.0749367  -0.06606894\n",
      "   0.79974978  0.19618586 -0.12957872]\n",
      " [-0.40716395  0.51348205 -0.42936346 -0.81227887 -0.0245596  -0.06615709\n",
      "  -1.13466905 -0.39147256  0.23179133]\n",
      " [ 0.20027594 -0.31892393  0.33833101 -0.60541173  0.28586122 -0.4535\n",
      "  -0.51603471  0.21253779  0.37703801]\n",
      " [ 0.64309445 -0.46852185  0.3223165   0.65265068 -0.00837506  0.12807897\n",
      "   1.10255062 -0.11595287 -0.52846257]\n",
      " [-0.59226776  0.4893748  -0.2963297  -0.4966771   0.06527562 -0.12575717\n",
      "  -1.0672382  -0.2688265   0.33188489]\n",
      " [ 0.03044902  0.05368136  0.28090729 -0.64963416 -0.04912941 -0.03484964\n",
      "  -0.67252976  0.05206709  0.07953838]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.11640625  0.65109268 -1.26664748 -0.06077751  0.71233102 -1.10415594\n",
      "  -0.25918751]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:28 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.62433889]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 28 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.46321853 -0.32968659  0.19063619  0.67956751 -0.06581007 -0.05694231\n",
      "   0.80887641  0.19618586 -0.12957872]\n",
      " [-0.41699938  0.51348205 -0.42936346 -0.81227887 -0.03439502 -0.07599252\n",
      "  -1.14450447 -0.39147256  0.23179133]\n",
      " [ 0.19525135 -0.31892393  0.33833101 -0.60541173  0.28083664 -0.45852458\n",
      "  -0.52105929  0.21253779  0.37703801]\n",
      " [ 0.6526353  -0.46852185  0.3223165   0.65265068  0.00116579  0.13761982\n",
      "   1.11209146 -0.11595287 -0.52846257]\n",
      " [-0.60202774  0.4893748  -0.2963297  -0.4966771   0.05551564 -0.13551715\n",
      "  -1.07699818 -0.2688265   0.33188489]\n",
      " [ 0.02342034  0.05368136  0.28090729 -0.64963416 -0.0561581  -0.04187833\n",
      "  -0.67955844  0.05206709  0.07953838]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.0723525   0.68425006 -1.25944551 -0.0439732   0.75047801 -1.09746537\n",
      "  -0.24482334]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:28 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.55381391]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 28 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.45009608 -0.32968659  0.19063619  0.66644506 -0.06581007 -0.07006476\n",
      "   0.80887641  0.19618586 -0.14270117]\n",
      " [-0.4030545   0.51348205 -0.42936346 -0.798334   -0.03439502 -0.06204764\n",
      "  -1.14450447 -0.39147256  0.24573621]\n",
      " [ 0.20317314 -0.31892393  0.33833101 -0.59748995  0.28083664 -0.4506028\n",
      "  -0.52105929  0.21253779  0.3849598 ]\n",
      " [ 0.63985567 -0.46852185  0.3223165   0.63987106  0.00116579  0.12484019\n",
      "   1.11209146 -0.11595287 -0.54124219]\n",
      " [-0.5893522   0.4893748  -0.2963297  -0.48400156  0.05551564 -0.12284162\n",
      "  -1.07699818 -0.2688265   0.34456042]\n",
      " [ 0.03266436  0.05368136  0.28090729 -0.64039013 -0.0561581  -0.0326343\n",
      "  -0.67955844  0.05206709  0.0887824 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.14077733  0.63482178 -1.27687619 -0.06994081  0.70162894 -1.11721076\n",
      "  -0.26924873]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:28 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.64664978]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 28 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.45913258 -0.32065009  0.19063619  0.67548155 -0.06581007 -0.06102826\n",
      "   0.8179129   0.19618586 -0.14270117]\n",
      " [-0.4117458   0.50479075 -0.42936346 -0.8070253  -0.03439502 -0.07073894\n",
      "  -1.15319577 -0.39147256  0.24573621]\n",
      " [ 0.19419668 -0.32790039  0.33833101 -0.60646641  0.28083664 -0.45957926\n",
      "  -0.53003575  0.21253779  0.3849598 ]\n",
      " [ 0.64822221 -0.46015532  0.3223165   0.6482376   0.00116579  0.13320673\n",
      "   1.120458   -0.11595287 -0.54124219]\n",
      " [-0.59822     0.480507   -0.2963297  -0.49286936  0.05551564 -0.13170941\n",
      "  -1.08586598 -0.2688265   0.34456042]\n",
      " [ 0.0238955   0.04491249  0.28090729 -0.649159   -0.0561581  -0.04140317\n",
      "  -0.68832731  0.05206709  0.0887824 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.10040816  0.66797987 -1.27159845 -0.06362516  0.73738631 -1.11140437\n",
      "  -0.26037147]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:28 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.66616276]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 28 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.46742054 -0.31236213  0.19063619  0.68376952 -0.06581007 -0.06102826\n",
      "   0.82620087  0.19618586 -0.14270117]\n",
      " [-0.41978232  0.49675423 -0.42936346 -0.81506182 -0.03439502 -0.07073894\n",
      "  -1.16123229 -0.39147256  0.24573621]\n",
      " [ 0.18612608 -0.33597098  0.33833101 -0.614537    0.28083664 -0.45957926\n",
      "  -0.53810635  0.21253779  0.3849598 ]\n",
      " [ 0.65610238 -0.45227515  0.3223165   0.65611777  0.00116579  0.13320673\n",
      "   1.12833817 -0.11595287 -0.54124219]\n",
      " [-0.60646495  0.47226205 -0.2963297  -0.50111431  0.05551564 -0.13170941\n",
      "  -1.09411093 -0.2688265   0.34456042]\n",
      " [ 0.01582769  0.03684469  0.28090729 -0.65722681 -0.0561581  -0.04140317\n",
      "  -0.69639512  0.05206709  0.0887824 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.06328714  0.69902888 -1.2666291  -0.05548742  0.76991108 -1.10565322\n",
      "  -0.25222401]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:28 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.69374819]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 28 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 4.74664994e-01 -3.12362126e-01  1.97880641e-01  6.83769517e-01\n",
      "  -6.58100733e-02 -5.37838087e-02  8.33445320e-01  1.96185864e-01\n",
      "  -1.42701172e-01]\n",
      " [-4.26460602e-01  4.96754226e-01 -4.36041736e-01 -8.15061820e-01\n",
      "  -3.43950246e-02 -7.74172202e-02 -1.16791057e+00 -3.91472560e-01\n",
      "   2.45736209e-01]\n",
      " [ 1.82484845e-01 -3.35970984e-01  3.34689769e-01 -6.14537004e-01\n",
      "   2.80836635e-01 -4.63220496e-01 -5.41747589e-01  2.12537785e-01\n",
      "   3.84959801e-01]\n",
      " [ 6.62439518e-01 -4.52275146e-01  3.28653631e-01  6.56117771e-01\n",
      "   1.16578983e-03  1.39543867e-01  1.13467531e+00 -1.15952867e-01\n",
      "  -5.41242192e-01]\n",
      " [-6.13046360e-01  4.72262052e-01 -3.02911111e-01 -5.01114307e-01\n",
      "   5.55156401e-02 -1.38290821e-01 -1.10069233e+00 -2.68826499e-01\n",
      "   3.44560422e-01]\n",
      " [ 1.24093313e-02  3.68446867e-02  2.77488935e-01 -6.57226807e-01\n",
      "  -5.61580984e-02 -4.48215249e-02 -6.99813475e-01  5.20670857e-02\n",
      "   8.87824048e-02]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.03075376  0.72524571 -1.26301966 -0.0429994   0.79931414 -1.10219341\n",
      "  -0.2394876 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:28 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.46624364]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 29 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 4.65628066e-01 -3.21399054e-01  1.97880641e-01  6.74732589e-01\n",
      "  -6.58100733e-02 -5.37838087e-02  8.33445320e-01  1.96185864e-01\n",
      "  -1.51738100e-01]\n",
      " [-4.19655339e-01  5.03559489e-01 -4.36041736e-01 -8.08256557e-01\n",
      "  -3.43950246e-02 -7.74172202e-02 -1.16791057e+00 -3.91472560e-01\n",
      "   2.52541471e-01]\n",
      " [ 1.87841752e-01 -3.30614077e-01  3.34689769e-01 -6.09180096e-01\n",
      "   2.80836635e-01 -4.63220496e-01 -5.41747589e-01  2.12537785e-01\n",
      "   3.90316709e-01]\n",
      " [ 6.57847592e-01 -4.56867072e-01  3.28653631e-01  6.51525845e-01\n",
      "   1.16578983e-03  1.39543867e-01  1.13467531e+00 -1.15952867e-01\n",
      "  -5.45834119e-01]\n",
      " [-6.08827785e-01  4.76480627e-01 -3.02911111e-01 -4.96895732e-01\n",
      "   5.55156401e-02 -1.38290821e-01 -1.10069233e+00 -2.68826499e-01\n",
      "   3.48778997e-01]\n",
      " [ 1.94540155e-02  4.38893709e-02  2.77488935e-01 -6.50182123e-01\n",
      "  -5.61580984e-02 -4.48215249e-02 -6.99813475e-01  5.20670857e-02\n",
      "   9.58270890e-02]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.08876857  0.68643758 -1.28493581 -0.06651793  0.76563353 -1.1269198\n",
      "  -0.26112954]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:29 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.67598203]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 29 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 4.73565943e-01 -3.21399054e-01  2.05818517e-01  6.74732589e-01\n",
      "  -6.58100733e-02 -5.37838087e-02  8.41383196e-01  1.96185864e-01\n",
      "  -1.51738100e-01]\n",
      " [-4.27059891e-01  5.03559489e-01 -4.43446289e-01 -8.08256557e-01\n",
      "  -3.43950246e-02 -7.74172202e-02 -1.17531513e+00 -3.91472560e-01\n",
      "   2.52541471e-01]\n",
      " [ 1.87671298e-01 -3.30614077e-01  3.34519315e-01 -6.09180096e-01\n",
      "   2.80836635e-01 -4.63220496e-01 -5.41918043e-01  2.12537785e-01\n",
      "   3.90316709e-01]\n",
      " [ 6.65042973e-01 -4.56867072e-01  3.35849012e-01  6.51525845e-01\n",
      "   1.16578983e-03  1.39543867e-01  1.14187069e+00 -1.15952867e-01\n",
      "  -5.45834119e-01]\n",
      " [-6.16254699e-01  4.76480627e-01 -3.10338025e-01 -4.96895732e-01\n",
      "   5.55156401e-02 -1.38290821e-01 -1.10811925e+00 -2.68826499e-01\n",
      "   3.48778997e-01]\n",
      " [ 1.60212452e-02  4.38893709e-02  2.74056165e-01 -6.50182123e-01\n",
      "  -5.61580984e-02 -4.48215249e-02 -7.03246245e-01  5.20670857e-02\n",
      "   9.58270890e-02]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.05328369  0.71543298 -1.28079308 -0.04894595  0.79731955 -1.122736\n",
      "  -0.24691348]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:29 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.6518257]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 29 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.45777922 -0.33718578  0.1900318   0.67473259 -0.06581007 -0.05378381\n",
      "   0.82559647  0.19618586 -0.1517381 ]\n",
      " [-0.41050111  0.52011827 -0.4268875  -0.80825656 -0.03439502 -0.07741722\n",
      "  -1.15875634 -0.39147256  0.25254147]\n",
      " [ 0.19395481 -0.32433056  0.34080283 -0.6091801   0.28083664 -0.4632205\n",
      "  -0.53563453  0.21253779  0.39031671]\n",
      " [ 0.64859757 -0.47331247  0.31940361  0.65152584  0.00116579  0.13954387\n",
      "   1.12542529 -0.11595287 -0.54583412]\n",
      " [-0.59969719  0.49303813 -0.29378052 -0.49689573  0.05551564 -0.13829082\n",
      "  -1.09156174 -0.2688265   0.348779  ]\n",
      " [ 0.02262213  0.05049026  0.28065705 -0.65018212 -0.0561581  -0.04482152\n",
      "  -0.69664536  0.05206709  0.09582709]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.12724927  0.65859691 -1.29382631 -0.0795159   0.734916   -1.13559866\n",
      "  -0.27714433]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:29 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.51702775]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 29 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.46848112 -0.33718578  0.20073369  0.67473259 -0.06581007 -0.05378381\n",
      "   0.82559647  0.20688776 -0.1517381 ]\n",
      " [-0.42347881  0.52011827 -0.43986521 -0.80825656 -0.03439502 -0.07741722\n",
      "  -1.15875634 -0.40445026  0.25254147]\n",
      " [ 0.20378333 -0.32433056  0.35063135 -0.6091801   0.28083664 -0.4632205\n",
      "  -0.53563453  0.2223663   0.39031671]\n",
      " [ 0.6593668  -0.47331247  0.33017284  0.65152584  0.00116579  0.13954387\n",
      "   1.12542529 -0.10518364 -0.54583412]\n",
      " [-0.61241733  0.49303813 -0.30650066 -0.49689573  0.05551564 -0.13829082\n",
      "  -1.09156174 -0.28154664  0.348779  ]\n",
      " [ 0.0278135   0.05049026  0.28584842 -0.65018212 -0.0561581  -0.04482152\n",
      "  -0.69664536  0.05725845  0.09582709]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.06694776  0.70076614 -1.2801752  -0.03859595  0.77718716 -1.12123189\n",
      "  -0.24169226]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:29 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.53965893]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 29 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.47967151 -0.33718578  0.20073369  0.68592298 -0.06581007 -0.05378381\n",
      "   0.82559647  0.20688776 -0.14054771]\n",
      " [-0.4345927   0.52011827 -0.43986521 -0.81937044 -0.03439502 -0.07741722\n",
      "  -1.15875634 -0.40445026  0.24142758]\n",
      " [ 0.20356777 -0.32433056  0.35063135 -0.60939566  0.28083664 -0.4632205\n",
      "  -0.53563453  0.2223663   0.39010115]\n",
      " [ 0.66884718 -0.47331247  0.33017284  0.66100622  0.00116579  0.13954387\n",
      "   1.12542529 -0.10518364 -0.53635374]\n",
      " [-0.62185718  0.49303813 -0.30650066 -0.50633558  0.05551564 -0.13829082\n",
      "  -1.09156174 -0.28154664  0.33933915]\n",
      " [ 0.02078502  0.05049026  0.28584842 -0.6572106  -0.0561581  -0.04482152\n",
      "  -0.69664536  0.05725845  0.08879861]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.00976714  0.74247249 -1.26456194 -0.01022121  0.81621013 -1.10301813\n",
      "  -0.22045974]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:29 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.44034147]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 29 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.47672923 -0.33718578  0.20073369  0.68592298 -0.06875235 -0.05672609\n",
      "   0.82559647  0.20688776 -0.14348999]\n",
      " [-0.43055047  0.52011827 -0.43986521 -0.81937044 -0.0303528  -0.073375\n",
      "  -1.15875634 -0.40445026  0.24546981]\n",
      " [ 0.19821822 -0.32433056  0.35063135 -0.60939566  0.27548708 -0.46857005\n",
      "  -0.53563453  0.2223663   0.3847516 ]\n",
      " [ 0.66520954 -0.47331247  0.33017284  0.66100622 -0.00247184  0.13590623\n",
      "   1.12542529 -0.10518364 -0.53999137]\n",
      " [-0.61706375  0.49303813 -0.30650066 -0.50633558  0.06030906 -0.1334974\n",
      "  -1.09156174 -0.28154664  0.34413258]\n",
      " [ 0.02066831  0.05049026  0.28584842 -0.6572106  -0.05627481 -0.04493823\n",
      "  -0.69664536  0.05725845  0.0886819 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.06402621  0.71237699 -1.28758629 -0.04285239  0.78539754 -1.12524692\n",
      "  -0.24770598]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:29 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.29910683]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 29 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 4.77739449e-01 -3.36175558e-01  2.00733693e-01  6.85922981e-01\n",
      "  -6.77421351e-02 -5.57158705e-02  8.25596474e-01  2.06887761e-01\n",
      "  -1.42479770e-01]\n",
      " [-4.32339483e-01  5.18329266e-01 -4.39865207e-01 -8.19370444e-01\n",
      "  -3.21418098e-02 -7.51640054e-02 -1.15875634e+00 -4.04450265e-01\n",
      "   2.43680799e-01]\n",
      " [ 1.97704929e-01 -3.24843851e-01  3.50631347e-01 -6.09395656e-01\n",
      "   2.74973793e-01 -4.69083338e-01 -5.35634528e-01  2.22366303e-01\n",
      "   3.84238308e-01]\n",
      " [ 6.66872848e-01 -4.71649166e-01  3.30172837e-01  6.61006224e-01\n",
      "  -8.08539153e-04  1.37569538e-01  1.12542529e+00 -1.05183642e-01\n",
      "  -5.38328068e-01]\n",
      " [-6.18209135e-01  4.91892754e-01 -3.06500660e-01 -5.06335575e-01\n",
      "   5.91636827e-02 -1.34642778e-01 -1.09156174e+00 -2.81546640e-01\n",
      "   3.42987196e-01]\n",
      " [ 2.02091762e-02  5.00311212e-02  2.85848420e-01 -6.57210601e-01\n",
      "  -5.67339445e-02 -4.53973711e-02 -6.96645358e-01  5.72584537e-02\n",
      "   8.82227647e-02]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.09537888  0.6977137  -1.30506763 -0.05904238  0.77139732 -1.14207276\n",
      "  -0.26384171]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:29 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.58955105]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 29 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.46508323 -0.34883178  0.20073369  0.68592298 -0.08039835 -0.06837209\n",
      "   0.81294026  0.20688776 -0.14247977]\n",
      " [-0.41720526  0.53346349 -0.43986521 -0.81937044 -0.01700759 -0.06002978\n",
      "  -1.14362212 -0.40445026  0.2436808 ]\n",
      " [ 0.21049107 -0.31205772  0.35063135 -0.60939566  0.28775993 -0.4562972\n",
      "  -0.52284839  0.2223663   0.38423831]\n",
      " [ 0.65094723 -0.48757478  0.33017284  0.66100622 -0.01673415  0.12164392\n",
      "   1.10949967 -0.10518364 -0.53832807]\n",
      " [-0.60262379  0.5074781  -0.30650066 -0.50633558  0.07474903 -0.11905744\n",
      "  -1.0759764  -0.28154664  0.3429872 ]\n",
      " [ 0.03161869  0.06144064  0.28584842 -0.6572106  -0.04532443 -0.03398786\n",
      "  -0.68523584  0.05725845  0.08822276]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.16670884  0.64783662 -1.32183297 -0.08029807  0.713539   -1.15742904\n",
      "  -0.28706076]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:29 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.67786435]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 29 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.47253652 -0.34883178  0.20073369  0.69337627 -0.08039835 -0.06837209\n",
      "   0.82039354  0.20688776 -0.14247977]\n",
      " [-0.42369501  0.53346349 -0.43986521 -0.8258602  -0.01700759 -0.06002978\n",
      "  -1.15011187 -0.40445026  0.2436808 ]\n",
      " [ 0.20389037 -0.31205772  0.35063135 -0.61599635  0.28775993 -0.4562972\n",
      "  -0.52944909  0.2223663   0.38423831]\n",
      " [ 0.65732623 -0.48757478  0.33017284  0.66738522 -0.01673415  0.12164392\n",
      "   1.11587867 -0.10518364 -0.53832807]\n",
      " [-0.60960829  0.5074781  -0.30650066 -0.51332007  0.07474903 -0.11905744\n",
      "  -1.0829609  -0.28154664  0.3429872 ]\n",
      " [ 0.02390754  0.06144064  0.28584842 -0.66492175 -0.04532443 -0.03398786\n",
      "  -0.69294699  0.05725845  0.08822276]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.13153738  0.67868056 -1.31885411 -0.07028824  0.74584218 -1.1538728\n",
      "  -0.27959216]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:29 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.6291643]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 29 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.48160484 -0.34883178  0.20073369  0.69337627 -0.07133003 -0.05930377\n",
      "   0.82946187  0.20688776 -0.14247977]\n",
      " [-0.43334174  0.53346349 -0.43986521 -0.8258602  -0.02665431 -0.06967651\n",
      "  -1.1597586  -0.40445026  0.2436808 ]\n",
      " [ 0.19885991 -0.31205772  0.35063135 -0.61599635  0.28272947 -0.46132766\n",
      "  -0.53447954  0.2223663   0.38423831]\n",
      " [ 0.66667163 -0.48757478  0.33017284  0.66738522 -0.00738876  0.13098932\n",
      "   1.12522407 -0.10518364 -0.53832807]\n",
      " [-0.61917327  0.5074781  -0.30650066 -0.51332007  0.06518404 -0.12862242\n",
      "  -1.09252588 -0.28154664  0.3429872 ]\n",
      " [ 0.01684914  0.06144064  0.28584842 -0.66492175 -0.05238283 -0.04104626\n",
      "  -0.70000539  0.05725845  0.08822276]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.08827632  0.71149159 -1.3118893  -0.05389536  0.78336658 -1.14739619\n",
      "  -0.26569754]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:29 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.55381797]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 29 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.46835091 -0.34883178  0.20073369  0.68012234 -0.07133003 -0.07255769\n",
      "   0.82946187  0.20688776 -0.1557337 ]\n",
      " [-0.41932572  0.53346349 -0.43986521 -0.81184418 -0.02665431 -0.05566049\n",
      "  -1.1597586  -0.40445026  0.25769681]\n",
      " [ 0.20681834 -0.31205772  0.35063135 -0.60803793  0.28272947 -0.45336924\n",
      "  -0.53447954  0.2223663   0.39219673]\n",
      " [ 0.65378862 -0.48757478  0.33017284  0.65450221 -0.00738876  0.11810631\n",
      "   1.12522407 -0.10518364 -0.55121108]\n",
      " [-0.60636233  0.5074781  -0.30650066 -0.50050913  0.06518404 -0.11581147\n",
      "  -1.09252588 -0.28154664  0.35579814]\n",
      " [ 0.02625348  0.06144064  0.28584842 -0.65551741 -0.05238283 -0.03164192\n",
      "  -0.70000539  0.05725845  0.0976271 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.15670153  0.66183155 -1.32916829 -0.07982158  0.73434614 -1.1669206\n",
      "  -0.28992956]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:29 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.65286671]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 29 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.47715653 -0.34002616  0.20073369  0.68892795 -0.07133003 -0.06375208\n",
      "   0.83826748  0.20688776 -0.1557337 ]\n",
      " [-0.42776054  0.52502867 -0.43986521 -0.820279   -0.02665431 -0.06409531\n",
      "  -1.16819341 -0.40445026  0.25769681]\n",
      " [ 0.19808565 -0.3207904   0.35063135 -0.61677061  0.28272947 -0.46210192\n",
      "  -0.54321223  0.2223663   0.39219673]\n",
      " [ 0.66190386 -0.47945954  0.33017284  0.66261746 -0.00738876  0.12622155\n",
      "   1.13333931 -0.10518364 -0.55121108]\n",
      " [-0.61497044  0.49886998 -0.30650066 -0.50911724  0.06518404 -0.12441959\n",
      "  -1.10113399 -0.28154664  0.35579814]\n",
      " [ 0.0176482   0.05283536  0.28584842 -0.66412269 -0.05238283 -0.0402472\n",
      "  -0.70861067  0.05725845  0.0976271 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.11736582  0.69431723 -1.32410853 -0.07375136  0.76925105 -1.1613777\n",
      "  -0.28150172]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:29 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.67194597]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 29 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.48521086 -0.33197183  0.20073369  0.69698229 -0.07133003 -0.06375208\n",
      "   0.84632181  0.20688776 -0.1557337 ]\n",
      " [-0.43555034  0.51723887 -0.43986521 -0.8280688  -0.02665431 -0.06409531\n",
      "  -1.17598322 -0.40445026  0.25769681]\n",
      " [ 0.190203   -0.32867305  0.35063135 -0.62465326  0.28272947 -0.46210192\n",
      "  -0.55109488  0.2223663   0.39219673]\n",
      " [ 0.66953861 -0.47182479  0.33017284  0.67025221 -0.00738876  0.12622155\n",
      "   1.14097406 -0.10518364 -0.55121108]\n",
      " [-0.62297502  0.4908654  -0.30650066 -0.51712182  0.06518404 -0.12441959\n",
      "  -1.10913858 -0.28154664  0.35579814]\n",
      " [ 0.00973394  0.0449211   0.28584842 -0.67203695 -0.05238283 -0.0402472\n",
      "  -0.71652493  0.05725845  0.0976271 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.08120859  0.72471888 -1.31936806 -0.06590188  0.80101556 -1.15591597\n",
      "  -0.27377179]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:29 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.6987442]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 29 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.49229473 -0.33197183  0.20781756  0.69698229 -0.07133003 -0.05666821\n",
      "   0.85340568  0.20688776 -0.1557337 ]\n",
      " [-0.44199118  0.51723887 -0.44630605 -0.8280688  -0.02665431 -0.07053615\n",
      "  -1.18242406 -0.40445026  0.25769681]\n",
      " [ 0.18666001 -0.32867305  0.34708835 -0.62465326  0.28272947 -0.46564492\n",
      "  -0.55463787  0.2223663   0.39219673]\n",
      " [ 0.67565421 -0.47182479  0.33628843  0.67025221 -0.00738876  0.13233715\n",
      "   1.14708966 -0.10518364 -0.55121108]\n",
      " [-0.62931807  0.4908654  -0.31284371 -0.51712182  0.06518404 -0.13076263\n",
      "  -1.11548162 -0.28154664  0.35579814]\n",
      " [ 0.00626587  0.0449211   0.28238035 -0.67203695 -0.05238283 -0.04371526\n",
      "  -0.719993    0.05725845  0.0976271 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.04950131  0.75049156 -1.31595635 -0.05372447  0.8297455  -1.15264633\n",
      "  -0.26151047]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:29 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.45645731]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 30 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.4834916  -0.34077496  0.20781756  0.68817916 -0.07133003 -0.05666821\n",
      "   0.85340568  0.20688776 -0.16453682]\n",
      " [-0.43539475  0.5238353  -0.44630605 -0.82147237 -0.02665431 -0.07053615\n",
      "  -1.18242406 -0.40445026  0.26429325]\n",
      " [ 0.19177946 -0.32355359  0.34708835 -0.6195338   0.28272947 -0.46564492\n",
      "  -0.55463787  0.2223663   0.39731619]\n",
      " [ 0.67120069 -0.4762783   0.33628843  0.66579869 -0.00738876  0.13233715\n",
      "   1.14708966 -0.10518364 -0.55566459]\n",
      " [-0.62516833  0.49501514 -0.31284371 -0.51297209  0.06518404 -0.13076263\n",
      "  -1.11548162 -0.28154664  0.35994788]\n",
      " [ 0.01318801  0.05184324  0.28238035 -0.66511481 -0.05238283 -0.04371526\n",
      "  -0.719993    0.05725845  0.10454924]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.10612576  0.71263607 -1.33739931 -0.07679675  0.79690198 -1.17674639\n",
      "  -0.28258037]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:30 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.68157465]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 30 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.49122727 -0.34077496  0.21555322  0.68817916 -0.07133003 -0.05666821\n",
      "   0.86114135  0.20688776 -0.16453682]\n",
      " [-0.44252334  0.5238353  -0.45343463 -0.82147237 -0.02665431 -0.07053615\n",
      "  -1.18955264 -0.40445026  0.26429325]\n",
      " [ 0.19164324 -0.32355359  0.34695213 -0.6195338   0.28272947 -0.46564492\n",
      "  -0.5547741   0.2223663   0.39731619]\n",
      " [ 0.67813253 -0.4762783   0.34322027  0.66579869 -0.00738876  0.13233715\n",
      "   1.15402149 -0.10518364 -0.55566459]\n",
      " [-0.63231877  0.49501514 -0.31999415 -0.51297209  0.06518404 -0.13076263\n",
      "  -1.12263206 -0.28154664  0.35994788]\n",
      " [ 0.0096819   0.05184324  0.27887424 -0.66511481 -0.05238283 -0.04371526\n",
      "  -0.72349911  0.05725845  0.10454924]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.07157173  0.74111374 -1.33350739 -0.05965596  0.8278657  -1.1728176\n",
      "  -0.26891568]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:30 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.65386054]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 30 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.47531024 -0.35669198  0.1996362   0.68817916 -0.07133003 -0.05666821\n",
      "   0.84522432  0.20688776 -0.16453682]\n",
      " [-0.42596033  0.54039831 -0.43687162 -0.82147237 -0.02665431 -0.07053615\n",
      "  -1.17298963 -0.40445026  0.26429325]\n",
      " [ 0.1977498  -0.31744703  0.35305869 -0.6195338   0.28272947 -0.46564492\n",
      "  -0.54866753  0.2223663   0.39731619]\n",
      " [ 0.66170233 -0.4927085   0.32679007  0.66579869 -0.00738876  0.13233715\n",
      "   1.13759129 -0.10518364 -0.55566459]\n",
      " [-0.61576156  0.51157235 -0.30343693 -0.51297209  0.06518404 -0.13076263\n",
      "  -1.10607485 -0.28154664  0.35994788]\n",
      " [ 0.01651479  0.05867614  0.28570713 -0.66511481 -0.05238283 -0.04371526\n",
      "  -0.71666622  0.05725845  0.10454924]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.14556485  0.68389346 -1.34633827 -0.09042781  0.76531068 -1.18545603\n",
      "  -0.29891098]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:30 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.51788656]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 30 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.48630033 -0.35669198  0.21062629  0.68817916 -0.07133003 -0.05666821\n",
      "   0.84522432  0.21787786 -0.16453682]\n",
      " [-0.4390371   0.54039831 -0.4499484  -0.82147237 -0.02665431 -0.07053615\n",
      "  -1.17298963 -0.41752704  0.26429325]\n",
      " [ 0.20780454 -0.31744703  0.36311343 -0.6195338   0.28272947 -0.46564492\n",
      "  -0.54866753  0.23242104  0.39731619]\n",
      " [ 0.67270407 -0.4927085   0.33779181  0.66579869 -0.00738876  0.13233715\n",
      "   1.13759129 -0.0941819  -0.55566459]\n",
      " [-0.62861272  0.51157235 -0.3162881  -0.51297209  0.06518404 -0.13076263\n",
      "  -1.10607485 -0.29439781  0.35994788]\n",
      " [ 0.02175276  0.05867614  0.2909451  -0.66511481 -0.05238283 -0.04371526\n",
      "  -0.71666622  0.06249642  0.10454924]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.08537779  0.72645788 -1.33311379 -0.04924739  0.80789347 -1.17153219\n",
      "  -0.26346594]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:30 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.53810199]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 30 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.49764599 -0.35669198  0.21062629  0.69952482 -0.07133003 -0.05666821\n",
      "   0.84522432  0.21787786 -0.15319117]\n",
      " [-0.45029995  0.54039831 -0.4499484  -0.83273522 -0.02665431 -0.07053615\n",
      "  -1.17298963 -0.41752704  0.25303039]\n",
      " [ 0.20759771 -0.31744703  0.36311343 -0.61974063  0.28272947 -0.46564492\n",
      "  -0.54866753  0.23242104  0.39710936]\n",
      " [ 0.68237869 -0.4927085   0.33779181  0.67547331 -0.00738876  0.13233715\n",
      "   1.13759129 -0.0941819  -0.54598998]\n",
      " [-0.63827681  0.51157235 -0.3162881  -0.52263618  0.06518404 -0.13076263\n",
      "  -1.10607485 -0.29439781  0.35028379]\n",
      " [ 0.01455566  0.05867614  0.2909451  -0.67231191 -0.05238283 -0.04371526\n",
      "  -0.71666622  0.06249642  0.09735214]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.02797582  0.76853403 -1.31763329 -0.02075324  0.84728796 -1.15350986\n",
      "  -0.24231539]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:30 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.42638928]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 30 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.49485708 -0.35669198  0.21062629  0.69952482 -0.07411894 -0.05945712\n",
      "   0.84522432  0.21787786 -0.15598007]\n",
      " [-0.44654343  0.54039831 -0.4499484  -0.83273522 -0.02289779 -0.06677963\n",
      "  -1.17298963 -0.41752704  0.25678692]\n",
      " [ 0.20233677 -0.31744703  0.36311343 -0.61974063  0.27746853 -0.47090586\n",
      "  -0.54866753  0.23242104  0.39184842]\n",
      " [ 0.67902944 -0.4927085   0.33779181  0.67547331 -0.010738    0.12898791\n",
      "   1.13759129 -0.0941819  -0.54933922]\n",
      " [-0.6338088   0.51157235 -0.3162881  -0.52263618  0.06965205 -0.12629462\n",
      "  -1.10607485 -0.29439781  0.3547518 ]\n",
      " [ 0.01434958  0.05867614  0.2909451  -0.67231191 -0.05258891 -0.04392134\n",
      "  -0.71666622  0.06249642  0.09714606]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.08011928  0.73965157 -1.33989397 -0.05224329  0.81782873 -1.1750199\n",
      "  -0.26859321]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:30 with online instance: 5-------------\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.28060211]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 30 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.49592288 -0.35562618  0.21062629  0.69952482 -0.07305314 -0.05839132\n",
      "   0.84522432  0.21787786 -0.15491428]\n",
      " [-0.44836007  0.53858167 -0.4499484  -0.83273522 -0.02471443 -0.06859627\n",
      "  -1.17298963 -0.41752704  0.25497027]\n",
      " [ 0.20174799 -0.31803582  0.36311343 -0.61974063  0.27687975 -0.47149464\n",
      "  -0.54866753  0.23242104  0.39125964]\n",
      " [ 0.68073681 -0.49100113  0.33779181  0.67547331 -0.00903063  0.13069528\n",
      "   1.13759129 -0.0941819  -0.54763185]\n",
      " [-0.63504448  0.51033667 -0.3162881  -0.52263618  0.06841638 -0.1275303\n",
      "  -1.10607485 -0.29439781  0.35351612]\n",
      " [ 0.01382873  0.05815529  0.2909451  -0.67231191 -0.05310976 -0.04444219\n",
      "  -0.71666622  0.06249642  0.09662521]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.10844109  0.72656054 -1.35589221 -0.06699366  0.80539229 -1.19042286\n",
      "  -0.28327543]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:30 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.59037108]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 30 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.48315423 -0.36839484  0.21062629  0.69952482 -0.08582179 -0.07115997\n",
      "   0.83245567  0.21787786 -0.15491428]\n",
      " [-0.43323336  0.55370838 -0.4499484  -0.83273522 -0.00958772 -0.05346956\n",
      "  -1.15786292 -0.41752704  0.25497027]\n",
      " [ 0.21457028 -0.30521352  0.36311343 -0.61974063  0.28970204 -0.45867235\n",
      "  -0.53584524  0.23242104  0.39125964]\n",
      " [ 0.66480778 -0.50693017  0.33779181  0.67547331 -0.02495967  0.11476624\n",
      "   1.12166226 -0.0941819  -0.54763185]\n",
      " [-0.61945806  0.5259231  -0.3162881  -0.52263618  0.0840028  -0.11194388\n",
      "  -1.09048842 -0.29439781  0.35351612]\n",
      " [ 0.02540588  0.06973244  0.2909451  -0.67231191 -0.04153261 -0.03286504\n",
      "  -0.70508906  0.06249642  0.09662521]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.17982672  0.67648912 -1.37272194 -0.08822583  0.74758113 -1.2058329\n",
      "  -0.30629843]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:30 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.68776912]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 30 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.49016579 -0.36839484  0.21062629  0.70653638 -0.08582179 -0.07115997\n",
      "   0.83946723  0.21787786 -0.15491428]\n",
      " [-0.43930755  0.55370838 -0.4499484  -0.83880941 -0.00958772 -0.05346956\n",
      "  -1.16393711 -0.41752704  0.25497027]\n",
      " [ 0.20820073 -0.30521352  0.36311343 -0.62611018  0.28970204 -0.45867235\n",
      "  -0.5422148   0.23242104  0.39125964]\n",
      " [ 0.67078307 -0.50693017  0.33779181  0.6814486  -0.02495967  0.11476624\n",
      "   1.12763755 -0.0941819  -0.54763185]\n",
      " [-0.62600564  0.5259231  -0.3162881  -0.52918377  0.0840028  -0.11194388\n",
      "  -1.09703601 -0.29439781  0.35351612]\n",
      " [ 0.01800447  0.06973244  0.2909451  -0.67971333 -0.04153261 -0.03286504\n",
      "  -0.71249048  0.06249642  0.09662521]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.14630206  0.70607051 -1.36999392 -0.07881562  0.77847178 -1.20258562\n",
      "  -0.29940762]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:30 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.63387527]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 30 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.49915837 -0.36839484  0.21062629  0.70653638 -0.07682922 -0.0621674\n",
      "   0.8484598   0.21787786 -0.15491428]\n",
      " [-0.44876985  0.55370838 -0.4499484  -0.83880941 -0.01905002 -0.06293186\n",
      "  -1.17339941 -0.41752704  0.25497027]\n",
      " [ 0.20318259 -0.30521352  0.36311343 -0.62611018  0.2846839  -0.46369049\n",
      "  -0.54723294  0.23242104  0.39125964]\n",
      " [ 0.67994174 -0.50693017  0.33779181  0.6814486  -0.01580099  0.12392491\n",
      "   1.13679622 -0.0941819  -0.54763185]\n",
      " [-0.63538189  0.5259231  -0.3162881  -0.52918377  0.07462655 -0.12132012\n",
      "  -1.10641226 -0.29439781  0.35351612]\n",
      " [ 0.01093532  0.06973244  0.2909451  -0.67971333 -0.04860176 -0.03993419\n",
      "  -0.71955963  0.06249642  0.09662521]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.10381742  0.7385128  -1.36324236 -0.06280572  0.81537195 -1.19630115\n",
      "  -0.28595186]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:30 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.5539324]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 30 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.48578642 -0.36839484  0.21062629  0.69316443 -0.07682922 -0.07553935\n",
      "   0.8484598   0.21787786 -0.16828622]\n",
      " [-0.43469058  0.55370838 -0.4499484  -0.82473014 -0.01905002 -0.04885259\n",
      "  -1.17339941 -0.41752704  0.26904955]\n",
      " [ 0.21115831 -0.30521352  0.36311343 -0.61813446  0.2846839  -0.45571476\n",
      "  -0.54723294  0.23242104  0.39923536]\n",
      " [ 0.6669663  -0.50693017  0.33779181  0.66847315 -0.01580099  0.11094947\n",
      "   1.13679622 -0.0941819  -0.56060729]\n",
      " [-0.62245004  0.5259231  -0.3162881  -0.51625191  0.07462655 -0.10838827\n",
      "  -1.10641226 -0.29439781  0.36644797]\n",
      " [ 0.0204848   0.06973244  0.2909451  -0.67016385 -0.04860176 -0.03038471\n",
      "  -0.71955963  0.06249642  0.10617469]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.17225335  0.68863574 -1.38039078 -0.08871776  0.76619173 -1.21563048\n",
      "  -0.31001279]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:30 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.65878219]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 30 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.49436726 -0.35981399  0.21062629  0.70174528 -0.07682922 -0.0669585\n",
      "   0.85704065  0.21787786 -0.16828622]\n",
      " [-0.44288696  0.545512   -0.4499484  -0.83292652 -0.01905002 -0.05704897\n",
      "  -1.18159579 -0.41752704  0.26904955]\n",
      " [ 0.20265711 -0.31371472  0.36311343 -0.62663566  0.2846839  -0.46421597\n",
      "  -0.55573414  0.23242104  0.39923536]\n",
      " [ 0.67485012 -0.49904635  0.33779181  0.67635698 -0.01580099  0.11883329\n",
      "   1.14468005 -0.0941819  -0.56060729]\n",
      " [-0.6308143   0.51755884 -0.3162881  -0.52461617  0.07462655 -0.11675253\n",
      "  -1.11477651 -0.29439781  0.36644797]\n",
      " [ 0.01204785  0.0612955   0.2909451  -0.67860079 -0.04860176 -0.03882165\n",
      "  -0.72799657  0.06249642  0.10617469]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.13390248  0.72045531 -1.37552114 -0.08286931  0.80026931 -1.21031786\n",
      "  -0.30199065]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:30 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.67754315]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 30 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.5021935  -0.35198776  0.21062629  0.70957152 -0.07682922 -0.0669585\n",
      "   0.86486688  0.21787786 -0.16828622]\n",
      " [-0.45044234  0.53795662 -0.4499484  -0.84048189 -0.01905002 -0.05704897\n",
      "  -1.18915117 -0.41752704  0.26904955]\n",
      " [ 0.19496048 -0.32141136  0.36311343 -0.6343323   0.2846839  -0.46421597\n",
      "  -0.56343077  0.23242104  0.39923536]\n",
      " [ 0.68225301 -0.49164346  0.33779181  0.68375986 -0.01580099  0.11883329\n",
      "   1.15208293 -0.0941819  -0.56060729]\n",
      " [-0.63858662  0.50978652 -0.3162881  -0.53238849  0.07462655 -0.11675253\n",
      "  -1.12254884 -0.29439781  0.36644797]\n",
      " [ 0.0042942   0.05354184  0.2909451  -0.68635445 -0.04860176 -0.03882165\n",
      "  -0.73575023  0.06249642  0.10617469]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.09867755  0.7502085  -1.3709853  -0.07528632  0.83128363 -1.20511466\n",
      "  -0.29464197]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:30 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.7036679]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 30 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 5.09107473e-01 -3.51987756e-01  2.17540266e-01  7.09571516e-01\n",
      "  -7.68292191e-02 -6.00445276e-02  8.71780856e-01  2.17877858e-01\n",
      "  -1.68286224e-01]\n",
      " [-4.56656422e-01  5.37956620e-01 -4.56162483e-01 -8.40481895e-01\n",
      "  -1.90500230e-02 -6.32630603e-02 -1.19536526e+00 -4.17527037e-01\n",
      "   2.69049548e-01]\n",
      " [ 1.91526362e-01 -3.21411358e-01  3.59679312e-01 -6.34332297e-01\n",
      "   2.84683902e-01 -4.67650079e-01 -5.66864883e-01  2.32421038e-01\n",
      "   3.99235361e-01]\n",
      " [ 6.88158456e-01 -4.91643463e-01  3.43697264e-01  6.83759860e-01\n",
      "  -1.58009947e-02  1.24738742e-01  1.15798838e+00 -9.41818983e-02\n",
      "  -5.60607293e-01]\n",
      " [-6.44702623e-01  5.09786521e-01 -3.22404102e-01 -5.32388490e-01\n",
      "   7.46265514e-02 -1.22868533e-01 -1.12866484e+00 -2.94397805e-01\n",
      "   3.66447972e-01]\n",
      " [ 7.96667729e-04  5.35418420e-02  2.87447571e-01 -6.86354447e-01\n",
      "  -4.86017572e-02 -4.23191803e-02 -7.39247757e-01  6.24964197e-02\n",
      "   1.06174689e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.06778206  0.77551714 -1.36775208 -0.06340029  0.85934062 -1.20201662\n",
      "  -0.28282723]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:30 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.44670906]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 31 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.50055476 -0.36054047  0.21754027  0.7010188  -0.07682922 -0.06004453\n",
      "   0.87178086  0.21787786 -0.17683894]\n",
      " [-0.45028259  0.54433045 -0.45616248 -0.83410806 -0.01905002 -0.06326306\n",
      "  -1.19536526 -0.41752704  0.27542338]\n",
      " [ 0.19639941 -0.31653831  0.35967931 -0.62945925  0.2846839  -0.46765008\n",
      "  -0.56686488  0.23242104  0.40410841]\n",
      " [ 0.68385751 -0.49594441  0.34369726  0.67945892 -0.01580099  0.12473874\n",
      "   1.15798838 -0.0941819  -0.56490824]\n",
      " [-0.64064303  0.51384611 -0.3224041  -0.5283289   0.07462655 -0.12286853\n",
      "  -1.12866484 -0.29439781  0.37050756]\n",
      " [ 0.00757442  0.0603196   0.28744757 -0.67957669 -0.04860176 -0.04231918\n",
      "  -0.73924776  0.06249642  0.11295244]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.12298638  0.73864981 -1.38872213 -0.08602049  0.82736389 -1.22549768\n",
      "  -0.30333494]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:31 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.68719492]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 31 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.50807564 -0.36054047  0.22506115  0.7010188  -0.07682922 -0.06004453\n",
      "   0.87930174  0.21787786 -0.17683894]\n",
      " [-0.45714093  0.54433045 -0.46302082 -0.83410806 -0.01905002 -0.06326306\n",
      "  -1.20222359 -0.41752704  0.27542338]\n",
      " [ 0.19630876 -0.31653831  0.35958866 -0.62945925  0.2846839  -0.46765008\n",
      "  -0.56695554  0.23242104  0.40410841]\n",
      " [ 0.69053253 -0.49594441  0.35037229  0.67945892 -0.01580099  0.12473874\n",
      "   1.1646634  -0.0941819  -0.56490824]\n",
      " [-0.64752248  0.51384611 -0.32928354 -0.5283289   0.07462655 -0.12286853\n",
      "  -1.13554428 -0.29439781  0.37050756]\n",
      " [ 0.00401901  0.0603196   0.28389216 -0.67957669 -0.04860176 -0.04231918\n",
      "  -0.74280317  0.06249642  0.11295244]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.08936639  0.76657453 -1.38506021 -0.06930115  0.85758637 -1.22180267\n",
      "  -0.29019845]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:31 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.65580007]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 31 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.49205262 -0.37656349  0.20903813  0.7010188  -0.07682922 -0.06004453\n",
      "   0.86327872  0.21787786 -0.17683894]\n",
      " [-0.4405779   0.56089348 -0.44645779 -0.83410806 -0.01905002 -0.06326306\n",
      "  -1.18566056 -0.41752704  0.27542338]\n",
      " [ 0.2022108  -0.31063626  0.3654907  -0.62945925  0.2846839  -0.46765008\n",
      "  -0.56105349  0.23242104  0.40410841]\n",
      " [ 0.67411557 -0.51236137  0.33395532  0.67945892 -0.01580099  0.12473874\n",
      "   1.14824643 -0.0941819  -0.56490824]\n",
      " [-0.63096998  0.5303986  -0.31273105 -0.5283289   0.07462655 -0.12286853\n",
      "  -1.11899179 -0.29439781  0.37050756]\n",
      " [ 0.01104319  0.06734377  0.29091634 -0.67957669 -0.04860176 -0.04231918\n",
      "  -0.73577899  0.06249642  0.11295244]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.16338207  0.70901785 -1.39772215 -0.10030081  0.79491082 -1.23425143\n",
      "  -0.31999843]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:31 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.51926922]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 31 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.50329277 -0.37656349  0.22027828  0.7010188  -0.07682922 -0.06004453\n",
      "   0.86327872  0.229118   -0.17683894]\n",
      " [-0.45371734  0.56089348 -0.45959723 -0.83410806 -0.01905002 -0.06326306\n",
      "  -1.18566056 -0.43066648  0.27542338]\n",
      " [ 0.21247992 -0.31063626  0.37575982 -0.62945925  0.2846839  -0.46765008\n",
      "  -0.56105349  0.24269016  0.40410841]\n",
      " [ 0.68531795 -0.51236137  0.3451577   0.67945892 -0.01580099  0.12473874\n",
      "   1.14824643 -0.08297952 -0.56490824]\n",
      " [-0.64391453  0.5303986  -0.32567559 -0.5283289   0.07462655 -0.12286853\n",
      "  -1.11899179 -0.30734235  0.37050756]\n",
      " [ 0.01633262  0.06734377  0.29620577 -0.67957669 -0.04860176 -0.04231918\n",
      "  -0.73577899  0.06778585  0.11295244]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.10337997  0.75190926 -1.38491782 -0.05889932  0.83774004 -1.22076493\n",
      "  -0.28459007]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:31 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.53690006]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 31 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.51477471 -0.37656349  0.22027828  0.71250074 -0.07682922 -0.06004453\n",
      "   0.86327872  0.229118   -0.165357  ]\n",
      " [-0.46511127  0.56089348 -0.45959723 -0.84550199 -0.01905002 -0.06326306\n",
      "  -1.18566056 -0.43066648  0.26402945]\n",
      " [ 0.21229468 -0.31063626  0.37575982 -0.62964449  0.2846839  -0.46765008\n",
      "  -0.56105349  0.24269016  0.40392317]\n",
      " [ 0.69516902 -0.51236137  0.3451577   0.68930999 -0.01580099  0.12473874\n",
      "   1.14824643 -0.08297952 -0.55505716]\n",
      " [-0.65378159  0.5303986  -0.32567559 -0.53819597  0.07462655 -0.12286853\n",
      "  -1.11899179 -0.30734235  0.3606405 ]\n",
      " [ 0.00898284  0.06734377  0.29620577 -0.68692647 -0.04860176 -0.04231918\n",
      "  -0.73577899  0.06778585  0.10560267]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.04580776  0.79430691 -1.36957424 -0.03029846  0.87746177 -1.20293744\n",
      "  -0.26353034]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:31 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.41255447]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 31 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.51214811 -0.37656349  0.22027828  0.71250074 -0.07945582 -0.06267112\n",
      "   0.86327872  0.229118   -0.1679836 ]\n",
      " [-0.46163958  0.56089348 -0.45959723 -0.84550199 -0.01557833 -0.05979137\n",
      "  -1.18566056 -0.43066648  0.26750114]\n",
      " [ 0.20712626 -0.31063626  0.37575982 -0.62964449  0.27951548 -0.4728185\n",
      "  -0.56105349  0.24269016  0.39875474]\n",
      " [ 0.69210417 -0.51236137  0.3451577   0.68930999 -0.01886584  0.12167389\n",
      "   1.14824643 -0.08297952 -0.55812201]\n",
      " [-0.64963692  0.5303986  -0.32567559 -0.53819597  0.07877122 -0.11872387\n",
      "  -1.11899179 -0.30734235  0.36478516]\n",
      " [ 0.00868713  0.06734377  0.29620577 -0.68692647 -0.04889748 -0.0426149\n",
      "  -0.73577899  0.06778585  0.10530695]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.09579972  0.76666451 -1.39105188 -0.06062608  0.84936916 -1.22370778\n",
      "  -0.28882207]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:31 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.26255628]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 31 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.51324874 -0.37546286  0.22027828  0.71250074 -0.07835519 -0.0615705\n",
      "   0.86327872  0.229118   -0.16688297]\n",
      " [-0.46345244  0.55908062 -0.45959723 -0.84550199 -0.01739119 -0.06160423\n",
      "  -1.18566056 -0.43066648  0.26568828]\n",
      " [ 0.20648014 -0.31128238  0.37575982 -0.62964449  0.27886937 -0.47346461\n",
      "  -0.56105349  0.24269016  0.39810863]\n",
      " [ 0.69382247 -0.51064308  0.3451577   0.68930999 -0.01714755  0.12339219\n",
      "   1.14824643 -0.08297952 -0.55640371]\n",
      " [-0.65092967  0.52910586 -0.32567559 -0.53819597  0.07747847 -0.12001661\n",
      "  -1.11899179 -0.30734235  0.36349242]\n",
      " [ 0.00811748  0.06677412  0.29620577 -0.68692647 -0.04946712 -0.04318455\n",
      "  -0.73577899  0.06778585  0.1047373 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.12121786  0.75506167 -1.40559957 -0.07398238  0.83840022 -1.23771872\n",
      "  -0.30210155]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:31 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.59063975]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 31 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.50040795 -0.38830365  0.22027828  0.71250074 -0.09119597 -0.07441128\n",
      "   0.85043794  0.229118   -0.16688297]\n",
      " [-0.44835659  0.57417647 -0.45959723 -0.84550199 -0.00229534 -0.04650838\n",
      "  -1.17056471 -0.43066648  0.26568828]\n",
      " [ 0.21931425 -0.29844827  0.37575982 -0.62964449  0.29170348 -0.4606305\n",
      "  -0.54821938  0.24269016  0.39810863]\n",
      " [ 0.6779029  -0.52656265  0.3451577   0.68930999 -0.03306712  0.10747262\n",
      "   1.13232686 -0.08297952 -0.55640371]\n",
      " [-0.6353607   0.54467482 -0.32567559 -0.53819597  0.09304744 -0.10444765\n",
      "  -1.10342282 -0.30734235  0.36349242]\n",
      " [ 0.01982654  0.07848318  0.29620577 -0.68692647 -0.03775806 -0.03147549\n",
      "  -0.72406993  0.06778585  0.1047373 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.19262161  0.70487208 -1.42252446 -0.0952068   0.7806993  -1.2532119\n",
      "  -0.32495437]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:31 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.69741305]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 31 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.5069961  -0.38830365  0.22027828  0.71908888 -0.09119597 -0.07441128\n",
      "   0.85702608  0.229118   -0.16688297]\n",
      " [-0.45404099  0.57417647 -0.45959723 -0.85118639 -0.00229534 -0.04650838\n",
      "  -1.17624911 -0.43066648  0.26568828]\n",
      " [ 0.2131829  -0.29844827  0.37575982 -0.63577584  0.29170348 -0.4606305\n",
      "  -0.55435073  0.24269016  0.39810863]\n",
      " [ 0.68349959 -0.52656265  0.3451577   0.69490668 -0.03306712  0.10747262\n",
      "   1.13792355 -0.08297952 -0.55640371]\n",
      " [-0.64149562  0.54467482 -0.32567559 -0.54433089  0.09304744 -0.10444765\n",
      "  -1.10955775 -0.30734235  0.36349242]\n",
      " [ 0.01274074  0.07848318  0.29620577 -0.69401226 -0.03775806 -0.03147549\n",
      "  -0.73115572  0.06778585  0.1047373 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.16069444  0.73320072 -1.42002168 -0.08635759  0.81020351 -1.25024116\n",
      "  -0.3185938 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:31 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.63843638]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 31 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.51590032 -0.38830365  0.22027828  0.71908888 -0.08229175 -0.06550706\n",
      "   0.8659303   0.229118   -0.16688297]\n",
      " [-0.46332518  0.57417647 -0.45959723 -0.85118639 -0.01157953 -0.05579257\n",
      "  -1.1855333  -0.43066648  0.26568828]\n",
      " [ 0.20819293 -0.29844827  0.37575982 -0.63577584  0.2867135  -0.46562048\n",
      "  -0.55934071  0.24269016  0.39810863]\n",
      " [ 0.69248145 -0.52656265  0.3451577   0.69490668 -0.02408526  0.11645448\n",
      "   1.14690541 -0.08297952 -0.55640371]\n",
      " [-0.65069111  0.54467482 -0.32567559 -0.54433089  0.08385195 -0.11364314\n",
      "  -1.11875324 -0.30734235  0.36349242]\n",
      " [ 0.00567685  0.07848318  0.29620577 -0.69401226 -0.04482195 -0.03853938\n",
      "  -0.73821961  0.06778585  0.1047373 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.1189636   0.76526013 -1.41346094 -0.07070138  0.84648508 -1.24412829\n",
      "  -0.30554566]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:31 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.55413699]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 31 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.50242247 -0.38830365  0.22027828  0.70561103 -0.08229175 -0.07898491\n",
      "   0.8659303   0.229118   -0.18036082]\n",
      " [-0.44918966  0.57417647 -0.45959723 -0.83705088 -0.01157953 -0.04165705\n",
      "  -1.1855333  -0.43066648  0.27982379]\n",
      " [ 0.21616714 -0.29844827  0.37575982 -0.62780163  0.2867135  -0.45764626\n",
      "  -0.55934071  0.24269016  0.40608285]\n",
      " [ 0.67942341 -0.52656265  0.3451577   0.68184865 -0.02408526  0.10339644\n",
      "   1.14690541 -0.08297952 -0.56946174]\n",
      " [-0.6376513   0.54467482 -0.32567559 -0.53129107  0.08385195 -0.10060332\n",
      "  -1.11875324 -0.30734235  0.37653223]\n",
      " [ 0.01535671  0.07848318  0.29620577 -0.68433241 -0.04482195 -0.02885952\n",
      "  -0.73821961  0.06778585  0.11441716]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.18741869  0.71518048 -1.43049777 -0.09662491  0.79715635 -1.2632859\n",
      "  -0.32945672]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:31 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.66437577]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 31 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.51078784 -0.37993827  0.22027828  0.7139764  -0.08229175 -0.07061954\n",
      "   0.87429568  0.229118   -0.18036082]\n",
      " [-0.4571661   0.56620003 -0.45959723 -0.84502732 -0.01157953 -0.04963349\n",
      "  -1.19350974 -0.43066648  0.27982379]\n",
      " [ 0.20788397 -0.30673144  0.37575982 -0.6360848   0.2867135  -0.46592944\n",
      "  -0.56762389  0.24269016  0.40608285]\n",
      " [ 0.68709563 -0.51889043  0.3451577   0.68952087 -0.02408526  0.11106866\n",
      "   1.15457763 -0.08297952 -0.56946174]\n",
      " [-0.64578874  0.53653738 -0.32567559 -0.53942851  0.08385195 -0.10874076\n",
      "  -1.12689068 -0.30734235  0.37653223]\n",
      " [ 0.00708872  0.07021519  0.29620577 -0.6926004  -0.04482195 -0.03712751\n",
      "  -0.7464876   0.06778585  0.11441716]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.14999984  0.74634849 -1.42579311 -0.09097606  0.83043809 -1.25817361\n",
      "  -0.3217995 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:31 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.68293837]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 31 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 5.18393703e-01 -3.72332416e-01  2.20278279e-01  7.21582262e-01\n",
      "  -8.22917485e-02 -7.06195354e-02  8.81901538e-01  2.29118004e-01\n",
      "  -1.80360823e-01]\n",
      " [-4.64499980e-01  5.58866152e-01 -4.59597234e-01 -8.52361194e-01\n",
      "  -1.15795312e-02 -4.96334910e-02 -1.20084362e+00 -4.30666481e-01\n",
      "   2.79823794e-01]\n",
      " [ 2.00369812e-01 -3.14245596e-01  3.75759822e-01 -6.43598956e-01\n",
      "   2.86713502e-01 -4.65929437e-01 -5.75138042e-01  2.42690157e-01\n",
      "   4.06082850e-01]\n",
      " [ 6.94280501e-01 -5.11705560e-01  3.45157699e-01  6.96705737e-01\n",
      "  -2.40852606e-02  1.11068664e-01  1.16176249e+00 -8.29795177e-02\n",
      "  -5.69461744e-01]\n",
      " [-6.53338615e-01  5.28987505e-01 -3.25675593e-01 -5.46978388e-01\n",
      "   8.38519483e-02 -1.08740758e-01 -1.13444055e+00 -3.07342347e-01\n",
      "   3.76532235e-01]\n",
      " [-5.00761185e-04  6.26257140e-02  2.96205769e-01 -7.00189877e-01\n",
      "  -4.48219540e-02 -3.71275115e-02 -7.54077083e-01  6.77858509e-02\n",
      "   1.14417155e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.1156726   0.77545852 -1.42144023 -0.08363912  0.86071756 -1.25320128\n",
      "  -0.31479844]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:31 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.70850081]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 31 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.52513242 -0.37233242  0.227017    0.72158226 -0.08229175 -0.06388081\n",
      "   0.88864026  0.229118   -0.18036082]\n",
      " [-0.4704986   0.55886615 -0.46559585 -0.85236119 -0.01157953 -0.05563211\n",
      "  -1.20684223 -0.43066648  0.27982379]\n",
      " [ 0.19705348 -0.3142456   0.37244349 -0.64359896  0.2867135  -0.46924577\n",
      "  -0.57845438  0.24269016  0.40608285]\n",
      " [ 0.69998731 -0.51170556  0.35086451  0.69670574 -0.02408526  0.11677548\n",
      "   1.16746931 -0.08297952 -0.56946174]\n",
      " [-0.6592394   0.5289875  -0.33157638 -0.54697839  0.08385195 -0.11464154\n",
      "  -1.14034134 -0.30734235  0.37653223]\n",
      " [-0.00400973  0.06262571  0.2926968  -0.70018988 -0.04482195 -0.04063648\n",
      "  -0.75758605  0.06778585  0.11441716]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.08557131  0.80028929 -1.41836818 -0.07202559  0.88810636 -1.25025813\n",
      "  -0.30340213]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:31 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.43701453]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 32 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.5168436  -0.38062124  0.227017    0.71329344 -0.08229175 -0.06388081\n",
      "   0.88864026  0.229118   -0.18864964]\n",
      " [-0.4643583   0.56500645 -0.46559585 -0.84622089 -0.01157953 -0.05563211\n",
      "  -1.20684223 -0.43066648  0.28596409]\n",
      " [ 0.20167389 -0.30962518  0.37244349 -0.63897854  0.2867135  -0.46924577\n",
      "  -0.57845438  0.24269016  0.41070326]\n",
      " [ 0.69585047 -0.5158424   0.35086451  0.69256889 -0.02408526  0.11677548\n",
      "   1.16746931 -0.08297952 -0.57359859]\n",
      " [-0.65528802  0.53293889 -0.33157638 -0.543027    0.08385195 -0.11464154\n",
      "  -1.14034134 -0.30734235  0.38048362]\n",
      " [ 0.00260496  0.06924041  0.2926968  -0.69357518 -0.04482195 -0.04063648\n",
      "  -0.75758605  0.06778585  0.12103185]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.13933128  0.76443887 -1.43886513 -0.09418766  0.85702055 -1.27312691\n",
      "  -0.32335677]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:32 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.69281893]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 32 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 5.24141375e-01 -3.80621236e-01  2.34314771e-01  7.13293442e-01\n",
      "  -8.22917485e-02 -6.38808147e-02  8.95938030e-01  2.29118004e-01\n",
      "  -1.88649642e-01]\n",
      " [-4.70953591e-01  5.65006452e-01 -4.72191145e-01 -8.46220894e-01\n",
      "  -1.15795312e-02 -5.56321082e-02 -1.21343753e+00 -4.30666481e-01\n",
      "   2.85964095e-01]\n",
      " [ 2.01638449e-01 -3.09625184e-01  3.72408047e-01 -6.38978544e-01\n",
      "   2.86713502e-01 -4.69245772e-01 -5.78489817e-01  2.42690157e-01\n",
      "   4.10703263e-01]\n",
      " [ 7.02276331e-01 -5.15842403e-01  3.57290372e-01  6.92568893e-01\n",
      "  -2.40852606e-02  1.16775476e-01  1.17389517e+00 -8.29795177e-02\n",
      "  -5.73598587e-01]\n",
      " [-6.61903513e-01  5.32938888e-01 -3.38191875e-01 -5.43027004e-01\n",
      "   8.38519483e-02 -1.14641543e-01 -1.14695684e+00 -3.07342347e-01\n",
      "   3.80483618e-01]\n",
      " [-9.77887131e-04  6.92404080e-02  2.89113949e-01 -6.93575183e-01\n",
      "  -4.48219540e-02 -4.06364819e-02 -7.61168903e-01  6.77858509e-02\n",
      "   1.21031849e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.10664401  0.79178216 -1.43541428 -0.07787947  0.88648865 -1.26964635\n",
      "  -0.31072497]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:32 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.65762712]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 32 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.50803263 -0.39672998  0.21820603  0.71329344 -0.08229175 -0.06388081\n",
      "   0.87982929  0.229118   -0.18864964]\n",
      " [-0.454393    0.58156705 -0.45563055 -0.84622089 -0.01157953 -0.05563211\n",
      "  -1.19687693 -0.43066648  0.28596409]\n",
      " [ 0.20731033 -0.3039533   0.37807993 -0.63897854  0.2867135  -0.46924577\n",
      "  -0.57281793  0.24269016  0.41070326]\n",
      " [ 0.68586996 -0.53224877  0.34088401  0.69256889 -0.02408526  0.11677548\n",
      "   1.1574888  -0.08297952 -0.57359859]\n",
      " [-0.64535819  0.54948421 -0.32164655 -0.543027    0.08385195 -0.11464154\n",
      "  -1.13041151 -0.30734235  0.38048362]\n",
      " [ 0.00619912  0.07641741  0.29629095 -0.69357518 -0.04482195 -0.04063648\n",
      "  -0.7539919   0.06778585  0.12103185]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.18067759  0.73393321 -1.44793797 -0.10913067  0.82372135 -1.28193728\n",
      "  -0.34036813]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:32 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.52114319]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 32 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.51948518 -0.39672998  0.22965858  0.71329344 -0.08229175 -0.06388081\n",
      "   0.87982929  0.24057055 -0.18864964]\n",
      " [-0.46756113  0.58156705 -0.46879869 -0.84622089 -0.01157953 -0.05563211\n",
      "  -1.19687693 -0.44383462  0.28596409]\n",
      " [ 0.21778005 -0.3039533   0.38854965 -0.63897854  0.2867135  -0.46924577\n",
      "  -0.57281793  0.25315987  0.41070326]\n",
      " [ 0.69724174 -0.53224877  0.35225579  0.69256889 -0.02408526  0.11677548\n",
      "   1.1574888  -0.07160774 -0.57359859]\n",
      " [-0.65836068  0.54948421 -0.33464904 -0.543027    0.08385195 -0.11464154\n",
      "  -1.13041151 -0.32034484  0.38048362]\n",
      " [ 0.01154479  0.07641741  0.30163662 -0.69357518 -0.04482195 -0.04063648\n",
      "  -0.7539919   0.07313152  0.12103185]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.12092752  0.77708418 -1.43554601 -0.06754768  0.86673312 -1.26888106\n",
      "  -0.30502443]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:32 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.53604527]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 32 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.53108537 -0.39672998  0.22965858  0.72489363 -0.08229175 -0.06388081\n",
      "   0.87982929  0.24057055 -0.17704945]\n",
      " [-0.47906914  0.58156705 -0.46879869 -0.8577289  -0.01157953 -0.05563211\n",
      "  -1.19687693 -0.44383462  0.27445609]\n",
      " [ 0.21762868 -0.3039533   0.38854965 -0.63912992  0.2867135  -0.46924577\n",
      "  -0.57281793  0.25315987  0.41055189]\n",
      " [ 0.70725226 -0.53224877  0.35225579  0.70257941 -0.02408526  0.11677548\n",
      "   1.1574888  -0.07160774 -0.56358807]\n",
      " [-0.66841044  0.54948421 -0.33464904 -0.55307676  0.08385195 -0.11464154\n",
      "  -1.13041151 -0.32034484  0.37043386]\n",
      " [ 0.00405804  0.07641741  0.30163662 -0.70106193 -0.04482195 -0.04063648\n",
      "  -0.7539919   0.07313152  0.1135451 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.06323458  0.81975653 -1.4203435  -0.03885258  0.90673924 -1.25125172\n",
      "  -0.28406363]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:32 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.39887619]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 32 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.52862701 -0.39672998  0.22965858  0.72489363 -0.08475011 -0.06633917\n",
      "   0.87982929  0.24057055 -0.17950781]\n",
      " [-0.47587877  0.58156705 -0.46879869 -0.8577289  -0.00838916 -0.05244174\n",
      "  -1.19687693 -0.44383462  0.27764646]\n",
      " [ 0.21255695 -0.3039533   0.38854965 -0.63912992  0.28164178 -0.4743175\n",
      "  -0.57281793  0.25315987  0.40548017]\n",
      " [ 0.70446574 -0.53224877  0.35225579  0.70257941 -0.02687178  0.11398895\n",
      "   1.1574888  -0.07160774 -0.56637459]\n",
      " [-0.66458436  0.54948421 -0.33464904 -0.55307676  0.08767803 -0.11081546\n",
      "  -1.13041151 -0.32034484  0.37425994]\n",
      " [ 0.00367385  0.07641741  0.30163662 -0.70106193 -0.04520615 -0.04102067\n",
      "  -0.7539919   0.07313152  0.11316091]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.11105465  0.7933704  -1.44102372 -0.06800389  0.88001666 -1.27126637\n",
      "  -0.30835792]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:32 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.24505725]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 32 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.52974202 -0.39561497  0.22965858  0.72489363 -0.0836351  -0.06522417\n",
      "   0.87982929  0.24057055 -0.1783928 ]\n",
      " [-0.47765971  0.5797861  -0.46879869 -0.8577289  -0.0101701  -0.05422268\n",
      "  -1.19687693 -0.44383462  0.27586552]\n",
      " [ 0.21187145 -0.3046388   0.38854965 -0.63912992  0.28095628 -0.475003\n",
      "  -0.57281793  0.25315987  0.40479467]\n",
      " [ 0.70616539 -0.53054912  0.35225579  0.70257941 -0.02517214  0.1156886\n",
      "   1.1574888  -0.07160774 -0.56467494]\n",
      " [-0.66590346  0.54816511 -0.33464904 -0.55307676  0.08635893 -0.11213456\n",
      "  -1.13041151 -0.32034484  0.37294084]\n",
      " [ 0.00306906  0.07581262  0.30163662 -0.70106193 -0.04581093 -0.04162546\n",
      "  -0.7539919   0.07313152  0.11255612]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.13372296  0.7831586  -1.45416986 -0.08002523  0.87040899 -1.28393191\n",
      "  -0.32029802]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:32 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.59031024]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 32 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.51686697 -0.40849002  0.22965858  0.72489363 -0.09651015 -0.07809922\n",
      "   0.86695423  0.24057055 -0.1783928 ]\n",
      " [-0.46261844  0.59482738 -0.46879869 -0.8577289   0.00487117 -0.0391814\n",
      "  -1.18183566 -0.44383462  0.27586552]\n",
      " [ 0.22469361 -0.29181664  0.38854965 -0.63912992  0.29377844 -0.46218084\n",
      "  -0.55999578  0.25315987  0.40479467]\n",
      " [ 0.69026977 -0.54644475  0.35225579  0.70257941 -0.04106776  0.09979298\n",
      "   1.14159318 -0.07160774 -0.56467494]\n",
      " [-0.65037125  0.56369733 -0.33464904 -0.55307676  0.10189114 -0.09660235\n",
      "  -1.1148793  -0.32034484  0.37294084]\n",
      " [ 0.01487585  0.08761942  0.30163662 -0.70106193 -0.03400414 -0.02981867\n",
      "  -0.74218511  0.07313152  0.11255612]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.20510447  0.73292568 -1.4712174  -0.10125525  0.81288223 -1.29953491\n",
      "  -0.34300403]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:32 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.70676621]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 32 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.52305243 -0.40849002  0.22965858  0.73107909 -0.09651015 -0.07809922\n",
      "   0.8731397   0.24057055 -0.1783928 ]\n",
      " [-0.46793866  0.59482738 -0.46879869 -0.86304912  0.00487117 -0.0391814\n",
      "  -1.18715588 -0.44383462  0.27586552]\n",
      " [ 0.21880368 -0.29181664  0.38854965 -0.64501985  0.29377844 -0.46218084\n",
      "  -0.56588571  0.25315987  0.40479467]\n",
      " [ 0.69551268 -0.54644475  0.35225579  0.70782233 -0.04106776  0.09979298\n",
      "   1.14683609 -0.07160774 -0.56467494]\n",
      " [-0.6561185   0.56369733 -0.33464904 -0.55882402  0.10189114 -0.09660235\n",
      "  -1.12062655 -0.32034484  0.37294084]\n",
      " [ 0.00810648  0.08761942  0.30163662 -0.70783131 -0.03400414 -0.02981867\n",
      "  -0.74895448  0.07313152  0.11255612]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.17471845  0.76002234 -1.46891702 -0.09292954  0.84103553 -1.29681182\n",
      "  -0.33712881]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:32 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.64281849]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 32 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 5.31859910e-01 -4.08490024e-01  2.29658577e-01  7.31079094e-01\n",
      "  -8.77026710e-02 -6.92917372e-02  8.81947178e-01  2.40570554e-01\n",
      "  -1.78392805e-01]\n",
      " [-4.77052588e-01  5.94827379e-01 -4.68798689e-01 -8.63049119e-01\n",
      "  -4.24276009e-03 -4.82953370e-02 -1.19626981e+00 -4.43834619e-01\n",
      "   2.75865518e-01]\n",
      " [ 2.13855386e-01 -2.91816643e-01  3.88549646e-01 -6.45019851e-01\n",
      "   2.88830147e-01 -4.67129127e-01 -5.70834001e-01  2.53159872e-01\n",
      "   4.04794668e-01]\n",
      " [ 7.04328462e-01 -5.46444746e-01  3.52255785e-01  7.07822328e-01\n",
      "  -3.22519770e-02  1.08608760e-01  1.15565187e+00 -7.16077384e-02\n",
      "  -5.64674942e-01]\n",
      " [-6.65142483e-01  5.63697326e-01 -3.34649042e-01 -5.58824022e-01\n",
      "   9.28671623e-02 -1.05626329e-01 -1.12965053e+00 -3.20344840e-01\n",
      "   3.72940840e-01]\n",
      " [ 1.06097066e-03  8.76194153e-02  3.01636624e-01 -7.07831306e-01\n",
      "  -4.10496495e-02 -3.68641774e-02 -7.55999991e-01  7.31315223e-02\n",
      "   1.12556124e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.1337135   0.79169218 -1.46252623 -0.07759743  0.87671037 -1.29085129\n",
      "  -0.32445716]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:32 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.55441272]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 32 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.51828701 -0.40849002  0.22965858  0.71750619 -0.08770267 -0.08286464\n",
      "   0.88194718  0.24057055 -0.1919657 ]\n",
      " [-0.46286709  0.59482738 -0.46879869 -0.84886362 -0.00424276 -0.03410984\n",
      "  -1.19626981 -0.44383462  0.29005101]\n",
      " [ 0.22180997 -0.29181664  0.38854965 -0.63706527  0.28883015 -0.45917455\n",
      "  -0.570834    0.25315987  0.41274925]\n",
      " [ 0.6911967  -0.54644475  0.35225579  0.69469056 -0.03225198  0.09547699\n",
      "   1.15565187 -0.07160774 -0.57780671]\n",
      " [-0.65200624  0.56369733 -0.33464904 -0.54568778  0.09286716 -0.09249008\n",
      "  -1.12965053 -0.32034484  0.38607708]\n",
      " [ 0.01085702  0.08761942  0.30163662 -0.69803526 -0.04104965 -0.02706813\n",
      "  -0.75599999  0.07313152  0.12235217]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.20219435  0.741424   -1.47946834 -0.10355645  0.8272441  -1.30985808\n",
      "  -0.34823824]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:32 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.66963417]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 32 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.52644841 -0.40032862  0.22965858  0.7256676  -0.08770267 -0.07470323\n",
      "   0.89010858  0.24057055 -0.1919657 ]\n",
      " [-0.47064205  0.58705243 -0.46879869 -0.85663858 -0.00424276 -0.04188479\n",
      "  -1.20404476 -0.44383462  0.29005101]\n",
      " [ 0.21373066 -0.29989595  0.38854965 -0.64514457  0.28883015 -0.46725385\n",
      "  -0.5789133   0.25315987  0.41274925]\n",
      " [ 0.69867672 -0.53896472  0.35225579  0.70217059 -0.03225198  0.10295702\n",
      "   1.1631319  -0.07160774 -0.57780671]\n",
      " [-0.65993445  0.55576911 -0.33464904 -0.55361599  0.09286716 -0.1004183\n",
      "  -1.13757875 -0.32034484  0.38607708]\n",
      " [ 0.00275529  0.07951769  0.30163662 -0.70613698 -0.04104965 -0.03516985\n",
      "  -0.76410172  0.07313152  0.12235217]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.16565189  0.77196159 -1.47490602 -0.09808662  0.85976629 -1.30491922\n",
      "  -0.34090828]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:32 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.6881209]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 32 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.53384309 -0.39293395  0.22965858  0.73306227 -0.08770267 -0.07470323\n",
      "   0.89750325  0.24057055 -0.1919657 ]\n",
      " [-0.47776755  0.57992692 -0.46879869 -0.86376408 -0.00424276 -0.04188479\n",
      "  -1.21117027 -0.44383462  0.29005101]\n",
      " [ 0.2063942  -0.30723241  0.38854965 -0.65248104  0.28883015 -0.46725385\n",
      "  -0.58624977  0.25315987  0.41274925]\n",
      " [ 0.70565737 -0.53198407  0.35225579  0.70915124 -0.03225198  0.10295702\n",
      "   1.17011255 -0.07160774 -0.57780671]\n",
      " [-0.66727277  0.5484308  -0.33464904 -0.56095431  0.09286716 -0.1004183\n",
      "  -1.14491707 -0.32034484  0.38607708]\n",
      " [-0.00466926  0.07209314  0.30163662 -0.71356153 -0.04104965 -0.03516985\n",
      "  -0.77152627  0.07313152  0.12235217]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.13218562  0.80043896 -1.47071674 -0.09097673  0.88933034 -1.30015307\n",
      "  -0.33422397]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:32 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.71322741]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 32 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.54040443 -0.39293395  0.23621992  0.73306227 -0.08770267 -0.06814189\n",
      "   0.9040646   0.24057055 -0.1919657 ]\n",
      " [-0.48356225  0.57992692 -0.47459339 -0.86376408 -0.00424276 -0.04767949\n",
      "  -1.21696497 -0.44383462  0.29005101]\n",
      " [ 0.20320284 -0.30723241  0.38535828 -0.65248104  0.28883015 -0.47044521\n",
      "  -0.58944113  0.25315987  0.41274925]\n",
      " [ 0.71117698 -0.53198407  0.35777539  0.70915124 -0.03225198  0.10847662\n",
      "   1.17563215 -0.07160774 -0.57780671]\n",
      " [-0.67297033  0.5484308  -0.3403466  -0.56095431  0.09286716 -0.10611586\n",
      "  -1.15061463 -0.32034484  0.38607708]\n",
      " [-0.00817389  0.07209314  0.29813199 -0.71356153 -0.04104965 -0.03867449\n",
      "  -0.7750309   0.07313152  0.12235217]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.10285823  0.82478348 -1.46779032 -0.07961742  0.91605963 -1.29734977\n",
      "  -0.32321866]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:32 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.42738614]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 33 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.5323901  -0.40094828  0.23621992  0.72504794 -0.08770267 -0.06814189\n",
      "   0.9040646   0.24057055 -0.19998004]\n",
      " [-0.47766377  0.5858254  -0.47459339 -0.85786561 -0.00424276 -0.04767949\n",
      "  -1.21696497 -0.44383462  0.29594949]\n",
      " [ 0.20756701 -0.30286823  0.38535828 -0.64811686  0.28883015 -0.47044521\n",
      "  -0.58944113  0.25315987  0.41711342]\n",
      " [ 0.7072133  -0.53594774  0.35777539  0.70518756 -0.03225198  0.10847662\n",
      "   1.17563215 -0.07160774 -0.58177039]\n",
      " [-0.66914216  0.55225897 -0.3403466  -0.55712614  0.09286716 -0.10611586\n",
      "  -1.15061463 -0.32034484  0.38990526]\n",
      " [-0.00173786  0.07852917  0.29813199 -0.7071255  -0.04104965 -0.03867449\n",
      "  -0.7750309   0.07313152  0.12878821]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.15515475  0.78997241 -1.48781352 -0.10131504  0.88588369 -1.3196124\n",
      "  -0.34262847]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:33 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.69842488]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 33 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.53945999 -0.40094828  0.24328982  0.72504794 -0.08770267 -0.06814189\n",
      "   0.91113449  0.24057055 -0.19998004]\n",
      " [-0.4840043   0.5858254  -0.48093391 -0.85786561 -0.00424276 -0.04767949\n",
      "  -1.2233055  -0.44383462  0.29594949]\n",
      " [ 0.20759468 -0.30286823  0.38538595 -0.64811686  0.28883015 -0.47044521\n",
      "  -0.58941346  0.25315987  0.41711342]\n",
      " [ 0.71339832 -0.53594774  0.36396041  0.70518756 -0.03225198  0.10847662\n",
      "   1.18181718 -0.07160774 -0.58177039]\n",
      " [-0.67550192  0.55225897 -0.34670636 -0.55712614  0.09286716 -0.10611586\n",
      "  -1.15697438 -0.32034484  0.38990526]\n",
      " [-0.00532862  0.07852917  0.29454122 -0.7071255  -0.04104965 -0.03867449\n",
      "  -0.77862167  0.07313152  0.12878821]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.12339473  0.81671219 -1.48455658 -0.08540737  0.91458961 -1.3163287\n",
      "  -0.33047791]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:33 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.65932644]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 33 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.5232823  -0.41712597  0.22711213  0.72504794 -0.08770267 -0.06814189\n",
      "   0.8949568   0.24057055 -0.19998004]\n",
      " [-0.46744728  0.60238242 -0.46437689 -0.85786561 -0.00424276 -0.04767949\n",
      "  -1.20674847 -0.44383462  0.29594949]\n",
      " [ 0.21301304 -0.29744988  0.39080431 -0.64811686  0.28883015 -0.47044521\n",
      "  -0.58399511  0.25315987  0.41711342]\n",
      " [ 0.6969995  -0.55234657  0.34756159  0.70518756 -0.03225198  0.10847662\n",
      "   1.16541836 -0.07160774 -0.58177039]\n",
      " [-0.65896468  0.56879621 -0.33016912 -0.55712614  0.09286716 -0.10611586\n",
      "  -1.14043714 -0.32034484  0.38990526]\n",
      " [ 0.00196532  0.08582311  0.30183516 -0.7071255  -0.04104965 -0.03867449\n",
      "  -0.77132773  0.07313152  0.12878821]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.19744206  0.75861141 -1.49697016 -0.11693142  0.85175718 -1.32849114\n",
      "  -0.36000066]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:33 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.52347121]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 33 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.53491058 -0.41712597  0.23874041  0.72504794 -0.08770267 -0.06814189\n",
      "   0.8949568   0.25219884 -0.19998004]\n",
      " [-0.48061272  0.60238242 -0.47754233 -0.85786561 -0.00424276 -0.04767949\n",
      "  -1.20674847 -0.45700006  0.29594949]\n",
      " [ 0.2236677  -0.29744988  0.40145897 -0.64811686  0.28883015 -0.47044521\n",
      "  -0.58399511  0.26381453  0.41711342]\n",
      " [ 0.70851038 -0.55234657  0.35907247  0.70518756 -0.03225198  0.10847662\n",
      "   1.16541836 -0.06009686 -0.58177039]\n",
      " [-0.67199213  0.56879621 -0.34319657 -0.55712614  0.09286716 -0.10611586\n",
      "  -1.14043714 -0.33337229  0.38990526]\n",
      " [ 0.00737172  0.08582311  0.30724157 -0.7071255  -0.04104965 -0.03867449\n",
      "  -0.77132773  0.07853793  0.12878821]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.13800722  0.80195596 -1.48498173 -0.07520645  0.8948895  -1.31585683\n",
      "  -0.3247479 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:33 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.53552565]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 33 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 5.46612015e-01 -4.17125972e-01  2.38740409e-01  7.36749367e-01\n",
      "  -8.77026710e-02 -6.81418913e-02  8.94956804e-01  2.52198836e-01\n",
      "  -1.88278608e-01]\n",
      " [-4.92218728e-01  6.02382418e-01 -4.77542330e-01 -8.69471616e-01\n",
      "  -4.24276009e-03 -4.76794924e-02 -1.20674847e+00 -4.57000056e-01\n",
      "   2.84343479e-01]\n",
      " [ 2.23561759e-01 -2.97449876e-01  4.01458967e-01 -6.48222799e-01\n",
      "   2.88830147e-01 -4.70445211e-01 -5.83995106e-01  2.63814533e-01\n",
      "   4.17007485e-01]\n",
      " [ 7.18664141e-01 -5.52346566e-01  3.59072466e-01  7.15341326e-01\n",
      "  -3.22519770e-02  1.08476619e-01  1.16541836e+00 -6.00968621e-02\n",
      "  -5.71616623e-01]\n",
      " [-6.82205375e-01  5.68796208e-01 -3.43196572e-01 -5.67339382e-01\n",
      "   9.28671623e-02 -1.06115860e-01 -1.14043714e+00 -3.33372294e-01\n",
      "   3.79692010e-01]\n",
      " [-2.36777401e-04  8.58231122e-02  3.07241570e-01 -7.14733999e-01\n",
      "  -4.10496495e-02 -3.86744876e-02 -7.71327732e-01  7.85379286e-02\n",
      "   1.21179710e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.08024103  0.84485813 -1.46992426 -0.0464293   0.93513881 -1.29842874\n",
      "  -0.3038935 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:33 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.38538586]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 33 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 5.44325042e-01 -4.17125972e-01  2.38740409e-01  7.36749367e-01\n",
      "  -8.99896442e-02 -7.04288645e-02  8.94956804e-01  2.52198836e-01\n",
      "  -1.90565581e-01]\n",
      " [-4.89303776e-01  6.02382418e-01 -4.77542330e-01 -8.69471616e-01\n",
      "  -1.32780786e-03 -4.47645401e-02 -1.20674847e+00 -4.57000056e-01\n",
      "   2.87258431e-01]\n",
      " [ 2.18591242e-01 -2.97449876e-01  4.01458967e-01 -6.48222799e-01\n",
      "   2.83859630e-01 -4.75415728e-01 -5.83995106e-01  2.63814533e-01\n",
      "   4.12036968e-01]\n",
      " [ 7.16147986e-01 -5.52346566e-01  3.59072466e-01  7.15341326e-01\n",
      "  -3.47681322e-02  1.05960463e-01  1.16541836e+00 -6.00968621e-02\n",
      "  -5.74132779e-01]\n",
      " [-6.78690715e-01  5.68796208e-01 -3.43196572e-01 -5.67339382e-01\n",
      "   9.63818224e-02 -1.02601200e-01 -1.14043714e+00 -3.33372294e-01\n",
      "   3.83206670e-01]\n",
      " [-7.06904137e-04  8.58231122e-02  3.07241570e-01 -7.14733999e-01\n",
      "  -4.15197762e-02 -3.91446144e-02 -7.71327732e-01  7.85379286e-02\n",
      "   1.20709583e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.12588297  0.81973452 -1.48979738 -0.07439713  0.90978073 -1.31767635\n",
      "  -0.32718473]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:33 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.22817867]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 33 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.54543519 -0.41601582  0.23874041  0.73674937 -0.08887949 -0.06931871\n",
      "   0.8949568   0.25219884 -0.18945543]\n",
      " [-0.49102853  0.60065767 -0.47754233 -0.86947162 -0.00305256 -0.04648929\n",
      "  -1.20674847 -0.45700006  0.28553368]\n",
      " [ 0.21788341 -0.29815771  0.40145897 -0.6482228   0.28315179 -0.47612356\n",
      "  -0.58399511  0.26381453  0.41132913]\n",
      " [ 0.71780348 -0.55069108  0.35907247  0.71534133 -0.03311264  0.10761595\n",
      "   1.16541836 -0.06009686 -0.57247729]\n",
      " [-0.68000872  0.5674782  -0.34319657 -0.56733938  0.09506382 -0.1039192\n",
      "  -1.14043714 -0.33337229  0.38188867]\n",
      " [-0.00133315  0.08519686  0.30724157 -0.714734   -0.04214603 -0.03977086\n",
      "  -0.77132773  0.07853793  0.12008333]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.1459756   0.81080765 -1.50160474 -0.08515365  0.90142182 -1.3290564\n",
      "  -0.33785893]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:33 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.58934473]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 33 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.53256158 -0.42888943  0.23874041  0.73674937 -0.10175311 -0.08219233\n",
      "   0.88208319  0.25219884 -0.18945543]\n",
      " [-0.47606609  0.61562011 -0.47754233 -0.86947162  0.01190988 -0.03152685\n",
      "  -1.19178603 -0.45700006  0.28553368]\n",
      " [ 0.23067051 -0.28537061  0.40145897 -0.6482228   0.2959389  -0.46333646\n",
      "  -0.571208    0.26381453  0.41132913]\n",
      " [ 0.70194809 -0.56654646  0.35907247  0.71534133 -0.04896803  0.09176057\n",
      "   1.14956297 -0.06009686 -0.57247729]\n",
      " [-0.66453354  0.58295338 -0.34319657 -0.56733938  0.11053899 -0.08844403\n",
      "  -1.12496197 -0.33337229  0.38188867]\n",
      " [ 0.01053882  0.09706884  0.30724157 -0.714734   -0.03027405 -0.02789889\n",
      "  -0.75945575  0.07853793  0.12008333]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.21729148  0.76060485 -1.51879896 -0.10639979  0.84413394 -1.34479298\n",
      "  -0.36043846]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:33 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.71580826]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 33 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.53836645 -0.42888943  0.23874041  0.74255423 -0.10175311 -0.08219233\n",
      "   0.88788806  0.25219884 -0.18945543]\n",
      " [-0.48104706  0.61562011 -0.47754233 -0.87445259  0.01190988 -0.03152685\n",
      "  -1.19676701 -0.45700006  0.28553368]\n",
      " [ 0.22502198 -0.28537061  0.40145897 -0.65387133  0.2959389  -0.46333646\n",
      "  -0.57685653  0.26381453  0.41132913]\n",
      " [ 0.70686135 -0.56654646  0.35907247  0.72025458 -0.04896803  0.09176057\n",
      "   1.15447623 -0.06009686 -0.57247729]\n",
      " [-0.66991813  0.58295338 -0.34319657 -0.57272397  0.11053899 -0.08844403\n",
      "  -1.13034656 -0.33337229  0.38188867]\n",
      " [ 0.00408257  0.09706884  0.30724157 -0.72119025 -0.03027405 -0.02789889\n",
      "  -0.765912    0.07853793  0.12008333]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.18838537  0.78649891 -1.51668064 -0.0985617   0.87097897 -1.3422919\n",
      "  -0.35500673]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:33 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.6469994]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 33 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.5470723  -0.42888943  0.23874041  0.74255423 -0.09304725 -0.07348647\n",
      "   0.89659391  0.25219884 -0.18945543]\n",
      " [-0.48999967  0.61562011 -0.47754233 -0.87445259  0.00295727 -0.04047946\n",
      "  -1.20571962 -0.45700006  0.28553368]\n",
      " [ 0.2201267  -0.28537061  0.40145897 -0.65387133  0.29104362 -0.46823174\n",
      "  -0.58175181  0.26381453  0.41132913]\n",
      " [ 0.71552227 -0.56654646  0.35907247  0.72025458 -0.0403071   0.10042149\n",
      "   1.16313715 -0.06009686 -0.57247729]\n",
      " [-0.67878069  0.58295338 -0.34319657 -0.57272397  0.10167644 -0.09730658\n",
      "  -1.13920911 -0.33337229  0.38188867]\n",
      " [-0.00293412  0.09706884  0.30724157 -0.72119025 -0.03729074 -0.03491558\n",
      "  -0.7729287   0.07853793  0.12008333]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Theta two: \n",
      "[[-0.14807426  0.81777895 -1.51044044 -0.08352437  0.90606408 -1.33646577\n",
      "  -0.34268108]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:33 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.55474279]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 33 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.53341405 -0.42888943  0.23874041  0.72889598 -0.09304725 -0.08714472\n",
      "   0.89659391  0.25219884 -0.20311369]\n",
      " [-0.47576979  0.61562011 -0.47754233 -0.8602227   0.00295727 -0.02624958\n",
      "  -1.20571962 -0.45700006  0.29976357]\n",
      " [ 0.22804431 -0.28537061  0.40145897 -0.64595372  0.29104362 -0.46031413\n",
      "  -0.58175181  0.26381453  0.41924675]\n",
      " [ 0.70232472 -0.56654646  0.35907247  0.70705702 -0.0403071   0.08722394\n",
      "   1.16313715 -0.06009686 -0.58567485]\n",
      " [-0.66555828  0.58295338 -0.34319657 -0.55950157  0.10167644 -0.08408418\n",
      "  -1.13920911 -0.33337229  0.39511107]\n",
      " [ 0.00696469  0.09706884  0.30724157 -0.71129144 -0.03729074 -0.02501677\n",
      "  -0.7729287   0.07853793  0.12998214]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.21658589  0.76733571 -1.52730273 -0.10954105  0.85647082 -1.35534033\n",
      "  -0.36635064]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:33 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.67455083]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 33 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 5.41384357e-01 -4.20919123e-01  2.38740409e-01  7.36866287e-01\n",
      "  -9.30472493e-02 -7.91744141e-02  9.04564223e-01  2.52198836e-01\n",
      "  -2.03113685e-01]\n",
      " [-4.83361294e-01  6.08028600e-01 -4.77542330e-01 -8.67814211e-01\n",
      "   2.95726899e-03 -3.38410836e-02 -1.21331113e+00 -4.57000056e-01\n",
      "   2.99763570e-01]\n",
      " [ 2.20154442e-01 -2.93260478e-01  4.01458967e-01 -6.53843586e-01\n",
      "   2.91043616e-01 -4.68203998e-01 -5.89641684e-01  2.63814533e-01\n",
      "   4.19246746e-01]\n",
      " [ 7.09631255e-01 -5.59239921e-01  3.59072466e-01  7.14363564e-01\n",
      "  -4.03071013e-02  9.45304775e-02  1.17044369e+00 -6.00968621e-02\n",
      "  -5.85674845e-01]\n",
      " [-6.73294895e-01  5.75216769e-01 -3.43196572e-01 -5.67238178e-01\n",
      "   1.01676438e-01 -9.18207899e-02 -1.14694572e+00 -3.33372294e-01\n",
      "   3.95111071e-01]\n",
      " [-9.76002337e-04  8.91281522e-02  3.07241570e-01 -7.19232130e-01\n",
      "  -3.72907436e-02 -3.29574617e-02 -7.80869389e-01  7.85379286e-02\n",
      "   1.29982143e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.18086263  0.79726912 -1.52286239 -0.10423132  0.8882731  -1.35055074\n",
      "  -0.35931346]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:33 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.69308495]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 33 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.54857793 -0.41372555  0.23874041  0.74405986 -0.09304725 -0.07917441\n",
      "   0.91175779  0.25219884 -0.20311369]\n",
      " [-0.49029141  0.60109848 -0.47754233 -0.87474433  0.00295727 -0.03384108\n",
      "  -1.22024125 -0.45700006  0.29976357]\n",
      " [ 0.21298997 -0.30042495  0.40145897 -0.66100806  0.29104362 -0.468204\n",
      "  -0.59680616  0.26381453  0.41924675]\n",
      " [ 0.71642117 -0.55245001  0.35907247  0.72115348 -0.0403071   0.09453048\n",
      "   1.17723361 -0.06009686 -0.58567485]\n",
      " [-0.68043306  0.5680786  -0.34319657 -0.57437634  0.10167644 -0.09182079\n",
      "  -1.15408389 -0.33337229  0.39511107]\n",
      " [-0.00823708  0.08186707  0.30724157 -0.72649321 -0.03729074 -0.03295746\n",
      "  -0.78813047  0.07853793  0.12998214]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.14821943  0.82512837 -1.5188195  -0.09733101  0.91714417 -1.34596878\n",
      "  -0.35291778]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:33 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.71783531]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 33 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.55496233 -0.41372555  0.24512481  0.74405986 -0.09304725 -0.07279001\n",
      "   0.91814219  0.25219884 -0.20311369]\n",
      " [-0.49589373  0.60109848 -0.48314465 -0.87474433  0.00295727 -0.0394434\n",
      "  -1.22584357 -0.45700006  0.29976357]\n",
      " [ 0.20992915 -0.30042495  0.39839815 -0.66100806  0.29104362 -0.47126481\n",
      "  -0.59986697  0.26381453  0.41924675]\n",
      " [ 0.72176473 -0.55245001  0.36441603  0.72115348 -0.0403071   0.09987404\n",
      "   1.18257717 -0.06009686 -0.58567485]\n",
      " [-0.68593931  0.5680786  -0.34870282 -0.57437634  0.10167644 -0.09732704\n",
      "  -1.15959014 -0.33337229  0.39511107]\n",
      " [-0.0117238   0.08186707  0.30375486 -0.72649321 -0.03729074 -0.03644417\n",
      "  -0.79161718  0.07853793  0.12998214]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.11964351  0.84898302 -1.51602478 -0.08620848  0.94322587 -1.34329185\n",
      "  -0.3422771 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:33 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.41783402]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 34 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.54723039 -0.42145749  0.24512481  0.73632792 -0.09304725 -0.07279001\n",
      "   0.91814219  0.25219884 -0.21084562]\n",
      " [-0.49024291  0.6067493  -0.48314465 -0.86909351  0.00295727 -0.0394434\n",
      "  -1.22584357 -0.45700006  0.30541439]\n",
      " [ 0.21403599 -0.29631812  0.39839815 -0.65690123  0.29104362 -0.47126481\n",
      "  -0.59986697  0.26381453  0.42335358]\n",
      " [ 0.71798099 -0.55623376  0.36441603  0.71736973 -0.0403071   0.09987404\n",
      "   1.18257717 -0.06009686 -0.58945859]\n",
      " [-0.68224652  0.57177139 -0.34870282 -0.57068355  0.10167644 -0.09732704\n",
      "  -1.15959014 -0.33337229  0.39880386]\n",
      " [-0.00547908  0.08811179  0.30375486 -0.72024849 -0.03729074 -0.03644417\n",
      "  -0.79161718  0.07853793  0.13622686]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.17046231  0.81522786 -1.53557321 -0.10743506  0.91397386 -1.36495388\n",
      "  -0.3611494 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:34 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.70399332]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 34 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.5540706  -0.42145749  0.25196502  0.73632792 -0.09304725 -0.07279001\n",
      "   0.92498241  0.25219884 -0.21084562]\n",
      " [-0.49633769  0.6067493  -0.48923943 -0.86909351  0.00295727 -0.0394434\n",
      "  -1.23193834 -0.45700006  0.30541439]\n",
      " [ 0.21413288 -0.29631812  0.39849505 -0.65690123  0.29104362 -0.47126481\n",
      "  -0.59977008  0.26381453  0.42335358]\n",
      " [ 0.72393394 -0.55623376  0.37036899  0.71736973 -0.0403071   0.09987404\n",
      "   1.18853013 -0.06009686 -0.58945859]\n",
      " [-0.68835954  0.57177139 -0.35481585 -0.57068355  0.10167644 -0.09732704\n",
      "  -1.16570316 -0.33337229  0.39880386]\n",
      " [-0.0090606   0.08811179  0.30017334 -0.72024849 -0.03729074 -0.03644417\n",
      "  -0.79519871  0.07853793  0.13622686]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.13962038  0.84134792 -1.53249459 -0.0919172   0.94191463 -1.36185108\n",
      "  -0.34945701]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:34 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.66088501]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 34 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.53783778 -0.43769031  0.2357322   0.73632792 -0.09304725 -0.07279001\n",
      "   0.90874959  0.25219884 -0.21084562]\n",
      " [-0.4797844   0.62330258 -0.47268614 -0.86909351  0.00295727 -0.0394434\n",
      "  -1.21538506 -0.45700006  0.30541439]\n",
      " [ 0.21927695 -0.29117406  0.40363911 -0.65690123  0.29104362 -0.47126481\n",
      "  -0.59462602  0.26381453  0.42335358]\n",
      " [ 0.70753939 -0.57262831  0.35397443  0.71736973 -0.0403071   0.09987404\n",
      "   1.17213557 -0.06009686 -0.58945859]\n",
      " [-0.67183017  0.58830077 -0.33828647 -0.57068355  0.10167644 -0.09732704\n",
      "  -1.14917379 -0.33337229  0.39880386]\n",
      " [-0.00168288  0.09548951  0.30755105 -0.72024849 -0.03729074 -0.03644417\n",
      "  -0.78782099  0.07853793  0.13622686]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.21367784  0.78303214 -1.54482384 -0.1237327   0.87904156 -1.37391201\n",
      "  -0.37889341]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:34 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.52621298]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 34 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.5496065  -0.43769031  0.24750092  0.73632792 -0.09304725 -0.07279001\n",
      "   0.90874959  0.26396755 -0.21084562]\n",
      " [-0.49291842  0.62330258 -0.48582016 -0.86909351  0.00295727 -0.0394434\n",
      "  -1.21538506 -0.47013407  0.30541439]\n",
      " [ 0.2300992  -0.29117406  0.41446136 -0.65690123  0.29104362 -0.47126481\n",
      "  -0.59462602  0.27463679  0.42335358]\n",
      " [ 0.71916024 -0.57262831  0.36559529  0.71736973 -0.0403071   0.09987404\n",
      "   1.17213557 -0.04847601 -0.58945859]\n",
      " [-0.68485217  0.58830077 -0.35130847 -0.57068355  0.10167644 -0.09732704\n",
      "  -1.14917379 -0.34639429  0.39880386]\n",
      " [ 0.00378832  0.09548951  0.31302225 -0.72024849 -0.03729074 -0.03644417\n",
      "  -0.78782099  0.08400912  0.13622686]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.15461723  0.82650635 -1.53322921 -0.08190509  0.92223476 -1.3616902\n",
      "  -0.34375617]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:34 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.53532625]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 34 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.56139323 -0.43769031  0.24750092  0.74811465 -0.09304725 -0.07279001\n",
      "   0.90874959  0.26396755 -0.1990589 ]\n",
      " [-0.50460734  0.62330258 -0.48582016 -0.88078243  0.00295727 -0.0394434\n",
      "  -1.21538506 -0.47013407  0.29372547]\n",
      " [ 0.23004937 -0.29117406  0.41446136 -0.65695105  0.29104362 -0.47126481\n",
      "  -0.59462602  0.27463679  0.42330376]\n",
      " [ 0.72944191 -0.57262831  0.36559529  0.72765139 -0.0403071   0.09987404\n",
      "   1.17213557 -0.04847601 -0.57917693]\n",
      " [-0.69521079  0.58830077 -0.35130847 -0.58104218  0.10167644 -0.09732704\n",
      "  -1.14917379 -0.34639429  0.38844524]\n",
      " [-0.00392739  0.09548951  0.31302225 -0.72796419 -0.03729074 -0.03644417\n",
      "  -0.78782099  0.08400912  0.12851116]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.09682296  0.86959565 -1.51832058 -0.05305778  0.96268801 -1.3444662\n",
      "  -0.32301502]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:34 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.37210881]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 34 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.55927825 -0.43769031  0.24750092  0.74811465 -0.09516222 -0.07490499\n",
      "   0.90874959  0.26396755 -0.20117387]\n",
      " [-0.50195975  0.62330258 -0.48582016 -0.88078243  0.00560486 -0.03679581\n",
      "  -1.21538506 -0.47013407  0.29637306]\n",
      " [ 0.2251849  -0.29117406  0.41446136 -0.65695105  0.28617915 -0.47612928\n",
      "  -0.59462602  0.27463679  0.41843929]\n",
      " [ 0.72718645 -0.57262831  0.36559529  0.72765139 -0.04256256  0.09761859\n",
      "   1.17213557 -0.04847601 -0.58143238]\n",
      " [-0.69199826  0.58830077 -0.35130847 -0.58104218  0.10488897 -0.09411451\n",
      "  -1.14917379 -0.34639429  0.39165777]\n",
      " [-0.00447964  0.09548951  0.31302225 -0.72796419 -0.03784299 -0.03699642\n",
      "  -0.78782099  0.08400912  0.12795891]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.14029342  0.8457318  -1.53738114 -0.07984101  0.93868073 -1.36293971\n",
      "  -0.34530274]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:34 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.21198063]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 34 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.56036625 -0.43660232  0.24750092  0.74811465 -0.09407423 -0.07381699\n",
      "   0.90874959  0.26396755 -0.20008587]\n",
      " [-0.50360827  0.62165406 -0.48582016 -0.88078243  0.00395633 -0.03844434\n",
      "  -1.21538506 -0.47013407  0.29472454]\n",
      " [ 0.22447036 -0.2918886   0.41446136 -0.65695105  0.28546461 -0.47684382\n",
      "  -0.59462602  0.27463679  0.41772475]\n",
      " [ 0.72877663 -0.57103813  0.36559529  0.72765139 -0.04097238  0.09920877\n",
      "   1.17213557 -0.04847601 -0.57984221]\n",
      " [-0.69329148  0.58700755 -0.35130847 -0.58104218  0.10359575 -0.09540773\n",
      "  -1.14917379 -0.34639429  0.39036455]\n",
      " [-0.00511429  0.09485485  0.31302225 -0.72796419 -0.03847765 -0.03763108\n",
      "  -0.78782099  0.08400912  0.12732426]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.15799856  0.83797857 -1.54792359 -0.08941127  0.93145524 -1.37310481\n",
      "  -0.35479216]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:34 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.58771483]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 34 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.54752788 -0.44944069  0.24750092  0.74811465 -0.1069126  -0.08665536\n",
      "   0.89591122  0.26396755 -0.20008587]\n",
      " [-0.48874958  0.63651275 -0.48582016 -0.88078243  0.01881503 -0.02358565\n",
      "  -1.20052637 -0.47013407  0.29472454]\n",
      " [ 0.23720004 -0.27915892  0.41446136 -0.65695105  0.29819428 -0.46411415\n",
      "  -0.58189634  0.27463679  0.41772475]\n",
      " [ 0.71297974 -0.58683502  0.36559529  0.72765139 -0.05676927  0.08341188\n",
      "   1.15633868 -0.04847601 -0.57984221]\n",
      " [-0.67789478  0.60240425 -0.35130847 -0.58104218  0.11899245 -0.08001103\n",
      "  -1.13377709 -0.34639429  0.39036455]\n",
      " [ 0.00679197  0.10676112  0.31302225 -0.72796419 -0.02657138 -0.02572481\n",
      "  -0.77591472  0.08400912  0.12732426]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.22920201  0.78787801 -1.5652847  -0.11068082  0.8744718  -1.38899551\n",
      "  -0.37726209]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:34 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.72452755]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 34 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 5.52974720e-01 -4.49440685e-01  2.47500917e-01  7.53561491e-01\n",
      "  -1.06912598e-01 -8.66553616e-02  9.01358059e-01  2.63967553e-01\n",
      "  -2.00085875e-01]\n",
      " [-4.93415197e-01  6.36512750e-01 -4.85820159e-01 -8.85448043e-01\n",
      "   1.88150263e-02 -2.35856467e-02 -1.20519198e+00 -4.70134073e-01\n",
      "   2.94724535e-01]\n",
      " [ 2.31790313e-01 -2.79158923e-01  4.14461362e-01 -6.62360775e-01\n",
      "   2.98194281e-01 -4.64114147e-01 -5.87306066e-01  2.74636785e-01\n",
      "   4.17724747e-01]\n",
      " [ 7.17586405e-01 -5.86835021e-01  3.65595290e-01  7.32258057e-01\n",
      "  -5.67692684e-02  8.34118766e-02  1.16094535e+00 -4.84760054e-02\n",
      "  -5.79842207e-01]\n",
      " [-6.82941122e-01  6.02404248e-01 -3.51308469e-01 -5.86088517e-01\n",
      "   1.18992447e-01 -8.00110316e-02 -1.13882343e+00 -3.46394292e-01\n",
      "   3.90364552e-01]\n",
      " [ 6.42410670e-04  1.06761117e-01  3.13022250e-01 -7.34113757e-01\n",
      "  -2.65713839e-02 -2.57248148e-02 -7.82064285e-01  8.40091245e-02\n",
      "   1.27324256e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.20171159  0.81260532 -1.5633304  -0.10329641  0.9000562  -1.38669374\n",
      "  -0.37223519]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:34 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.65096402]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 34 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.56157691 -0.44944069  0.24750092  0.75356149 -0.09831041 -0.07805317\n",
      "   0.90996025  0.26396755 -0.20008587]\n",
      " [-0.50221608  0.63651275 -0.48582016 -0.88544804  0.01001414 -0.03238653\n",
      "  -1.21399287 -0.47013407  0.29472454]\n",
      " [ 0.22695732 -0.27915892  0.41446136 -0.66236078  0.29336129 -0.46894714\n",
      "  -0.59213906  0.27463679  0.41772475]\n",
      " [ 0.72610384 -0.58683502  0.36559529  0.73225806 -0.04825183  0.09192932\n",
      "   1.16946279 -0.04847601 -0.57984221]\n",
      " [-0.69165279  0.60240425 -0.35130847 -0.58608852  0.11028078 -0.0887227\n",
      "  -1.1475351  -0.34639429  0.39036455]\n",
      " [-0.00633747  0.10676112  0.31302225 -0.73411376 -0.03355127 -0.0327047\n",
      "  -0.78904417  0.08400912  0.12732426]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.16205938  0.84350064 -1.55722298 -0.08852531  0.9345725  -1.38098541\n",
      "  -0.36022614]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:34 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.55511324]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 34 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.54784189 -0.44944069  0.24750092  0.73982647 -0.09831041 -0.09178819\n",
      "   0.90996025  0.26396755 -0.21382089]\n",
      " [-0.48794675  0.63651275 -0.48582016 -0.87117871  0.01001414 -0.0181172\n",
      "  -1.21399287 -0.47013407  0.30899387]\n",
      " [ 0.23482156 -0.27915892  0.41446136 -0.65449653  0.29336129 -0.4610829\n",
      "  -0.59213906  0.27463679  0.42558899]\n",
      " [ 0.71284758 -0.58683502  0.36559529  0.7190018  -0.04825183  0.07867305\n",
      "   1.16946279 -0.04847601 -0.59309847]\n",
      " [-0.67835332  0.60240425 -0.35130847 -0.57278905  0.11028078 -0.07542323\n",
      "  -1.1475351  -0.34639429  0.40366402]\n",
      " [ 0.00365153  0.10676112  0.31302225 -0.72412476 -0.03355127 -0.0227157\n",
      "  -0.78904417  0.08400912  0.13731325]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.23060547  0.79289508 -1.57401846 -0.11462003  0.88486226 -1.39974417\n",
      "  -0.38380116]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:34 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.6791254]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 34 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.55563472 -0.44164786  0.24750092  0.7476193  -0.09831041 -0.08399537\n",
      "   0.91775307  0.26396755 -0.21382089]\n",
      " [-0.49537215  0.62908735 -0.48582016 -0.87860411  0.01001414 -0.0255426\n",
      "  -1.22141826 -0.47013407  0.30899387]\n",
      " [ 0.22710675 -0.28687374  0.41446136 -0.66221135  0.29336129 -0.46879771\n",
      "  -0.59985387  0.27463679  0.42558899]\n",
      " [ 0.71999843 -0.57968418  0.36559529  0.72615264 -0.04825183  0.0858239\n",
      "   1.17661363 -0.04847601 -0.59309847]\n",
      " [-0.68591556  0.59484201 -0.35130847 -0.58035129  0.11028078 -0.08298547\n",
      "  -1.15509734 -0.34639429  0.40366402]\n",
      " [-0.0041352   0.09897439  0.31302225 -0.73191148 -0.03355127 -0.03050242\n",
      "  -0.79683089  0.08400912  0.13731325]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.19564392  0.82225419 -1.56968186 -0.10945313  0.91598628 -1.39508221\n",
      "  -0.37702543]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:34 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.69782921]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 34 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.56263767 -0.43464491  0.24750092  0.75462225 -0.09831041 -0.08399537\n",
      "   0.92475603  0.26396755 -0.21382089]\n",
      " [-0.50211945  0.62234005 -0.48582016 -0.88535141  0.01001414 -0.0255426\n",
      "  -1.22816557 -0.47013407  0.30899387]\n",
      " [ 0.22010792 -0.29387257  0.41446136 -0.66921018  0.29336129 -0.46879771\n",
      "  -0.6068527   0.27463679  0.42558899]\n",
      " [ 0.72661053 -0.57307207  0.36559529  0.73276474 -0.04825183  0.0858239\n",
      "   1.18322573 -0.04847601 -0.59309847]\n",
      " [-0.69286511  0.58789246 -0.35130847 -0.58730084  0.11028078 -0.08298547\n",
      "  -1.16204689 -0.34639429  0.40366402]\n",
      " [-0.01123591  0.09187368  0.31302225 -0.7390122  -0.03355127 -0.03050242\n",
      "  -0.80393161  0.08400912  0.13731325]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.16378551  0.84951281 -1.56577009 -0.1027465   0.94418883 -1.39066494\n",
      "  -0.37089296]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:34 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.72231529]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 34 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.56884753 -0.43464491  0.25371078  0.75462225 -0.09831041 -0.07778551\n",
      "   0.93096589  0.26396755 -0.21382089]\n",
      " [-0.50754071  0.62234005 -0.49124142 -0.88535141  0.01001414 -0.03096386\n",
      "  -1.23358683 -0.47013407  0.30899387]\n",
      " [ 0.21718171 -0.29387257  0.41153516 -0.66921018  0.29336129 -0.47172392\n",
      "  -0.60977891  0.27463679  0.42558899]\n",
      " [ 0.73178885 -0.57307207  0.37077362  0.73276474 -0.04825183  0.09100222\n",
      "   1.18840406 -0.04847601 -0.59309847]\n",
      " [-0.69819168  0.58789246 -0.35663504 -0.58730084  0.11028078 -0.08831204\n",
      "  -1.16737347 -0.34639429  0.40366402]\n",
      " [-0.01469319  0.09187368  0.30956497 -0.7390122  -0.03355127 -0.0339597\n",
      "  -0.80738889  0.08400912  0.13731325]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.13593708  0.87287799 -1.56309462 -0.0918443   0.96963741 -1.3881023\n",
      "  -0.36059184]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:34 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.40836687]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 35 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.56140344 -0.442089    0.25371078  0.74717816 -0.09831041 -0.07778551\n",
      "   0.93096589  0.26396755 -0.22126499]\n",
      " [-0.5021411   0.62773966 -0.49124142 -0.8799518   0.01001414 -0.03096386\n",
      "  -1.23358683 -0.47013407  0.31439348]\n",
      " [ 0.22103245 -0.29002183  0.41153516 -0.66535944  0.29336129 -0.47172392\n",
      "  -0.60977891  0.27463679  0.42943973]\n",
      " [ 0.72818965 -0.57667128  0.37077362  0.72916554 -0.04825183  0.09100222\n",
      "   1.18840406 -0.04847601 -0.59669768]\n",
      " [-0.6946438   0.59144034 -0.35663504 -0.58375296  0.11028078 -0.08831204\n",
      "  -1.16737347 -0.34639429  0.40721191]\n",
      " [-0.00864967  0.0979172   0.30956497 -0.73296868 -0.03355127 -0.0339597\n",
      "  -0.80738889  0.08400912  0.14335677]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.18526849  0.84018988 -1.58216695 -0.11259308  0.94131886 -1.40916881\n",
      "  -0.37893307]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:35 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.70950732]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 35 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.56801455 -0.442089    0.26032189  0.74717816 -0.09831041 -0.07778551\n",
      "   0.937577    0.26396755 -0.22126499]\n",
      " [-0.50799958  0.62773966 -0.4970999  -0.8799518   0.01001414 -0.03096386\n",
      "  -1.23944531 -0.47013407  0.31439348]\n",
      " [ 0.22120298 -0.29002183  0.41170569 -0.66535944  0.29336129 -0.47172392\n",
      "  -0.60960838  0.27463679  0.42943973]\n",
      " [ 0.73391955 -0.57667128  0.37650351  0.72916554 -0.04825183  0.09100222\n",
      "   1.19413396 -0.04847601 -0.59669768]\n",
      " [-0.70051959  0.59144034 -0.36251083 -0.58375296  0.11028078 -0.08831204\n",
      "  -1.17324926 -0.34639429  0.40721191]\n",
      " [-0.0122071   0.0979172   0.30600754 -0.73296868 -0.03355127 -0.0339597\n",
      "  -0.81094631  0.08400912  0.14335677]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.15533224  0.86567932 -1.57925251 -0.09745441  0.96849574 -1.40623243\n",
      "  -0.36767643]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:35 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.66229215]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 35 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.55173792 -0.45836563  0.24404527  0.74717816 -0.09831041 -0.07778551\n",
      "   0.92130038  0.26396755 -0.22126499]\n",
      " [-0.49144954  0.64428971 -0.48054986 -0.8799518   0.01001414 -0.03096386\n",
      "  -1.22289526 -0.47013407  0.31439348]\n",
      " [ 0.22605483 -0.28516998  0.41655753 -0.66535944  0.29336129 -0.47172392\n",
      "  -0.60475653  0.27463679  0.42943973]\n",
      " [ 0.71752594 -0.59306489  0.3601099   0.72916554 -0.04825183  0.09100222\n",
      "   1.17774035 -0.04847601 -0.59669768]\n",
      " [-0.68399706  0.60796287 -0.3459883  -0.58375296  0.11028078 -0.08831204\n",
      "  -1.15672673 -0.34639429  0.40721191]\n",
      " [-0.00477593  0.10534837  0.31343872 -0.73296868 -0.03355127 -0.0339597\n",
      "  -0.80351514  0.08400912  0.14335677]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.22939679  0.80718186 -1.59152101 -0.12957711  0.90560444 -1.41821664\n",
      "  -0.39705794]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:35 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.52932644]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 35 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 5.63613456e-01 -4.58365629e-01  2.55920798e-01  7.47178156e-01\n",
      "  -9.83104076e-02 -7.77855055e-02  9.21300376e-01  2.75843086e-01\n",
      "  -2.21264988e-01]\n",
      " [-5.04526111e-01  6.44289708e-01 -4.93626431e-01 -8.79951802e-01\n",
      "   1.00141435e-02 -3.09638556e-02 -1.22289526e+00 -4.83210648e-01\n",
      "   3.14393477e-01]\n",
      " [ 2.37025828e-01 -2.85169976e-01  4.27528528e-01 -6.65359436e-01\n",
      "   2.93361290e-01 -4.71723920e-01 -6.04756529e-01  2.85607779e-01\n",
      "   4.29439730e-01]\n",
      " [ 7.29229049e-01 -5.93064890e-01  3.71813016e-01  7.29165536e-01\n",
      "  -4.82518292e-02  9.10022247e-02  1.17774035e+00 -3.67728925e-02\n",
      "  -5.96697675e-01]\n",
      " [-6.96985834e-01  6.07962871e-01 -3.58977075e-01 -5.83752956e-01\n",
      "   1.10280779e-01 -8.83120435e-02 -1.15672673e+00 -3.59383065e-01\n",
      "   4.07211906e-01]\n",
      " [ 7.63544952e-04  1.05348369e-01  3.18978190e-01 -7.32968680e-01\n",
      "  -3.35512662e-02 -3.39597011e-02 -8.03515139e-01  8.95485970e-02\n",
      "   1.43356771e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.170765    0.85072435 -1.58030972 -0.08768579  0.94880159 -1.40639699\n",
      "  -0.36205911]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:35 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.53542997]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 35 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.57547061 -0.45836563  0.2559208   0.75903531 -0.09831041 -0.07778551\n",
      "   0.92130038  0.27584309 -0.20940784]\n",
      " [-0.51628382  0.64428971 -0.49362643 -0.89170951  0.01001414 -0.03096386\n",
      "  -1.22289526 -0.48321065  0.30263577]\n",
      " [ 0.23704181 -0.28516998  0.42752853 -0.66534346  0.29336129 -0.47172392\n",
      "  -0.60475653  0.28560778  0.42945571]\n",
      " [ 0.73962415 -0.59306489  0.37181302  0.73956064 -0.04825183  0.09100222\n",
      "   1.17774035 -0.03677289 -0.58630258]\n",
      " [-0.70747284  0.60796287 -0.35897708 -0.59423997  0.11028078 -0.08831204\n",
      "  -1.15672673 -0.35938307  0.3967249 ]\n",
      " [-0.00704559  0.10534837  0.31897819 -0.74077782 -0.03355127 -0.0339597\n",
      "  -0.80351514  0.0895486   0.13554763]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.11298533  0.89396045 -1.56555354 -0.05877997  0.98942156 -1.38937967\n",
      "  -0.34143759]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:35 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.35906597]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 35 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.57352598 -0.45836563  0.2559208   0.75903531 -0.10025504 -0.07973014\n",
      "   0.92130038  0.27584309 -0.21135247]\n",
      " [-0.51389362  0.64428971 -0.49362643 -0.89170951  0.01240434 -0.02857366\n",
      "  -1.22289526 -0.48321065  0.30502597]\n",
      " [ 0.23228852 -0.28516998  0.42752853 -0.66534346  0.288608   -0.47647721\n",
      "  -0.60475653  0.28560778  0.42470242]\n",
      " [ 0.7376182  -0.59306489  0.37181302  0.73956064 -0.05025778  0.08899628\n",
      "   1.17774035 -0.03677289 -0.58830853]\n",
      " [-0.70455128  0.60796287 -0.35897708 -0.59423997  0.11320234 -0.08539048\n",
      "  -1.15672673 -0.35938307  0.39964646]\n",
      " [-0.007675    0.10534837  0.31897819 -0.74077782 -0.03418068 -0.03458911\n",
      "  -0.80351514  0.0895486   0.13491822]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.15430262  0.87134545 -1.5838     -0.08438293  0.96674409 -1.40707598\n",
      "  -0.36272604]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:35 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.19651057]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 35 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.57457696 -0.45731465  0.2559208   0.75903531 -0.09920406 -0.07867916\n",
      "   0.92130038  0.27584309 -0.21030149]\n",
      " [-0.51545027  0.64273305 -0.49362643 -0.89170951  0.01084769 -0.03013031\n",
      "  -1.22289526 -0.48321065  0.30346932]\n",
      " [ 0.23158107 -0.28587743  0.42752853 -0.66534346  0.28790055 -0.47718466\n",
      "  -0.60475653  0.28560778  0.42399497]\n",
      " [ 0.73912635 -0.59155674  0.37181302  0.73956064 -0.04874963  0.09050443\n",
      "   1.17774035 -0.03677289 -0.58680037]\n",
      " [-0.70580007  0.60671409 -0.35897708 -0.59423997  0.11195356 -0.08663927\n",
      "  -1.15672673 -0.35938307  0.39839767]\n",
      " [-0.00830614  0.10471723  0.31897819 -0.74077782 -0.03481181 -0.03522025\n",
      "  -0.80351514  0.0895486   0.13428709]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.16981655  0.86465288 -1.59315963 -0.09285135  0.96053685 -1.41610464\n",
      "  -0.37111697]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:35 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.58540171]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 35 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.56180595 -0.47008566  0.2559208   0.75903531 -0.11197507 -0.09145017\n",
      "   0.90852936  0.27584309 -0.21030149]\n",
      " [-0.50072092  0.6574624  -0.49362643 -0.89170951  0.02557703 -0.01540096\n",
      "  -1.20816592 -0.48321065  0.30346932]\n",
      " [ 0.24423171 -0.27322679  0.42752853 -0.66534346  0.30055119 -0.46453402\n",
      "  -0.59210589  0.28560778  0.42399497]\n",
      " [ 0.72340827 -0.60727482  0.37181302  0.73956064 -0.06446771  0.07478635\n",
      "   1.16202227 -0.03677289 -0.58680037]\n",
      " [-0.69050455  0.6220096  -0.35897708 -0.59423997  0.12724907 -0.07134375\n",
      "  -1.14143121 -0.35938307  0.39839767]\n",
      " [ 0.0036051   0.11662848  0.31897819 -0.74077782 -0.02290057 -0.023309\n",
      "  -0.7916039   0.0895486   0.13428709]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.24085697  0.81472549 -1.61070373 -0.11414797  0.90392435 -1.43216646\n",
      "  -0.39349026]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:35 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.73291991]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 35 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.56691718 -0.47008566  0.2559208   0.76414654 -0.11197507 -0.09145017\n",
      "   0.9136406   0.27584309 -0.21030149]\n",
      " [-0.50509377  0.6574624  -0.49362643 -0.89608235  0.02557703 -0.01540096\n",
      "  -1.21253876 -0.48321065  0.30346932]\n",
      " [ 0.23905618 -0.27322679  0.42752853 -0.67051898  0.30055119 -0.46453402\n",
      "  -0.59728141  0.28560778  0.42399497]\n",
      " [ 0.72773014 -0.60727482  0.37181302  0.74388251 -0.06446771  0.07478635\n",
      "   1.16634414 -0.03677289 -0.58680037]\n",
      " [-0.69523607  0.6220096  -0.35897708 -0.59897148  0.12724907 -0.07134375\n",
      "  -1.14616273 -0.35938307  0.39839767]\n",
      " [-0.00224651  0.11662848  0.31897819 -0.74662943 -0.02290057 -0.023309\n",
      "  -0.79745551  0.0895486   0.13428709]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.21471673  0.83832637 -1.60889743 -0.10718551  0.92829889 -1.43004392\n",
      "  -0.38883278]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:35 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.65470428]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 35 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.57541588 -0.47008566  0.2559208   0.76414654 -0.10347637 -0.08295147\n",
      "   0.9221393   0.27584309 -0.21030149]\n",
      " [-0.51375278  0.6574624  -0.49362643 -0.89608235  0.01691802 -0.02405998\n",
      "  -1.22119777 -0.48321065  0.30346932]\n",
      " [ 0.23429293 -0.27322679  0.42752853 -0.67051898  0.29578794 -0.46929727\n",
      "  -0.60204466  0.28560778  0.42399497]\n",
      " [ 0.73611532 -0.60727482  0.37181302  0.74388251 -0.05608253  0.08317153\n",
      "   1.17472932 -0.03677289 -0.58680037]\n",
      " [-0.70380749  0.6220096  -0.35897708 -0.59897148  0.11867765 -0.07991517\n",
      "  -1.15473415 -0.35938307  0.39839767]\n",
      " [-0.00918368  0.11662848  0.31897819 -0.74662943 -0.02983774 -0.03024618\n",
      "  -0.80439268  0.0895486   0.13428709]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.17568682  0.86884625 -1.60290658 -0.09265326  0.96227005 -1.42443821\n",
      "  -0.37711244]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:35 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.55551336]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 35 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 5.61611656e-01 -4.70085660e-01  2.55920798e-01  7.50342320e-01\n",
      "  -1.03476372e-01 -9.67556896e-02  9.22139295e-01  2.75843086e-01\n",
      "  -2.24105708e-01]\n",
      " [-4.99448341e-01  6.57462401e-01 -4.93626431e-01 -8.81777909e-01\n",
      "   1.69180202e-02 -9.75553866e-03 -1.22119777e+00 -4.83210648e-01\n",
      "   3.17773758e-01]\n",
      " [ 2.42088435e-01 -2.73226786e-01  4.27528528e-01 -6.62723480e-01\n",
      "   2.95787942e-01 -4.61501764e-01 -6.02044664e-01  2.85607779e-01\n",
      "   4.31790473e-01]\n",
      " [ 7.22806601e-01 -6.07274820e-01  3.71813016e-01  7.30573785e-01\n",
      "  -5.60825264e-02  6.98628056e-02  1.17472932e+00 -3.67728925e-02\n",
      "  -6.00109095e-01]\n",
      " [-6.90438949e-01  6.22009601e-01 -3.58977075e-01 -5.85602943e-01\n",
      "   1.18677651e-01 -6.65466298e-02 -1.15473415e+00 -3.59383065e-01\n",
      "   4.11766215e-01]\n",
      " [ 8.83891718e-04  1.16628477e-01  3.18978190e-01 -7.36561857e-01\n",
      "  -2.98377422e-02 -2.01786056e-02 -8.04392678e-01  8.95485970e-02\n",
      "   1.44354657e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.24427001  0.81809018 -1.61964654 -0.11884459  0.9124521  -1.44309559\n",
      "  -0.40060838]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:35 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.68336304]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 35 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.56924079 -0.46245652  0.2559208   0.75797146 -0.10347637 -0.08912655\n",
      "   0.92976843  0.27584309 -0.22410571]\n",
      " [-0.50672403  0.65018671 -0.49362643 -0.8890536   0.01691802 -0.01703123\n",
      "  -1.22847347 -0.48321065  0.31777376]\n",
      " [ 0.23453464 -0.28078058  0.42752853 -0.67027728  0.29578794 -0.46905556\n",
      "  -0.60959846  0.28560778  0.43179047]\n",
      " [ 0.72981846 -0.60026296  0.37181302  0.73758565 -0.05608253  0.07687467\n",
      "   1.18174118 -0.03677289 -0.6001091 ]\n",
      " [-0.69784336  0.61460519 -0.35897708 -0.59300735  0.11867765 -0.07395104\n",
      "  -1.16213856 -0.35938307  0.41176622]\n",
      " [-0.00675718  0.10898741  0.31897819 -0.74420293 -0.02983774 -0.02781967\n",
      "  -0.81203375  0.0895486   0.14435466]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.21001338  0.84690717 -1.61539735 -0.11380491  0.94294036 -1.43854191\n",
      "  -0.39406586]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:35 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.70235639]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 35 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.57606367 -0.45563365  0.2559208   0.76479433 -0.10347637 -0.08912655\n",
      "   0.9365913   0.27584309 -0.22410571]\n",
      " [-0.5133005   0.64361025 -0.49362643 -0.89563006  0.01691802 -0.01703123\n",
      "  -1.23504993 -0.48321065  0.31777376]\n",
      " [ 0.22769475 -0.28762047  0.42752853 -0.67711717  0.29578794 -0.46905556\n",
      "  -0.61643835  0.28560778  0.43179047]\n",
      " [ 0.73626497 -0.59381645  0.37181302  0.74403216 -0.05608253  0.07687467\n",
      "   1.18818769 -0.03677289 -0.6001091 ]\n",
      " [-0.70461563  0.60783292 -0.35897708 -0.59977962  0.11867765 -0.07395104\n",
      "  -1.16891083 -0.35938307  0.41176622]\n",
      " [-0.0137018   0.10204279  0.31897819 -0.75114755 -0.02983774 -0.02781967\n",
      "  -0.81897837  0.0895486   0.14435466]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.1789019   0.87358468 -1.61160321 -0.10727768  0.97049998 -1.4342721\n",
      "  -0.38817379]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:35 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.72666113]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 35 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.58210284 -0.45563365  0.26195997  0.76479433 -0.10347637 -0.08308737\n",
      "   0.94263048  0.27584309 -0.22410571]\n",
      " [-0.51855162  0.64361025 -0.49887756 -0.89563006  0.01691802 -0.02228236\n",
      "  -1.24030105 -0.48321065  0.31777376]\n",
      " [ 0.22490581 -0.28762047  0.42473959 -0.67711717  0.29578794 -0.4718445\n",
      "  -0.61922729  0.28560778  0.43179047]\n",
      " [ 0.74128836 -0.59381645  0.37683641  0.74403216 -0.05608253  0.08189806\n",
      "   1.19321108 -0.03677289 -0.6001091 ]\n",
      " [-0.70977372  0.60783292 -0.36413516 -0.59977962  0.11867765 -0.07910913\n",
      "  -1.17406892 -0.35938307  0.41176622]\n",
      " [-0.01712004  0.10204279  0.31555994 -0.75114755 -0.02983774 -0.03123792\n",
      "  -0.82239661  0.0895486   0.14435466]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.15175597  0.89646413 -1.60903588 -0.0965805   0.99533189 -1.43181295\n",
      "  -0.37818861]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:35 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.3989927]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 36 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.57494977 -0.46278672  0.26195997  0.75764126 -0.10347637 -0.08308737\n",
      "   0.94263048  0.27584309 -0.23125878]\n",
      " [-0.51340466  0.64875721 -0.49887756 -0.8904831   0.01691802 -0.02228236\n",
      "  -1.24030105 -0.48321065  0.32292072]\n",
      " [ 0.22850386 -0.28402243  0.42473959 -0.67351912  0.29578794 -0.4718445\n",
      "  -0.61922729  0.28560778  0.43538852]\n",
      " [ 0.73787632 -0.59722849  0.37683641  0.74062011 -0.05608253  0.08189806\n",
      "   1.19321108 -0.03677289 -0.60352114]\n",
      " [-0.70637784  0.6112288  -0.36413516 -0.59638374  0.11867765 -0.07910913\n",
      "  -1.17406892 -0.35938307  0.4151621 ]\n",
      " [-0.01128503  0.1078778   0.31555994 -0.74531253 -0.02983774 -0.03123792\n",
      "  -0.82239661  0.0895486   0.15018967]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.1995947   0.86484911 -1.62763059 -0.11684465  0.96795198 -1.45228865\n",
      "  -0.39600447]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:36 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.7149524]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 36 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.58133428 -0.46278672  0.26834448  0.75764126 -0.10347637 -0.08308737\n",
      "   0.94901499  0.27584309 -0.23125878]\n",
      " [-0.51903654  0.64875721 -0.50450944 -0.8904831   0.01691802 -0.02228236\n",
      "  -1.24593294 -0.48321065  0.32292072]\n",
      " [ 0.22875079 -0.28402243  0.42498652 -0.67351912  0.29578794 -0.4718445\n",
      "  -0.61898035  0.28560778  0.43538852]\n",
      " [ 0.74339226 -0.59722849  0.38235235  0.74062011 -0.05608253  0.08189806\n",
      "   1.19872703 -0.03677289 -0.60352114]\n",
      " [-0.71202616  0.6112288  -0.36978349 -0.59638374  0.11867765 -0.07910913\n",
      "  -1.17971725 -0.35938307  0.4151621 ]\n",
      " [-0.01480572  0.1078778   0.31203926 -0.74531253 -0.02983774 -0.03123792\n",
      "  -0.8259173   0.0895486   0.15018967]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.17054899  0.88970178 -1.62486757 -0.10207482  0.99436996 -1.44950555\n",
      "  -0.38516195]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:36 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.66353958]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 36 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.56502309 -0.47909791  0.2520333   0.75764126 -0.10347637 -0.08308737\n",
      "   0.93270381  0.27584309 -0.23125878]\n",
      " [-0.50248879  0.66530496 -0.48796169 -0.8904831   0.01691802 -0.02228236\n",
      "  -1.22938519 -0.48321065  0.32292072]\n",
      " [ 0.23329555 -0.27947766  0.42953129 -0.67351912  0.29578794 -0.4718445\n",
      "  -0.61443559  0.28560778  0.43538852]\n",
      " [ 0.72699637 -0.61362439  0.36595646  0.74062011 -0.05608253  0.08189806\n",
      "   1.18233113 -0.03677289 -0.60352114]\n",
      " [-0.69550892  0.62774604 -0.35326625 -0.59638374  0.11867765 -0.07910913\n",
      "  -1.1632     -0.35938307  0.4151621 ]\n",
      " [-0.00734855  0.11533497  0.31949642 -0.74531253 -0.02983774 -0.03123792\n",
      "  -0.81846014  0.0895486   0.15018967]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.24461819  0.8310526  -1.63709679 -0.13451745  0.9314808  -1.46143578\n",
      "  -0.41451735]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:36 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.532769]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 36 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.57697375 -0.47909791  0.26398395  0.75764126 -0.10347637 -0.08308737\n",
      "   0.93270381  0.28779374 -0.23125878]\n",
      " [-0.51548459  0.66530496 -0.50095749 -0.8904831   0.01691802 -0.02228236\n",
      "  -1.22938519 -0.49620645  0.32292072]\n",
      " [ 0.24439521 -0.27947766  0.44063094 -0.67351912  0.29578794 -0.4718445\n",
      "  -0.61443559  0.29670744  0.43538852]\n",
      " [ 0.73875557 -0.61362439  0.37771566  0.74062011 -0.05608253  0.08189806\n",
      "   1.18233113 -0.02501369 -0.60352114]\n",
      " [-0.70843936  0.62774604 -0.36619669 -0.59638374  0.11867765 -0.07910913\n",
      "  -1.1632     -0.37231351  0.4151621 ]\n",
      " [-0.00173798  0.11533497  0.325107   -0.74531253 -0.02983774 -0.03123792\n",
      "  -0.81846014  0.09515917  0.15018967]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.18646518  0.87460498 -1.62625783 -0.09260074  0.97462801 -1.45000728\n",
      "  -0.37967815]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:36 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.53581845]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 36 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.5888875  -0.47909791  0.26398395  0.76955501 -0.10347637 -0.08308737\n",
      "   0.93270381  0.28779374 -0.21934503]\n",
      " [-0.52729792  0.66530496 -0.50095749 -0.90229643  0.01691802 -0.02228236\n",
      "  -1.22938519 -0.49620645  0.3111074 ]\n",
      " [ 0.24448561 -0.27947766  0.44063094 -0.67342872  0.29578794 -0.4718445\n",
      "  -0.61443559  0.29670744  0.43547892]\n",
      " [ 0.7492505  -0.61362439  0.37771566  0.75111505 -0.05608253  0.08189806\n",
      "   1.18233113 -0.02501369 -0.59302621]\n",
      " [-0.71903886  0.62774604 -0.36619669 -0.60698324  0.11867765 -0.07910913\n",
      "  -1.1632     -0.37231351  0.4045626 ]\n",
      " [-0.00962765  0.11533497  0.325107   -0.7532022  -0.02983774 -0.03123792\n",
      "  -0.81846014  0.09515917  0.1423    ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.12874025  0.91794997 -1.61165748 -0.06364787  1.01537964 -1.43319894\n",
      "  -0.35918221]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:36 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.34627523]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 36 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.58710955 -0.47909791  0.26398395  0.76955501 -0.10525432 -0.08486532\n",
      "   0.93270381  0.28779374 -0.22112298]\n",
      " [-0.52515349  0.66530496 -0.50095749 -0.90229643  0.01906244 -0.02013794\n",
      "  -1.22938519 -0.49620645  0.31325182]\n",
      " [ 0.23984883 -0.27947766  0.44063094 -0.67342872  0.29115116 -0.47648128\n",
      "  -0.61443559  0.29670744  0.43084214]\n",
      " [ 0.74748153 -0.61362439  0.37771566  0.75111505 -0.0578515   0.08012909\n",
      "   1.18233113 -0.02501369 -0.59479518]\n",
      " [-0.71639551  0.62774604 -0.36619669 -0.60698324  0.121321   -0.07646578\n",
      "  -1.1632     -0.37231351  0.40720595]\n",
      " [-0.01032827  0.11533497  0.325107   -0.7532022  -0.03053837 -0.03193854\n",
      "  -0.81846014  0.09515917  0.14159938]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.16793318  0.89656562 -1.62909194 -0.08807985  0.99400441 -1.45011864\n",
      "  -0.3794799 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:36 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.18180402]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 36 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.5881114  -0.47809606  0.26398395  0.76955501 -0.10425247 -0.08386347\n",
      "   0.93270381  0.28779374 -0.22012113]\n",
      " [-0.52660694  0.66385151 -0.50095749 -0.90229643  0.01760899 -0.02159139\n",
      "  -1.22938519 -0.49620645  0.31179837]\n",
      " [ 0.23916018 -0.28016632  0.44063094 -0.67342872  0.29046251 -0.47716994\n",
      "  -0.61443559  0.29670744  0.43015349]\n",
      " [ 0.74889529 -0.61221063  0.37771566  0.75111505 -0.05643774  0.08154285\n",
      "   1.18233113 -0.02501369 -0.59338142]\n",
      " [-0.71758432  0.62655723 -0.36619669 -0.60698324  0.12013219 -0.07765459\n",
      "  -1.1632     -0.37231351  0.40601714]\n",
      " [-0.01094546  0.11471777  0.325107   -0.7532022  -0.03115556 -0.03255574\n",
      "  -0.81846014  0.09515917  0.14098219]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.18145498  0.89082199 -1.63735635 -0.09553428  0.98870305 -1.45809468\n",
      "  -0.38686149]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:36 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.58239612]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 36 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 5.75438344e-01 -4.90769112e-01  2.63983952e-01  7.69555005e-01\n",
      "  -1.16925523e-01 -9.65365257e-02  9.20030750e-01  2.87793738e-01\n",
      "  -2.20121127e-01]\n",
      " [-5.12033168e-01  6.78425285e-01 -5.00957490e-01 -9.02296425e-01\n",
      "   3.21827683e-02 -7.01761025e-03 -1.21481141e+00 -4.96206447e-01\n",
      "   3.11798368e-01]\n",
      " [ 2.51710991e-01 -2.67615508e-01  4.40630943e-01 -6.73428716e-01\n",
      "   3.03013319e-01 -4.64619124e-01 -6.01884782e-01  2.96707437e-01\n",
      "   4.30153489e-01]\n",
      " [ 7.33278402e-01 -6.27827521e-01  3.77715658e-01  7.51115046e-01\n",
      "  -7.20546291e-02  6.59259555e-02  1.16671424e+00 -2.50136905e-02\n",
      "  -5.93381418e-01]\n",
      " [-7.02413967e-01  6.41727585e-01 -3.66196690e-01 -6.06983240e-01\n",
      "   1.35302545e-01 -6.24842354e-02 -1.14802965e+00 -3.72313508e-01\n",
      "   4.06017143e-01]\n",
      " [ 9.42985658e-04  1.26606221e-01  3.25106997e-01 -7.53202203e-01\n",
      "  -1.92671099e-02 -2.06672892e-02 -8.06571687e-01  9.51591719e-02\n",
      "   1.40982186e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.25227751  0.84113773 -1.65509506 -0.11685769  0.93252897 -1.47434078\n",
      "  -0.40914686]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:36 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.74098739]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 36 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.58023574 -0.49076911  0.26398395  0.7743524  -0.11692552 -0.09653653\n",
      "   0.92482815  0.28779374 -0.22012113]\n",
      " [-0.51613438  0.67842529 -0.50095749 -0.90639763  0.03218277 -0.00701761\n",
      "  -1.21891262 -0.49620645  0.31179837]\n",
      " [ 0.24676356 -0.26761551  0.44063094 -0.67837614  0.30301332 -0.46461912\n",
      "  -0.60683221  0.29670744  0.43015349]\n",
      " [ 0.73733588 -0.62782752  0.37771566  0.75517252 -0.07205463  0.06592596\n",
      "   1.17077172 -0.02501369 -0.59338142]\n",
      " [-0.70685284  0.64172759 -0.36619669 -0.61142211  0.13530254 -0.06248424\n",
      "  -1.15246853 -0.37231351  0.40601714]\n",
      " [-0.00462099  0.12660622  0.325107   -0.75876618 -0.01926711 -0.02066729\n",
      "  -0.81213566  0.09515917  0.14098219]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Theta two: \n",
      "[[-0.22742201  0.86365529 -1.65342263 -0.11028773  0.95574598 -1.47237975\n",
      "  -0.40482661]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:36 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.65821871]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 36 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.58863276 -0.49076911  0.26398395  0.7743524  -0.1085285  -0.08813951\n",
      "   0.93322517  0.28779374 -0.22012113]\n",
      " [-0.52466133  0.67842529 -0.50095749 -0.90639763  0.02365582 -0.01554456\n",
      "  -1.22743957 -0.49620645  0.31179837]\n",
      " [ 0.24207588 -0.26761551  0.44063094 -0.67837614  0.29832564 -0.4693068\n",
      "  -0.61151989  0.29670744  0.43015349]\n",
      " [ 0.74559963 -0.62782752  0.37771566  0.75517252 -0.06379088  0.07418971\n",
      "   1.17903547 -0.02501369 -0.59338142]\n",
      " [-0.71529444  0.64172759 -0.36619669 -0.61142211  0.12686094 -0.07092584\n",
      "  -1.16091013 -0.37231351  0.40601714]\n",
      " [-0.01151133  0.12660622  0.325107   -0.75876618 -0.02615745 -0.02755763\n",
      "  -0.819026    0.09515917  0.14098219]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.18897728  0.89381204 -1.6475337  -0.09596857  0.98919733 -1.46686286\n",
      "  -0.39336889]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:36 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.55593572]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 36 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 5.74765923e-01 -4.90769112e-01  2.63983952e-01  7.60485563e-01\n",
      "  -1.08528502e-01 -1.02006343e-01  9.33225167e-01  2.87793738e-01\n",
      "  -2.33987964e-01]\n",
      " [-5.10325513e-01  6.78425285e-01 -5.00957490e-01 -8.92061818e-01\n",
      "   2.36558158e-02 -1.20874822e-03 -1.22743957e+00 -4.96206447e-01\n",
      "   3.26134183e-01]\n",
      " [ 2.49788419e-01 -2.67615508e-01  4.40630943e-01 -6.70663610e-01\n",
      "   2.98325641e-01 -4.61594267e-01 -6.11519889e-01  2.96707437e-01\n",
      "   4.37866024e-01]\n",
      " [ 7.32243874e-01 -6.27827521e-01  3.77715658e-01  7.41816768e-01\n",
      "  -6.37908787e-02  6.08339512e-02  1.17903547e+00 -2.50136905e-02\n",
      "  -6.06737173e-01]\n",
      " [-7.01863784e-01  6.41727585e-01 -3.66196690e-01 -5.97991455e-01\n",
      "   1.26860943e-01 -5.74951810e-02 -1.16091013e+00 -3.72313508e-01\n",
      "   4.19447799e-01]\n",
      " [-1.37578155e-03  1.26606221e-01  3.25106997e-01 -7.48630632e-01\n",
      "  -2.61574477e-02 -1.74220814e-02 -8.19026000e-01  9.51591719e-02\n",
      "   1.51117731e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.25759954  0.8429162  -1.66422781 -0.12227329  0.93928001 -1.48543144\n",
      "  -0.41679975]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:36 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.68727367]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 36 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.58224497 -0.48329006  0.26398395  0.76796461 -0.1085285  -0.09452729\n",
      "   0.94070422  0.28779374 -0.23398796]\n",
      " [-0.51746682  0.67128398 -0.50095749 -0.89920313  0.02365582 -0.00835006\n",
      "  -1.23458088 -0.49620645  0.32613418]\n",
      " [ 0.24238218 -0.27502175  0.44063094 -0.67806985  0.29832564 -0.46900051\n",
      "  -0.61892613  0.29670744  0.43786602]\n",
      " [ 0.73913226 -0.62093914  0.37771566  0.74870515 -0.06379088  0.06772233\n",
      "   1.18592385 -0.02501369 -0.60673717]\n",
      " [-0.70912598  0.63446539 -0.36619669 -0.60525365  0.12686094 -0.06475737\n",
      "  -1.16817232 -0.37231351  0.4194478 ]\n",
      " [-0.00888024  0.11910176  0.325107   -0.75613509 -0.02615745 -0.02492654\n",
      "  -0.82653046  0.09515917  0.15111773]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.22399262  0.87122446 -1.66005151 -0.11734687  0.96917487 -1.48096886\n",
      "  -0.41046516]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:36 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.7066726]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 36 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.58889805 -0.47663699  0.26398395  0.77461769 -0.1085285  -0.09452729\n",
      "   0.94735729  0.28779374 -0.23398796]\n",
      " [-0.52388367  0.66486713 -0.50095749 -0.90561998  0.02365582 -0.00835006\n",
      "  -1.24099773 -0.49620645  0.32613418]\n",
      " [ 0.23569436 -0.28170957  0.44063094 -0.68475767  0.29832564 -0.46900051\n",
      "  -0.62561395  0.29670744  0.43786602]\n",
      " [ 0.74542458 -0.61464682  0.37771566  0.75499747 -0.06379088  0.06772233\n",
      "   1.19221617 -0.02501369 -0.60673717]\n",
      " [-0.71573187  0.6278595  -0.36619669 -0.61185954  0.12686094 -0.06475737\n",
      "  -1.17477821 -0.37231351  0.4194478 ]\n",
      " [-0.01567379  0.11230821  0.325107   -0.76292864 -0.02615745 -0.02492654\n",
      "  -0.83332401  0.09515917  0.15111773]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.19359123  0.89734152 -1.65636314 -0.1109863   0.99611758 -1.47683129\n",
      "  -0.40479313]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:36 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.73086938]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 36 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.59477142 -0.47663699  0.26985733  0.77461769 -0.1085285  -0.08865392\n",
      "   0.95323067  0.28779374 -0.23398796]\n",
      " [-0.52897508  0.66486713 -0.5060489  -0.90561998  0.02365582 -0.01344147\n",
      "  -1.24608914 -0.49620645  0.32613418]\n",
      " [ 0.23304407 -0.28170957  0.43798066 -0.68475767  0.29832564 -0.47165079\n",
      "  -0.62826424  0.29670744  0.43786602]\n",
      " [ 0.75030278 -0.61464682  0.38259386  0.75499747 -0.06379088  0.07260053\n",
      "   1.19709437 -0.02501369 -0.60673717]\n",
      " [-0.72073212  0.6278595  -0.37119694 -0.61185954  0.12686094 -0.06975762\n",
      "  -1.17977846 -0.37231351  0.4194478 ]\n",
      " [-0.01904514  0.11230821  0.32173565 -0.76292864 -0.02615745 -0.02829789\n",
      "  -0.83669536  0.09515917  0.15111773]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.16712232  0.91974161 -1.65389404 -0.1004801   1.0203506  -1.47446601\n",
      "  -0.39510183]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:36 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.38971923]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 37 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.58791048 -0.48349793  0.26985733  0.76775674 -0.1085285  -0.08865392\n",
      "   0.95323067  0.28779374 -0.24084891]\n",
      " [-0.52408026  0.66976195 -0.5060489  -0.90072515  0.02365582 -0.01344147\n",
      "  -1.24608914 -0.49620645  0.33102901]\n",
      " [ 0.23639477 -0.27835887  0.43798066 -0.68140698  0.29832564 -0.47165079\n",
      "  -0.62826424  0.29670744  0.44121672]\n",
      " [ 0.74707866 -0.61787093  0.38259386  0.75177335 -0.06379088  0.07260053\n",
      "   1.19709437 -0.02501369 -0.60996129]\n",
      " [-0.71749309  0.63109853 -0.37119694 -0.60862051  0.12686094 -0.06975762\n",
      "  -1.17977846 -0.37231351  0.42268683]\n",
      " [-0.01342355  0.1179298   0.32173565 -0.75730706 -0.02615745 -0.02829789\n",
      "  -0.83669536  0.09515917  0.15673932]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.21346737  0.88920102 -1.67200962 -0.12025293  0.99391046 -1.49435539\n",
      "  -0.41239743]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:37 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.72031647]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 37 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.59407235 -0.48349793  0.2760192   0.76775674 -0.1085285  -0.08865392\n",
      "   0.95939254  0.28779374 -0.24084891]\n",
      " [-0.52949527  0.66976195 -0.51146391 -0.90072515  0.02365582 -0.01344147\n",
      "  -1.25150415 -0.49620645  0.33102901]\n",
      " [ 0.23671936 -0.27835887  0.43830525 -0.68140698  0.29832564 -0.47165079\n",
      "  -0.62793964  0.29670744  0.44121672]\n",
      " [ 0.75238972 -0.61787093  0.38790492  0.75177335 -0.06379088  0.07260053\n",
      "   1.20240543 -0.02501369 -0.60996129]\n",
      " [-0.72292378  0.63109853 -0.37662763 -0.60862051  0.12686094 -0.06975762\n",
      "  -1.18520915 -0.37231351  0.42268683]\n",
      " [-0.01689692  0.1179298   0.31826229 -0.75730706 -0.02615745 -0.02829789\n",
      "  -0.84016872  0.09515917  0.15673932]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.18529476  0.91341489 -1.66938642 -0.10584192  1.01957768 -1.4917137\n",
      "  -0.40194838]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:37 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.66462133]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 37 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.57773415 -0.49983613  0.259681    0.76775674 -0.1085285  -0.08865392\n",
      "   0.94305434  0.28779374 -0.24084891]\n",
      " [-0.51294865  0.68630857 -0.49491729 -0.90072515  0.02365582 -0.01344147\n",
      "  -1.23495753 -0.49620645  0.33102901]\n",
      " [ 0.24094533 -0.27413291  0.44253122 -0.68140698  0.29832564 -0.47165079\n",
      "  -0.62371367  0.29670744  0.44121672]\n",
      " [ 0.73598852 -0.63427214  0.37150372  0.75177335 -0.06379088  0.07260053\n",
      "   1.18600423 -0.02501369 -0.60996129]\n",
      " [-0.70640997  0.64761234 -0.36011382 -0.60862051  0.12686094 -0.06975762\n",
      "  -1.16869534 -0.37231351  0.42268683]\n",
      " [-0.00943838  0.12538834  0.32572083 -0.75730706 -0.02615745 -0.02829789\n",
      "  -0.83271018  0.09515917  0.15673932]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.25936674  0.8546408  -1.6815959  -0.13861417  0.9567091  -1.50361074\n",
      "  -0.43130368]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:37 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.53649847]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 37 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.58973032 -0.49983613  0.27167717  0.76775674 -0.1085285  -0.08865392\n",
      "   0.94305434  0.29978991 -0.24084891]\n",
      " [-0.52584296  0.68630857 -0.5078116  -0.90072515  0.02365582 -0.01344147\n",
      "  -1.23495753 -0.50910076  0.33102901]\n",
      " [ 0.25215265 -0.27413291  0.45373854 -0.68140698  0.29832564 -0.47165079\n",
      "  -0.62371367  0.30791475  0.44121672]\n",
      " [ 0.74777933 -0.63427214  0.38329452  0.75177335 -0.06379088  0.07260053\n",
      "   1.18600423 -0.01322288 -0.60996129]\n",
      " [-0.71925962  0.64761234 -0.37296347 -0.60862051  0.12686094 -0.06975762\n",
      "  -1.16869534 -0.38516316  0.42268683]\n",
      " [-0.00375459  0.12538834  0.33140461 -0.75730706 -0.02615745 -0.02829789\n",
      "  -0.83271018  0.10084295  0.15673932]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.20173778  0.898148   -1.67111786 -0.09670957  0.99975579 -1.4925618\n",
      "  -0.39664368]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:37 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.53647259]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 37 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.60168785 -0.49983613  0.27167717  0.77971427 -0.1085285  -0.08865392\n",
      "   0.94305434  0.29978991 -0.22889138]\n",
      " [-0.53769966  0.68630857 -0.5078116  -0.91258186  0.02365582 -0.01344147\n",
      "  -1.23495753 -0.50910076  0.3191723 ]\n",
      " [ 0.252325   -0.27413291  0.45373854 -0.68123463  0.29832564 -0.47165079\n",
      "  -0.62371367  0.30791475  0.44138907]\n",
      " [ 0.75836135 -0.63427214  0.38329452  0.76235538 -0.06379088  0.07260053\n",
      "   1.18600423 -0.01322288 -0.59937926]\n",
      " [-0.72995677  0.64761234 -0.37296347 -0.61931766  0.12686094 -0.06975762\n",
      "  -1.16869534 -0.38516316  0.41198967]\n",
      " [-0.01171279  0.12538834  0.33140461 -0.76526525 -0.02615745 -0.02829789\n",
      "  -0.83271018  0.10084295  0.14878113]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.14410515  0.94156637 -1.65667647 -0.0677209   1.04060613 -1.4759645\n",
      "  -0.37627888]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:37 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.33375257]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 37 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.60007122 -0.49983613  0.27167717  0.77971427 -0.11014513 -0.09027054\n",
      "   0.94305434  0.29978991 -0.230508  ]\n",
      " [-0.53578802  0.68630857 -0.5078116  -0.91258186  0.02556747 -0.01152982\n",
      "  -1.23495753 -0.50910076  0.32108395]\n",
      " [ 0.24781011 -0.27413291  0.45373854 -0.68123463  0.29381076 -0.47616568\n",
      "  -0.62371367  0.30791475  0.43687419]\n",
      " [ 0.7568157  -0.63427214  0.38329452  0.76235538 -0.06533653  0.07105488\n",
      "   1.18600423 -0.01322288 -0.60092492]\n",
      " [-0.72757752  0.64761234 -0.37296347 -0.61931766  0.1292402  -0.06737837\n",
      "  -1.16869534 -0.38516316  0.41436893]\n",
      " [-0.01247786  0.12538834  0.33140461 -0.76526525 -0.02692252 -0.02906296\n",
      "  -0.83271018  0.10084295  0.14801605]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.18121206  0.92138797 -1.67330442 -0.09099579  1.02049976 -1.49211162\n",
      "  -0.39559828]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:37 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.16788544]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 37 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.60101468 -0.49889268  0.27167717  0.77971427 -0.10920168 -0.08932709\n",
      "   0.94305434  0.29978991 -0.22956455]\n",
      " [-0.53713102  0.68496557 -0.5078116  -0.91258186  0.02422446 -0.01287282\n",
      "  -1.23495753 -0.50910076  0.31974095]\n",
      " [ 0.24714975 -0.27479327  0.45373854 -0.68123463  0.29315039 -0.47682605\n",
      "  -0.62371367  0.30791475  0.43621382]\n",
      " [ 0.7581268  -0.63296103  0.38329452  0.76235538 -0.06402543  0.07236598\n",
      "   1.18600423 -0.01322288 -0.59961381]\n",
      " [-0.72869481  0.64649505 -0.37296347 -0.61931766  0.12812291 -0.06849566\n",
      "  -1.16869534 -0.38516316  0.41325164]\n",
      " [-0.01307242  0.12479378  0.33140461 -0.76526525 -0.02751708 -0.02965752\n",
      "  -0.83271018  0.10084295  0.14742149]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.19293886  0.91648531 -1.68056424 -0.0975253   1.01599683 -1.49912178\n",
      "  -0.40206041]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:37 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57869822]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 37 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.58846876 -0.5114386   0.27167717  0.77971427 -0.1217476  -0.10187301\n",
      "   0.93050842  0.29978991 -0.22956455]\n",
      " [-0.52273952  0.69935707 -0.5078116  -0.91258186  0.03861596  0.00151868\n",
      "  -1.22056603 -0.50910076  0.31974095]\n",
      " [ 0.25958078 -0.26236224  0.45373854 -0.68123463  0.30558143 -0.46439501\n",
      "  -0.61128263  0.30791475  0.43621382]\n",
      " [ 0.74263547 -0.64845236  0.38329452  0.76235538 -0.07951676  0.05687465\n",
      "   1.1705129  -0.01322288 -0.59961381]\n",
      " [-0.7136748   0.66151505 -0.37296347 -0.61931766  0.14314291 -0.05347565\n",
      "  -1.15367533 -0.38516316  0.41325164]\n",
      " [-0.00123307  0.13663312  0.33140461 -0.76526525 -0.01567773 -0.01781818\n",
      "  -0.82087084  0.10084295  0.14742149]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.26348408  0.8671133  -1.69850439 -0.11887101  0.96032958 -1.5155611\n",
      "  -0.42426211]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:37 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.74873683]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 37 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.59297314 -0.5114386   0.27167717  0.78421866 -0.1217476  -0.10187301\n",
      "   0.9350128   0.29978991 -0.22956455]\n",
      " [-0.52658871  0.69935707 -0.5078116  -0.91643105  0.03861596  0.00151868\n",
      "  -1.22441523 -0.50910076  0.31974095]\n",
      " [ 0.2548543  -0.26236224  0.45373854 -0.68596111  0.30558143 -0.46439501\n",
      "  -0.61600912  0.30791475  0.43621382]\n",
      " [ 0.74644749 -0.64845236  0.38329452  0.7661674  -0.07951676  0.05687465\n",
      "   1.17432491 -0.01322288 -0.59961381]\n",
      " [-0.71784179  0.66151505 -0.37296347 -0.62348465  0.14314291 -0.05347565\n",
      "  -1.15784232 -0.38516316  0.41325164]\n",
      " [-0.00652072  0.13663312  0.33140461 -0.77055289 -0.01567773 -0.01781818\n",
      "  -0.82615848  0.10084295  0.14742149]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.23984901  0.8885921  -1.69695329 -0.11266645  0.98244174 -1.51374597\n",
      "  -0.42024995]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:37 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.66151187]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 37 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.60127143 -0.5114386   0.27167717  0.78421866 -0.11344931 -0.09357472\n",
      "   0.94331109  0.29978991 -0.22956455]\n",
      " [-0.53499307  0.69935707 -0.5078116  -0.91643105  0.0302116  -0.00688568\n",
      "  -1.23281959 -0.50910076  0.31974095]\n",
      " [ 0.25024663 -0.26236224  0.45373854 -0.68596111  0.30097375 -0.46900268\n",
      "  -0.62061679  0.30791475  0.43621382]\n",
      " [ 0.7546     -0.64845236  0.38329452  0.7661674  -0.07136424  0.06502717\n",
      "   1.18247743 -0.01322288 -0.59961381]\n",
      " [-0.72616354  0.66151505 -0.37296347 -0.62348465  0.13482116 -0.06179741\n",
      "  -1.16616407 -0.38516316  0.41325164]\n",
      " [-0.01336153  0.13663312  0.33140461 -0.77055289 -0.02251855 -0.02465899\n",
      "  -0.8329993   0.10084295  0.14742149]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.20195291  0.91839999 -1.69115325 -0.09853652  1.01539921 -1.50830554\n",
      "  -0.40903078]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:37 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.55637606]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 37 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.58734764 -0.5114386   0.27167717  0.77029487 -0.11344931 -0.10749851\n",
      "   0.94331109  0.29978991 -0.24348834]\n",
      " [-0.52062903  0.69935707 -0.5078116  -0.90206701  0.0302116   0.00747837\n",
      "  -1.23281959 -0.50910076  0.33410499]\n",
      " [ 0.25786317 -0.26236224  0.45373854 -0.67834456  0.30097375 -0.46138614\n",
      "  -0.62061679  0.30791475  0.44383036]\n",
      " [ 0.74120184 -0.64845236  0.38329452  0.75276923 -0.07136424  0.05162901\n",
      "   1.18247743 -0.01322288 -0.61301198]\n",
      " [-0.71267674  0.66151505 -0.37296347 -0.60999785  0.13482116 -0.04831061\n",
      "  -1.16616407 -0.38516316  0.42673843]\n",
      " [-0.00316756  0.13663312  0.33140461 -0.76035893 -0.02251855 -0.01446502\n",
      "  -0.8329993   0.10084295  0.15761546]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.27061576  0.86737391 -1.70780971 -0.1249697   0.96538979 -1.52679621\n",
      "  -0.43240917]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:37 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.69087123]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 37 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 5.94689688e-01 -5.04096546e-01  2.71677171e-01  7.77636918e-01\n",
      "  -1.13449311e-01 -1.00156461e-01  9.50653141e-01  2.99789906e-01\n",
      "  -2.43488342e-01]\n",
      " [-5.27650075e-01  6.92336023e-01 -5.07811600e-01 -9.09088055e-01\n",
      "   3.02116003e-02  4.57317650e-04 -1.23984064e+00 -5.09100757e-01\n",
      "   3.34104994e-01]\n",
      " [ 2.50591773e-01 -2.69633637e-01  4.53738537e-01 -6.85615962e-01\n",
      "   3.00973754e-01 -4.68657537e-01 -6.27888189e-01  3.07914754e-01\n",
      "   4.43830361e-01]\n",
      " [ 7.47980969e-01 -6.41673232e-01  3.83294523e-01  7.59548360e-01\n",
      "  -7.13642406e-02  5.84081367e-02  1.18925656e+00 -1.32228828e-02\n",
      "  -6.13011980e-01]\n",
      " [-7.19811247e-01  6.54380549e-01 -3.72963468e-01 -6.17132355e-01\n",
      "   1.34821158e-01 -5.54451136e-02 -1.17329858e+00 -3.85163156e-01\n",
      "   4.26738433e-01]\n",
      " [-1.05447744e-02  1.29255912e-01  3.31404609e-01 -7.67736137e-01\n",
      "  -2.25185485e-02 -2.18422343e-02 -8.40376506e-01  1.00842955e-01\n",
      "   1.57615461e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.23760573  0.89520703 -1.70369344 -0.12014415  0.99473264 -1.52240947\n",
      "  -0.42626002]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:37 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.71078673]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 37 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 6.01182798e-01 -4.97603435e-01  2.71677171e-01  7.84130028e-01\n",
      "  -1.13449311e-01 -1.00156461e-01  9.57146252e-01  2.99789906e-01\n",
      "  -2.43488342e-01]\n",
      " [-5.33917711e-01  6.86068387e-01 -5.07811600e-01 -9.15355691e-01\n",
      "   3.02116003e-02  4.57317650e-04 -1.24610827e+00 -5.09100757e-01\n",
      "   3.34104994e-01]\n",
      " [ 2.44049170e-01 -2.76176240e-01  4.53738537e-01 -6.92158566e-01\n",
      "   3.00973754e-01 -4.68657537e-01 -6.34430792e-01  3.07914754e-01\n",
      "   4.43830361e-01]\n",
      " [ 7.54129604e-01 -6.35524597e-01  3.83294523e-01  7.65696995e-01\n",
      "  -7.13642406e-02  5.84081367e-02  1.19540520e+00 -1.32228828e-02\n",
      "  -6.13011980e-01]\n",
      " [-7.26261046e-01  6.47930751e-01 -3.72963468e-01 -6.23582154e-01\n",
      "   1.34821158e-01 -5.54451136e-02 -1.17974837e+00 -3.85163156e-01\n",
      "   4.26738433e-01]\n",
      " [-1.71927082e-02  1.22607978e-01  3.31404609e-01 -7.74384070e-01\n",
      "  -2.25185485e-02 -2.18422343e-02 -8.47024440e-01  1.00842955e-01\n",
      "   1.57615461e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.20787909  0.92078478 -1.70010046 -0.11393905  1.02108421 -1.51839084\n",
      "  -0.42078998]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:37 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.73493899]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 37 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.60689594 -0.49760344  0.27739031  0.78413003 -0.11344931 -0.09444332\n",
      "   0.96285939  0.29978991 -0.24348834]\n",
      " [-0.53885923  0.68606839 -0.51275312 -0.91535569  0.0302116  -0.0044842\n",
      "  -1.25104979 -0.50910076  0.33410499]\n",
      " [ 0.24153777 -0.27617624  0.45122714 -0.69215857  0.30097375 -0.47116894\n",
      "  -0.63694219  0.30791475  0.44383036]\n",
      " [ 0.75887174 -0.6355246   0.38803666  0.76569699 -0.07136424  0.06315027\n",
      "   1.20014733 -0.01322288 -0.61301198]\n",
      " [-0.73111347  0.64793075 -0.37781589 -0.62358215  0.13482116 -0.06029754\n",
      "  -1.1846008  -0.38516316  0.42673843]\n",
      " [-0.02051084  0.12260798  0.32808648 -0.77438407 -0.02251855 -0.02516037\n",
      "  -0.85034257  0.10084295  0.15761546]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.18206166  0.94271393 -1.69772078 -0.10361106  1.04473702 -1.51611085\n",
      "  -0.41137216]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:37 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.3805543]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 38 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.60032635 -0.50417302  0.27739031  0.77756044 -0.11344931 -0.09444332\n",
      "   0.96285939  0.29978991 -0.25005793]\n",
      " [-0.53421427  0.69071335 -0.51275312 -0.91071073  0.0302116  -0.0044842\n",
      "  -1.25104979 -0.50910076  0.33874995]\n",
      " [ 0.24464818 -0.27306583  0.45122714 -0.68904816  0.30097375 -0.47116894\n",
      "  -0.63694219  0.30791475  0.44694077]\n",
      " [ 0.75583464 -0.6385617   0.38803666  0.76265989 -0.07136424  0.06315027\n",
      "   1.20014733 -0.01322288 -0.61604908]\n",
      " [-0.72803412  0.6510101  -0.37781589 -0.6205028   0.13482116 -0.06029754\n",
      "  -1.1846008  -0.38516316  0.42981779]\n",
      " [-0.01510543  0.12801339  0.32808648 -0.76897866 -0.02251855 -0.02516037\n",
      "  -0.85034257  0.10084295  0.16302087]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.22691621  0.91324468 -1.71535587 -0.12288626  1.01923393 -1.53541837\n",
      "  -0.42815221]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:38 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.72558957]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 38 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.60627068 -0.50417302  0.28333463  0.77756044 -0.11344931 -0.09444332\n",
      "   0.96880372  0.29978991 -0.25005793]\n",
      " [-0.53942205  0.69071335 -0.5179609  -0.91071073  0.0302116  -0.0044842\n",
      "  -1.25625757 -0.50910076  0.33874995]\n",
      " [ 0.24505032 -0.27306583  0.45162928 -0.68904816  0.30097375 -0.47116894\n",
      "  -0.63654004  0.30791475  0.44694077]\n",
      " [ 0.76094974 -0.6385617   0.39315176  0.76265989 -0.07136424  0.06315027\n",
      "   1.20526244 -0.01322288 -0.61604908]\n",
      " [-0.73325693  0.6510101  -0.3830387  -0.6205028   0.13482116 -0.06029754\n",
      "  -1.18982361 -0.38516316  0.42981779]\n",
      " [-0.01852276  0.12801339  0.32466914 -0.76897866 -0.02251855 -0.02516037\n",
      "  -0.85375991  0.10084295  0.16302087]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.19959737  0.9368213  -1.71286201 -0.10882445  1.04416121 -1.5329073\n",
      "  -0.41807698]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:38 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.66553363]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 38 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.58991161 -0.52053209  0.26697556  0.77756044 -0.11344931 -0.09444332\n",
      "   0.95244465  0.29978991 -0.25005793]\n",
      " [-0.52287531  0.70726009 -0.50141415 -0.91071073  0.0302116  -0.0044842\n",
      "  -1.23971083 -0.50910076  0.33874995]\n",
      " [ 0.24894901 -0.26916715  0.45552797 -0.68904816  0.30097375 -0.47116894\n",
      "  -0.63264136  0.30791475  0.44694077]\n",
      " [ 0.74454052 -0.65497092  0.37674254  0.76265989 -0.07136424  0.06315027\n",
      "   1.18885322 -0.01322288 -0.61604908]\n",
      " [-0.71674458  0.66752245 -0.36652635 -0.6205028   0.13482116 -0.06029754\n",
      "  -1.17331126 -0.38516316  0.42981779]\n",
      " [-0.01108468  0.13545147  0.33210722 -0.76897866 -0.02251855 -0.02516037\n",
      "  -0.84632183  0.10084295  0.16302087]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.2736708   0.87794616 -1.72506943 -0.14193296  0.98132975 -1.54479014\n",
      "  -0.44745542]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:38 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.54047376]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 38 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.6019259  -0.52053209  0.27898986  0.77756044 -0.11344931 -0.09444332\n",
      "   0.95244465  0.3118042  -0.25005793]\n",
      " [-0.53564995  0.70726009 -0.5141888  -0.91071073  0.0302116  -0.0044842\n",
      "  -1.23971083 -0.5218754   0.33874995]\n",
      " [ 0.26024238 -0.26916715  0.46682135 -0.68904816  0.30097375 -0.47116894\n",
      "  -0.63264136  0.31920813  0.44694077]\n",
      " [ 0.75634022 -0.65497092  0.38854224  0.76265989 -0.07136424  0.06315027\n",
      "   1.18885322 -0.00142318 -0.61604908]\n",
      " [-0.72949354  0.66752245 -0.37927532 -0.6205028   0.13482116 -0.06029754\n",
      "  -1.17331126 -0.39791212  0.42981779]\n",
      " [-0.00532633  0.13545147  0.33786557 -0.76897866 -0.02251855 -0.02516037\n",
      "  -0.84632183  0.1066013   0.16302087]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.2166064   0.92135667 -1.7149406  -0.10007691  1.02422889 -1.53410878\n",
      "  -0.41299254]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:38 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.53737305]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 38 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.61391534 -0.52053209  0.27898986  0.78954989 -0.11344931 -0.09444332\n",
      "   0.95244465  0.3118042  -0.23806848]\n",
      " [-0.54753868  0.70726009 -0.5141888  -0.92259946  0.0302116  -0.0044842\n",
      "  -1.23971083 -0.5218754   0.32686122]\n",
      " [ 0.26050308 -0.26916715  0.46682135 -0.68878747  0.30097375 -0.47116894\n",
      "  -0.63264136  0.31920813  0.44720146]\n",
      " [ 0.76699741 -0.65497092  0.38854224  0.77331709 -0.07136424  0.06315027\n",
      "   1.18885322 -0.00142318 -0.60539189]\n",
      " [-0.74027454  0.66752245 -0.37927532 -0.63128379  0.13482116 -0.06029754\n",
      "  -1.17331126 -0.39791212  0.41903679]\n",
      " [-0.01334195  0.13545147  0.33786557 -0.77699427 -0.02251855 -0.02516037\n",
      "  -0.84632183  0.1066013   0.15500526]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.15910112  0.96481533 -1.70066108 -0.07106356  1.0651471  -1.51772429\n",
      "  -0.39276412]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:38 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.32151287]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 38 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.61245323 -0.52053209  0.27898986  0.78954989 -0.11491142 -0.09590543\n",
      "   0.95244465  0.3118042  -0.23953059]\n",
      " [-0.5458457   0.70726009 -0.5141888  -0.92259946  0.03190458 -0.00279122\n",
      "  -1.23971083 -0.5218754   0.3285542 ]\n",
      " [ 0.25611539 -0.26916715  0.46682135 -0.68878747  0.29658606 -0.47555663\n",
      "  -0.63264136  0.31920813  0.44281377]\n",
      " [ 0.76566052 -0.65497092  0.38854224  0.77331709 -0.07270114  0.06181338\n",
      "   1.18885322 -0.00142318 -0.60672878]\n",
      " [-0.73814419  0.66752245 -0.37927532 -0.63128379  0.13695151 -0.05816719\n",
      "  -1.17331126 -0.39791212  0.42116714]\n",
      " [-0.01416409  0.13545147  0.33786557 -0.77699427 -0.02334069 -0.0259825\n",
      "  -0.84632183  0.1066013   0.15418312]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.19416891  0.94581244 -1.71649125 -0.0931995   1.04627106 -1.53310615\n",
      "  -0.41112136]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:38 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.15476898]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 38 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.61333181 -0.51965352  0.27898986  0.78954989 -0.11403285 -0.09502686\n",
      "   0.95244465  0.3118042  -0.23865202]\n",
      " [-0.54707472  0.70603108 -0.5141888  -0.92259946  0.03067557 -0.00402024\n",
      "  -1.23971083 -0.5218754   0.32732519]\n",
      " [ 0.25549057 -0.26979197  0.46682135 -0.68878747  0.29596124 -0.47618145\n",
      "  -0.63264136  0.31920813  0.44218895]\n",
      " [ 0.76686442 -0.65376702  0.38854224  0.77331709 -0.07149723  0.06301728\n",
      "   1.18885322 -0.00142318 -0.60552488]\n",
      " [-0.73918216  0.66648449 -0.37927532 -0.63128379  0.13591354 -0.05920516\n",
      "  -1.17331126 -0.39791212  0.42012917]\n",
      " [-0.01472916  0.1348864   0.33786557 -0.77699427 -0.02390576 -0.02654758\n",
      "  -0.84632183  0.1066013   0.15361805]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.204292    0.94164841 -1.72283775 -0.09889243  1.04246565 -1.53923785\n",
      "  -0.41675281]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:38 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57431739]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 38 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.60094083 -0.5320445   0.27898986  0.78954989 -0.12642383 -0.10741784\n",
      "   0.94005366  0.3118042  -0.23865202]\n",
      " [-0.53289244  0.72021335 -0.5141888  -0.92259946  0.04485784  0.01016204\n",
      "  -1.22552855 -0.5218754   0.32732519]\n",
      " [ 0.2677828  -0.25749973  0.46682135 -0.68878747  0.30825348 -0.46388921\n",
      "  -0.62034912  0.31920813  0.44218895]\n",
      " [ 0.75152481 -0.66910663  0.38854224  0.77331709 -0.08683684  0.04767767\n",
      "   1.17351361 -0.00142318 -0.60552488]\n",
      " [-0.72433869  0.68132796 -0.37927532 -0.63128379  0.15075701 -0.04436169\n",
      "  -1.15846779 -0.39791212  0.42012917]\n",
      " [-0.0029638   0.14665175  0.33786557 -0.77699427 -0.0121404  -0.01478222\n",
      "  -0.83455647  0.1066013   0.15361805]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.27449568  0.89265703 -1.7409811  -0.12025157  0.98737443 -1.55587482\n",
      "  -0.43887043]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:38 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.7561787]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 38 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.60517186 -0.5320445   0.27898986  0.79378093 -0.12642383 -0.10741784\n",
      "   0.9442847   0.3118042  -0.23865202]\n",
      " [-0.53650772  0.72021335 -0.5141888  -0.92621474  0.04485784  0.01016204\n",
      "  -1.22914383 -0.5218754   0.32732519]\n",
      " [ 0.26326943 -0.25749973  0.46682135 -0.69330084  0.30825348 -0.46388921\n",
      "  -0.62486249  0.31920813  0.44218895]\n",
      " [ 0.75510883 -0.66910663  0.38854224  0.7769011  -0.08683684  0.04767767\n",
      "   1.17709762 -0.00142318 -0.60552488]\n",
      " [-0.72825306  0.68132796 -0.37927532 -0.63519817  0.15075701 -0.04436169\n",
      "  -1.16238217 -0.39791212  0.42012917]\n",
      " [-0.00798695  0.14665175  0.33786557 -0.78201742 -0.0121404  -0.01478222\n",
      "  -0.83957962  0.1066013   0.15361805]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.25201871  0.91314198 -1.73954027 -0.11438758  1.00843377 -1.55419183\n",
      "  -0.43514009]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:38 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.66459357]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 38 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.61337506 -0.5320445   0.27898986  0.79378093 -0.11822063 -0.09921465\n",
      "   0.9524879   0.3118042  -0.23865202]\n",
      " [-0.54479841  0.72021335 -0.5141888  -0.92621474  0.03656716  0.00187135\n",
      "  -1.23743452 -0.5218754   0.32732519]\n",
      " [ 0.258745   -0.25749973  0.46682135 -0.69330084  0.30372905 -0.46841364\n",
      "  -0.62938692  0.31920813  0.44218895]\n",
      " [ 0.76315951 -0.66910663  0.38854224  0.7769011  -0.07878616  0.05572835\n",
      "   1.1851483  -0.00142318 -0.60552488]\n",
      " [-0.73646427  0.68132796 -0.37927532 -0.63519817  0.1425458  -0.0525729\n",
      "  -1.17059337 -0.39791212  0.42012917]\n",
      " [-0.01477667  0.14665175  0.33786557 -0.78201742 -0.01893012 -0.02157194\n",
      "  -0.84636934  0.1066013   0.15361805]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.21463616  0.9426163  -1.73381768 -0.10042518  1.04092304 -1.54881695\n",
      "  -0.42413762]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:38 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.55683288]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 38 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.59939913 -0.5320445   0.27898986  0.779805   -0.11822063 -0.11319058\n",
      "   0.9524879   0.3118042  -0.25262795]\n",
      " [-0.53040869  0.72021335 -0.5141888  -0.91182503  0.03656716  0.01626107\n",
      "  -1.23743452 -0.5218754   0.3417149 ]\n",
      " [ 0.2662538  -0.25749973  0.46682135 -0.68579204  0.30372905 -0.46090484\n",
      "  -0.62938692  0.31920813  0.44969774]\n",
      " [ 0.74972276 -0.66910663  0.38854224  0.76346436 -0.07878616  0.04229161\n",
      "   1.1851483  -0.00142318 -0.61896162]\n",
      " [-0.72292637  0.68132796 -0.37927532 -0.62166027  0.1425458  -0.039035\n",
      "  -1.17059337 -0.39791212  0.43366707]\n",
      " [-0.00453278  0.14665175  0.33786557 -0.77177353 -0.01893012 -0.01132805\n",
      "  -0.84636934  0.1066013   0.16386194]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.28334099  0.89146818 -1.75044335 -0.12700026  0.99082759 -1.56723911\n",
      "  -0.44747485]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:38 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.69417284]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 38 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.60661654 -0.52482709  0.27898986  0.78702241 -0.11822063 -0.10597317\n",
      "   0.95970531  0.3118042  -0.25262795]\n",
      " [-0.53732235  0.71329969 -0.5141888  -0.91873869  0.03656716  0.00934741\n",
      "  -1.24434818 -0.5218754   0.3417149 ]\n",
      " [ 0.2591054  -0.26464813  0.46682135 -0.69294045  0.30372905 -0.46805324\n",
      "  -0.63653532  0.31920813  0.44969774]\n",
      " [ 0.75640554 -0.66242385  0.38854224  0.77014713 -0.07878616  0.04897438\n",
      "   1.19183108 -0.00142318 -0.61896162]\n",
      " [-0.72994652  0.6743078  -0.37927532 -0.62868043  0.1425458  -0.04605515\n",
      "  -1.17761353 -0.39791212  0.43366707]\n",
      " [-0.01179208  0.13939245  0.33786557 -0.77903283 -0.01893012 -0.01858735\n",
      "  -0.85362864  0.1066013   0.16386194]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.25087791  0.91885915 -1.74637582 -0.12226471  1.01965819 -1.56291477\n",
      "  -0.44149127]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:38 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.71470983]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 38 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.61295894 -0.51848469  0.27898986  0.79336481 -0.11822063 -0.10597317\n",
      "   0.96604771  0.3118042  -0.25262795]\n",
      " [-0.54345028  0.70717175 -0.5141888  -0.92486662  0.03656716  0.00934741\n",
      "  -1.25047611 -0.5218754   0.3417149 ]\n",
      " [ 0.25270131 -0.27105222  0.46682135 -0.69934453  0.30372905 -0.46805324\n",
      "  -0.64293941  0.31920813  0.44969774]\n",
      " [ 0.76242006 -0.65640933  0.38854224  0.77616165 -0.07878616  0.04897438\n",
      "   1.1978456  -0.00142318 -0.61896162]\n",
      " [-0.7362498   0.66800453 -0.37927532 -0.63498371  0.1425458  -0.04605515\n",
      "  -1.18391681 -0.39791212  0.43366707]\n",
      " [-0.01830001  0.13288452  0.33786557 -0.78554076 -0.01893012 -0.01858735\n",
      "  -0.86013657  0.1066013   0.16386194]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.22179262  0.9439186  -1.74286921 -0.11620536  1.0454437  -1.55900344\n",
      "  -0.43620729]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:38 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.73887091]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 38 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.6185178  -0.51848469  0.28454872  0.79336481 -0.11822063 -0.10041431\n",
      "   0.97160657  0.3118042  -0.25262795]\n",
      " [-0.5482511   0.70717175 -0.51898961 -0.92486662  0.03656716  0.0045466\n",
      "  -1.25527693 -0.5218754   0.3417149 ]\n",
      " [ 0.250328   -0.27105222  0.46444804 -0.69934453  0.30372905 -0.47042655\n",
      "  -0.64531272  0.31920813  0.44969774]\n",
      " [ 0.76703461 -0.65640933  0.3931568   0.77616165 -0.07878616  0.05358894\n",
      "   1.20246015 -0.00142318 -0.61896162]\n",
      " [-0.74096376  0.66800453 -0.38398927 -0.63498371  0.1425458  -0.0507691\n",
      "  -1.18863076 -0.39791212  0.43366707]\n",
      " [-0.02155996  0.13288452  0.33460562 -0.78554076 -0.01893012 -0.0218473\n",
      "  -0.86339651  0.1066013   0.16386194]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.19660141  0.96538675 -1.74057114 -0.10604415  1.06853543 -1.55680111\n",
      "  -0.42704418]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:38 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.37150593]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 39 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.6122371  -0.52476539  0.28454872  0.7870841  -0.11822063 -0.10041431\n",
      "   0.97160657  0.3118042  -0.25890866]\n",
      " [-0.54385214  0.71157071 -0.51898961 -0.92046767  0.03656716  0.0045466\n",
      "  -1.25527693 -0.5218754   0.34611386]\n",
      " [ 0.25320666 -0.26817356  0.46444804 -0.69646588  0.30372905 -0.47042655\n",
      "  -0.64531272  0.31920813  0.4525764 ]\n",
      " [ 0.76418209 -0.65926186  0.3931568   0.77330913 -0.07878616  0.05358894\n",
      "   1.20246015 -0.00142318 -0.62181415]\n",
      " [-0.73804507  0.67092322 -0.38398927 -0.63206501  0.1425458  -0.0507691\n",
      "  -1.18863076 -0.39791212  0.43658576]\n",
      " [-0.01637152  0.13807296  0.33460562 -0.78035232 -0.01893012 -0.0218473\n",
      "  -0.86339651  0.1066013   0.16905038]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.23997273  0.93698168 -1.7577247  -0.12481595  1.04396302 -1.57553135\n",
      "  -0.44331322]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:39 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.73076368]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 39 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.61796979 -0.52476539  0.29028141  0.7870841  -0.11822063 -0.10041431\n",
      "   0.97733926  0.3118042  -0.25890866]\n",
      " [-0.54886212  0.71157071 -0.52399959 -0.92046767  0.03656716  0.0045466\n",
      "  -1.26028691 -0.5218754   0.34611386]\n",
      " [ 0.25368504 -0.26817356  0.46492642 -0.69646588  0.30372905 -0.47042655\n",
      "  -0.64483433  0.31920813  0.4525764 ]\n",
      " [ 0.76910995 -0.65926186  0.39808466  0.77330913 -0.07878616  0.05358894\n",
      "   1.20738801 -0.00142318 -0.62181415]\n",
      " [-0.74306956  0.67092322 -0.38901377 -0.63206501  0.1425458  -0.0507691\n",
      "  -1.19365526 -0.39791212  0.43658576]\n",
      " [-0.01972583  0.13807296  0.33125131 -0.78035232 -0.01893012 -0.0218473\n",
      "  -0.86675083  0.1066013   0.16905038]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.21348686  0.95992565 -1.75535068 -0.11109421  1.06816342 -1.57314117\n",
      "  -0.43359324]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:39 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.66627477]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 39 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.60159486 -0.54114032  0.27390648  0.7870841  -0.11822063 -0.10041431\n",
      "   0.96096433  0.3118042  -0.25890866]\n",
      " [-0.53231405  0.72811878 -0.50745152 -0.92046767  0.03656716  0.0045466\n",
      "  -1.24373884 -0.5218754   0.34611386]\n",
      " [ 0.25725118 -0.26460743  0.46849256 -0.69646588  0.30372905 -0.47042655\n",
      "  -0.6412682   0.31920813  0.4525764 ]\n",
      " [ 0.75269037 -0.67568143  0.38166508  0.77330913 -0.07878616  0.05358894\n",
      "   1.19096843 -0.00142318 -0.62181415]\n",
      " [-0.72655672  0.68743606 -0.37250093 -0.63206501  0.1425458  -0.0507691\n",
      "  -1.17714242 -0.39791212  0.43658576]\n",
      " [-0.01232735  0.14547144  0.33864979 -0.78035232 -0.01893012 -0.0218473\n",
      "  -0.85935235  0.1066013   0.16905038]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.28756086  0.90097062 -1.76757195 -0.14454258  1.00538385 -1.58502703\n",
      "  -0.46301531]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:39 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.54465548]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 39 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.61360216 -0.54114032  0.28591378  0.7870841  -0.11822063 -0.10041431\n",
      "   0.96096433  0.3238115  -0.25890866]\n",
      " [-0.54495327  0.72811878 -0.52009074 -0.92046767  0.03656716  0.0045466\n",
      "  -1.24373884 -0.53451462  0.34611386]\n",
      " [ 0.26860874 -0.26460743  0.47985012 -0.69646588  0.30372905 -0.47042655\n",
      "  -0.6412682   0.33056569  0.4525764 ]\n",
      " [ 0.76447808 -0.67568143  0.39345279  0.77330913 -0.07878616  0.05358894\n",
      "   1.19096843  0.01036453 -0.62181415]\n",
      " [-0.73918759  0.68743606 -0.3851318  -0.63206501  0.1425458  -0.0507691\n",
      "  -1.17714242 -0.41054299  0.43658576]\n",
      " [-0.00649382  0.14547144  0.34448331 -0.78035232 -0.01893012 -0.0218473\n",
      "  -0.85935235  0.11243483  0.16905038]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.2310968   0.9442367  -1.75778045 -0.10277022  1.04809211 -1.574701\n",
      "  -0.42876581]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:39 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.53850056]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 39 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.62561256 -0.54114032  0.28591378  0.7990945  -0.11822063 -0.10041431\n",
      "   0.96096433  0.3238115  -0.24689826]\n",
      " [-0.55686351  0.72811878 -0.52009074 -0.9323779   0.03656716  0.0045466\n",
      "  -1.24373884 -0.53451462  0.33420362]\n",
      " [ 0.26896307 -0.26460743  0.47985012 -0.69611155  0.30372905 -0.47042655\n",
      "  -0.6412682   0.33056569  0.45293073]\n",
      " [ 0.7751993  -0.67568143  0.39345279  0.78403035 -0.07878616  0.05358894\n",
      "   1.19096843  0.01036453 -0.61109293]\n",
      " [-0.75003958  0.68743606 -0.3851318  -0.642917    0.1425458  -0.0507691\n",
      "  -1.17714242 -0.41054299  0.42573378]\n",
      " [-0.01455664  0.14547144  0.34448331 -0.78841513 -0.01893012 -0.0218473\n",
      "  -0.85935235  0.11243483  0.16098757]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.17375141  0.98770484 -1.74366546 -0.07374316  1.08904939 -1.55853081\n",
      "  -0.4086787 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:39 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.30957039]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 39 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.62429702 -0.54114032  0.28591378  0.7990945  -0.11953618 -0.10172985\n",
      "   0.96096433  0.3238115  -0.2482138 ]\n",
      " [-0.55537428  0.72811878 -0.52009074 -0.9323779   0.03805639  0.00603583\n",
      "  -1.24373884 -0.53451462  0.33569285]\n",
      " [ 0.26470759 -0.26460743  0.47985012 -0.69611155  0.29947357 -0.47468203\n",
      "  -0.6412682   0.33056569  0.44867525]\n",
      " [ 0.77405594 -0.67568143  0.39345279  0.78403035 -0.07992953  0.05244557\n",
      "   1.19096843  0.01036453 -0.6122363 ]\n",
      " [-0.74814214  0.68743606 -0.3851318  -0.642917    0.14444324 -0.04887166\n",
      "  -1.17714242 -0.41054299  0.42763122]\n",
      " [-0.01542804  0.14547144  0.34448331 -0.78841513 -0.01980153 -0.02271871\n",
      "  -0.85935235  0.11243483  0.16011616]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.20683466  0.96984205 -1.75870967 -0.09476228  1.07136071 -1.57315785\n",
      "  -0.42609336]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:39 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.14245917]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 39 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.62510684 -0.5403305   0.28591378  0.7990945  -0.11872635 -0.10092002\n",
      "   0.96096433  0.3238115  -0.24740397]\n",
      " [-0.55648901  0.72700406 -0.52009074 -0.9323779   0.03694166  0.0049211\n",
      "  -1.24373884 -0.53451462  0.33457813]\n",
      " [ 0.26412346 -0.26519156  0.47985012 -0.69611155  0.29888944 -0.47526616\n",
      "  -0.6412682   0.33056569  0.44809112]\n",
      " [ 0.77515136 -0.67458601  0.39345279  0.78403035 -0.0788341   0.053541\n",
      "   1.19096843  0.01036453 -0.61114087]\n",
      " [-0.74909633  0.68648187 -0.3851318  -0.642917    0.14348905 -0.04982585\n",
      "  -1.17714242 -0.41054299  0.42667703]\n",
      " [-0.01595859  0.1449409   0.34448331 -0.78841513 -0.02033208 -0.02324925\n",
      "  -0.85935235  0.11243483  0.15958561]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.21553639  0.9663213  -1.76423285 -0.09970459  1.06815957 -1.57849729\n",
      "  -0.43098021]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:39 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.56927183]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 39 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.61289717 -0.55254017  0.28591378  0.7990945  -0.13093602 -0.11312969\n",
      "   0.94875466  0.3238115  -0.24740397]\n",
      " [-0.54254281  0.74095025 -0.52009074 -0.9323779   0.05088785  0.01886729\n",
      "  -1.22979265 -0.53451462  0.33457813]\n",
      " [ 0.27625887 -0.25305614  0.47985012 -0.69611155  0.31102485 -0.46313075\n",
      "  -0.62913279  0.33056569  0.44809112]\n",
      " [ 0.75999111 -0.68974626  0.39345279  0.78403035 -0.09399436  0.03838074\n",
      "   1.17580818  0.01036453 -0.61114087]\n",
      " [-0.73445631  0.70112188 -0.3851318  -0.642917    0.15812907 -0.03518584\n",
      "  -1.1625024  -0.41054299  0.42667703]\n",
      " [-0.00429071  0.15660877  0.34448331 -0.78841513 -0.0086642  -0.01158138\n",
      "  -0.84768447  0.11243483  0.15958561]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.28532952  0.9177782  -1.7825759  -0.12106387  1.01371405 -1.59533162\n",
      "  -0.45300866]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:39 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: [1.] Net Result: [[0.76332583]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 39 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.61687329 -0.55254017  0.28591378  0.80307062 -0.13093602 -0.11312969\n",
      "   0.95273079  0.3238115  -0.24740397]\n",
      " [-0.5459408   0.74095025 -0.52009074 -0.93577589  0.05088785  0.01886729\n",
      "  -1.23319063 -0.53451462  0.33457813]\n",
      " [ 0.27195037 -0.25305614  0.47985012 -0.70042005  0.31102485 -0.46313075\n",
      "  -0.63344129  0.33056569  0.44809112]\n",
      " [ 0.76336317 -0.68974626  0.39345279  0.78740241 -0.09399436  0.03838074\n",
      "   1.17918023  0.01036453 -0.61114087]\n",
      " [-0.73813588  0.70112188 -0.3851318  -0.64659657  0.15812907 -0.03518584\n",
      "  -1.16618197 -0.41054299  0.42667703]\n",
      " [-0.00906136  0.15660877  0.34448331 -0.79318578 -0.0086642  -0.01158138\n",
      "  -0.85245512  0.11243483  0.15958561]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.2639508   0.93731374 -1.78123555 -0.1155178   1.03377128 -1.59376863\n",
      "  -0.4495365 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:39 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.66747795]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 39 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.62498537 -0.55254017  0.28591378  0.80307062 -0.12282394 -0.10501761\n",
      "   0.96084286  0.3238115  -0.24740397]\n",
      " [-0.55412599  0.74095025 -0.52009074 -0.93577589  0.04270266  0.0106821\n",
      "  -1.24137582 -0.53451462  0.33457813]\n",
      " [ 0.26751142 -0.25305614  0.47985012 -0.70042005  0.30658591 -0.4675697\n",
      "  -0.63788024  0.33056569  0.44809112]\n",
      " [ 0.77132045 -0.68974626  0.39345279  0.78740241 -0.08603707  0.04633803\n",
      "   1.18713752  0.01036453 -0.61114087]\n",
      " [-0.74624501  0.70112188 -0.3851318  -0.64659657  0.15001994 -0.04329496\n",
      "  -1.1742911  -0.41054299  0.42667703]\n",
      " [-0.01579925  0.15660877  0.34448331 -0.79318578 -0.01540209 -0.01831926\n",
      "  -0.85919301  0.11243483  0.15958561]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.22704898  0.96646993 -1.77558054 -0.10170357  1.06581701 -1.58844984\n",
      "  -0.43873118]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:39 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.55730693]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 39 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.6109613  -0.55254017  0.28591378  0.78904656 -0.12282394 -0.11904168\n",
      "   0.96084286  0.3238115  -0.26142804]\n",
      " [-0.5397126   0.74095025 -0.52009074 -0.92136251  0.04270266  0.02509549\n",
      "  -1.24137582 -0.53451462  0.34899151]\n",
      " [ 0.27490201 -0.25305614  0.47985012 -0.69302947  0.30658591 -0.46017911\n",
      "  -0.63788024  0.33056569  0.4554817 ]\n",
      " [ 0.75784821 -0.68974626  0.39345279  0.77393016 -0.08603707  0.03286578\n",
      "   1.18713752  0.01036453 -0.62461311]\n",
      " [-0.73266017  0.70112188 -0.3851318  -0.63301174  0.15001994 -0.02971013\n",
      "  -1.1742911  -0.41054299  0.44026186]\n",
      " [-0.0055129   0.15660877  0.34448331 -0.78289943 -0.01540209 -0.00803292\n",
      "  -0.85919301  0.11243483  0.16987196]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.29579722  0.91520664 -1.79218112 -0.12843243  1.01564031 -1.60681151\n",
      "  -0.46203737]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:39 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.69719809]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 39 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.61806556 -0.54543591  0.28591378  0.79615081 -0.12282394 -0.11193742\n",
      "   0.96794712  0.3238115  -0.26142804]\n",
      " [-0.54653048  0.73413237 -0.52009074 -0.92818039  0.04270266  0.01827761\n",
      "  -1.2481937  -0.53451462  0.34899151]\n",
      " [ 0.26786572 -0.26009243  0.47985012 -0.70006576  0.30658591 -0.4672154\n",
      "  -0.64491653  0.33056569  0.4554817 ]\n",
      " [ 0.76444619 -0.68314828  0.39345279  0.78052814 -0.08603707  0.03946376\n",
      "   1.1937355   0.01036453 -0.62461311]\n",
      " [-0.73957807  0.69420398 -0.3851318  -0.63992963  0.15001994 -0.03662803\n",
      "  -1.18120899 -0.41054299  0.44026186]\n",
      " [-0.01266336  0.14945831  0.34448331 -0.79004989 -0.01540209 -0.01518338\n",
      "  -0.86634347  0.11243483  0.16987196]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.26383453  0.94218725 -1.78815244 -0.1237775   1.04399628 -1.6025378\n",
      "  -0.45620193]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:39 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.71845452]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 39 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.62426583 -0.53923564  0.28591378  0.80235108 -0.12282394 -0.11193742\n",
      "   0.97414739  0.3238115  -0.26142804]\n",
      " [-0.55252732  0.72813553 -0.52009074 -0.93417722  0.04270266  0.01827761\n",
      "  -1.25419054 -0.53451462  0.34899151]\n",
      " [ 0.2615937  -0.26636445  0.47985012 -0.70633777  0.30658591 -0.4672154\n",
      "  -0.65118854  0.33056569  0.4554817 ]\n",
      " [ 0.77033521 -0.67725926  0.39345279  0.78641717 -0.08603707  0.03946376\n",
      "   1.19962453  0.01036453 -0.62461311]\n",
      " [-0.74574361  0.68803844 -0.3851318  -0.64609518  0.15001994 -0.03662803\n",
      "  -1.18737454 -0.41054299  0.44026186]\n",
      " [-0.01903685  0.14308482  0.34448331 -0.79642338 -0.01540209 -0.01518338\n",
      "  -0.87271696  0.11243483  0.16987196]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.23535935  0.96674883 -1.7847244  -0.11785553  1.06923976 -1.59872367\n",
      "  -0.45109005]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:39 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.7426677]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 39 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.62967656 -0.53923564  0.29132451  0.80235108 -0.12282394 -0.10652669\n",
      "   0.97955812  0.3238115  -0.26142804]\n",
      " [-0.55719595  0.72813553 -0.52475937 -0.93417722  0.04270266  0.01360898\n",
      "  -1.25885917 -0.53451462  0.34899151]\n",
      " [ 0.25935678 -0.26636445  0.4776132  -0.70633777  0.30658591 -0.46945232\n",
      "  -0.65342547  0.33056569  0.4554817 ]\n",
      " [ 0.77483002 -0.67725926  0.3979476   0.78641717 -0.08603707  0.04395857\n",
      "   1.20411934  0.01036453 -0.62461311]\n",
      " [-0.75032776  0.68803844 -0.38971594 -0.64609518  0.15001994 -0.04121217\n",
      "  -1.19195868 -0.41054299  0.44026186]\n",
      " [-0.02223483  0.14308482  0.34128533 -0.79642338 -0.01540209 -0.01838136\n",
      "  -0.87591494  0.11243483  0.16987196]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.21076966  0.98776698 -1.78250103 -0.10785096  1.09178965 -1.59659226\n",
      "  -0.44216449]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:39 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.36258228]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 40 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.62368075 -0.54523145  0.29132451  0.79635527 -0.12282394 -0.10652669\n",
      "   0.97955812  0.3238115  -0.26742385]\n",
      " [-0.55303774  0.73229374 -0.52475937 -0.93001901  0.04270266  0.01360898\n",
      "  -1.25885917 -0.53451462  0.35314972]\n",
      " [ 0.26201345 -0.26370778  0.4776132  -0.7036811   0.30658591 -0.46945232\n",
      "  -0.65342547  0.33056569  0.45813838]\n",
      " [ 0.7721583  -0.67993098  0.3979476   0.78374545 -0.08603707  0.04395857\n",
      "   1.20411934  0.01036453 -0.62728484]\n",
      " [-0.74756909  0.6907971  -0.38971594 -0.64333651  0.15001994 -0.04121217\n",
      "  -1.19195868 -0.41054299  0.44302053]\n",
      " [-0.01726243  0.14805722  0.34128533 -0.79145098 -0.01540209 -0.01838136\n",
      "  -0.87591494  0.11243483  0.17484436]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.25266901  0.96041514 -1.79917255 -0.12611442  1.0681382  -1.6147501\n",
      "  -0.4579271 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:40 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.73583244]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 40 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.62920831 -0.54523145  0.29685207  0.79635527 -0.12282394 -0.10652669\n",
      "   0.98508567  0.3238115  -0.26742385]\n",
      " [-0.5578591   0.73229374 -0.52958073 -0.93001901  0.04270266  0.01360898\n",
      "  -1.26368053 -0.53451462  0.35314972]\n",
      " [ 0.26256573 -0.26370778  0.47816547 -0.7036811   0.30658591 -0.46945232\n",
      "  -0.65287319  0.33056569  0.45813838]\n",
      " [ 0.77690736 -0.67993098  0.40269666  0.78374545 -0.08603707  0.04395857\n",
      "   1.2088684   0.01036453 -0.62728484]\n",
      " [-0.75240457  0.6907971  -0.39455142 -0.64333651  0.15001994 -0.04121217\n",
      "  -1.19679416 -0.41054299  0.44302053]\n",
      " [-0.02054825  0.14805722  0.33799952 -0.79145098 -0.01540209 -0.01838136\n",
      "  -0.87920076  0.11243483  0.17484436]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.22699416  0.98273365 -1.79690976 -0.11272404  1.09162659 -1.61247198\n",
      "  -0.44854485]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:40 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.66684484]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 40 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.61282162 -0.56161814  0.28046538  0.79635527 -0.12282394 -0.10652669\n",
      "   0.96869899  0.3238115  -0.26742385]\n",
      " [-0.54130863  0.74884421 -0.51303026 -0.93001901  0.04270266  0.01360898\n",
      "  -1.24713006 -0.53451462  0.35314972]\n",
      " [ 0.26579716 -0.26047635  0.48139691 -0.7036811   0.30658591 -0.46945232\n",
      "  -0.64964176  0.33056569  0.45813838]\n",
      " [ 0.76047552 -0.69636282  0.38626482  0.78374545 -0.08603707  0.04395857\n",
      "   1.19243656  0.01036453 -0.62728484]\n",
      " [-0.73588944  0.70731224 -0.37803629 -0.64333651  0.15001994 -0.04121217\n",
      "  -1.18027902 -0.41054299  0.44302053]\n",
      " [-0.01320593  0.15539954  0.34534183 -0.79145098 -0.01540209 -0.01838136\n",
      "  -0.87185844  0.11243483  0.17484436]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.30106822  0.92371742 -1.80915915 -0.14651296  1.02891201 -1.62437644\n",
      "  -0.47802833]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:40 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.54900632]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 40 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.62479911 -0.56161814  0.29244287  0.79635527 -0.12282394 -0.10652669\n",
      "   0.96869899  0.33578899 -0.26742385]\n",
      " [-0.55379895  0.74884421 -0.52552058 -0.93001901  0.04270266  0.01360898\n",
      "  -1.24713006 -0.54700494  0.35314972]\n",
      " [ 0.27719713 -0.26047635  0.49279688 -0.7036811   0.30658591 -0.46945232\n",
      "  -0.64964176  0.34196567  0.45813838]\n",
      " [ 0.7722322  -0.69636282  0.3980215   0.78374545 -0.08603707  0.04395857\n",
      "   1.19243656  0.02212121 -0.62728484]\n",
      " [-0.74838718  0.70731224 -0.39053403 -0.64333651  0.15001994 -0.04121217\n",
      "  -1.18027902 -0.42304074  0.44302053]\n",
      " [-0.00729733  0.15539954  0.35125044 -0.79145098 -0.01540209 -0.01838136\n",
      "  -0.87185844  0.11834343  0.17484436]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.24523556  0.96679524 -1.79969299 -0.1048579   1.07138984 -1.61439331\n",
      "  -0.44400684]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:40 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.53983613]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 40 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.63682034 -0.56161814  0.29244287  0.8083765  -0.12282394 -0.10652669\n",
      "   0.96869899  0.33578899 -0.25540262]\n",
      " [-0.56572096  0.74884421 -0.52552058 -0.94194102  0.04270266  0.01360898\n",
      "  -1.24713006 -0.54700494  0.34122772]\n",
      " [ 0.27764933 -0.26047635  0.49279688 -0.70322891  0.30658591 -0.46945232\n",
      "  -0.64964176  0.34196567  0.45859057]\n",
      " [ 0.78300705 -0.69636282  0.3980215   0.79452029 -0.08603707  0.04395857\n",
      "   1.19243656  0.02212121 -0.61650999]\n",
      " [-0.75929821  0.70731224 -0.39053403 -0.65424754  0.15001994 -0.04121217\n",
      "  -1.18027902 -0.42304074  0.4321095 ]\n",
      " [-0.01539798  0.15539954  0.35125044 -0.79955163 -0.01540209 -0.01838136\n",
      "  -0.87185844  0.11834343  0.16674371]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.1880802   1.01024427 -1.78574492 -0.07582796  1.11235938 -1.59843864\n",
      "  -0.42406567]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:40 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.29793888]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 40 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.63564255 -0.56161814  0.29244287  0.8083765  -0.12400173 -0.10770447\n",
      "   0.96869899  0.33578899 -0.25658041]\n",
      " [-0.56442004  0.74884421 -0.52552058 -0.94194102  0.04400358  0.0149099\n",
      "  -1.24713006 -0.54700494  0.34252864]\n",
      " [ 0.27353062 -0.26047635  0.49279688 -0.70322891  0.3024672  -0.47357103\n",
      "  -0.64964176  0.34196567  0.45447186]\n",
      " [ 0.78204157 -0.69636282  0.3980215   0.79452029 -0.08700255  0.04299309\n",
      "   1.19243656  0.02212121 -0.61747547]\n",
      " [-0.75761714  0.70731224 -0.39053403 -0.65424754  0.15170102 -0.0395311\n",
      "  -1.18027902 -0.42304074  0.43379057]\n",
      " [-0.01631064  0.15539954  0.35125044 -0.79955163 -0.01631474 -0.01929402\n",
      "  -0.87185844  0.11834343  0.16583106]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.21924033  0.99348187 -1.80001792 -0.09575615  1.09581134 -1.61232424\n",
      "  -0.4405605 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:40 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.13095178]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 40 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.63638207 -0.56087862  0.29244287  0.8083765  -0.1232622  -0.10696495\n",
      "   0.96869899  0.33578899 -0.25584088]\n",
      " [-0.56542288  0.74784138 -0.52552058 -0.94194102  0.04300075  0.01390707\n",
      "  -1.24713006 -0.54700494  0.3415258 ]\n",
      " [ 0.27299038 -0.26101658  0.49279688 -0.70322891  0.30192696 -0.47411127\n",
      "  -0.64964176  0.34196567  0.45393162]\n",
      " [ 0.78302999 -0.6953744   0.3980215   0.79452029 -0.08601413  0.04398151\n",
      "   1.19243656  0.02212121 -0.61648705]\n",
      " [-0.758486    0.70644337 -0.39053403 -0.65424754  0.15083215 -0.04039996\n",
      "  -1.18027902 -0.42304074  0.4329217 ]\n",
      " [-0.01680333  0.15490684  0.35125044 -0.79955163 -0.01680744 -0.01978671\n",
      "  -0.87185844  0.11834343  0.16533836]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.22669171  0.99051704 -1.80480481 -0.10003002  1.0931296  -1.61695481\n",
      "  -0.44478486]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:40 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.56358812]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 40 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.62437858 -0.57288211  0.29244287  0.8083765  -0.1352657  -0.11896845\n",
      "   0.95669549  0.33578899 -0.25584088]\n",
      " [-0.55173914  0.76152511 -0.52552058 -0.94194102  0.05668448  0.0275908\n",
      "  -1.23344632 -0.54700494  0.3415258 ]\n",
      " [ 0.28495205 -0.24905491  0.49279688 -0.70322891  0.31388863 -0.4621496\n",
      "  -0.63768009  0.34196567  0.45393162]\n",
      " [ 0.76807775 -0.71032663  0.3980215   0.79452029 -0.10096637  0.02902928\n",
      "   1.17748432  0.02212121 -0.61648705]\n",
      " [-0.7440767   0.72085268 -0.39053403 -0.65424754  0.16524146 -0.02599066\n",
      "  -1.16586972 -0.42304074  0.4329217 ]\n",
      " [-0.00525501  0.16645517  0.35125044 -0.79955163 -0.00525912 -0.00823839\n",
      "  -0.86031012  0.11834343  0.16533836]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.29600081  0.94248889 -1.82333875 -0.12137181  1.03939938 -1.63398135\n",
      "  -0.46671451]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:40 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.77019245]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 40 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.62811693 -0.57288211  0.29244287  0.81211485 -0.1352657  -0.11896845\n",
      "   0.96043384  0.33578899 -0.25584088]\n",
      " [-0.55493506  0.76152511 -0.52552058 -0.94513694  0.05668448  0.0275908\n",
      "  -1.23664224 -0.54700494  0.3415258 ]\n",
      " [ 0.28084    -0.24905491  0.49279688 -0.70734096  0.31388863 -0.4621496\n",
      "  -0.64179214  0.34196567  0.45393162]\n",
      " [ 0.77125254 -0.71032663  0.3980215   0.79769508 -0.10096637  0.02902928\n",
      "   1.18065911  0.02212121 -0.61648705]\n",
      " [-0.74753782  0.72085268 -0.39053403 -0.65770867  0.16524146 -0.02599066\n",
      "  -1.16933084 -0.42304074  0.4329217 ]\n",
      " [-0.00978508  0.16645517  0.35125044 -0.8040817  -0.00525912 -0.00823839\n",
      "  -0.86484018  0.11834343  0.16533836]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.2756633   0.96111844 -1.8220902  -0.11612305  1.05850344 -1.63252765\n",
      "  -0.46347929]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:40 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.6701826]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 40 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.6361419  -0.57288211  0.29244287  0.81211485 -0.12724073 -0.11094347\n",
      "   0.96845881  0.33578899 -0.25584088]\n",
      " [-0.56302208  0.76152511 -0.52552058 -0.94513694  0.04859747  0.01950379\n",
      "  -1.24472926 -0.54700494  0.3415258 ]\n",
      " [ 0.27648793 -0.24905491  0.49279688 -0.70734096  0.30953656 -0.46650167\n",
      "  -0.64614421  0.34196567  0.45393162]\n",
      " [ 0.77912385 -0.71032663  0.3980215   0.79769508 -0.09309506  0.03690059\n",
      "   1.18853042  0.02212121 -0.61648705]\n",
      " [-0.75555239  0.72085268 -0.39053403 -0.65770867  0.15722689 -0.03400522\n",
      "  -1.17734541 -0.42304074  0.4329217 ]\n",
      " [-0.01647097  0.16645517  0.35125044 -0.8040817  -0.01194501 -0.01492428\n",
      "  -0.87152608  0.11834343  0.16533836]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.23921223  0.98997142 -1.81649445 -0.10244012  1.09012869 -1.62725692\n",
      "  -0.4528539 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:40 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.55780062]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 40 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.62207298 -0.57288211  0.29244287  0.79804593 -0.12724073 -0.12501239\n",
      "   0.96845881  0.33578899 -0.2699098 ]\n",
      " [-0.54858649  0.76152511 -0.52552058 -0.93070136  0.04859747  0.03393937\n",
      "  -1.24472926 -0.54700494  0.35596138]\n",
      " [ 0.28375114 -0.24905491  0.49279688 -0.70007775  0.30953656 -0.45923845\n",
      "  -0.64614421  0.34196567  0.46119484]\n",
      " [ 0.76561846 -0.71032663  0.3980215   0.78418968 -0.09309506  0.02339519\n",
      "   1.18853042  0.02212121 -0.62999244]\n",
      " [-0.74192398  0.72085268 -0.39053403 -0.64408026  0.15722689 -0.02037682\n",
      "  -1.17734541 -0.42304074  0.44655011]\n",
      " [-0.00614864  0.16645517  0.35125044 -0.79375937 -0.01194501 -0.00460195\n",
      "  -0.87152608  0.11834343  0.17566069]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.30800552  0.93859848 -1.83307458 -0.12933318  1.03987422 -1.64556491\n",
      "  -0.47613808]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:40 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.69996821]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 40 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.6290746  -0.56588049  0.29244287  0.80504755 -0.12724073 -0.11801077\n",
      "   0.97546044  0.33578899 -0.2699098 ]\n",
      " [-0.55531892  0.75479269 -0.52552058 -0.93743379  0.04859747  0.02720694\n",
      "  -1.25146169 -0.54700494  0.35596138]\n",
      " [ 0.27681707 -0.25598898  0.49279688 -0.70701182  0.30953656 -0.46617252\n",
      "  -0.65307828  0.34196567  0.46119484]\n",
      " [ 0.77214188 -0.70380321  0.3980215   0.7907131  -0.09309506  0.02991861\n",
      "   1.19505384  0.02212121 -0.62999244]\n",
      " [-0.74875046  0.7140262  -0.39053403 -0.65090674  0.15722689 -0.02720329\n",
      "  -1.18417189 -0.42304074  0.44655011]\n",
      " [-0.01319887  0.15940494  0.35125044 -0.8008096  -0.01194501 -0.01165218\n",
      "  -0.87857631  0.11834343  0.17566069]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.27650028  0.9651988  -1.82907622 -0.12475082  1.06779069 -1.6413316\n",
      "  -0.4704356 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:40 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.72203432]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 40 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.63514059 -0.55981451  0.29244287  0.81111354 -0.12724073 -0.11801077\n",
      "   0.98152642  0.33578899 -0.2699098 ]\n",
      " [-0.56119238  0.74891923 -0.52552058 -0.94330724  0.04859747  0.02720694\n",
      "  -1.25733514 -0.54700494  0.35596138]\n",
      " [ 0.27067102 -0.26213503  0.49279688 -0.71315787  0.30953656 -0.46617252\n",
      "  -0.65922433  0.34196567  0.46119484]\n",
      " [ 0.77791311 -0.69803199  0.3980215   0.79648433 -0.09309506  0.02991861\n",
      "   1.20082507  0.02212121 -0.62999244]\n",
      " [-0.75478624  0.70799042 -0.39053403 -0.65694252  0.15722689 -0.02720329\n",
      "  -1.19020767 -0.42304074  0.44655011]\n",
      " [-0.0194433   0.1531605   0.35125044 -0.80705403 -0.01194501 -0.01165218\n",
      "  -0.88482074  0.11834343  0.17566069]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.24860631  0.98928206 -1.82572008 -0.11895916  1.09251483 -1.63760595\n",
      "  -0.46548362]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:40 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.74633309]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 40 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.64040937 -0.55981451  0.29771165  0.81111354 -0.12724073 -0.11274199\n",
      "   0.9867952   0.33578899 -0.2699098 ]\n",
      " [-0.56573669  0.74891923 -0.5300649  -0.94330724  0.04859747  0.02266263\n",
      "  -1.26187946 -0.54700494  0.35596138]\n",
      " [ 0.26856798 -0.26213503  0.49069384 -0.71315787  0.30953656 -0.46827557\n",
      "  -0.66132738  0.34196567  0.46119484]\n",
      " [ 0.78229536 -0.69803199  0.40240376  0.79648433 -0.09309506  0.03430087\n",
      "   1.20520732  0.02212121 -0.62999244]\n",
      " [-0.75924855  0.70799042 -0.39499634 -0.65694252  0.15722689 -0.0316656\n",
      "  -1.19466998 -0.42304074  0.44655011]\n",
      " [-0.02257654  0.1531605   0.34811719 -0.80705403 -0.01194501 -0.01478542\n",
      "  -0.88795398  0.11834343  0.17566069]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.2245942   1.00986192 -1.82356528 -0.10910236  1.11454199 -1.63553949\n",
      "  -0.45677998]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:40 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.35379149]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 41 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.63469316 -0.56553072  0.29771165  0.80539732 -0.12724073 -0.11274199\n",
      "   0.9867952   0.33578899 -0.27562601]\n",
      " [-0.56181279  0.75284314 -0.5300649  -0.93938334  0.04859747  0.02266263\n",
      "  -1.26187946 -0.54700494  0.35988529]\n",
      " [ 0.2710134  -0.25968961  0.49069384 -0.71071245  0.30953656 -0.46827557\n",
      "  -0.66132738  0.34196567  0.46364026]\n",
      " [ 0.77979951 -0.70052783  0.40240376  0.79398848 -0.09309506  0.03430087\n",
      "   1.20520732  0.02212121 -0.63248829]\n",
      " [-0.75664787  0.7105911  -0.39499634 -0.65434184  0.15722689 -0.0316656\n",
      "  -1.19466998 -0.42304074  0.44915079]\n",
      " [-0.01781773  0.15791931  0.34811719 -0.80229522 -0.01194501 -0.01478542\n",
      "  -0.88795398  0.11834343  0.1804195 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.26503665  0.98354895 -1.8397549  -0.12685353  1.09179867 -1.65313026\n",
      "  -0.47204096]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:41 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.74079091]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 41 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.64002246 -0.56553072  0.30304095  0.80539732 -0.12724073 -0.11274199\n",
      "   0.9921245   0.33578899 -0.27562601]\n",
      " [-0.56645437  0.75284314 -0.53470649 -0.93938334  0.04859747  0.02266263\n",
      "  -1.26652104 -0.54700494  0.35988529]\n",
      " [ 0.27163636 -0.25968961  0.4913168  -0.71071245  0.30953656 -0.46827557\n",
      "  -0.66070442  0.34196567  0.46364026]\n",
      " [ 0.78437791 -0.70052783  0.40698216  0.79398848 -0.09309506  0.03430087\n",
      "   1.20978572  0.02212121 -0.63248829]\n",
      " [-0.76130331  0.7105911  -0.39965178 -0.65434184  0.15722689 -0.0316656\n",
      "  -1.19932542 -0.42304074  0.44915079]\n",
      " [-0.02103092  0.15791931  0.34490401 -0.80229522 -0.01194501 -0.01478542\n",
      "  -0.89116717  0.11834343  0.1804195 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.24015002  1.00525132 -1.83759555 -0.11378621  1.1145914  -1.65095618\n",
      "  -0.46297998]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:41 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.66724546]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 41 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.6236274  -0.58192578  0.28664589  0.80539732 -0.12724073 -0.11274199\n",
      "   0.97572944  0.33578899 -0.27562601]\n",
      " [-0.54990062  0.76939689 -0.51815273 -0.93938334  0.04859747  0.02266263\n",
      "  -1.24996729 -0.54700494  0.35988529]\n",
      " [ 0.27453393 -0.25679204  0.49421437 -0.71071245  0.30953656 -0.46827557\n",
      "  -0.65780685  0.34196567  0.46364026]\n",
      " [ 0.76793235 -0.7169734   0.3905366   0.79398848 -0.09309506  0.03430087\n",
      "   1.19334016  0.02212121 -0.63248829]\n",
      " [-0.7447843   0.72711012 -0.38313277 -0.65434184  0.15722689 -0.0316656\n",
      "  -1.1828064  -0.42304074  0.44915079]\n",
      " [-0.01375888  0.16519136  0.35217605 -0.80229522 -0.01194501 -0.01478542\n",
      "  -0.88389513  0.11834343  0.1804195 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.31422393  0.94619036 -1.84988573 -0.1479136   1.05195336 -1.66289326\n",
      "  -0.49254006]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:41 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.55349129]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 41 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.63555455 -0.58192578  0.29857304  0.80539732 -0.12724073 -0.11274199\n",
      "   0.97572944  0.34771614 -0.27562601]\n",
      " [-0.56223074  0.76939689 -0.53048285 -0.93938334  0.04859747  0.02266263\n",
      "  -1.24996729 -0.55933506  0.35988529]\n",
      " [ 0.28595494 -0.25679204  0.50563538 -0.71071245  0.30953656 -0.46827557\n",
      "  -0.65780685  0.35338668  0.46364026]\n",
      " [ 0.77964082 -0.7169734   0.40224506  0.79398848 -0.09309506  0.03430087\n",
      "   1.19334016  0.03382968 -0.63248829]\n",
      " [-0.75713611  0.72711012 -0.39548459 -0.65434184  0.15722689 -0.0316656\n",
      "  -1.1828064  -0.43539255  0.44915079]\n",
      " [-0.00777598  0.16519136  0.35815895 -0.80229522 -0.01194501 -0.01478542\n",
      "  -0.88389513  0.12432633  0.1804195 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.25904914  0.98904007 -1.84073292 -0.10640769  1.09416505 -1.65324055\n",
      "  -0.45875955]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:41 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.54136115]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 41 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.64757726 -0.58192578  0.29857304  0.81742003 -0.12724073 -0.11274199\n",
      "   0.97572944  0.34771614 -0.2636033 ]\n",
      " [-0.57415551  0.76939689 -0.53048285 -0.95130811  0.04859747  0.02266263\n",
      "  -1.24996729 -0.55933506  0.34796052]\n",
      " [ 0.2865082  -0.25679204  0.50563538 -0.71015918  0.30953656 -0.46827557\n",
      "  -0.65780685  0.35338668  0.46419352]\n",
      " [ 0.79045958 -0.7169734   0.40224506  0.80480725 -0.09309506  0.03430087\n",
      "   1.19334016  0.03382968 -0.62166953]\n",
      " [-0.7680951   0.72711012 -0.39548459 -0.66530082  0.15722689 -0.0316656\n",
      "  -1.1828064  -0.43539255  0.43819181]\n",
      " [-0.01590591  0.16519136  0.35815895 -0.81042515 -0.01194501 -0.01478542\n",
      "  -0.88389513  0.12432633  0.17228957]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.20211159  1.03244351 -1.82695386 -0.07738552  1.13512194 -1.6375023\n",
      "  -0.43896866]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:41 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.28663151]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 41 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.64652782 -0.58192578  0.29857304  0.81742003 -0.12829017 -0.11379143\n",
      "   0.97572944  0.34771614 -0.26465275]\n",
      " [-0.57302723  0.76939689 -0.53048285 -0.95130811  0.04972575  0.02379091\n",
      "  -1.24996729 -0.55933506  0.3490888 ]\n",
      " [ 0.28253022 -0.25679204  0.50563538 -0.71015918  0.30555858 -0.47225355\n",
      "  -0.65780685  0.35338668  0.46021554]\n",
      " [ 0.78965617 -0.7169734   0.40224506  0.80480725 -0.09389846  0.03349746\n",
      "   1.19334016  0.03382968 -0.62247294]\n",
      " [-0.76661359  0.72711012 -0.39548459 -0.66530082  0.1587084  -0.0301841\n",
      "  -1.1828064  -0.43539255  0.43967332]\n",
      " [-0.01685178  0.16519136  0.35815895 -0.81042515 -0.01289088 -0.01573129\n",
      "  -0.88389513  0.12432633  0.1713437 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.23141592  1.01673827 -1.84047323 -0.09625209  1.11966475 -1.65066262\n",
      "  -0.45456934]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:41 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.12023456]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 41 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.64719744 -0.58125615  0.29857304  0.81742003 -0.12762054 -0.1131218\n",
      "   0.97572944  0.34771614 -0.26398312]\n",
      " [-0.57392271  0.7685014  -0.53048285 -0.95130811  0.04883026  0.02289542\n",
      "  -1.24996729 -0.55933506  0.34819331]\n",
      " [ 0.28203539 -0.25728687  0.50563538 -0.71015918  0.30506375 -0.47274838\n",
      "  -0.65780685  0.35338668  0.45972071]\n",
      " [ 0.79054126 -0.71608831  0.40224506  0.80480725 -0.09301337  0.03438255\n",
      "   1.19334016  0.03382968 -0.62158785]\n",
      " [-0.76739799  0.72632571 -0.39548459 -0.66530082  0.157924   -0.0309685\n",
      "  -1.1828064  -0.43539255  0.43888891]\n",
      " [-0.01730483  0.1647383   0.35815895 -0.81042515 -0.01334394 -0.01618435\n",
      "  -0.88389513  0.12432633  0.17089064]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.23777502  1.01425038 -1.84460655 -0.09993493  1.11742625 -1.65466363\n",
      "  -0.45820837]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:41 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.55730056]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 41 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.63542331 -0.59303028  0.29857304  0.81742003 -0.13939467 -0.12489593\n",
      "   0.96395531  0.34771614 -0.26398312]\n",
      " [-0.56052687  0.78189725 -0.53048285 -0.95130811  0.06222611  0.03629126\n",
      "  -1.23657145 -0.55933506  0.34819331]\n",
      " [ 0.29380762 -0.24551464  0.50563538 -0.71015918  0.31683598 -0.46097615\n",
      "  -0.64603461  0.35338668  0.45972071]\n",
      " [ 0.77582622 -0.73080335  0.40224506  0.80480725 -0.10772841  0.01966751\n",
      "   1.17862512  0.03382968 -0.62158785]\n",
      " [-0.75324651  0.7404772  -0.39548459 -0.66530082  0.17207548 -0.01681701\n",
      "  -1.16865492 -0.43539255  0.43888891]\n",
      " [-0.00589668  0.17614646  0.35815895 -0.81042515 -0.00193578 -0.00477619\n",
      "  -0.87248697  0.12432633  0.17089064]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.30652268  0.9668026  -1.86331729 -0.12123753  1.06448019 -1.67187233\n",
      "  -0.48002525]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:41 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.77679333]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 41 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.63893979 -0.59303028  0.29857304  0.82093651 -0.13939467 -0.12489593\n",
      "   0.96747178  0.34771614 -0.26398312]\n",
      " [-0.56353465  0.78189725 -0.53048285 -0.95431588  0.06222611  0.03629126\n",
      "  -1.23957923 -0.55933506  0.34819331]\n",
      " [ 0.28988359 -0.24551464  0.50563538 -0.71408322  0.31683598 -0.46097615\n",
      "  -0.64995864  0.35338668  0.45972071]\n",
      " [ 0.77881717 -0.73080335  0.40224506  0.80779819 -0.10772841  0.01966751\n",
      "   1.18161606  0.03382968 -0.62158785]\n",
      " [-0.7565042   0.7404772  -0.39548459 -0.66855851  0.17207548 -0.01681701\n",
      "  -1.17191261 -0.43539255  0.43888891]\n",
      " [-0.01019782  0.17614646  0.35815895 -0.8147263  -0.00193578 -0.00477619\n",
      "  -0.87678812  0.12432633  0.17089064]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.28717228  0.98456818 -1.86215285 -0.11626731  1.08267796 -1.67051844\n",
      "  -0.4770079 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:41 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.67272756]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 41 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.64688149 -0.59303028  0.29857304  0.82093651 -0.13145297 -0.11695423\n",
      "   0.97541348  0.34771614 -0.26398312]\n",
      " [-0.57152988  0.78189725 -0.53048285 -0.95431588  0.05423087  0.02829603\n",
      "  -1.24757446 -0.55933506  0.34819331]\n",
      " [ 0.28561909 -0.24551464  0.50563538 -0.71408322  0.31257148 -0.46524065\n",
      "  -0.65422315  0.35338668  0.45972071]\n",
      " [ 0.78660882 -0.73080335  0.40224506  0.80779819 -0.09993676  0.02745916\n",
      "   1.18940771  0.03382968 -0.62158785]\n",
      " [-0.76443072  0.7404772  -0.39548459 -0.66855851  0.16414896 -0.02474353\n",
      "  -1.17983913 -0.43539255  0.43888891]\n",
      " [-0.01683193  0.17614646  0.35815895 -0.8147263  -0.00856988 -0.0114103\n",
      "  -0.88342222  0.12432633  0.17089064]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.25114529  1.01313178 -1.85660952 -0.10270132  1.11390373 -1.66528913\n",
      "  -0.46654755]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:41 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.55831732]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 41 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 6.32770352e-01 -5.93030280e-01  2.98573040e-01  8.06825371e-01\n",
      "  -1.31452969e-01 -1.31065370e-01  9.75413483e-01  3.47716139e-01\n",
      "  -2.78094253e-01]\n",
      " [-5.57073098e-01  7.81897247e-01 -5.30482851e-01 -9.39859097e-01\n",
      "   5.42308711e-02  4.27528147e-02 -1.24757446e+00 -5.59335058e-01\n",
      "   3.62650097e-01]\n",
      " [ 2.92747056e-01 -2.45514640e-01  5.05635378e-01 -7.06955245e-01\n",
      "   3.12571478e-01 -4.58112681e-01 -6.54223148e-01  3.53386680e-01\n",
      "   4.66848677e-01]\n",
      " [ 7.73071975e-01 -7.30803349e-01  4.02245065e-01  7.94261345e-01\n",
      "  -9.99367611e-02  1.39223185e-02  1.18940771e+00  3.38296834e-02\n",
      "  -6.35124692e-01]\n",
      " [-7.50761364e-01  7.40477197e-01 -3.95484586e-01 -6.54889155e-01\n",
      "   1.64148959e-01 -1.10741804e-02 -1.17983913e+00 -4.35392554e-01\n",
      "   4.52558268e-01]\n",
      " [-6.47915326e-03  1.76146455e-01  3.58158951e-01 -8.04373521e-01\n",
      "  -8.56988475e-03 -1.05752219e-03 -8.83422224e-01  1.24326334e-01\n",
      "   1.81243421e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.31998556  0.96165344 -1.87317295 -0.12976762  1.06357369 -1.68354919\n",
      "  -0.48981775]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:41 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.70250538]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 41 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.63967886 -0.58612178  0.29857304  0.81373387 -0.13145297 -0.12415687\n",
      "   0.98232199  0.34771614 -0.27809425]\n",
      " [-0.56372918  0.77524117 -0.53048285 -0.94651518  0.05423087  0.03609674\n",
      "  -1.25423054 -0.55933506  0.3626501 ]\n",
      " [ 0.28590634 -0.25235536  0.50563538 -0.71379597  0.31257148 -0.4649534\n",
      "  -0.66106387  0.35338668  0.46684868]\n",
      " [ 0.77952979 -0.72434554  0.40224506  0.80071916 -0.09993676  0.02038013\n",
      "   1.19586553  0.03382968 -0.63512469]\n",
      " [-0.75750602  0.73373254 -0.39548459 -0.66163381  0.16414896 -0.01781883\n",
      "  -1.18658378 -0.43539255  0.45255827]\n",
      " [-0.01343717  0.16918843  0.35815895 -0.81133154 -0.00856988 -0.00801554\n",
      "  -0.89038024  0.12432633  0.18124342]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.28889862  0.98790152 -1.86919763 -0.12525111  1.09108311 -1.67934749\n",
      "  -0.48423511]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:41 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.72546316]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 41 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.64561768 -0.58018295  0.29857304  0.8196727  -0.13145297 -0.12415687\n",
      "   0.98826081  0.34771614 -0.27809425]\n",
      " [-0.56948611  0.76948424 -0.53048285 -0.95227211  0.05423087  0.03609674\n",
      "  -1.25998747 -0.55933506  0.3626501 ]\n",
      " [ 0.27988052 -0.25838117  0.50563538 -0.71982178  0.31257148 -0.4649534\n",
      "  -0.66708968  0.35338668  0.46684868]\n",
      " [ 0.78519002 -0.7186853   0.40224506  0.80637939 -0.09993676  0.02038013\n",
      "   1.20152576  0.03382968 -0.63512469]\n",
      " [-0.7634192   0.72781936 -0.39548459 -0.66754699  0.16414896 -0.01781883\n",
      "  -1.19249696 -0.43539255  0.45255827]\n",
      " [-0.01955764  0.16306797  0.35815895 -0.81745201 -0.00856988 -0.00801554\n",
      "  -0.89650071  0.12432633  0.18124342]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.26155937  1.01152486 -1.86590768 -0.11958382  1.11530914 -1.67570285\n",
      "  -0.47943242]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:41 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.74987155]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 41 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.65075059 -0.58018295  0.30370595  0.8196727  -0.13145297 -0.11902396\n",
      "   0.99339372  0.34771614 -0.27809425]\n",
      " [-0.57391333  0.76948424 -0.53491008 -0.95227211  0.05423087  0.03166951\n",
      "  -1.2644147  -0.55933506  0.3626501 ]\n",
      " [ 0.27790816 -0.25838117  0.50366301 -0.71982178  0.31257148 -0.46692577\n",
      "  -0.66906205  0.35338668  0.46684868]\n",
      " [ 0.78946629 -0.7186853   0.40652133  0.80637939 -0.09993676  0.0246564\n",
      "   1.20580203  0.03382968 -0.63512469]\n",
      " [-0.76776699  0.72781936 -0.39983238 -0.66754699  0.16414896 -0.02216663\n",
      "  -1.19684476 -0.43539255  0.45255827]\n",
      " [-0.02262424  0.16306797  0.35509235 -0.81745201 -0.00856988 -0.01108214\n",
      "  -0.89956731  0.12432633  0.18124342]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.2381018   1.03167857 -1.86381606 -0.10986711  1.13683233 -1.67369605\n",
      "  -0.47093655]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:41 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.34514143]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 42 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.64530755 -0.58562599  0.30370595  0.81422966 -0.13145297 -0.11902396\n",
      "   0.99339372  0.34771614 -0.28353729]\n",
      " [-0.5702163   0.77318127 -0.53491008 -0.94857507  0.05423087  0.03166951\n",
      "  -1.2644147  -0.55933506  0.36634713]\n",
      " [ 0.28015377 -0.25613556  0.50366301 -0.71757617  0.31257148 -0.46692577\n",
      "  -0.66906205  0.35338668  0.46909429]\n",
      " [ 0.78714041 -0.72101118  0.40652133  0.80405352 -0.09993676  0.0246564\n",
      "   1.20580203  0.03382968 -0.63745057]\n",
      " [-0.76532105  0.73026531 -0.39983238 -0.66510105  0.16414896 -0.02216663\n",
      "  -1.19684476 -0.43539255  0.45500421]\n",
      " [-0.01807529  0.16761691  0.35509235 -0.81290306 -0.00856988 -0.01108214\n",
      "  -0.89956731  0.12432633  0.18579237]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.27710603  1.00638705 -1.87952472 -0.12710316  1.11498156 -1.69072564\n",
      "  -0.48570108]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:42 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.74563525]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 42 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.65044572 -0.58562599  0.30884413  0.81422966 -0.13145297 -0.11902396\n",
      "   0.99853189  0.34771614 -0.28353729]\n",
      " [-0.57468663  0.77318127 -0.53938041 -0.94857507  0.05423087  0.03166951\n",
      "  -1.26888503 -0.55933506  0.36634713]\n",
      " [ 0.28084352 -0.25613556  0.50435276 -0.71757617  0.31257148 -0.46692577\n",
      "  -0.6683723   0.35338668  0.46909429]\n",
      " [ 0.79155598 -0.72101118  0.4109369   0.80405352 -0.09993676  0.0246564\n",
      "   1.2102176   0.03382968 -0.63745057]\n",
      " [-0.76980508  0.73026531 -0.40431641 -0.66510105  0.16414896 -0.02216663\n",
      "  -1.20132879 -0.43539255  0.45500421]\n",
      " [-0.02121288  0.16761691  0.35195477 -0.81290306 -0.00856988 -0.01108214\n",
      "  -0.9027049   0.12432633  0.18579237]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.2529842   1.0274844  -1.8774617  -0.11435098  1.13709613 -1.68864835\n",
      "  -0.47694587]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:42 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.66747955]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 42 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.6340451  -0.60202661  0.29244351  0.81422966 -0.13145297 -0.11902396\n",
      "   0.98213127  0.34771614 -0.28353729]\n",
      " [-0.55812895  0.78973895 -0.52282273 -0.94857507  0.05423087  0.03166951\n",
      "  -1.25232735 -0.55933506  0.36634713]\n",
      " [ 0.28341088 -0.25356821  0.50692012 -0.71757617  0.31257148 -0.46692577\n",
      "  -0.66580494  0.35338668  0.46909429]\n",
      " [ 0.77509571 -0.73747145  0.39447663  0.80405352 -0.09993676  0.0246564\n",
      "   1.19375732  0.03382968 -0.63745057]\n",
      " [-0.75328089  0.7467895  -0.38779222 -0.66510105  0.16414896 -0.02216663\n",
      "  -1.1848046  -0.43539255  0.45500421]\n",
      " [-0.01402293  0.17480686  0.35914472 -0.81290306 -0.00856988 -0.01108214\n",
      "  -0.89551495  0.12432633  0.18579237]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.32705794  0.9683932  -1.8898039  -0.14881219  1.07454474 -1.70063057\n",
      "  -0.50659527]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:42 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.55807787]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 42 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.64590363 -0.60202661  0.30430203  0.81422966 -0.13145297 -0.11902396\n",
      "   0.98213127  0.35957467 -0.28353729]\n",
      " [-0.57028955  0.78973895 -0.53498333 -0.94857507  0.05423087  0.03166951\n",
      "  -1.25232735 -0.57149566  0.36634713]\n",
      " [ 0.29483223 -0.25356821  0.51834147 -0.71757617  0.31257148 -0.46692577\n",
      "  -0.66580494  0.36480803  0.46909429]\n",
      " [ 0.78674059 -0.73747145  0.40612151  0.80405352 -0.09993676  0.0246564\n",
      "   1.19375732  0.04547456 -0.63745057]\n",
      " [-0.76547608  0.7467895  -0.39998741 -0.66510105  0.16414896 -0.02216663\n",
      "  -1.1848046  -0.44758775  0.45500421]\n",
      " [-0.00796711  0.17480686  0.36520053 -0.81290306 -0.00856988 -0.01108214\n",
      "  -0.89551495  0.13038215  0.18579237]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.27256299  1.01097892 -1.8809525  -0.10748531  1.11645841 -1.69129577\n",
      "  -0.47306708]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:42 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.54305747]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 42 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.65791922 -0.60202661  0.30430203  0.82624524 -0.13145297 -0.11902396\n",
      "   0.98213127  0.35957467 -0.27152171]\n",
      " [-0.58220877  0.78973895 -0.53498333 -0.9604943   0.05423087  0.03166951\n",
      "  -1.25232735 -0.57149566  0.35442791]\n",
      " [ 0.29548882 -0.25356821  0.51834147 -0.71691957  0.31257148 -0.46692577\n",
      "  -0.66580494  0.36480803  0.46975089]\n",
      " [ 0.79759421 -0.73747145  0.40612151  0.81490714 -0.09993676  0.0246564\n",
      "   1.19375732  0.04547456 -0.62659694]\n",
      " [-0.77647272  0.7467895  -0.39998741 -0.67609768  0.16414896 -0.02216663\n",
      "  -1.1848046  -0.44758775  0.44400757]\n",
      " [-0.01611851  0.17480686  0.36520053 -0.82105446 -0.00856988 -0.01108214\n",
      "  -0.89551495  0.13038215  0.17764097]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.21586874  1.05431233 -1.86734421 -0.07848135  1.15737959 -1.67577453\n",
      "  -0.45343045]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:42 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.27566058]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 42 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.65698836 -0.60202661  0.30430203  0.82624524 -0.13238382 -0.11995481\n",
      "   0.98213127  0.35957467 -0.27245256]\n",
      " [-0.5812375   0.78973895 -0.53498333 -0.9604943   0.05520214  0.03264078\n",
      "  -1.25232735 -0.57149566  0.35539918]\n",
      " [ 0.29165477 -0.25356821  0.51834147 -0.71691957  0.30873742 -0.47075982\n",
      "  -0.66580494  0.36480803  0.46591683]\n",
      " [ 0.79693717 -0.73747145  0.40612151  0.81490714 -0.10059381  0.02399935\n",
      "   1.19375732  0.04547456 -0.62725399]\n",
      " [-0.77517397  0.7467895  -0.39998741 -0.67609768  0.1654477  -0.02086789\n",
      "  -1.1848046  -0.44758775  0.44530632]\n",
      " [-0.01708972  0.17480686  0.36520053 -0.82105446 -0.00954109 -0.01205334\n",
      "  -0.89551495  0.13038215  0.17666976]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.24338957  1.03961819 -1.88013009 -0.09631878  1.14296113 -1.68822833\n",
      "  -0.46816533]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:42 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.1102882]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 42 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.6575901  -0.60142488  0.30430203  0.82624524 -0.13178209 -0.11935307\n",
      "   0.98213127  0.35957467 -0.27185083]\n",
      " [-0.58203177  0.78894468 -0.53498333 -0.9604943   0.05440787  0.03184651\n",
      "  -1.25232735 -0.57149566  0.35460491]\n",
      " [ 0.29120542 -0.25401755  0.51834147 -0.71691957  0.30828807 -0.47120917\n",
      "  -0.66580494  0.36480803  0.46546748]\n",
      " [ 0.79772428 -0.73668434  0.40612151  0.81490714 -0.09980669  0.02478646\n",
      "   1.19375732  0.04547456 -0.62646687]\n",
      " [-0.77587669  0.74608679 -0.39998741 -0.67609768  0.16474499 -0.0215706\n",
      "  -1.1848046  -0.44758775  0.44460361]\n",
      " [-0.01750266  0.17439392  0.36520053 -0.82105446 -0.00995403 -0.01246629\n",
      "  -0.89551495  0.13038215  0.17625682]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.24880057  1.03753682 -1.88368729 -0.09948243  1.14109835 -1.69167405\n",
      "  -0.47129054]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:42 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.55045036]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 42 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 6.46066681e-01 -6.12948295e-01  3.04302034e-01  8.26245242e-01\n",
      "  -1.43305505e-01 -1.30876494e-01  9.70607855e-01  3.59574668e-01\n",
      "  -2.71850825e-01]\n",
      " [-5.68947850e-01  8.02028601e-01 -5.34983327e-01 -9.60494295e-01\n",
      "   6.74917935e-02  4.49304300e-02 -1.23924342e+00 -5.71495656e-01\n",
      "   3.54604907e-01]\n",
      " [ 3.02773890e-01 -2.42449084e-01  5.18341470e-01 -7.16919569e-01\n",
      "   3.19856545e-01 -4.59640700e-01 -6.54236471e-01  3.64808031e-01\n",
      "   4.65467482e-01]\n",
      " [ 7.83275496e-01 -7.51133125e-01  4.06121509e-01  8.14907140e-01\n",
      "  -1.14255479e-01  1.03376796e-02  1.17930854e+00  4.54745641e-02\n",
      "  -6.26466874e-01]\n",
      " [-7.62009470e-01  7.59954001e-01 -3.99987413e-01 -6.76097682e-01\n",
      "   1.78612207e-01 -7.70338179e-03 -1.17093738e+00 -4.47587748e-01\n",
      "   4.44603606e-01]\n",
      " [-6.25374747e-03  1.85642832e-01  3.65200534e-01 -8.21054465e-01\n",
      "   1.29488179e-03 -1.21737407e-03 -8.84266032e-01  1.30382152e-01\n",
      "   1.76256816e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.31690635  0.99073306 -1.90255575 -0.12072044  1.08900372 -1.70905009\n",
      "  -0.49297676]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:42 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.78314304]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 42 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 6.49375979e-01 -6.12948295e-01  3.04302034e-01  8.29554540e-01\n",
      "  -1.43305505e-01 -1.30876494e-01  9.73917153e-01  3.59574668e-01\n",
      "  -2.71850825e-01]\n",
      " [-5.71780213e-01  8.02028601e-01 -5.34983327e-01 -9.63326658e-01\n",
      "   6.74917935e-02  4.49304300e-02 -1.24207579e+00 -5.71495656e-01\n",
      "   3.54604907e-01]\n",
      " [ 2.99029549e-01 -2.42449084e-01  5.18341470e-01 -7.20663910e-01\n",
      "   3.19856545e-01 -4.59640700e-01 -6.57980812e-01  3.64808031e-01\n",
      "   4.65467482e-01]\n",
      " [ 7.86094875e-01 -7.51133125e-01  4.06121509e-01  8.17726519e-01\n",
      "  -1.14255479e-01  1.03376796e-02  1.18212792e+00  4.54745641e-02\n",
      "  -6.26466874e-01]\n",
      " [-7.65077480e-01  7.59954001e-01 -3.99987413e-01 -6.79165692e-01\n",
      "   1.78612207e-01 -7.70338179e-03 -1.17400539e+00 -4.47587748e-01\n",
      "   4.44603606e-01]\n",
      " [-1.03372659e-02  1.85642832e-01  3.65200534e-01 -8.25137983e-01\n",
      "   1.29488179e-03 -1.21737407e-03 -8.88349550e-01  1.30382152e-01\n",
      "   1.76256816e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.29849194  1.00767509 -1.90146857 -0.11601167  1.10633991 -1.70778762\n",
      "  -0.4901601 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:42 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.67513433]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 42 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.65723788 -0.61294829  0.30430203  0.82955454 -0.1354436  -0.12301459\n",
      "   0.98177906  0.35957467 -0.27185083]\n",
      " [-0.57968909  0.8020286  -0.53498333 -0.96332666  0.05958292  0.03702155\n",
      "  -1.24998466 -0.57149566  0.35460491]\n",
      " [ 0.29485272 -0.24244908  0.51834147 -0.72066391  0.31567971 -0.46381753\n",
      "  -0.66215764  0.36480803  0.46546748]\n",
      " [ 0.79381209 -0.75113313  0.40612151  0.81772652 -0.10653827  0.01805489\n",
      "   1.18984513  0.04547456 -0.62646687]\n",
      " [-0.77292143  0.759954   -0.39998741 -0.67916569  0.17076826 -0.01554733\n",
      "  -1.18184934 -0.44758775  0.44460361]\n",
      " [-0.01691999  0.18564283  0.36520053 -0.82513798 -0.00528785 -0.0078001\n",
      "  -0.89493228  0.13038215  0.17625682]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.26286587  1.03596163 -1.89597225 -0.1025508   1.13718489 -1.70259443\n",
      "  -0.47985218]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:42 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.55886075]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 42 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.6430866  -0.61294829  0.30430203  0.81540326 -0.1354436  -0.13716587\n",
      "   0.98177906  0.35957467 -0.28600211]\n",
      " [-0.56521167  0.8020286  -0.53498333 -0.94884924  0.05958292  0.05149897\n",
      "  -1.24998466 -0.57149566  0.36908233]\n",
      " [ 0.30183882 -0.24244908  0.51834147 -0.71367781  0.31567971 -0.45683143\n",
      "  -0.66215764  0.36480803  0.47245358]\n",
      " [ 0.78024489 -0.75113313  0.40612151  0.80415932 -0.10653827  0.00448769\n",
      "   1.18984513  0.04547456 -0.64003407]\n",
      " [-0.75921311  0.759954   -0.39998741 -0.66545737  0.17076826 -0.00183901\n",
      "  -1.18184934 -0.44758775  0.45831193]\n",
      " [-0.00654146  0.18564283  0.36520053 -0.81475945 -0.00528785  0.00257843\n",
      "  -0.89493228  0.13038215  0.18663535]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.33175536  0.98438101 -1.91252194 -0.12979806  1.08678032 -1.72081139\n",
      "  -0.50311554]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:42 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.70483204]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 42 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.64991049 -0.6061244   0.30430203  0.82222715 -0.1354436  -0.13034198\n",
      "   0.98860295  0.35957467 -0.28600211]\n",
      " [-0.57179933  0.79544094 -0.53498333 -0.9554369   0.05958292  0.04491131\n",
      "  -1.25657232 -0.57149566  0.36908233]\n",
      " [ 0.29508357 -0.24920433  0.51834147 -0.72043305  0.31567971 -0.46358668\n",
      "  -0.66891289  0.36480803  0.47245358]\n",
      " [ 0.78664482 -0.74473319  0.40612151  0.81055925 -0.10653827  0.01088762\n",
      "   1.19624506  0.04547456 -0.64003407]\n",
      " [-0.76588434  0.75328277 -0.39998741 -0.6721286   0.17076826 -0.00851024\n",
      "  -1.18852057 -0.44758775  0.45831193]\n",
      " [-0.01341462  0.17876967  0.36520053 -0.82163261 -0.00528785 -0.00429473\n",
      "  -0.90180544  0.13038215  0.18663535]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.30105142  1.01030264 -1.90856351 -0.12534184  1.11391232 -1.7166338\n",
      "  -0.49764149]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:42 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.72875487]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 42 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.65572856 -0.60030633  0.30430203  0.82804522 -0.1354436  -0.13034198\n",
      "   0.99442102  0.35957467 -0.28600211]\n",
      " [-0.57744579  0.78979448 -0.53498333 -0.96108336  0.05958292  0.04491131\n",
      "  -1.26221878 -0.57149566  0.36908233]\n",
      " [ 0.28917267 -0.25511523  0.51834147 -0.72634395  0.31567971 -0.46358668\n",
      "  -0.67482379  0.36480803  0.47245358]\n",
      " [ 0.79220002 -0.73917799  0.40612151  0.81611445 -0.10653827  0.01088762\n",
      "   1.20180026  0.04547456 -0.64003407]\n",
      " [-0.77168131  0.7474858  -0.39998741 -0.67792557  0.17076826 -0.00851024\n",
      "  -1.19431753 -0.44758775  0.45831193]\n",
      " [-0.01941586  0.17276843  0.36520053 -0.82763385 -0.00528785 -0.00429473\n",
      "  -0.90780668  0.13038215  0.18663535]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.27424274  1.03348325 -1.90533495 -0.11979404  1.13765992 -1.7130638\n",
      "  -0.49297888]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:42 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.753288]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 42 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.66073152 -0.60030633  0.309305    0.82804522 -0.1354436  -0.12533902\n",
      "   0.99942398  0.35957467 -0.28600211]\n",
      " [-0.58176256  0.78979448 -0.5393001  -0.96108336  0.05958292  0.04059454\n",
      "  -1.26653555 -0.57149566  0.36908233]\n",
      " [ 0.28732719 -0.25511523  0.51649599 -0.72634395  0.31567971 -0.46543216\n",
      "  -0.67666927  0.36480803  0.47245358]\n",
      " [ 0.7963763  -0.73917799  0.41029778  0.81611445 -0.10653827  0.0150639\n",
      "   1.20597654  0.04547456 -0.64003407]\n",
      " [-0.77592128  0.7474858  -0.40422739 -0.67792557  0.17076826 -0.01275022\n",
      "  -1.19855751 -0.44758775  0.45831193]\n",
      " [-0.02241465  0.17276843  0.36220175 -0.82763385 -0.00528785 -0.00729351\n",
      "  -0.91080547  0.13038215  0.18663535]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.25131762  1.05322312 -1.90330172 -0.11021082  1.15869745 -1.71111199\n",
      "  -0.48467799]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:42 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.33663938]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 43 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.6555543  -0.60548356  0.309305    0.82286799 -0.1354436  -0.12533902\n",
      "   0.99942398  0.35957467 -0.29117933]\n",
      " [-0.57828419  0.79327285 -0.5393001  -0.95760499  0.05958292  0.04059454\n",
      "  -1.26653555 -0.57149566  0.3725607 ]\n",
      " [ 0.28938492 -0.2530575   0.51649599 -0.72428622  0.31567971 -0.46543216\n",
      "  -0.67666927  0.36480803  0.47451131]\n",
      " [ 0.79421373 -0.74134056  0.41029778  0.81395189 -0.10653827  0.0150639\n",
      "   1.20597654  0.04547456 -0.64219664]\n",
      " [-0.77362585  0.74978123 -0.40422739 -0.67563014  0.17076826 -0.01275022\n",
      "  -1.19855751 -0.44758775  0.46060736]\n",
      " [-0.01807077  0.17711231  0.36220175 -0.82328998 -0.00528785 -0.00729351\n",
      "  -0.91080547  0.13038215  0.19097923]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.28890565  1.02893296 -1.91853126 -0.12693021  1.1377212  -1.72758696\n",
      "  -0.49895173]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:43 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.75036261]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 43 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.6605086  -0.60548356  0.31425929  0.82286799 -0.1354436  -0.12533902\n",
      "   1.00437828  0.35957467 -0.29117933]\n",
      " [-0.58259141  0.79327285 -0.54360733 -0.95760499  0.05958292  0.04059454\n",
      "  -1.27084278 -0.57149566  0.3725607 ]\n",
      " [ 0.29013705 -0.2530575   0.51724812 -0.72428622  0.31567971 -0.46543216\n",
      "  -0.67591714  0.36480803  0.47451131]\n",
      " [ 0.79847396 -0.74134056  0.41455801  0.81395189 -0.10653827  0.0150639\n",
      "   1.21023677  0.04547456 -0.64219664]\n",
      " [-0.77794673  0.74978123 -0.40854827 -0.67563014  0.17076826 -0.01275022\n",
      "  -1.20287839 -0.44758775  0.46060736]\n",
      " [-0.0211308   0.17711231  0.35914172 -0.82328998 -0.00528785 -0.00729351\n",
      "  -0.9138655   0.13038215  0.19097923]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.26552479  1.04943787 -1.91655811 -0.11448556  1.15917603 -1.72559984\n",
      "  -0.49048775]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:43 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.66755097]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 43 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.6441048  -0.62188736  0.2978555   0.82286799 -0.1354436  -0.12533902\n",
      "   0.98797448  0.35957467 -0.29117933]\n",
      " [-0.56602942  0.80983485 -0.52704533 -0.95760499  0.05958292  0.04059454\n",
      "  -1.25428079 -0.57149566  0.3725607 ]\n",
      " [ 0.29238039 -0.25081415  0.51949146 -0.72428622  0.31567971 -0.46543216\n",
      "  -0.6736738   0.36480803  0.47451131]\n",
      " [ 0.78199845 -0.75781607  0.3980825   0.81395189 -0.10653827  0.0150639\n",
      "   1.19376125  0.04547456 -0.64219664]\n",
      " [-0.76141637  0.76631159 -0.39201791 -0.67563014  0.17076826 -0.01275022\n",
      "  -1.18634803 -0.44758775  0.46060736]\n",
      " [-0.01403263  0.18421048  0.36623989 -0.82328998 -0.00528785 -0.00729351\n",
      "  -0.90676732  0.13038215  0.19097923]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.33959847  0.9903292  -1.92896216 -0.14927353  1.09672009 -1.73763834\n",
      "  -0.52023682]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:43 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.56273611]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 43 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.65587859 -0.62188736  0.30962929  0.82286799 -0.1354436  -0.12533902\n",
      "   0.98797448  0.37134846 -0.29117933]\n",
      " [-0.57801304  0.80983485 -0.53902895 -0.95760499  0.05958292  0.04059454\n",
      "  -1.25428079 -0.58347928  0.3725607 ]\n",
      " [ 0.30378236 -0.25081415  0.53089342 -0.72428622  0.31567971 -0.46543216\n",
      "  -0.6736738   0.37621     0.47451131]\n",
      " [ 0.79356614 -0.75781607  0.40965019  0.81395189 -0.10653827  0.0150639\n",
      "   1.19376125  0.05704225 -0.64219664]\n",
      " [-0.7734462   0.76631159 -0.40404773 -0.67563014  0.17076826 -0.01275022\n",
      "  -1.18634803 -0.45961757  0.46060736]\n",
      " [-0.0079058   0.18421048  0.37236672 -0.82328998 -0.00528785 -0.00729351\n",
      "  -0.90676732  0.13650898  0.19097923]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.28580098  1.03261904 -1.92040032 -0.1081534   1.13830769 -1.72860903\n",
      "  -0.48697065]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:43 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.54490738]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 43 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.66787913 -0.62188736  0.30962929  0.83486853 -0.1354436  -0.12533902\n",
      "   0.98797448  0.37134846 -0.2791788 ]\n",
      " [-0.58991905  0.80983485 -0.53902895 -0.96951099  0.05958292  0.04059454\n",
      "  -1.25428079 -0.58347928  0.36065469]\n",
      " [ 0.30454368 -0.25081415  0.53089342 -0.7235249   0.31567971 -0.46543216\n",
      "  -0.6736738   0.37621     0.47527264]\n",
      " [ 0.80444618 -0.75781607  0.40965019  0.82483193 -0.10653827  0.0150639\n",
      "   1.19376125  0.05704225 -0.6313166 ]\n",
      " [-0.78447093  0.76631159 -0.40404773 -0.68665487  0.17076826 -0.01275022\n",
      "  -1.18634803 -0.45961757  0.44958262]\n",
      " [-0.01607158  0.18421048  0.37236672 -0.83145575 -0.00528785 -0.00729351\n",
      "  -0.90676732  0.13650898  0.18281345]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.22937329  1.0758599  -1.9069642  -0.07917786  1.17917183 -1.71330502\n",
      "  -0.46749195]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:43 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.26503716]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 43 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.66705699 -0.62188736  0.30962929  0.83486853 -0.13626574 -0.12616116\n",
      "   0.98797448  0.37134846 -0.28000093]\n",
      " [-0.58908946  0.80983485 -0.53902895 -0.96951099  0.06041251  0.04142413\n",
      "  -1.25428079 -0.58347928  0.36148428]\n",
      " [ 0.30085589 -0.25081415  0.53089342 -0.7235249   0.31199192 -0.46911995\n",
      "  -0.6736738   0.37621     0.47158484]\n",
      " [ 0.8039201  -0.75781607  0.40965019  0.82483193 -0.10706434  0.01453782\n",
      "   1.19376125  0.05704225 -0.63184267]\n",
      " [-0.78333839  0.76631159 -0.40404773 -0.68665487  0.17190079 -0.01161768\n",
      "  -1.18634803 -0.45961757  0.45071516]\n",
      " [-0.01706055  0.18421048  0.37236672 -0.83145575 -0.00627682 -0.00828249\n",
      "  -0.90676732  0.13650898  0.18182448]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.25518691  1.06212871 -1.91903911 -0.09602135  1.16573836 -1.72507338\n",
      "  -0.48139165]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:43 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.10108729]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 43 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.66759406 -0.62135029  0.30962929  0.83486853 -0.13572867 -0.12562409\n",
      "   0.98797448  0.37134846 -0.27946387]\n",
      " [-0.58978974  0.80913457 -0.53902895 -0.96951099  0.05971223  0.04072385\n",
      "  -1.25428079 -0.58347928  0.360784  ]\n",
      " [ 0.30045097 -0.25121908  0.53089342 -0.7235249   0.311587   -0.46952487\n",
      "  -0.6736738   0.37621     0.47117992]\n",
      " [ 0.80461577 -0.7571204   0.40965019  0.82483193 -0.10636868  0.01523349\n",
      "   1.19376125  0.05704225 -0.63114701]\n",
      " [-0.78396361  0.76568637 -0.40404773 -0.68665487  0.17127558 -0.0122429\n",
      "  -1.18634803 -0.45961757  0.45008994]\n",
      " [-0.01743399  0.18383704  0.37236672 -0.83145575 -0.00665026 -0.00865593\n",
      "  -0.90676732  0.13650898  0.18145104]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.25977975  1.06039177 -1.92209172 -0.09873171  1.16419219 -1.72803243\n",
      "  -0.48406851]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:43 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.54308469]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 43 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 6.56340653e-01 -6.32603698e-01  3.09629290e-01  8.34868529e-01\n",
      "  -1.46982077e-01 -1.36877497e-01  9.76721073e-01  3.71348463e-01\n",
      "  -2.79463867e-01]\n",
      " [-5.77039877e-01  8.21884429e-01 -5.39028954e-01 -9.69510990e-01\n",
      "   7.24620877e-02  5.34737132e-02 -1.24153092e+00 -5.83479277e-01\n",
      "   3.60784001e-01]\n",
      " [ 3.11802839e-01 -2.39867202e-01  5.30893423e-01 -7.23524899e-01\n",
      "   3.22938871e-01 -4.58172997e-01 -6.62321926e-01  3.76209995e-01\n",
      "   4.71179922e-01]\n",
      " [ 7.90461529e-01 -7.71274644e-01  4.09650188e-01  8.24831930e-01\n",
      "  -1.20522916e-01  1.07924632e-03  1.17960701e+00  5.70422541e-02\n",
      "  -6.31147007e-01]\n",
      " [-7.70405888e-01  7.79244096e-01 -4.04047733e-01 -6.86654872e-01\n",
      "   1.84833300e-01  1.31482309e-03 -1.17279031e+00 -4.59617570e-01\n",
      "   4.50089940e-01]\n",
      " [-6.36176105e-03  1.94909268e-01  3.72366717e-01 -8.31455749e-01\n",
      "   4.42196905e-03  2.41630031e-03 -8.95695095e-01  1.36508977e-01\n",
      "   1.81451042e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.32716127  1.01429334 -1.94109416 -0.11987652  1.11301369 -1.7455565\n",
      "  -0.50560277]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:43 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.78925551]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 43 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 6.59456363e-01 -6.32603698e-01  3.09629290e-01  8.37984239e-01\n",
      "  -1.46982077e-01 -1.36877497e-01  9.79836784e-01  3.71348463e-01\n",
      "  -2.79463867e-01]\n",
      " [-5.79708487e-01  8.21884429e-01 -5.39028954e-01 -9.72179600e-01\n",
      "   7.24620877e-02  5.34737132e-02 -1.24419953e+00 -5.83479277e-01\n",
      "   3.60784001e-01]\n",
      " [ 3.08230036e-01 -2.39867202e-01  5.30893423e-01 -7.27097702e-01\n",
      "   3.22938871e-01 -4.58172997e-01 -6.65894729e-01  3.76209995e-01\n",
      "   4.71179922e-01]\n",
      " [ 7.93120581e-01 -7.71274644e-01  4.09650188e-01  8.27490982e-01\n",
      "  -1.20522916e-01  1.07924632e-03  1.18226606e+00  5.70422541e-02\n",
      "  -6.31147007e-01]\n",
      " [-7.73296831e-01  7.79244096e-01 -4.04047733e-01 -6.89545815e-01\n",
      "   1.84833300e-01  1.31482309e-03 -1.17568125e+00 -4.59617570e-01\n",
      "   4.50089940e-01]\n",
      " [-1.02385138e-02  1.94909268e-01  3.72366717e-01 -8.35332502e-01\n",
      "   4.42196905e-03  2.41630031e-03 -8.99571848e-01  1.36508977e-01\n",
      "   1.81451042e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.30963457  1.03045055 -1.94007811 -0.11541361  1.12953084 -1.74437798\n",
      "  -0.50297133]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:43 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.67742498]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 43 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.66724149 -0.6326037   0.30962929  0.83798424 -0.13919695 -0.12909237\n",
      "   0.98762191  0.37134846 -0.27946387]\n",
      " [-0.58753546  0.82188443 -0.53902895 -0.9721796   0.06463511  0.04564674\n",
      "  -1.25202651 -0.58347928  0.360784  ]\n",
      " [ 0.30414049 -0.2398672   0.53089342 -0.7270977   0.31884932 -0.46226255\n",
      "  -0.66998428  0.37621     0.47117992]\n",
      " [ 0.8007675  -0.77127464  0.40965019  0.82749098 -0.112876    0.00872616\n",
      "   1.18991298  0.05704225 -0.63114701]\n",
      " [-0.78106266  0.7792441  -0.40404773 -0.68954582  0.17706747 -0.006451\n",
      "  -1.18344708 -0.45961757  0.45008994]\n",
      " [-0.01677033  0.19490927  0.37236672 -0.8353325  -0.00210985 -0.00411552\n",
      "  -0.90610367  0.13650898  0.18145104]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.27438997  1.0584706  -1.9346247  -0.10204847  1.1600112  -1.73921689\n",
      "  -0.49280537]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:43 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.55943432]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 43 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.65305165 -0.6326037   0.30962929  0.8237944  -0.13919695 -0.14328221\n",
      "   0.98762191  0.37134846 -0.29365371]\n",
      " [-0.57303762  0.82188443 -0.53902895 -0.95768175  0.06463511  0.06014458\n",
      "  -1.25202651 -0.58347928  0.37528185]\n",
      " [ 0.31097929 -0.2398672   0.53089342 -0.7202589   0.31884932 -0.45542375\n",
      "  -0.66998428  0.37621     0.47801872]\n",
      " [ 0.78717053 -0.77127464  0.40965019  0.81389401 -0.112876   -0.00487081\n",
      "   1.18991298  0.05704225 -0.64474398]\n",
      " [-0.7673168   0.7792441  -0.40404773 -0.67579996  0.17706747  0.00729485\n",
      "  -1.18344708 -0.45961757  0.4638358 ]\n",
      " [-0.00636997  0.19490927  0.37236672 -0.82493214 -0.00210985  0.00628484\n",
      "  -0.90610367  0.13650898  0.1918514 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.34333117  1.00678982 -1.95116299 -0.1294832   1.10953212 -1.75739476\n",
      "  -0.51606822]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:43 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.70697027]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 43 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 6.59798463e-01 -6.25856889e-01  3.09629290e-01  8.30541209e-01\n",
      "  -1.39196946e-01 -1.36535396e-01  9.94368723e-01  3.71348463e-01\n",
      "  -2.93653706e-01]\n",
      " [-5.79563694e-01  8.15358351e-01 -5.39028954e-01 -9.64207832e-01\n",
      "   6.46351133e-02  5.36185065e-02 -1.25855259e+00 -5.83479277e-01\n",
      "   3.75281847e-01]\n",
      " [ 3.04302609e-01 -2.46543879e-01  5.30893423e-01 -7.26935580e-01\n",
      "   3.18849322e-01 -4.62100425e-01 -6.76660954e-01  3.76209995e-01\n",
      "   4.78018721e-01]\n",
      " [ 7.93519162e-01 -7.64926009e-01  4.09650188e-01  8.20242647e-01\n",
      "  -1.12875999e-01  1.47782765e-03  1.19626162e+00  5.70422541e-02\n",
      "  -6.44743978e-01]\n",
      " [-7.73921887e-01  7.72639010e-01 -4.04047733e-01 -6.82405045e-01\n",
      "   1.77067473e-01  6.89766960e-04 -1.19005217e+00 -4.59617570e-01\n",
      "   4.63835797e-01]\n",
      " [-1.31649083e-02  1.88114330e-01  3.72366717e-01 -8.31727077e-01\n",
      "  -2.10984979e-03 -5.10094202e-04 -9.12898605e-01  1.36508977e-01\n",
      "   1.91851404e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.31297867  1.03240847 -1.94721633 -0.12508279  1.13631352 -1.753235\n",
      "  -0.51069319]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:43 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.73192272]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 43 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 6.65501510e-01 -6.20153843e-01  3.09629290e-01  8.36244255e-01\n",
      "  -1.39196946e-01 -1.36535396e-01  1.00007177e+00  3.71348463e-01\n",
      "  -2.93653706e-01]\n",
      " [-5.85104995e-01  8.09817049e-01 -5.39028954e-01 -9.69749134e-01\n",
      "   6.46351133e-02  5.36185065e-02 -1.26409389e+00 -5.83479277e-01\n",
      "   3.75281847e-01]\n",
      " [ 2.98501710e-01 -2.52344779e-01  5.30893423e-01 -7.32736480e-01\n",
      "   3.18849322e-01 -4.62100425e-01 -6.82461853e-01  3.76209995e-01\n",
      "   4.78018721e-01]\n",
      " [ 7.98974530e-01 -7.59470641e-01  4.09650188e-01  8.25698015e-01\n",
      "  -1.12875999e-01  1.47782765e-03  1.20171698e+00  5.70422541e-02\n",
      "  -6.44743978e-01]\n",
      " [-7.79608296e-01  7.66952602e-01 -4.04047733e-01 -6.88091453e-01\n",
      "   1.77067473e-01  6.89766960e-04 -1.19573857e+00 -4.59617570e-01\n",
      "   4.63835797e-01]\n",
      " [-1.90512953e-02  1.82227942e-01  3.72366717e-01 -8.37613464e-01\n",
      "  -2.10984979e-03 -5.10094202e-04 -9.18784992e-01  1.36508977e-01\n",
      "   1.91851404e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.2866787   1.05516224 -1.94404513 -0.11965049  1.15960086 -1.74973422\n",
      "  -0.50616269]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:43 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.7565874]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 43 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.67038024 -0.62015384  0.31450802  0.83624426 -0.13919695 -0.13165667\n",
      "   1.0049505   0.37134846 -0.29365371]\n",
      " [-0.58931738  0.80981705 -0.54324134 -0.96974913  0.06463511  0.04940612\n",
      "  -1.26830628 -0.58347928  0.37528185]\n",
      " [ 0.29677883 -0.25234478  0.52917054 -0.73273648  0.31884932 -0.46382331\n",
      "  -0.68418474  0.37621     0.47801872]\n",
      " [ 0.80305626 -0.75947064  0.41373192  0.82569802 -0.112876    0.00555956\n",
      "   1.20579872  0.05704225 -0.64474398]\n",
      " [-0.78374657  0.7669526  -0.408186   -0.68809145  0.17706747 -0.0034485\n",
      "  -1.19987685 -0.45961757  0.4638358 ]\n",
      " [-0.02198172  0.18222794  0.36943629 -0.83761346 -0.00210985 -0.00344052\n",
      "  -0.92171542  0.13650898  0.1918514 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.26426491  1.07450065 -1.94206606 -0.11019514  1.18017053 -1.74783328\n",
      "  -0.4980452 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:43 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.3282918]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 44 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.66546074 -0.62507334  0.31450802  0.83132475 -0.13919695 -0.13165667\n",
      "   1.0049505   0.37134846 -0.29857321]\n",
      " [-0.58604889  0.81308555 -0.54324134 -0.96648064  0.06463511  0.04940612\n",
      "  -1.26830628 -0.58347928  0.37855034]\n",
      " [ 0.29866085 -0.25046276  0.52917054 -0.73085446  0.31884932 -0.46382331\n",
      "  -0.68418474  0.37621     0.47990074]\n",
      " [ 0.80104975 -0.76147715  0.41373192  0.82369151 -0.112876    0.00555956\n",
      "   1.20579872  0.05704225 -0.64675049]\n",
      " [-0.78159664  0.76910253 -0.408186   -0.68594153  0.17706747 -0.0034485\n",
      "  -1.19987685 -0.45961757  0.46598572]\n",
      " [-0.01783726  0.1863724   0.36943629 -0.833469   -0.00210985 -0.00344052\n",
      "  -0.92171542  0.13650898  0.19599587]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.30046176  1.05118947 -1.95681927 -0.12639765  1.1600487  -1.7637609\n",
      "  -0.51183441]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:44 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.75497083]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 44 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.67023844 -0.62507334  0.31928572  0.83132475 -0.13919695 -0.13165667\n",
      "   1.0097282   0.37134846 -0.29857321]\n",
      " [-0.59020082  0.81308555 -0.54739327 -0.96648064  0.06463511  0.04940612\n",
      "  -1.27245821 -0.58347928  0.37855034]\n",
      " [ 0.29947056 -0.25046276  0.52998026 -0.73085446  0.31884932 -0.46382331\n",
      "  -0.68337502  0.37621     0.47990074]\n",
      " [ 0.8051618  -0.76147715  0.41784397  0.82369151 -0.112876    0.00555956\n",
      "   1.20991077  0.05704225 -0.64675049]\n",
      " [-0.78576227  0.76910253 -0.41235163 -0.68594153  0.17706747 -0.0034485\n",
      "  -1.20404247 -0.45961757  0.46598572]\n",
      " [-0.02081864  0.1863724   0.36645491 -0.833469   -0.00210985 -0.00344052\n",
      "  -0.9246968   0.13650898  0.19599587]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.2777978   1.07111573 -1.95493009 -0.11425316  1.1808629  -1.76185793\n",
      "  -0.50364796]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:44 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.66746425]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 44 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.65383351 -0.64147827  0.30288079  0.83132475 -0.13919695 -0.13165667\n",
      "   0.99332327  0.37134846 -0.29857321]\n",
      " [-0.57363438  0.82965198 -0.53082684 -0.96648064  0.06463511  0.04940612\n",
      "  -1.25589177 -0.58347928  0.37855034]\n",
      " [ 0.30139839 -0.24853493  0.53190808 -0.73085446  0.31884932 -0.46382331\n",
      "  -0.68144719  0.37621     0.47990074]\n",
      " [ 0.78867097 -0.77796798  0.40135314  0.82369151 -0.112876    0.00555956\n",
      "   1.19341994  0.05704225 -0.64675049]\n",
      " [-0.76922509  0.78563971 -0.39581445 -0.68594153  0.17706747 -0.0034485\n",
      "  -1.18750529 -0.45961757  0.46598572]\n",
      " [-0.01381996  0.19337108  0.37345359 -0.833469   -0.00210985 -0.00344052\n",
      "  -0.91769812  0.13650898  0.19599587]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.35187156  1.01200084 -1.96740455 -0.14935871  1.11851001 -1.7739625\n",
      "  -0.5335049 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:44 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.56743859]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 44 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.66550855 -0.64147827  0.31455583  0.83132475 -0.13919695 -0.13165667\n",
      "   0.99332327  0.3830235  -0.29857321]\n",
      " [-0.58543528  0.82965198 -0.54262773 -0.96648064  0.06463511  0.04940612\n",
      "  -1.25589177 -0.59528017  0.37855034]\n",
      " [ 0.31276241 -0.24853493  0.5432721  -0.73085446  0.31884932 -0.46382331\n",
      "  -0.68144719  0.38757402  0.47990074]\n",
      " [ 0.80014957 -0.77796798  0.41283174  0.82369151 -0.112876    0.00555956\n",
      "   1.19341994  0.06852086 -0.64675049]\n",
      " [-0.78108259  0.78563971 -0.40767196 -0.68594153  0.17706747 -0.0034485\n",
      "  -1.18750529 -0.47147507  0.46598572]\n",
      " [-0.00762449  0.19337108  0.37964906 -0.833469   -0.00210985 -0.00344052\n",
      "  -0.91769812  0.14270445  0.19599587]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.29878502  1.05396681 -1.95912056 -0.10847075  1.15974719 -1.76522639\n",
      "  -0.50050883]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:44 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.54689365]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 44 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.67748677 -0.64147827  0.31455583  0.84330297 -0.13919695 -0.13165667\n",
      "   0.99332327  0.3830235  -0.28659499]\n",
      " [-0.59732101  0.82965198 -0.54262773 -0.97836637  0.06463511  0.04940612\n",
      "  -1.25589177 -0.59528017  0.36666461]\n",
      " [ 0.31362907 -0.24853493  0.5432721  -0.7299878   0.31884932 -0.46382331\n",
      "  -0.68144719  0.38757402  0.4807674 ]\n",
      " [ 0.81104818 -0.77796798  0.41283174  0.83459011 -0.112876    0.00555956\n",
      "   1.19341994  0.06852086 -0.63585189]\n",
      " [-0.79212657  0.78563971 -0.40767196 -0.6969855   0.17706747 -0.0034485\n",
      "  -1.18750529 -0.47147507  0.45494175]\n",
      " [-0.01579817  0.19337108  0.37964906 -0.84164269 -0.00210985 -0.00344052\n",
      "  -0.91769812  0.14270445  0.18782218]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.24264492  1.09709448 -1.94585762 -0.07953349  1.20053468 -1.75013945\n",
      "  -0.4811913 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:44 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.25477067]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 44 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.67676357 -0.64147827  0.31455583  0.84330297 -0.13992014 -0.13237986\n",
      "   0.99332327  0.3830235  -0.28731818]\n",
      " [-0.5966183   0.82965198 -0.54262773 -0.97836637  0.06533783  0.05010883\n",
      "  -1.25589177 -0.59528017  0.36736732]\n",
      " [ 0.31008895 -0.24853493  0.5432721  -0.7299878   0.3153092  -0.46736343\n",
      "  -0.68144719  0.38757402  0.47722728]\n",
      " [ 0.81063822 -0.77796798  0.41283174  0.83459011 -0.11328596  0.0051496\n",
      "   1.19341994  0.06852086 -0.63626184]\n",
      " [-0.79114416  0.78563971 -0.40767196 -0.6969855   0.17804988 -0.0024661\n",
      "  -1.18750529 -0.47147507  0.45592416]\n",
      " [-0.0167978   0.19337108  0.37964906 -0.84164269 -0.00310948 -0.00444015\n",
      "  -0.91769812  0.14270445  0.18682255]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.26683062  1.0842767  -1.95724617 -0.0954206   1.18803156 -1.76124551\n",
      "  -0.4942884 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:44 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.09260136]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 44 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.67724004 -0.6410018   0.31455583  0.84330297 -0.13944367 -0.13190339\n",
      "   0.99332327  0.3830235  -0.28684171]\n",
      " [-0.59723245  0.82903783 -0.54262773 -0.97836637  0.06472368  0.04949468\n",
      "  -1.25589177 -0.59528017  0.36675317]\n",
      " [ 0.30972653 -0.24889735  0.5432721  -0.7299878   0.31494678 -0.46772585\n",
      "  -0.68144719  0.38757402  0.47686486]\n",
      " [ 0.8112497  -0.7773565   0.41283174  0.83459011 -0.11267448  0.00576108\n",
      "   1.19341994  0.06852086 -0.63565037]\n",
      " [-0.79169706  0.7850868  -0.40767196 -0.6969855   0.17749697 -0.003019\n",
      "  -1.18750529 -0.47147507  0.45537125]\n",
      " [-0.01713318  0.19303571  0.37964906 -0.84164269 -0.00344485 -0.00477552\n",
      "  -0.91769812  0.14270445  0.18648718]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.2707211   1.08283006 -1.95985937 -0.09773736  1.18675075 -1.76378048\n",
      "  -0.49657614]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:44 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.53525552]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 44 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.66627372 -0.65196813  0.31455583  0.84330297 -0.15040999 -0.14286972\n",
      "   0.98235694  0.3830235  -0.28684171]\n",
      " [-0.58483647  0.84143381 -0.54262773 -0.97836637  0.07711965  0.06189065\n",
      "  -1.2434958  -0.59528017  0.36675317]\n",
      " [ 0.32085059 -0.23777329  0.5432721  -0.7299878   0.32607084 -0.45660179\n",
      "  -0.67032313  0.38757402  0.47686486]\n",
      " [ 0.79741686 -0.79118934  0.41283174  0.83459011 -0.12650731 -0.00807176\n",
      "   1.1795871   0.06852086 -0.63565037]\n",
      " [-0.77847229  0.79831158 -0.40767196 -0.6969855   0.19072175  0.01020577\n",
      "  -1.17428052 -0.47147507  0.45537125]\n",
      " [-0.00625335  0.20391553  0.37964906 -0.84164269  0.00743497  0.0061043\n",
      "  -0.90681829  0.14270445  0.18648718]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.33729539  1.03749524 -1.97896798 -0.11875784  1.13654955 -1.78142919\n",
      "  -0.51793436]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:44 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.79514363]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 44 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.66920841 -0.65196813  0.31455583  0.84623766 -0.15040999 -0.14286972\n",
      "   0.98529163  0.3830235  -0.28684171]\n",
      " [-0.58735203  0.84143381 -0.54262773 -0.98088192  0.07711965  0.06189065\n",
      "  -1.24601135 -0.59528017  0.36675317]\n",
      " [ 0.3174414  -0.23777329  0.5432721  -0.73339699  0.32607084 -0.45660179\n",
      "  -0.67373232  0.38757402  0.47686486]\n",
      " [ 0.79992589 -0.79118934  0.41283174  0.83709914 -0.12650731 -0.00807176\n",
      "   1.18209613  0.06852086 -0.63565037]\n",
      " [-0.78119775  0.79831158 -0.40767196 -0.69971096  0.19072175  0.01020577\n",
      "  -1.17700598 -0.47147507  0.45537125]\n",
      " [-0.00993375  0.20391553  0.37964906 -0.84532308  0.00743497  0.0061043\n",
      "  -0.91049869  0.14270445  0.18648718]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.32061084  1.05290466 -1.97801757 -0.1145265   1.15228807 -1.78032792\n",
      "  -0.51547416]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:44 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.67962129]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 44 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 6.76919260e-01 -6.51968128e-01  3.14555829e-01  8.46237659e-01\n",
      "  -1.42699138e-01 -1.35158861e-01  9.93002485e-01  3.83023504e-01\n",
      "  -2.86841712e-01]\n",
      " [-5.95100631e-01  8.41433806e-01 -5.42627731e-01 -9.80881925e-01\n",
      "   6.93710456e-02  5.41420510e-02 -1.25375996e+00 -5.95280173e-01\n",
      "   3.66753174e-01]\n",
      " [ 3.13438330e-01 -2.37773290e-01  5.43272103e-01 -7.33396993e-01\n",
      "   3.22067776e-01 -4.60604854e-01 -6.77735393e-01  3.87574015e-01\n",
      "   4.76864861e-01]\n",
      " [ 8.07505630e-01 -7.91189338e-01  4.12831742e-01  8.37099139e-01\n",
      "  -1.18927577e-01 -4.92018418e-04  1.18967587e+00  6.85208562e-02\n",
      "  -6.35650366e-01]\n",
      " [-7.88888927e-01  7.98311578e-01 -4.07671958e-01 -6.99710958e-01\n",
      "   1.83030571e-01  2.51459432e-03 -1.18469716e+00 -4.71475074e-01\n",
      "   4.55371253e-01]\n",
      " [-1.64150999e-02  2.03915533e-01  3.79649064e-01 -8.45323084e-01\n",
      "   9.53618586e-04 -3.77052604e-04 -9.16980041e-01  1.42704450e-01\n",
      "   1.86487176e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.28573182  1.0806669  -1.9726042  -0.10125006  1.18241749 -1.77519611\n",
      "  -0.5054417 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:44 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.56004065]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 44 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 6.62692076e-01 -6.51968128e-01  3.14555829e-01  8.32010475e-01\n",
      "  -1.42699138e-01 -1.49386046e-01  9.93002485e-01  3.83023504e-01\n",
      "  -3.01068896e-01]\n",
      " [-5.80582287e-01  8.41433806e-01 -5.42627731e-01 -9.66363580e-01\n",
      "   6.93710456e-02  6.86603952e-02 -1.25375996e+00 -5.95280173e-01\n",
      "   3.81271518e-01]\n",
      " [ 3.20125515e-01 -2.37773290e-01  5.43272103e-01 -7.26709808e-01\n",
      "   3.22067776e-01 -4.53917670e-01 -6.77735393e-01  3.87574015e-01\n",
      "   4.83552045e-01]\n",
      " [ 7.93879053e-01 -7.91189338e-01  4.12831742e-01  8.23472562e-01\n",
      "  -1.18927577e-01 -1.41185954e-02  1.18967587e+00  6.85208562e-02\n",
      "  -6.49276943e-01]\n",
      " [-7.75106509e-01  7.98311578e-01 -4.07671958e-01 -6.85928540e-01\n",
      "   1.83030571e-01  1.62970119e-02 -1.18469716e+00 -4.71475074e-01\n",
      "   4.69153670e-01]\n",
      " [-5.99616559e-03  2.03915533e-01  3.79649064e-01 -8.34904150e-01\n",
      "   9.53618586e-04  1.00418817e-02 -9.16980041e-01  1.42704450e-01\n",
      "   1.96906110e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.35472746  1.02888727 -1.98913287 -0.12887758  1.13186307 -1.79333822\n",
      "  -0.52870963]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:44 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.70894129]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 44 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 6.69368403e-01 -6.45291801e-01  3.14555829e-01  8.38686802e-01\n",
      "  -1.42699138e-01 -1.42709719e-01  9.99678812e-01  3.83023504e-01\n",
      "  -3.01068896e-01]\n",
      " [-5.87052618e-01  8.34963476e-01 -5.42627731e-01 -9.72833911e-01\n",
      "   6.93710456e-02  6.21900647e-02 -1.26023029e+00 -5.95280173e-01\n",
      "   3.81271518e-01]\n",
      " [ 3.13521403e-01 -2.44377401e-01  5.43272103e-01 -7.33313920e-01\n",
      "   3.22067776e-01 -4.60521782e-01 -6.84339505e-01  3.87574015e-01\n",
      "   4.83552045e-01]\n",
      " [ 8.00181926e-01 -7.84886465e-01  4.12831742e-01  8.29775435e-01\n",
      "  -1.18927577e-01 -7.81572201e-03  1.19597874e+00  6.85208562e-02\n",
      "  -6.49276943e-01]\n",
      " [-7.81651688e-01  7.91766400e-01 -4.07671958e-01 -6.92473719e-01\n",
      "   1.83030571e-01  9.75183326e-03 -1.19124234e+00 -4.71475074e-01\n",
      "   4.69153670e-01]\n",
      " [-1.27188146e-02  1.97192884e-01  3.79649064e-01 -8.41626799e-01\n",
      "   9.53618586e-04  3.31923270e-03 -9.23702690e-01  1.42704450e-01\n",
      "   1.96906110e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.32469842  1.05422409 -1.98519382 -0.12452948  1.15831798 -1.78919107\n",
      "  -0.52342555]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:44 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.73497912]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 44 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 6.74961539e-01 -6.39698665e-01  3.14555829e-01  8.44279938e-01\n",
      "  -1.42699138e-01 -1.42709719e-01  1.00527195e+00  3.83023504e-01\n",
      "  -3.01068896e-01]\n",
      " [-5.92493415e-01  8.29522679e-01 -5.42627731e-01 -9.78274708e-01\n",
      "   6.93710456e-02  6.21900647e-02 -1.26567108e+00 -5.95280173e-01\n",
      "   3.81271518e-01]\n",
      " [ 3.07825992e-01 -2.50072812e-01  5.43272103e-01 -7.39009331e-01\n",
      "   3.22067776e-01 -4.60521782e-01 -6.90034916e-01  3.87574015e-01\n",
      "   4.83552045e-01]\n",
      " [ 8.05541966e-01 -7.79526425e-01  4.12831742e-01  8.35135475e-01\n",
      "  -1.18927577e-01 -7.81572201e-03  1.20133878e+00  6.85208562e-02\n",
      "  -6.49276943e-01]\n",
      " [-7.87232534e-01  7.86185554e-01 -4.07671958e-01 -6.98054565e-01\n",
      "   1.83030571e-01  9.75183326e-03 -1.19682318e+00 -4.71475074e-01\n",
      "   4.69153670e-01]\n",
      " [-1.84943297e-02  1.91417369e-01  3.79649064e-01 -8.47402314e-01\n",
      "   9.53618586e-04  3.31923270e-03 -9.29478205e-01  1.42704450e-01\n",
      "   1.96906110e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.2988874   1.07656562 -1.98207661 -0.11920946  1.18116177 -1.78575497\n",
      "  -0.51902027]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:44 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.75977458]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 44 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 6.79721516e-01 -6.39698665e-01  3.19315806e-01  8.44279938e-01\n",
      "  -1.42699138e-01 -1.37949742e-01  1.01003192e+00  3.83023504e-01\n",
      "  -3.01068896e-01]\n",
      " [-5.96606991e-01  8.29522679e-01 -5.46741308e-01 -9.78274708e-01\n",
      "   6.93710456e-02  5.80764884e-02 -1.26978466e+00 -5.95280173e-01\n",
      "   3.81271518e-01]\n",
      " [ 3.06221013e-01 -2.50072812e-01  5.41667124e-01 -7.39009331e-01\n",
      "   3.22067776e-01 -4.62126760e-01 -6.91639894e-01  3.87574015e-01\n",
      "   4.83552045e-01]\n",
      " [ 8.09534125e-01 -7.79526425e-01  4.16823901e-01  8.35135475e-01\n",
      "  -1.18927577e-01 -3.82356314e-03  1.20533094e+00  6.85208562e-02\n",
      "  -6.49276943e-01]\n",
      " [-7.91274691e-01  7.86185554e-01 -4.11714115e-01 -6.98054565e-01\n",
      "   1.83030571e-01  5.70967576e-03 -1.20086534e+00 -4.71475074e-01\n",
      "   4.69153670e-01]\n",
      " [-2.13563711e-02  1.91417369e-01  3.76787023e-01 -8.47402314e-01\n",
      "   9.53618586e-04  4.57191270e-04 -9.32340246e-01  1.42704450e-01\n",
      "   1.96906110e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.27696477  1.09551491 -1.98014794 -0.10987722  1.20128082 -1.78390124\n",
      "  -0.51107572]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:44 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.32010402]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 45 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 6.75051087e-01 -6.44369094e-01  3.19315806e-01  8.39609509e-01\n",
      "  -1.42699138e-01 -1.37949742e-01  1.01003192e+00  3.83023504e-01\n",
      "  -3.05739325e-01]\n",
      " [-5.93539192e-01  8.32590477e-01 -5.46741308e-01 -9.75206910e-01\n",
      "   6.93710456e-02  5.80764884e-02 -1.26978466e+00 -5.95280173e-01\n",
      "   3.84339316e-01]\n",
      " [ 3.07939539e-01 -2.48354287e-01  5.41667124e-01 -7.37290805e-01\n",
      "   3.22067776e-01 -4.62126760e-01 -6.91639894e-01  3.87574015e-01\n",
      "   4.85270571e-01]\n",
      " [ 8.07676022e-01 -7.81384528e-01  4.16823901e-01  8.33277372e-01\n",
      "  -1.18927577e-01 -3.82356314e-03  1.20533094e+00  6.85208562e-02\n",
      "  -6.51135047e-01]\n",
      " [-7.89264668e-01  7.88195577e-01 -4.11714115e-01 -6.96044542e-01\n",
      "   1.83030571e-01  5.70967576e-03 -1.20086534e+00 -4.71475074e-01\n",
      "   4.71163693e-01]\n",
      " [-1.74050147e-02  1.95368725e-01  3.76787023e-01 -8.43450958e-01\n",
      "   9.53618586e-04  4.57191270e-04 -9.32340246e-01  1.42704450e-01\n",
      "   2.00857466e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.31179808  1.07315851 -1.99442857 -0.12556395  1.1819916  -1.79928948\n",
      "  -0.52438725]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:45 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.75945833]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 45 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 6.79659439e-01 -6.44369094e-01  3.23924159e-01  8.39609509e-01\n",
      "  -1.42699138e-01 -1.37949742e-01  1.01464028e+00  3.83023504e-01\n",
      "  -3.05739325e-01]\n",
      " [-5.97543286e-01  8.32590477e-01 -5.50745401e-01 -9.75206910e-01\n",
      "   6.93710456e-02  5.80764884e-02 -1.27378875e+00 -5.95280173e-01\n",
      "   3.84339316e-01]\n",
      " [ 3.08801830e-01 -2.48354287e-01  5.42529415e-01 -7.37290805e-01\n",
      "   3.22067776e-01 -4.62126760e-01 -6.90777603e-01  3.87574015e-01\n",
      "   4.85270571e-01]\n",
      " [ 8.11646743e-01 -7.81384528e-01  4.20794622e-01  8.33277372e-01\n",
      "  -1.18927577e-01 -3.82356314e-03  1.20930166e+00  6.85208562e-02\n",
      "  -6.51135047e-01]\n",
      " [-7.93282576e-01  7.88195577e-01 -4.15732023e-01 -6.96044542e-01\n",
      "   1.83030571e-01  5.70967576e-03 -1.20488325e+00 -4.71475074e-01\n",
      "   4.71163693e-01]\n",
      " [-2.03073797e-02  1.95368725e-01  3.73884658e-01 -8.43450958e-01\n",
      "   9.53618586e-04  4.57191270e-04 -9.35242611e-01  1.42704450e-01\n",
      "   2.00857466e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.28982684  1.09252084 -1.99261796 -0.11371245  1.2021848  -1.79746516\n",
      "  -0.51646542]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:45 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.6672243]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 45 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 6.63255173e-01 -6.60773360e-01  3.07519892e-01  8.39609509e-01\n",
      "  -1.42699138e-01 -1.37949742e-01  9.98236011e-01  3.83023504e-01\n",
      "  -3.05739325e-01]\n",
      " [-5.80972540e-01  8.49161223e-01 -5.34174655e-01 -9.75206910e-01\n",
      "   6.93710456e-02  5.80764884e-02 -1.25721801e+00 -5.95280173e-01\n",
      "   3.84339316e-01]\n",
      " [ 3.10424635e-01 -2.46731482e-01  5.44152220e-01 -7.37290805e-01\n",
      "   3.22067776e-01 -4.62126760e-01 -6.89154798e-01  3.87574015e-01\n",
      "   4.85270571e-01]\n",
      " [ 7.95140926e-01 -7.97890346e-01  4.04288805e-01  8.33277372e-01\n",
      "  -1.18927577e-01 -3.82356314e-03  1.19279585e+00  6.85208562e-02\n",
      "  -6.51135047e-01]\n",
      " [-7.76738256e-01  8.04739897e-01 -3.99187703e-01 -6.96044542e-01\n",
      "   1.83030571e-01  5.70967576e-03 -1.18833893e+00 -4.71475074e-01\n",
      "   4.71163693e-01]\n",
      " [-1.34141291e-02  2.02261976e-01  3.80777908e-01 -8.43450958e-01\n",
      "   9.53618586e-04  4.57191270e-04 -9.28349361e-01  1.42704450e-01\n",
      "   2.00857466e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.36390076  1.03340969 -2.00517023 -0.14912451  1.13994151 -1.80964439\n",
      "  -0.5464364 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:45 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.5721604]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 45 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 6.74819426e-01 -6.60773360e-01  3.19084145e-01  8.39609509e-01\n",
      "  -1.42699138e-01 -1.37949742e-01  9.98236011e-01  3.94587757e-01\n",
      "  -3.05739325e-01]\n",
      " [-5.92586518e-01  8.49161223e-01 -5.45788633e-01 -9.75206910e-01\n",
      "   6.93710456e-02  5.80764884e-02 -1.25721801e+00 -6.06894150e-01\n",
      "   3.84339316e-01]\n",
      " [ 3.21733502e-01 -2.46731482e-01  5.55461087e-01 -7.37290805e-01\n",
      "   3.22067776e-01 -4.62126760e-01 -6.89154798e-01  3.98882882e-01\n",
      "   4.85270571e-01]\n",
      " [ 8.06520171e-01 -7.97890346e-01  4.15668050e-01  8.33277372e-01\n",
      "  -1.18927577e-01 -3.82356314e-03  1.19279585e+00  7.99001014e-02\n",
      "  -6.51135047e-01]\n",
      " [-7.88418151e-01  8.04739897e-01 -4.10867598e-01 -6.96044542e-01\n",
      "   1.83030571e-01  5.70967576e-03 -1.18833893e+00 -4.83154969e-01\n",
      "   4.71163693e-01]\n",
      " [-7.15272849e-03  2.02261976e-01  3.87039309e-01 -8.43450958e-01\n",
      "   9.53618586e-04  4.57191270e-04 -9.28349361e-01  1.48965850e-01\n",
      "   2.00857466e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.31153471  1.0750276  -1.99715256 -0.1084917   1.18080754 -1.80118935\n",
      "  -0.51371692]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:45 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.5489995]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 45 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 6.86768674e-01 -6.60773360e-01  3.19084145e-01  8.51558757e-01\n",
      "  -1.42699138e-01 -1.37949742e-01  9.98236011e-01  3.94587757e-01\n",
      "  -2.93790077e-01]\n",
      " [-6.04445508e-01  8.49161223e-01 -5.45788633e-01 -9.87065900e-01\n",
      "   6.93710456e-02  5.80764884e-02 -1.25721801e+00 -6.06894150e-01\n",
      "   3.72480326e-01]\n",
      " [ 3.22705407e-01 -2.46731482e-01  5.55461087e-01 -7.36318900e-01\n",
      "   3.22067776e-01 -4.62126760e-01 -6.89154798e-01  3.98882882e-01\n",
      "   4.86242476e-01]\n",
      " [ 8.17430009e-01 -7.97890346e-01  4.15668050e-01  8.44187210e-01\n",
      "  -1.18927577e-01 -3.82356314e-03  1.19279585e+00  7.99001014e-02\n",
      "  -6.40225208e-01]\n",
      " [-7.99473167e-01  8.04739897e-01 -4.10867598e-01 -7.07099558e-01\n",
      "   1.83030571e-01  5.70967576e-03 -1.18833893e+00 -4.83154969e-01\n",
      "   4.60108677e-01]\n",
      " [-1.53284771e-02  2.02261976e-01  3.87039309e-01 -8.51626706e-01\n",
      "   9.53618586e-04  4.57191270e-04 -9.28349361e-01  1.48965850e-01\n",
      "   1.92681718e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.25570106  1.11802325 -1.98406338 -0.07960219  1.22150042 -1.78631891\n",
      "  -0.49456337]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:45 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.2448685]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 45 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 6.86134916e-01 -6.60773360e-01  3.19084145e-01  8.51558757e-01\n",
      "  -1.43332897e-01 -1.38583501e-01  9.98236011e-01  3.94587757e-01\n",
      "  -2.94423835e-01]\n",
      " [-6.03855575e-01  8.49161223e-01 -5.45788633e-01 -9.87065900e-01\n",
      "   6.99609784e-02  5.86664212e-02 -1.25721801e+00 -6.06894150e-01\n",
      "   3.73070259e-01]\n",
      " [ 3.19313416e-01 -2.46731482e-01  5.55461087e-01 -7.36318900e-01\n",
      "   3.18675785e-01 -4.65518751e-01 -6.89154798e-01  3.98882882e-01\n",
      "   4.82850485e-01]\n",
      " [ 8.17122043e-01 -7.97890346e-01  4.15668050e-01  8.44187210e-01\n",
      "  -1.19235544e-01 -4.13152974e-03  1.19279585e+00  7.99001014e-02\n",
      "  -6.40533175e-01]\n",
      " [-7.98625486e-01  8.04739897e-01 -4.10867598e-01 -7.07099558e-01\n",
      "   1.83878252e-01  6.55735658e-03 -1.18833893e+00 -4.83154969e-01\n",
      "   4.60956358e-01]\n",
      " [-1.63321994e-02  2.02261976e-01  3.87039309e-01 -8.51626706e-01\n",
      "  -5.01037653e-05 -5.46531081e-04 -9.28349361e-01  1.48965850e-01\n",
      "   1.91677995e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.27834013  1.10606863 -1.9947919  -0.09457236  1.20987277 -1.79678755\n",
      "  -0.50689198]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:45 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.08479593]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 45 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 6.86555411e-01 -6.60352865e-01  3.19084145e-01  8.51558757e-01\n",
      "  -1.42912402e-01 -1.38163005e-01  9.98236011e-01  3.94587757e-01\n",
      "  -2.94003340e-01]\n",
      " [-6.04391709e-01  8.48625090e-01 -5.45788633e-01 -9.87065900e-01\n",
      "   6.94248445e-02  5.81302873e-02 -1.25721801e+00 -6.06894150e-01\n",
      "   3.72534125e-01]\n",
      " [ 3.18990964e-01 -2.47053933e-01  5.55461087e-01 -7.36318900e-01\n",
      "   3.18353333e-01 -4.65841203e-01 -6.89154798e-01  3.98882882e-01\n",
      "   4.82528034e-01]\n",
      " [ 8.17656946e-01 -7.97355443e-01  4.15668050e-01  8.44187210e-01\n",
      "  -1.18700640e-01 -3.59662643e-03  1.19279585e+00  7.99001014e-02\n",
      "  -6.39998271e-01]\n",
      " [-7.99111851e-01  8.04253532e-01 -4.10867598e-01 -7.07099558e-01\n",
      "   1.83391888e-01  6.07099207e-03 -1.18833893e+00 -4.83154969e-01\n",
      "   4.60469994e-01]\n",
      " [-1.66315570e-02  2.01962618e-01  3.87039309e-01 -8.51626706e-01\n",
      "  -3.49461386e-04 -8.45888702e-04 -9.28349361e-01  1.48965850e-01\n",
      "   1.91378638e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.28163044  1.10486556 -1.99702447 -0.09654902  1.20881334 -1.79895488\n",
      "  -0.50884364]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:45 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.52701834]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 45 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.67589085 -0.67101742  0.31908415  0.85155876 -0.15357696 -0.14882756\n",
      "   0.98757145  0.39458776 -0.29400334]\n",
      " [-0.59236677  0.86065003 -0.54578863 -0.9870659   0.08144979  0.07015523\n",
      "  -1.24519307 -0.60689415  0.37253412]\n",
      " [ 0.32987771 -0.23616718  0.55546109 -0.7363189   0.32924008 -0.45495445\n",
      "  -0.67826805  0.39888288  0.48252803]\n",
      " [ 0.80417031 -0.81084208  0.41566805  0.84418721 -0.13218728 -0.01708326\n",
      "   1.17930921  0.0799001  -0.63999827]\n",
      " [-0.78624121  0.81712417 -0.4108676  -0.70709956  0.19626253  0.01894163\n",
      "  -1.17546829 -0.48315497  0.46046999]\n",
      " [-0.00595803  0.21263615  0.38703931 -0.85162671  0.01032407  0.00982764\n",
      "  -0.91767583  0.14896585  0.19137864]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.34731538  1.06034886 -2.01620801 -0.11741223  1.15964604 -1.81670131\n",
      "  -0.52999967]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:45 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.80081907]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 45 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.67865615 -0.67101742  0.31908415  0.85432406 -0.15357696 -0.14882756\n",
      "   0.99033675  0.39458776 -0.29400334]\n",
      " [-0.59473911  0.86065003 -0.54578863 -0.98943825  0.08144979  0.07015523\n",
      "  -1.24756541 -0.60689415  0.37253412]\n",
      " [ 0.32662446 -0.23616718  0.55546109 -0.73957215  0.32924008 -0.45495445\n",
      "  -0.6815213   0.39888288  0.48252803]\n",
      " [ 0.80653881 -0.81084208  0.41566805  0.84655571 -0.13218728 -0.01708326\n",
      "   1.18167771  0.0799001  -0.63999827]\n",
      " [-0.78881185  0.81712417 -0.4108676  -0.7096702   0.19626253  0.01894163\n",
      "  -1.17803893 -0.48315497  0.46046999]\n",
      " [-0.00945201  0.21263615  0.38703931 -0.85512069  0.01032407  0.00982764\n",
      "  -0.92116982  0.14896585  0.19137864]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.33142991  1.0750459  -2.0153183  -0.11339931  1.17464435 -1.81567131\n",
      "  -0.52769803]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:45 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.68174409]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 45 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.68629468 -0.67101742  0.31908415  0.85432406 -0.14593843 -0.14118903\n",
      "   0.99797528  0.39458776 -0.29400334]\n",
      " [-0.60241201  0.86065003 -0.54578863 -0.98943825  0.07377689  0.06248233\n",
      "  -1.25523831 -0.60689415  0.37253412]\n",
      " [ 0.32270671 -0.23616718  0.55546109 -0.73957215  0.32532233 -0.45887221\n",
      "  -0.68543905  0.39888288  0.48252803]\n",
      " [ 0.81405354 -0.81084208  0.41566805  0.84655571 -0.12467254 -0.00956853\n",
      "   1.18919244  0.0799001  -0.63999827]\n",
      " [-0.79643094  0.81712417 -0.4108676  -0.7096702   0.18864344  0.01132254\n",
      "  -1.18565802 -0.48315497  0.46046999]\n",
      " [-0.01588325  0.21263615  0.38703931 -0.85512069  0.00389283  0.00339641\n",
      "  -0.92760105  0.14896585  0.19137864]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.29690407  1.10255708 -2.00994318 -0.10020671  1.20443408 -1.81056703\n",
      "  -0.51779244]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:45 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.5606812]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 45 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.67203107 -0.67101742  0.31908415  0.84006045 -0.14593843 -0.15545264\n",
      "   0.99797528  0.39458776 -0.30826695]\n",
      " [-0.5878729   0.86065003 -0.54578863 -0.97489913  0.07377689  0.07702145\n",
      "  -1.25523831 -0.60689415  0.38707324]\n",
      " [ 0.32923901 -0.23616718  0.55546109 -0.73303985  0.32532233 -0.45233991\n",
      "  -0.68543905  0.39888288  0.48906033]\n",
      " [ 0.8003972  -0.81084208  0.41566805  0.83289936 -0.12467254 -0.02322487\n",
      "   1.18919244  0.0799001  -0.65365462]\n",
      " [-0.78261259  0.81712417 -0.4108676  -0.69585184  0.18864344  0.02514089\n",
      "  -1.18565802 -0.48315497  0.47428835]\n",
      " [-0.00544844  0.21263615  0.38703931 -0.84468588  0.00389283  0.01383121\n",
      "  -0.92760105  0.14896585  0.20181345]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.36595694  1.05067934 -2.0264636  -0.12803122  1.15380284 -1.82867612\n",
      "  -0.54107041]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:45 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.71076498]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 45 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.67864266 -0.66440583  0.31908415  0.84667204 -0.14593843 -0.14884105\n",
      "   1.00458687  0.39458776 -0.30826695]\n",
      " [-0.59429241  0.85423051 -0.54578863 -0.98131865  0.07377689  0.07060193\n",
      "  -1.26165783 -0.60689415  0.38707324]\n",
      " [ 0.3227023  -0.24270389  0.55546109 -0.73957657  0.32532233 -0.45887662\n",
      "  -0.69197576  0.39888288  0.48906033]\n",
      " [ 0.8066589  -0.80458038  0.41566805  0.83916106 -0.12467254 -0.01696317\n",
      "   1.19545414  0.0799001  -0.65365462]\n",
      " [-0.78910316  0.8106336  -0.4108676  -0.70234241  0.18864344  0.01865032\n",
      "  -1.19214859 -0.48315497  0.47428835]\n",
      " [-0.01210404  0.20598055  0.38703931 -0.85134148  0.00389283  0.00717561\n",
      "  -0.93425665  0.14896585  0.20181345]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.33622675  1.0757532  -2.02252881 -0.12373278  1.17995281 -1.82453734\n",
      "  -0.5358705 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:45 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.73793536]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 45 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.68413045 -0.65891805  0.31908415  0.85215982 -0.14593843 -0.14884105\n",
      "   1.01007465  0.39458776 -0.30826695]\n",
      " [-0.59963678  0.84888615 -0.54578863 -0.98666301  0.07377689  0.07060193\n",
      "  -1.26700219 -0.60689415  0.38707324]\n",
      " [ 0.31710824 -0.24829795  0.55546109 -0.74517062  0.32532233 -0.45887662\n",
      "  -0.69756982  0.39888288  0.48906033]\n",
      " [ 0.81192751 -0.79931176  0.41566805  0.84442967 -0.12467254 -0.01696317\n",
      "   1.20072276  0.0799001  -0.65365462]\n",
      " [-0.79458284  0.80515392 -0.4108676  -0.7078221   0.18864344  0.01865032\n",
      "  -1.19762827 -0.48315497  0.47428835]\n",
      " [-0.0177723   0.20031228  0.38703931 -0.85700975  0.00389283  0.00717561\n",
      "  -0.93992492  0.14896585  0.20181345]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.31088683  1.09769596 -2.0194628  -0.11852248  1.2023684  -1.82116208\n",
      "  -0.53158446]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:45 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.762854]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 45 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.68877692 -0.65891805  0.32373062  0.85215982 -0.14593843 -0.14419457\n",
      "   1.01472113  0.39458776 -0.30826695]\n",
      " [-0.60365667  0.84888615 -0.54980852 -0.98666301  0.07377689  0.06658204\n",
      "  -1.27102208 -0.60689415  0.38707324]\n",
      " [ 0.31561616 -0.24829795  0.553969   -0.74517062  0.32532233 -0.4603687\n",
      "  -0.6990619   0.39888288  0.48906033]\n",
      " [ 0.81583463 -0.79931176  0.41957517  0.84442967 -0.12467254 -0.01305605\n",
      "   1.20462988  0.0799001  -0.65365462]\n",
      " [-0.79853401  0.80515392 -0.41481876 -0.7078221   0.18864344  0.01469916\n",
      "  -1.20157944 -0.48315497  0.47428835]\n",
      " [-0.02056637  0.20031228  0.38424524 -0.85700975  0.00389283  0.00438155\n",
      "  -0.94271898  0.14896585  0.20181345]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.28943605  1.11626835 -2.01758119 -0.10930929  1.22205355 -1.81935232\n",
      "  -0.52380336]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:45 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.31208014]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 46 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.68434653 -0.66334845  0.32373062  0.84772942 -0.14593843 -0.14419457\n",
      "   1.01472113  0.39458776 -0.31269734]\n",
      " [-0.60078018  0.85176264 -0.54980852 -0.98378652  0.07377689  0.06658204\n",
      "  -1.27102208 -0.60689415  0.38994973]\n",
      " [ 0.31718327 -0.24673084  0.553969   -0.74360351  0.32532233 -0.4603687\n",
      "  -0.6990619   0.39888288  0.49062744]\n",
      " [ 0.81411705 -0.80102935  0.41957517  0.84271209 -0.12467254 -0.01305605\n",
      "   1.20462988  0.0799001  -0.6553722 ]\n",
      " [-0.79665788  0.80703004 -0.41481876 -0.70594597  0.18864344  0.01469916\n",
      "  -1.20157944 -0.48315497  0.47616447]\n",
      " [-0.01680133  0.20407732  0.38424524 -0.85324471  0.00389283  0.00438155\n",
      "  -0.94271898  0.14896585  0.20557849]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.32293569  1.09484103 -2.03139394 -0.12448267  1.20357382 -1.83420987\n",
      "  -0.53664468]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:46 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.76382401]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 46 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.68879269 -0.66334845  0.32817679  0.84772942 -0.14593843 -0.14419457\n",
      "   1.0191673   0.39458776 -0.31269734]\n",
      " [-0.60464356  0.85176264 -0.55367191 -0.98378652  0.07377689  0.06658204\n",
      "  -1.27488547 -0.60689415  0.38994973]\n",
      " [ 0.31809302 -0.24673084  0.55487875 -0.74360351  0.32532233 -0.4603687\n",
      "  -0.69815216  0.39888288  0.49062744]\n",
      " [ 0.81795299 -0.80102935  0.42341111  0.84271209 -0.12467254 -0.01305605\n",
      "   1.20846582  0.0799001  -0.6553722 ]\n",
      " [-0.80053526  0.80703004 -0.41869614 -0.70594597  0.18864344  0.01469916\n",
      "  -1.20545681 -0.48315497  0.47616447]\n",
      " [-0.01962493  0.20407732  0.38142164 -0.85324471  0.00389283  0.00438155\n",
      "  -0.94554258  0.14896585  0.20557849]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.30163298  1.11365494 -2.02965693 -0.11291708  1.22316599 -1.83245916\n",
      "  -0.52897527]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:46 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.66683617]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 46 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.67239071 -0.67975043  0.31177481  0.84772942 -0.14593843 -0.14419457\n",
      "   1.00276532  0.39458776 -0.31269734]\n",
      " [-0.58806889  0.86833731 -0.53709723 -0.98378652  0.07377689  0.06658204\n",
      "  -1.25831079 -0.60689415  0.38994973]\n",
      " [ 0.31942299 -0.24540087  0.55620872 -0.74360351  0.32532233 -0.4603687\n",
      "  -0.69682219  0.39888288  0.49062744]\n",
      " [ 0.80143289 -0.81754944  0.40689102  0.84271209 -0.12467254 -0.01305605\n",
      "   1.19194572  0.0799001  -0.6553722 ]\n",
      " [-0.7839838   0.8235815  -0.40214468 -0.70594597  0.18864344  0.01469916\n",
      "  -1.18890535 -0.48315497  0.47616447]\n",
      " [-0.01284144  0.21086081  0.38820513 -0.85324471  0.00389283  0.00438155\n",
      "  -0.93875909  0.14896585  0.20557849]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.37570704  1.05455642 -2.04229336 -0.14862299  1.16103794 -1.8447205\n",
      "  -0.55906466]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:46 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.57687906]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 46 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.68383402 -0.67975043  0.32321811  0.84772942 -0.14593843 -0.14419457\n",
      "   1.00276532  0.40603106 -0.31269734]\n",
      " [-0.59949318  0.86833731 -0.54852152 -0.98378652  0.07377689  0.06658204\n",
      "  -1.25831079 -0.61831844  0.38994973]\n",
      " [ 0.33066096 -0.24540087  0.5674467  -0.74360351  0.32532233 -0.4603687\n",
      "  -0.69682219  0.41012086  0.49062744]\n",
      " [ 0.81270405 -0.81754944  0.41816218  0.84271209 -0.12467254 -0.01305605\n",
      "   1.19194572  0.09117126 -0.6553722 ]\n",
      " [-0.7954823   0.8235815  -0.41364318 -0.70594597  0.18864344  0.01469916\n",
      "  -1.18890535 -0.49465347  0.47616447]\n",
      " [-0.00651711  0.21086081  0.39452946 -0.85324471  0.00389283  0.00438155\n",
      "  -0.93875909  0.15529018  0.20557849]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.32406733  1.09580574 -2.03453066 -0.1082658   1.20151554 -1.83653459\n",
      "  -0.52662667]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:46 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.55120864]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 46 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.69574824 -0.67975043  0.32321811  0.85964364 -0.14593843 -0.14419457\n",
      "   1.00276532  0.40603106 -0.30078312]\n",
      " [-0.61131951  0.86833731 -0.54852152 -0.99561286  0.07377689  0.06658204\n",
      "  -1.25831079 -0.61831844  0.37812339]\n",
      " [ 0.33173742 -0.24540087  0.5674467  -0.74252705  0.32532233 -0.4603687\n",
      "  -0.69682219  0.41012086  0.4917039 ]\n",
      " [ 0.82361834 -0.81754944  0.41816218  0.85362637 -0.12467254 -0.01305605\n",
      "   1.19194572  0.09117126 -0.64445792]\n",
      " [-0.80654079  0.8235815  -0.41364318 -0.71700446  0.18864344  0.01469916\n",
      "  -1.18890535 -0.49465347  0.46510598]\n",
      " [-0.01468962  0.21086081  0.39452946 -0.86141722  0.00389283  0.00438155\n",
      "  -0.93875909  0.15529018  0.19740598]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.26855685  1.13865228 -2.02161538 -0.07943302  1.24209743 -1.82187963\n",
      "  -0.50763946]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:46 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.23533571]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 46 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.69519481 -0.67975043  0.32321811  0.85964364 -0.14649185 -0.144748\n",
      "   1.00276532  0.40603106 -0.30133655]\n",
      " [-0.61082912  0.86833731 -0.54852152 -0.99561286  0.07426728  0.06707243\n",
      "  -1.25831079 -0.61831844  0.37861378]\n",
      " [ 0.32849307 -0.24540087  0.5674467  -0.74252705  0.32207798 -0.46361305\n",
      "  -0.69682219  0.41012086  0.48845955]\n",
      " [ 0.8233991  -0.81754944  0.41816218  0.85362637 -0.12489177 -0.01327528\n",
      "   1.19194572  0.09117126 -0.64467715]\n",
      " [-0.80581325  0.8235815  -0.41364318 -0.71700446  0.18937098  0.0154267\n",
      "  -1.18890535 -0.49465347  0.46583352]\n",
      " [-0.01569149  0.21086081  0.39452946 -0.86141722  0.00289096  0.00337967\n",
      "  -0.93875909  0.15529018  0.1964041 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.28973151  1.12751051 -2.03171161 -0.09352708  1.23129081 -1.83173711\n",
      "  -0.51923476]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:46 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.07763359]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 46 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.69556423 -0.67938101  0.32321811  0.85964364 -0.14612243 -0.14437858\n",
      "   1.00276532  0.40603106 -0.30096713]\n",
      " [-0.6112953   0.86787113 -0.54852152 -0.99561286  0.0738011   0.06660625\n",
      "  -1.25831079 -0.61831844  0.3781476 ]\n",
      " [ 0.32820766 -0.24568628  0.5674467  -0.74252705  0.32179257 -0.46389846\n",
      "  -0.69682219  0.41012086  0.48817414]\n",
      " [ 0.82386508 -0.81708346  0.41816218  0.85362637 -0.12442579 -0.0128093\n",
      "   1.19194572  0.09117126 -0.64421117]\n",
      " [-0.80623913  0.82315563 -0.41364318 -0.71700446  0.1889451   0.01500082\n",
      "  -1.18890535 -0.49465347  0.46540764]\n",
      " [-0.01595729  0.21059501  0.39452946 -0.86141722  0.00262516  0.00311387\n",
      "  -0.93875909  0.15529018  0.1961383 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.29251105  1.12651101 -2.03361597 -0.09521115  1.23041534 -1.8335872\n",
      "  -0.5208974 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:46 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.51843085]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 46 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.68521362 -0.68973163  0.32321811  0.85964364 -0.15647305 -0.1547292\n",
      "   0.9924147   0.40603106 -0.30096713]\n",
      " [-0.59965557  0.87951086 -0.54852152 -0.99561286  0.08544083  0.07824598\n",
      "  -1.24667106 -0.61831844  0.3781476 ]\n",
      " [ 0.33884938 -0.23504456  0.5674467  -0.74252705  0.33243429 -0.45325674\n",
      "  -0.68618046  0.41012086  0.48817414]\n",
      " [ 0.81074683 -0.83020171  0.41816218  0.85362637 -0.13754405 -0.02592755\n",
      "   1.17882747  0.09117126 -0.64421117]\n",
      " [-0.79374112  0.83565363 -0.41364318 -0.71700446  0.20144311  0.02749883\n",
      "  -1.17640734 -0.49465347  0.46540764]\n",
      " [-0.00550205  0.22105026  0.39452946 -0.86141722  0.0130804   0.01356912\n",
      "  -0.92830385  0.15529018  0.1961383 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.35722685  1.0828625  -2.05284057 -0.11588311  1.18233308 -1.85140157\n",
      "  -0.54182377]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:46 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.8062922]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 46 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.68782032 -0.68973163  0.32321811  0.86225035 -0.15647305 -0.1547292\n",
      "   0.99502141  0.40603106 -0.30096713]\n",
      " [-0.6018938   0.87951086 -0.54852152 -0.99785109  0.08544083  0.07824598\n",
      "  -1.24890929 -0.61831844  0.3781476 ]\n",
      " [ 0.33574466 -0.23504456  0.5674467  -0.74563177  0.33243429 -0.45325674\n",
      "  -0.68928518  0.41012086  0.48817414]\n",
      " [ 0.81298354 -0.83020171  0.41816218  0.85586309 -0.13754405 -0.02592755\n",
      "   1.18106418  0.09117126 -0.64421117]\n",
      " [-0.79616678  0.83565363 -0.41364318 -0.71943012  0.20144311  0.02749883\n",
      "  -1.17883301 -0.49465347  0.46540764]\n",
      " [-0.00881912  0.22105026  0.39452946 -0.86473429  0.0130804   0.01356912\n",
      "  -0.93162092  0.15529018  0.1961383 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.34209972  1.09688102 -2.05200708 -0.11207645  1.19662771 -1.85043743\n",
      "  -0.53966916]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:46 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.68381263]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 46 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.69538795 -0.68973163  0.32321811  0.86225035 -0.14890542 -0.14716157\n",
      "   1.00258904  0.40603106 -0.30096713]\n",
      " [-0.60949288  0.87951086 -0.54852152 -0.99785109  0.07784175  0.07064691\n",
      "  -1.25650837 -0.61831844  0.3781476 ]\n",
      " [ 0.33191076 -0.23504456  0.5674467  -0.74563177  0.32860039 -0.45709064\n",
      "  -0.69311909  0.41012086  0.48817414]\n",
      " [ 0.82043461 -0.83020171  0.41816218  0.85586309 -0.13009298 -0.01847649\n",
      "   1.18851525  0.09117126 -0.64421117]\n",
      " [-0.80371553  0.83565363 -0.41364318 -0.71943012  0.19389436  0.01995008\n",
      "  -1.18638175 -0.49465347  0.46540764]\n",
      " [-0.01520046  0.22105026  0.39452946 -0.86473429  0.00669907  0.00718778\n",
      "  -0.93800226  0.15529018  0.1961383 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.30791782  1.12414604 -2.04666939 -0.09896481  1.22608671 -1.84535988\n",
      "  -0.52988545]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:46 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.56135598]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 46 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.68108863 -0.68973163  0.32321811  0.84795103 -0.14890542 -0.16146088\n",
      "   1.00258904  0.40603106 -0.31526645]\n",
      " [-0.59493259  0.87951086 -0.54852152 -0.9832908   0.07784175  0.08520719\n",
      "  -1.25650837 -0.61831844  0.39270789]\n",
      " [ 0.33828584 -0.23504456  0.5674467  -0.73925669  0.32860039 -0.45071556\n",
      "  -0.69311909  0.41012086  0.49454923]\n",
      " [ 0.80674811 -0.83020171  0.41816218  0.84217658 -0.13009298 -0.03216299\n",
      "   1.18851525  0.09117126 -0.65789767]\n",
      " [-0.78986162  0.83565363 -0.41364318 -0.70557621  0.19389436  0.03380399\n",
      "  -1.18638175 -0.49465347  0.47926155]\n",
      " [-0.00475201  0.22105026  0.39452946 -0.85428584  0.00669907  0.01763623\n",
      "  -0.93800226  0.15529018  0.20658675]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.37703069  1.07217055 -2.06318259 -0.12698945  1.17537679 -1.86343822\n",
      "  -0.55317778]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:46 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.71245957]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 46 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.68764046 -0.6831798   0.32321811  0.85450285 -0.14890542 -0.15490906\n",
      "   1.00914086  0.40603106 -0.31526645]\n",
      " [-0.60130543  0.87313802 -0.54852152 -0.98966364  0.07784175  0.07883435\n",
      "  -1.26288121 -0.61831844  0.39270789]\n",
      " [ 0.33181213 -0.24151827  0.5674467  -0.7457304   0.32860039 -0.45718927\n",
      "  -0.6995928   0.41012086  0.49454923]\n",
      " [ 0.8129724  -0.82397742  0.41816218  0.84840088 -0.13009298 -0.0259387\n",
      "   1.19473954  0.09117126 -0.65789767]\n",
      " [-0.79630204  0.82921321 -0.41364318 -0.71201664  0.19389436  0.02736357\n",
      "  -1.19282218 -0.49465347  0.47926155]\n",
      " [-0.01134516  0.21445711  0.39452946 -0.86087899  0.00669907  0.01104308\n",
      "  -0.94459541  0.15529018  0.20658675]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.34757779  1.09699824 -2.05924945 -0.12273876  1.20124101 -1.85930442\n",
      "  -0.54805642]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:46 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.74080141]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 46 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.69302695 -0.67779331  0.32321811  0.85988935 -0.14890542 -0.15490906\n",
      "   1.01452735  0.40603106 -0.31526645]\n",
      " [-0.60655694  0.86788651 -0.54852152 -0.99491515  0.07784175  0.07883435\n",
      "  -1.26813271 -0.61831844  0.39270789]\n",
      " [ 0.32631565 -0.24701475  0.5674467  -0.75122688  0.32860039 -0.45718927\n",
      "  -0.70508928  0.41012086  0.49454923]\n",
      " [ 0.81815296 -0.81879686  0.41816218  0.85358144 -0.13009298 -0.0259387\n",
      "   1.1999201   0.09117126 -0.65789767]\n",
      " [-0.80168445  0.8238308  -0.41364318 -0.71739905  0.19389436  0.02736357\n",
      "  -1.19820459 -0.49465347  0.47926155]\n",
      " [-0.01690946  0.2088928   0.39452946 -0.8664433   0.00669907  0.01104308\n",
      "  -0.95015971  0.15529018  0.20658675]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.32269282  1.11855456 -2.05623232 -0.11763612  1.22324256 -1.85598678\n",
      "  -0.54388441]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:46 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.76582967]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 46 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.69756494 -0.67779331  0.3277561   0.85988935 -0.14890542 -0.15037107\n",
      "   1.01906534  0.40603106 -0.31526645]\n",
      " [-0.61048787  0.86788651 -0.55245245 -0.99491515  0.07784175  0.07490342\n",
      "  -1.27206365 -0.61831844  0.39270789]\n",
      " [ 0.32493122 -0.24701475  0.56606226 -0.75122688  0.32860039 -0.4585737\n",
      "  -0.70647371  0.41012086  0.49454923]\n",
      " [ 0.82197921 -0.81879686  0.42198843  0.85358144 -0.13009298 -0.02211245\n",
      "   1.20374635  0.09117126 -0.65789767]\n",
      " [-0.80554934  0.8238308  -0.41750807 -0.71739905  0.19389436  0.02349868\n",
      "  -1.20206948 -0.49465347  0.47926155]\n",
      " [-0.01963632  0.2088928   0.39180261 -0.8664433   0.00669907  0.00831622\n",
      "  -0.95288657  0.15529018  0.20658675]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.3016954   1.13676217 -2.05439476 -0.10853854  1.24250998 -1.85421809\n",
      "  -0.53625812]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:46 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.30422285]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 47 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.69336531 -0.68199294  0.3277561   0.85568972 -0.14890542 -0.15037107\n",
      "   1.01906534  0.40603106 -0.31946608]\n",
      " [-0.60779326  0.87058113 -0.55245245 -0.99222053  0.07784175  0.07490342\n",
      "  -1.27206365 -0.61831844  0.39540251]\n",
      " [ 0.32635871 -0.24558726  0.56606226 -0.74979939  0.32860039 -0.4585737\n",
      "  -0.70647371  0.41012086  0.49597671]\n",
      " [ 0.82039418 -0.82038189  0.42198843  0.85199641 -0.13009298 -0.02211245\n",
      "   1.20374635  0.09117126 -0.6594827 ]\n",
      " [-0.80380084  0.82557929 -0.41750807 -0.71565056  0.19389436  0.02349868\n",
      "  -1.20206948 -0.49465347  0.48101005]\n",
      " [-0.01605049  0.21247863  0.39180261 -0.86285747  0.00669907  0.00831622\n",
      "  -0.95288657  0.15529018  0.21017258]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.33389303  1.11623719 -2.06774524 -0.12320225  1.22481567 -1.86855429\n",
      "  -0.54863728]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:47 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.76806714]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 47 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.69765634 -0.68199294  0.33204713  0.85568972 -0.14890542 -0.15037107\n",
      "   1.02335637  0.40603106 -0.31946608]\n",
      " [-0.61152274  0.87058113 -0.55618194 -0.99222053  0.07784175  0.07490342\n",
      "  -1.27579314 -0.61831844  0.39540251]\n",
      " [ 0.32731079 -0.24558726  0.56701435 -0.74979939  0.32860039 -0.4585737\n",
      "  -0.70552163  0.41012086  0.49597671]\n",
      " [ 0.82410159 -0.82038189  0.42569584  0.85199641 -0.13009298 -0.02211245\n",
      "   1.20745376  0.09117126 -0.6594827 ]\n",
      " [-0.80754455  0.82557929 -0.42125177 -0.71565056  0.19389436  0.02349868\n",
      "  -1.20581318 -0.49465347  0.48101005]\n",
      " [-0.0187961   0.21247863  0.389057   -0.86285747  0.00669907  0.00831622\n",
      "  -0.95563217  0.15529018  0.21017258]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.31323476  1.13451876 -2.06607725 -0.11191554  1.24382701 -1.86687256\n",
      "  -0.54120873]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:47 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.66630478]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 47 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.68125814 -0.69839114  0.31564893  0.85568972 -0.14890542 -0.15037107\n",
      "   1.00695817  0.40603106 -0.31946608]\n",
      " [-0.59494475  0.88715912 -0.53960395 -0.99222053  0.07784175  0.07490342\n",
      "  -1.25921515 -0.61831844  0.39540251]\n",
      " [ 0.32836149 -0.24453656  0.56806505 -0.74979939  0.32860039 -0.4585737\n",
      "  -0.70447093  0.41012086  0.49597671]\n",
      " [ 0.80756827 -0.83691521  0.40916252  0.85199641 -0.13009298 -0.02211245\n",
      "   1.19092045  0.09117126 -0.6594827 ]\n",
      " [-0.79098625  0.84213759 -0.40469347 -0.71565056  0.19389436  0.02349868\n",
      "  -1.18925488 -0.49465347  0.48101005]\n",
      " [-0.01212528  0.21914945  0.39572782 -0.86285747  0.00669907  0.00831622\n",
      "  -0.94896135  0.15529018  0.21017258]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.38730877  1.07544085 -2.07880324 -0.14790127  1.18181903 -1.87922242\n",
      "  -0.57141924]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:47 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.58157446]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 47 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.69257208 -0.69839114  0.32696287  0.85568972 -0.14890542 -0.15037107\n",
      "   1.00695817  0.41734501 -0.31946608]\n",
      " [-0.60617785  0.88715912 -0.55083704 -0.99222053  0.07784175  0.07490342\n",
      "  -1.25921515 -0.62955153  0.39540251]\n",
      " [ 0.3395144  -0.24453656  0.57921796 -0.74979939  0.32860039 -0.4585737\n",
      "  -0.70447093  0.42127377  0.49597671]\n",
      " [ 0.81872406 -0.83691521  0.42031831  0.85199641 -0.13009298 -0.02211245\n",
      "   1.19092045  0.10232705 -0.6594827 ]\n",
      " [-0.80230093  0.84213759 -0.41600815 -0.71565056  0.19389436  0.02349868\n",
      "  -1.18925488 -0.50596815  0.48101005]\n",
      " [-0.0057412   0.21914945  0.4021119  -0.86285747  0.00669907  0.00831622\n",
      "  -0.94896135  0.16167426  0.21017258]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.33639776  1.11630456 -2.0712844  -0.10783763  1.22189424 -1.87129395\n",
      "  -0.53926615]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:47 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.55350532]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 47 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.70444579 -0.69839114  0.32696287  0.86756343 -0.14890542 -0.15037107\n",
      "   1.00695817  0.41734501 -0.30759237]\n",
      " [-0.61796615  0.88715912 -0.55083704 -1.00400884  0.07784175  0.07490342\n",
      "  -1.25921515 -0.62955153  0.3836142 ]\n",
      " [ 0.34069419 -0.24453656  0.57921796 -0.74861959  0.32860039 -0.4585737\n",
      "  -0.70447093  0.42127377  0.49715651]\n",
      " [ 0.82963649 -0.83691521  0.42031831  0.86290884 -0.13009298 -0.02211245\n",
      "   1.19092045  0.10232705 -0.64857027]\n",
      " [-0.81335593  0.84213759 -0.41600815 -0.72670556  0.19389436  0.02349868\n",
      "  -1.18925488 -0.50596815  0.46995505]\n",
      " [-0.01390568  0.21914945  0.4021119  -0.87102195  0.00669907  0.00831622\n",
      "  -0.94896135  0.16167426  0.2020081 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.28122505  1.1589866  -2.05854267 -0.07907003  1.26235031 -1.85685301\n",
      "  -0.52044711]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:47 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.22617483]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 47 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.70396412 -0.69839114  0.32696287  0.86756343 -0.14938709 -0.15085274\n",
      "   1.00695817  0.41734501 -0.30807404]\n",
      " [-0.61756302  0.88715912 -0.55083704 -1.00400884  0.07824488  0.07530655\n",
      "  -1.25921515 -0.62955153  0.38401733]\n",
      " [ 0.33759611 -0.24453656  0.57921796 -0.74861959  0.3255023  -0.46167179\n",
      "  -0.70447093  0.42127377  0.49405842]\n",
      " [ 0.82949372 -0.83691521  0.42031831  0.86290884 -0.13023576 -0.02225522\n",
      "   1.19092045  0.10232705 -0.64871305]\n",
      " [-0.81273489  0.84213759 -0.41600815 -0.72670556  0.1945154   0.02411971\n",
      "  -1.18925488 -0.50596815  0.47057609]\n",
      " [-0.01490044  0.21914945  0.4021119  -0.87102195  0.00570431  0.00732146\n",
      "  -0.94896135  0.16167426  0.20101334]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.30101758  1.14860789 -2.06803535 -0.09232968  1.25231125 -1.86612659\n",
      "  -0.53134499]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:47 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.07107503]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 47 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.70428741 -0.69806785  0.32696287  0.86756343 -0.1490638  -0.15052945\n",
      "   1.00695817  0.41734501 -0.30775075]\n",
      " [-0.61796703  0.88675511 -0.55083704 -1.00400884  0.07784087  0.07490254\n",
      "  -1.25921515 -0.62955153  0.38361332]\n",
      " [ 0.3373446  -0.24478807  0.57921796 -0.74861959  0.32525079 -0.4619233\n",
      "  -0.70447093  0.42127377  0.49380691]\n",
      " [ 0.82989823 -0.8365107   0.42031831  0.86290884 -0.12983125 -0.02185071\n",
      "   1.19092045  0.10232705 -0.64830854]\n",
      " [-0.81310636  0.84176612 -0.41600815 -0.72670556  0.19414393  0.02374825\n",
      "  -1.18925488 -0.50596815  0.47020462]\n",
      " [-0.01513537  0.21891452  0.4021119  -0.87102195  0.00546938  0.00708653\n",
      "  -0.94896135  0.16167426  0.20077841]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.30336389  1.14777797 -2.06965784 -0.09376296  1.25158815 -1.86770402\n",
      "  -0.53275998]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:47 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.50955157]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 47 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.69426035 -0.70809491  0.32696287  0.86756343 -0.15909086 -0.16055651\n",
      "   0.99693111  0.41734501 -0.30775075]\n",
      " [-0.60672355  0.89799859 -0.55083704 -1.00400884  0.08908435  0.08614602\n",
      "  -1.24797167 -0.62955153  0.38361332]\n",
      " [ 0.34773539 -0.23439727  0.57921796 -0.74861959  0.33564159 -0.4515325\n",
      "  -0.69408013  0.42127377  0.49380691]\n",
      " [ 0.81716747 -0.84924145  0.42031831  0.86290884 -0.142562   -0.03458147\n",
      "   1.17818969  0.10232705 -0.64830854]\n",
      " [-0.80099645  0.85387603 -0.41600815 -0.72670556  0.20625384  0.03585816\n",
      "  -1.17714498 -0.50596815  0.47020462]\n",
      " [-0.00490845  0.22914144  0.4021119  -0.87102195  0.01569629  0.01731345\n",
      "  -0.93873443  0.16167426  0.20077841]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.36703459  1.10504265 -2.08888784 -0.11420948  1.20463581 -1.8855544\n",
      "  -0.55342868]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:47 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.81157214]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 47 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.6967185  -0.70809491  0.32696287  0.87002158 -0.15909086 -0.16055651\n",
      "   0.99938926  0.41734501 -0.30775075]\n",
      " [-0.6088361   0.89799859 -0.55083704 -1.00612138  0.08908435  0.08614602\n",
      "  -1.25008421 -0.62955153  0.38361332]\n",
      " [ 0.34477207 -0.23439727  0.57921796 -0.75158292  0.33564159 -0.4515325\n",
      "  -0.69704346  0.42127377  0.49380691]\n",
      " [ 0.81928051 -0.84924145  0.42031831  0.86502188 -0.142562   -0.03458147\n",
      "   1.18030273  0.10232705 -0.64830854]\n",
      " [-0.80328626  0.85387603 -0.41600815 -0.72899536  0.20625384  0.03585816\n",
      "  -1.17943478 -0.50596815  0.47020462]\n",
      " [-0.00805769  0.22914144  0.4021119  -0.87417119  0.01569629  0.01731345\n",
      "  -0.94188367  0.16167426  0.20077841]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.35262713  1.11841507 -2.08810648 -0.11059775  1.21826158 -1.88465123\n",
      "  -0.55141055]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:47 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.68584416]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 47 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.70421615 -0.70809491  0.32696287  0.87002158 -0.15159321 -0.15305886\n",
      "   1.00688691  0.41734501 -0.30775075]\n",
      " [-0.61636254  0.89799859 -0.55083704 -1.00612138  0.0815579   0.07861957\n",
      "  -1.25761066 -0.62955153  0.38361332]\n",
      " [ 0.34102027 -0.23439727  0.57921796 -0.75158292  0.33188979 -0.4552843\n",
      "  -0.70079526  0.42127377  0.49380691]\n",
      " [ 0.82666852 -0.84924145  0.42031831  0.86502188 -0.135174   -0.02719346\n",
      "   1.18769073  0.10232705 -0.64830854]\n",
      " [-0.81076569  0.85387603 -0.41600815 -0.72899536  0.1987744   0.02837872\n",
      "  -1.18691421 -0.50596815  0.47020462]\n",
      " [-0.0143892   0.22914144  0.4021119  -0.87417119  0.00936478  0.01098194\n",
      "  -0.94821518  0.16167426  0.20077841]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.31878282  1.14543712 -2.08280621 -0.09756596  1.24739675 -1.87960045\n",
      "  -0.54174517]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:47 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.56206344]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 47 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.68988172 -0.70809491  0.32696287  0.85568715 -0.15159321 -0.16739329\n",
      "   1.00688691  0.41734501 -0.32208518]\n",
      " [-0.60178064  0.89799859 -0.55083704 -0.99153948  0.0815579   0.09320147\n",
      "  -1.25761066 -0.62955153  0.39819523]\n",
      " [ 0.34723667 -0.23439727  0.57921796 -0.74536652  0.33188979 -0.4490679\n",
      "  -0.70079526  0.42127377  0.50002331]\n",
      " [ 0.81295134 -0.84924145  0.42031831  0.8513047  -0.135174   -0.04091064\n",
      "   1.18769073  0.10232705 -0.66202572]\n",
      " [-0.79687645  0.85387603 -0.41600815 -0.71510612  0.1987744   0.04226797\n",
      "  -1.18691421 -0.50596815  0.48409387]\n",
      " [-0.00392899  0.22914144  0.4021119  -0.86371098  0.00936478  0.02144215\n",
      "  -0.94821518  0.16167426  0.21123862]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.38795825  1.09336409 -2.09931297 -0.12579287  1.19660611 -1.8976499\n",
      "  -0.56505562]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:47 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.71404143]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 47 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.69637806 -0.70159857  0.32696287  0.8621835  -0.15159321 -0.16089695\n",
      "   1.01338325  0.41734501 -0.32208518]\n",
      " [-0.60811026  0.89166897 -0.55083704 -0.9978691   0.0815579   0.08687185\n",
      "  -1.26394028 -0.62955153  0.39819523]\n",
      " [ 0.34082222 -0.24081172  0.57921796 -0.75178097  0.33188979 -0.45548235\n",
      "  -0.7072097   0.42127377  0.50002331]\n",
      " [ 0.81914126 -0.84305153  0.42031831  0.85749463 -0.135174   -0.03472072\n",
      "   1.19388066  0.10232705 -0.66202572]\n",
      " [-0.80327047  0.847482   -0.41600815 -0.72150014  0.1987744   0.03587394\n",
      "  -1.19330824 -0.50596815  0.48409387]\n",
      " [-0.01046369  0.22260674  0.4021119  -0.87024568  0.00936478  0.01490745\n",
      "  -0.95474988  0.16167426  0.21123862]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.35876384  1.11796045 -2.09537948 -0.12158863  1.22220163 -1.89351841\n",
      "  -0.56000816]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:47 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.74358589]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 47 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.70166692 -0.69630972  0.32696287  0.86747235 -0.15159321 -0.16089695\n",
      "   1.01867211  0.41734501 -0.32208518]\n",
      " [-0.61327206  0.88650717 -0.55083704 -1.0030309   0.0815579   0.08687185\n",
      "  -1.26910208 -0.62955153  0.39819523]\n",
      " [ 0.33541986 -0.24621408  0.57921796 -0.75718332  0.33188979 -0.45548235\n",
      "  -0.71261206  0.42127377  0.50002331]\n",
      " [ 0.82423672 -0.83795608  0.42031831  0.86259008 -0.135174   -0.03472072\n",
      "   1.19897611  0.10232705 -0.66202572]\n",
      " [-0.80855906  0.84219341 -0.41600815 -0.72678873  0.1987744   0.03587394\n",
      "  -1.19859683 -0.50596815  0.48409387]\n",
      " [-0.01592703  0.2171434   0.4021119  -0.87570901  0.00936478  0.01490745\n",
      "  -0.96021322  0.16167426  0.21123862]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.33431913  1.13914175 -2.09240931 -0.11659198  1.24380222 -1.89025571\n",
      "  -0.55594562]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:47 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.76870508]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 47 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.70610123 -0.69630972  0.33139719  0.86747235 -0.15159321 -0.15646263\n",
      "   1.02310642  0.41734501 -0.32208518]\n",
      " [-0.61711844  0.88650717 -0.55468342 -1.0030309   0.0815579   0.08302547\n",
      "  -1.27294846 -0.62955153  0.39819523]\n",
      " [ 0.33413768 -0.24621408  0.57793578 -0.75718332  0.33188979 -0.45676453\n",
      "  -0.71389424  0.42127377  0.50002331]\n",
      " [ 0.82798593 -0.83795608  0.42406753  0.86259008 -0.135174   -0.0309715\n",
      "   1.20272533  0.10232705 -0.66202572]\n",
      " [-0.81234202  0.84219341 -0.41979111 -0.72678873  0.1987744   0.03209098\n",
      "  -1.20237979 -0.50596815  0.48409387]\n",
      " [-0.01858774  0.2171434   0.39945118 -0.87570901  0.00936478  0.01224673\n",
      "  -0.96287393  0.16167426  0.21123862]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.31375729  1.15699653 -2.09061309 -0.10760702  1.26266764 -1.88852549\n",
      "  -0.54846623]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:47 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.29653348]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 48 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.702123   -0.70028795  0.33139719  0.86349412 -0.15159321 -0.15646263\n",
      "   1.02310642  0.41734501 -0.32606341]\n",
      " [-0.61459633  0.88902928 -0.55468342 -1.00050879  0.0815579   0.08302547\n",
      "  -1.27294846 -0.62955153  0.40071734]\n",
      " [ 0.33543692 -0.24491484  0.57793578 -0.75588409  0.33188979 -0.45676453\n",
      "  -0.71389424  0.42127377  0.50132255]\n",
      " [ 0.82652553 -0.83941648  0.42406753  0.86112968 -0.135174   -0.0309715\n",
      "   1.20272533  0.10232705 -0.66348612]\n",
      " [-0.81071478  0.84382065 -0.41979111 -0.72516149  0.1987744   0.03209098\n",
      "  -1.20237979 -0.50596815  0.48572111]\n",
      " [-0.01517386  0.22055729  0.39945118 -0.87229513  0.00936478  0.01224673\n",
      "  -0.96287393  0.16167426  0.2146525 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.34468593  1.13734644 -2.10350773 -0.12176589  1.24573406 -1.90235026\n",
      "  -0.56039181]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:48 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.77218732]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 48 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.70626579 -0.70028795  0.33553998  0.86349412 -0.15159321 -0.15646263\n",
      "   1.02724921  0.41734501 -0.32606341]\n",
      " [-0.61819843  0.88902928 -0.55828552 -1.00050879  0.0815579   0.08302547\n",
      "  -1.27655056 -0.62955153  0.40071734]\n",
      " [ 0.33642632 -0.24491484  0.57892517 -0.75588409  0.33188979 -0.45676453\n",
      "  -0.71290485  0.42127377  0.50132255]\n",
      " [ 0.8301104  -0.83941648  0.4276524   0.86112968 -0.135174   -0.0309715\n",
      "   1.2063102   0.10232705 -0.66348612]\n",
      " [-0.81433135  0.84382065 -0.42340768 -0.72516149  0.1987744   0.03209098\n",
      "  -1.20599636 -0.50596815  0.48572111]\n",
      " [-0.01784264  0.22055729  0.3967824  -0.87229513  0.00936478  0.01224673\n",
      "  -0.96554271  0.16167426  0.2146525 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.32464821  1.1551122  -2.10190454 -0.11075105  1.26418487 -1.90073323\n",
      "  -0.55319315]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:48 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.66563478]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 48 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.68987279 -0.71668095  0.31914698  0.86349412 -0.15159321 -0.15646263\n",
      "   1.01085621  0.41734501 -0.32606341]\n",
      " [-0.60161795  0.90560977 -0.54170504 -1.00050879  0.0815579   0.08302547\n",
      "  -1.25997008 -0.62955153  0.40071734]\n",
      " [ 0.33721241 -0.24412875  0.57971126 -0.75588409  0.33188979 -0.45676453\n",
      "  -0.71211876  0.42127377  0.50132255]\n",
      " [ 0.81356521 -0.85596168  0.4111072   0.86112968 -0.135174   -0.0309715\n",
      "   1.189765    0.10232705 -0.66348612]\n",
      " [-0.79776681  0.86038519 -0.40684315 -0.72516149  0.1987744   0.03209098\n",
      "  -1.18943182 -0.50596815  0.48572111]\n",
      " [-0.01128615  0.22711377  0.40333889 -0.87229513  0.00936478  0.01224673\n",
      "  -0.95898623  0.16167426  0.2146525 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.39872175  1.09606218 -2.11472464 -0.1470015   1.20230115 -1.91317711\n",
      "  -0.58352603]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:48 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.58622878]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 48 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.70105059 -0.71668095  0.33032477  0.86349412 -0.15159321 -0.15646263\n",
      "   1.01085621  0.4285228  -0.32606341]\n",
      " [-0.6126595   0.90560977 -0.55274659 -1.00050879  0.0815579   0.08302547\n",
      "  -1.25997008 -0.64059308  0.40071734]\n",
      " [ 0.34826766 -0.24412875  0.59076651 -0.75588409  0.33188979 -0.45676453\n",
      "  -0.71211876  0.43232902  0.50132255]\n",
      " [ 0.82459969 -0.85596168  0.42214169  0.86112968 -0.135174   -0.0309715\n",
      "   1.189765    0.11336154 -0.66348612]\n",
      " [-0.80889647  0.86038519 -0.4179728  -0.72516149  0.1987744   0.03209098\n",
      "  -1.18943182 -0.51709781  0.48572111]\n",
      " [-0.00484565  0.22711377  0.40977939 -0.87229513  0.00936478  0.01224673\n",
      "  -0.95898623  0.16811477  0.2146525 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.34853862  1.13652656 -2.10743879 -0.10724676  1.24196307 -1.90549464\n",
      "  -0.55165975]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:48 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.55587432]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 48 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.71287886 -0.71668095  0.33032477  0.87532239 -0.15159321 -0.15646263\n",
      "   1.01085621  0.4285228  -0.31423513]\n",
      " [-0.62440493  0.90560977 -0.55274659 -1.01225422  0.0815579   0.08302547\n",
      "  -1.25997008 -0.64059308  0.38897191]\n",
      " [ 0.34954914 -0.24412875  0.59076651 -0.7546026   0.33188979 -0.45676453\n",
      "  -0.71211876  0.43232902  0.50260404]\n",
      " [ 0.83550446 -0.85596168  0.42214169  0.87203444 -0.135174   -0.0309715\n",
      "   1.189765    0.11336154 -0.65258135]\n",
      " [-0.81994158  0.86038519 -0.4179728  -0.7362066   0.1987744   0.03209098\n",
      "  -1.18943182 -0.51709781  0.47467599]\n",
      " [-0.01299778  0.22711377  0.40977939 -0.88044726  0.00936478  0.01224673\n",
      "  -0.95898623  0.16811477  0.20650037]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.29371618  1.17903037 -2.09486978 -0.07855218  1.28228001 -1.89126577\n",
      "  -0.53301021]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:48 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.21738579]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 48 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.71246093 -0.71668095  0.33032477  0.87532239 -0.15201114 -0.15688056\n",
      "   1.01085621  0.4285228  -0.31465306]\n",
      " [-0.62407781  0.90560977 -0.55274659 -1.01225422  0.08188503  0.08335259\n",
      "  -1.25997008 -0.64059308  0.38929903]\n",
      " [ 0.34659511 -0.24412875  0.59076651 -0.7546026   0.32893576 -0.45971856\n",
      "  -0.71211876  0.43232902  0.49965   ]\n",
      " [ 0.8354269  -0.85596168  0.42214169  0.87203444 -0.13525156 -0.03104906\n",
      "   1.189765    0.11336154 -0.65265891]\n",
      " [-0.81941444  0.86038519 -0.4179728  -0.7362066   0.19930155  0.03261812\n",
      "  -1.18943182 -0.51709781  0.47520313]\n",
      " [-0.01398083  0.22711377  0.40977939 -0.88044726  0.00838174  0.01126369\n",
      "  -0.95898623  0.16811477  0.20551732]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.31220802  1.16936596 -2.1037883  -0.09101952  1.27295653 -1.8999834\n",
      "  -0.54324677]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:48 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.06507992]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 48 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.71274295 -0.71639893  0.33032477  0.87532239 -0.15172913 -0.15659854\n",
      "   1.01085621  0.4285228  -0.31437105]\n",
      " [-0.62442698  0.90526059 -0.55274659 -1.01225422  0.08153586  0.08300342\n",
      "  -1.25997008 -0.64059308  0.38894986]\n",
      " [ 0.34637431 -0.24434956  0.59076651 -0.7546026   0.32871495 -0.45993936\n",
      "  -0.71211876  0.43232902  0.4994292 ]\n",
      " [ 0.83577703 -0.85561155  0.42214169  0.87203444 -0.13490143 -0.03069893\n",
      "   1.189765    0.11336154 -0.65230878]\n",
      " [-0.81973739  0.86006224 -0.4179728  -0.7362066   0.19897859  0.03229517\n",
      "  -1.18943182 -0.51709781  0.47488018]\n",
      " [-0.01418768  0.22690693  0.40977939 -0.88044726  0.00817489  0.01105684\n",
      "  -0.95898623  0.16811477  0.20531047]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.3141879   1.16867692 -2.10516954 -0.09223853  1.27235932 -1.90132727\n",
      "  -0.54445025]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:48 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.50043851]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 48 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.70304647 -0.72609541  0.33032477  0.87532239 -0.1614256  -0.16629502\n",
      "   1.00115973  0.4285228  -0.31437105]\n",
      " [-0.61358758  0.9161     -0.55274659 -1.01225422  0.09237526  0.09384282\n",
      "  -1.24913067 -0.64059308  0.38894986]\n",
      " [ 0.35651007 -0.2342138   0.59076651 -0.7546026   0.33885071 -0.4498036\n",
      "  -0.70198299  0.43232902  0.4994292 ]\n",
      " [ 0.82344949 -0.86793908  0.42214169  0.87203444 -0.14722896 -0.04302646\n",
      "   1.17743747  0.11336154 -0.65230878]\n",
      " [-0.80802783  0.8717718  -0.4179728  -0.7362066   0.21068816  0.04400474\n",
      "  -1.17772226 -0.51709781  0.47488018]\n",
      " [-0.00419718  0.23689742  0.40977939 -0.88044726  0.01816539  0.02104734\n",
      "  -0.94899573  0.16811477  0.20531047]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.37674266  1.12689424 -2.12436833 -0.11242595  1.22657492 -1.91918037\n",
      "  -0.56483344]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:48 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.81666687]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 48 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.70536542 -0.72609541  0.33032477  0.87764134 -0.1614256  -0.16629502\n",
      "   1.00347868  0.4285228  -0.31437105]\n",
      " [-0.61558226  0.9161     -0.55274659 -1.0142489   0.09237526  0.09384282\n",
      "  -1.25112536 -0.64059308  0.38894986]\n",
      " [ 0.35368127 -0.2342138   0.59076651 -0.7574314   0.33885071 -0.4498036\n",
      "  -0.7048118   0.43232902  0.4994292 ]\n",
      " [ 0.82544641 -0.86793908  0.42214169  0.87403136 -0.14722896 -0.04302646\n",
      "   1.17943439  0.11336154 -0.65230878]\n",
      " [-0.81019025  0.8717718  -0.4179728  -0.73836902  0.21068816  0.04400474\n",
      "  -1.17988468 -0.51709781  0.47488018]\n",
      " [-0.00718725  0.23689742  0.40977939 -0.88343734  0.01816539  0.02104734\n",
      "  -0.9519858   0.16811477  0.20531047]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.36301815  1.13965161 -2.12363537 -0.10899858  1.23956505 -1.9183337\n",
      "  -0.56294214]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:48 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.6878537]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 48 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.71279357 -0.72609541  0.33032477  0.87764134 -0.15399745 -0.15886687\n",
      "   1.01090683  0.4285228  -0.31437105]\n",
      " [-0.62303669  0.9161     -0.55274659 -1.0142489   0.08492083  0.08638839\n",
      "  -1.25857979 -0.64059308  0.38894986]\n",
      " [ 0.3500096  -0.2342138   0.59076651 -0.7574314   0.33517905 -0.45347527\n",
      "  -0.70848347  0.43232902  0.4994292 ]\n",
      " [ 0.83277135 -0.86793908  0.42214169  0.87403136 -0.13990403 -0.03570153\n",
      "   1.18675932  0.11336154 -0.65230878]\n",
      " [-0.8176008   0.8717718  -0.4179728  -0.73836902  0.20327761  0.03659419\n",
      "  -1.18729522 -0.51709781  0.47488018]\n",
      " [-0.01346887  0.23689742  0.40977939 -0.88343734  0.01188377  0.01476572\n",
      "  -0.95826742  0.16811477  0.20531047]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.32950753  1.16643234 -2.11837318 -0.09604701  1.2683815  -1.91331045\n",
      "  -0.55339275]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:48 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.5628005]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 48 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.69842459 -0.72609541  0.33032477  0.86327236 -0.15399745 -0.17323585\n",
      "   1.01090683  0.4285228  -0.32874003]\n",
      " [-0.60843275  0.9161     -0.55274659 -0.99964496  0.08492083  0.10099234\n",
      "  -1.25857979 -0.64059308  0.4035538 ]\n",
      " [ 0.35606661 -0.2342138   0.59076651 -0.75137439  0.33517905 -0.44741825\n",
      "  -0.70848347  0.43232902  0.50548622]\n",
      " [ 0.81902293 -0.86793908  0.42214169  0.86028295 -0.13990403 -0.04944994\n",
      "   1.18675932  0.11336154 -0.6660572 ]\n",
      " [-0.80367638  0.8717718  -0.4179728  -0.7244446   0.20327761  0.05051861\n",
      "  -1.18729522 -0.51709781  0.4888046 ]\n",
      " [-0.0029985   0.23689742  0.40977939 -0.87296696  0.01188377  0.02523609\n",
      "  -0.95826742  0.16811477  0.21578084]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.39874778  1.114262   -2.13487407 -0.12447735  1.21750811 -1.93133254\n",
      "  -0.57672458]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:48 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.71552495]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 48 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.70486914 -0.71965085  0.33032477  0.86971691 -0.15399745 -0.1667913\n",
      "   1.01735139  0.4285228  -0.32874003]\n",
      " [-0.61472203  0.90981072 -0.55274659 -1.00593424  0.08492083  0.09470306\n",
      "  -1.26486907 -0.64059308  0.4035538 ]\n",
      " [ 0.34970829 -0.24057212  0.59076651 -0.75773272  0.33517905 -0.45377658\n",
      "  -0.71484179  0.43232902  0.50548622]\n",
      " [ 0.82518093 -0.86178109  0.42214169  0.86644094 -0.13990403 -0.04329194\n",
      "   1.19291732  0.11336154 -0.6660572 ]\n",
      " [-0.81002713  0.86542105 -0.4179728  -0.73079535  0.20327761  0.04416786\n",
      "  -1.19364598 -0.51709781  0.4888046 ]\n",
      " [-0.00947822  0.2304177   0.40977939 -0.87944669  0.01188377  0.01875637\n",
      "  -0.96474714  0.16811477  0.21578084]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.36979547  1.13864018 -2.13093876 -0.12031879  1.24285012 -1.92720134\n",
      "  -0.57174718]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:48 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.74629606]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 48 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.71006365 -0.71445634  0.33032477  0.87491142 -0.15399745 -0.1667913\n",
      "   1.02254589  0.4285228  -0.32874003]\n",
      " [-0.61979693  0.90473581 -0.55274659 -1.01100915  0.08492083  0.09470306\n",
      "  -1.26994398 -0.64059308  0.4035538 ]\n",
      " [ 0.34439687 -0.24588354  0.59076651 -0.76304413  0.33517905 -0.45377658\n",
      "  -0.72015321  0.43232902  0.50548622]\n",
      " [ 0.83019386 -0.85676816  0.42214169  0.87145387 -0.13990403 -0.04329194\n",
      "   1.19793025  0.11336154 -0.6660572 ]\n",
      " [-0.81522498  0.8602232  -0.4179728  -0.7359932   0.20327761  0.04416786\n",
      "  -1.19884382 -0.51709781  0.4888046 ]\n",
      " [-0.01484332  0.2250526   0.40977939 -0.88481179  0.01188377  0.01875637\n",
      "  -0.97011224  0.16811477  0.21578084]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.34577754  1.15945706 -2.12801394 -0.11542677  1.26406194 -1.9239913\n",
      "  -0.56779006]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:48 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.7714832]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 48 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.71439889 -0.71445634  0.33466001  0.87491142 -0.15399745 -0.16245606\n",
      "   1.02688113  0.4285228  -0.32874003]\n",
      " [-0.62356287  0.90473581 -0.55651253 -1.01100915  0.08492083  0.09093713\n",
      "  -1.27370991 -0.64059308  0.4035538 ]\n",
      " [ 0.34321145 -0.24588354  0.58958109 -0.76304413  0.33517905 -0.454962\n",
      "  -0.72133863  0.43232902  0.50548622]\n",
      " [ 0.83386961 -0.85676816  0.42581744  0.87145387 -0.13990403 -0.03961619\n",
      "   1.201606    0.11336154 -0.6660572 ]\n",
      " [-0.81893006  0.8602232  -0.42167789 -0.7359932   0.20327761  0.04046277\n",
      "  -1.20254891 -0.51709781  0.4888046 ]\n",
      " [-0.01743921  0.2250526   0.40718351 -0.88481179  0.01188377  0.01616048\n",
      "  -0.97270813  0.16811477  0.21578084]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.32563414  1.17697083 -2.12625659 -0.10655178  1.28254064 -1.92229719\n",
      "  -0.56045029]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:48 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.28901206]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 49 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.71063271 -0.71822252  0.33466001  0.87114524 -0.15399745 -0.16245606\n",
      "   1.02688113  0.4285228  -0.33250621]\n",
      " [-0.62120407  0.90709461 -0.55651253 -1.00865035  0.08492083  0.09093713\n",
      "  -1.27370991 -0.64059308  0.4059126 ]\n",
      " [ 0.34439331 -0.24470169  0.58958109 -0.76186228  0.33517905 -0.454962\n",
      "  -0.72133863  0.43232902  0.50666807]\n",
      " [ 0.83252607 -0.8581117   0.42581744  0.87011033 -0.13990403 -0.03961619\n",
      "   1.201606    0.11336154 -0.66740074]\n",
      " [-0.81741769  0.86173556 -0.42167789 -0.73448084  0.20327761  0.04046277\n",
      "  -1.20254891 -0.51709781  0.49031697]\n",
      " [-0.01418995  0.22830186  0.40718351 -0.88156253  0.01188377  0.01616048\n",
      "  -0.97270813  0.16811477  0.2190301 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.35532783  1.15816779 -2.13870257 -0.12021171  1.26634278 -1.93562097\n",
      "  -0.57193134]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:49 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.77618447]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 49 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.71463399 -0.71822252  0.33866129  0.87114524 -0.15399745 -0.16245606\n",
      "   1.03088242  0.4285228  -0.33250621]\n",
      " [-0.62468502  0.90709461 -0.55999347 -1.00865035  0.08492083  0.09093713\n",
      "  -1.27719085 -0.64059308  0.4059126 ]\n",
      " [ 0.34541517 -0.24470169  0.59060296 -0.76186228  0.33517905 -0.454962\n",
      "  -0.72031677  0.43232902  0.50666807]\n",
      " [ 0.83599414 -0.8581117   0.42928551  0.87011033 -0.13990403 -0.03961619\n",
      "   1.20507407  0.11336154 -0.66740074]\n",
      " [-0.82091338  0.86173556 -0.42517357 -0.73448084  0.20327761  0.04046277\n",
      "  -1.20604459 -0.51709781  0.49031697]\n",
      " [-0.01678342  0.22830186  0.40459004 -0.88156253  0.01188377  0.01616048\n",
      "  -0.9753016   0.16811477  0.2190301 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.33588698  1.17543456 -2.13716024 -0.1094617   1.28425337 -1.93406469\n",
      "  -0.56495211]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:49 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.6648304]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 49 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.69824758 -0.73460893  0.32227488  0.87114524 -0.15399745 -0.16245606\n",
      "   1.014496    0.4285228  -0.33250621]\n",
      " [-0.60810307  0.92367655 -0.54341152 -1.00865035  0.08492083  0.09093713\n",
      "  -1.26060891 -0.64059308  0.4059126 ]\n",
      " [ 0.34595209 -0.24416477  0.59113987 -0.76186228  0.33517905 -0.454962\n",
      "  -0.71977985  0.43232902  0.50666807]\n",
      " [ 0.81943866 -0.87466718  0.41273003  0.87011033 -0.13990403 -0.03961619\n",
      "   1.18851859  0.11336154 -0.66740074]\n",
      " [-0.80434346  0.87830548 -0.40860365 -0.73448084  0.20327761  0.04046277\n",
      "  -1.18947467 -0.51709781  0.49031697]\n",
      " [-0.01034186  0.23474342  0.4110316  -0.88156253  0.01188377  0.01616048\n",
      "  -0.96886004  0.16811477  0.2190301 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.40995937  1.11641917 -2.1500783  -0.14596091  1.22249756 -1.94660724\n",
      "  -0.59540733]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:49 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.59082637]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 49 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.70928393 -0.73460893  0.33331123  0.87114524 -0.15399745 -0.16245606\n",
      "   1.014496    0.43955915 -0.33250621]\n",
      " [-0.61895374  0.92367655 -0.55426219 -1.00865035  0.08492083  0.09093713\n",
      "  -1.26060891 -0.65144375  0.4059126 ]\n",
      " [ 0.35689868 -0.24416477  0.60208647 -0.76186228  0.33517905 -0.454962\n",
      "  -0.71977985  0.44327561  0.50666807]\n",
      " [ 0.83034714 -0.87466718  0.42363851  0.87011033 -0.13990403 -0.03961619\n",
      "   1.18851859  0.12427002 -0.66740074]\n",
      " [-0.81528798  0.87830548 -0.41954817 -0.73448084  0.20327761  0.04046277\n",
      "  -1.18947467 -0.52804232  0.49031697]\n",
      " [-0.00384828  0.23474342  0.41752517 -0.88156253  0.01188377  0.01616048\n",
      "  -0.96886004  0.17460834  0.2190301 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.36050039  1.15647359 -2.14301484 -0.10652792  1.26173821 -1.93915961\n",
      "  -0.56382841]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:49 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.55830111]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 49 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.72106238 -0.73460893  0.33331123  0.88292369 -0.15399745 -0.16245606\n",
      "   1.014496    0.43955915 -0.32072776]\n",
      " [-0.63065195  0.92367655 -0.55426219 -1.02034857  0.08492083  0.09093713\n",
      "  -1.26060891 -0.65144375  0.39421438]\n",
      " [ 0.35827986 -0.24416477  0.60208647 -0.7604811   0.33517905 -0.454962\n",
      "  -0.71977985  0.44327561  0.50804924]\n",
      " [ 0.8412389  -0.87466718  0.42363851  0.88100209 -0.13990403 -0.03961619\n",
      "   1.18851859  0.12427002 -0.65650898]\n",
      " [-0.82631737  0.87830548 -0.41954817 -0.74551023  0.20327761  0.04046277\n",
      "  -1.18947467 -0.52804232  0.47928758]\n",
      " [-0.0119842   0.23474342  0.41752517 -0.88969845  0.01188377  0.01616048\n",
      "  -0.96886004  0.17460834  0.21089418]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.3060387   1.19878706 -2.13061724 -0.07791352  1.30190414 -1.9251404\n",
      "  -0.54534913]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:49 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.20896594]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 49 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.72070084 -0.73460893  0.33331123  0.88292369 -0.15435899 -0.1628176\n",
      "   1.014496    0.43955915 -0.3210893 ]\n",
      " [-0.63039062  0.92367655 -0.55426219 -1.02034857  0.08518215  0.09119845\n",
      "  -1.26060891 -0.65144375  0.39447571]\n",
      " [ 0.35546694 -0.24416477  0.60208647 -0.7604811   0.33236613 -0.45777491\n",
      "  -0.71977985  0.44327561  0.50523633]\n",
      " [ 0.84121641 -0.87466718  0.42363851  0.88100209 -0.13992652 -0.03963869\n",
      "   1.18851859  0.12427002 -0.65653147]\n",
      " [-0.82587258  0.87830548 -0.41954817 -0.74551023  0.2037224   0.04090756\n",
      "  -1.18947467 -0.52804232  0.47973237]\n",
      " [-0.01295162  0.23474342  0.41752517 -0.88969845  0.01091635  0.01519307\n",
      "  -0.96886004  0.17460834  0.20992677]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.32330965  1.18978962 -2.13899122 -0.08963058  1.29324617 -1.93333029\n",
      "  -0.55496035]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:49 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.05960783]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 49 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.7209462  -0.73436357  0.33331123  0.88292369 -0.15411363 -0.16257224\n",
      "   1.014496    0.43955915 -0.32084394]\n",
      " [-0.63069174  0.92337543 -0.55426219 -1.02034857  0.08488103  0.09089734\n",
      "  -1.26060891 -0.65144375  0.39417459]\n",
      " [ 0.3552737  -0.24435801  0.60208647 -0.7604811   0.33217289 -0.45796815\n",
      "  -0.71977985  0.44327561  0.50504309]\n",
      " [ 0.84151874 -0.87436485  0.42363851  0.88100209 -0.13962419 -0.03933635\n",
      "   1.18851859  0.12427002 -0.65622914]\n",
      " [-0.82615261  0.87802545 -0.41954817 -0.74551023  0.20344237  0.04062754\n",
      "  -1.18947467 -0.52804232  0.47945234]\n",
      " [-0.01313315  0.23456188  0.41752517 -0.88969845  0.01073482  0.01501153\n",
      "  -0.96886004  0.17460834  0.20974523]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.3249803   1.18921743 -2.14016657 -0.090667    1.29275275 -1.93447467\n",
      "  -0.55598362]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:49 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.49114801]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 49 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.71158483 -0.74372493  0.33331123  0.88292369 -0.16347499 -0.1719336\n",
      "   1.00513464  0.43955915 -0.32084394]\n",
      " [-0.62026106  0.93380611 -0.55426219 -1.02034857  0.09531171  0.10132801\n",
      "  -1.25017823 -0.65144375  0.39417459]\n",
      " [ 0.36515206 -0.23447965  0.60208647 -0.7604811   0.34205125 -0.44808979\n",
      "  -0.70990149  0.44327561  0.50504309]\n",
      " [ 0.8296066  -0.88627699  0.42363851  0.88100209 -0.15153633 -0.0512485\n",
      "   1.17660644  0.12427002 -0.65622914]\n",
      " [-0.8148523   0.88932576 -0.41954817 -0.74551023  0.21474268  0.05192784\n",
      "  -1.17817436 -0.52804232  0.47945234]\n",
      " [-0.00338524  0.2443098   0.41752517 -0.88969845  0.02048273  0.02475945\n",
      "  -0.95911212  0.17460834  0.20974523]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.38635456  1.14842083 -2.15929741 -0.11056289  1.24816709 -1.95229662\n",
      "  -0.57605436]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:49 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.82158338]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 49 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.71377332 -0.74372493  0.33331123  0.88511217 -0.16347499 -0.1719336\n",
      "   1.00732313  0.43955915 -0.32084394]\n",
      " [-0.6221452   0.93380611 -0.55426219 -1.0222327   0.09531171  0.10132801\n",
      "  -1.25206236 -0.65144375  0.39417459]\n",
      " [ 0.36245118 -0.23447965  0.60208647 -0.76318199  0.34205125 -0.44808979\n",
      "  -0.71260237  0.44327561  0.50504309]\n",
      " [ 0.83149443 -0.88627699  0.42363851  0.88288992 -0.15153633 -0.0512485\n",
      "   1.17849428  0.12427002 -0.65622914]\n",
      " [-0.81689523  0.88932576 -0.41954817 -0.74755316  0.21474268  0.05192784\n",
      "  -1.18021729 -0.52804232  0.47945234]\n",
      " [-0.00622443  0.2443098   0.41752517 -0.89253764  0.02048273  0.02475945\n",
      "  -0.96195132  0.17460834  0.20974523]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.37327803  1.16059296 -2.15860944 -0.10730996  1.26055332 -1.95150239\n",
      "  -0.57428098]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:49 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.68985386]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 49 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.72113206 -0.74372493  0.33331123  0.88511217 -0.15611626 -0.16457486\n",
      "   1.01468187  0.43955915 -0.32084394]\n",
      " [-0.62952774  0.93380611 -0.55426219 -1.0222327   0.08792917  0.09394547\n",
      "  -1.25944491 -0.65144375  0.39417459]\n",
      " [ 0.35885746 -0.23447965  0.60208647 -0.76318199  0.33845754 -0.4516835\n",
      "  -0.71619609  0.44327561  0.50504309]\n",
      " [ 0.8387558  -0.88627699  0.42363851  0.88288992 -0.14427496 -0.04398713\n",
      "   1.18575565  0.12427002 -0.65622914]\n",
      " [-0.82423683  0.88932576 -0.41954817 -0.74755316  0.20740108  0.04458625\n",
      "  -1.18755889 -0.52804232  0.47945234]\n",
      " [-0.01245595  0.2443098   0.41752517 -0.89253764  0.01425121  0.01852793\n",
      "  -0.96818284  0.17460834  0.20974523]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.3400993   1.18713273 -2.15338652 -0.09444025  1.28905464 -1.94650796\n",
      "  -0.56484626]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:49 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.56356268]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 49 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.70672909 -0.74372493  0.33331123  0.8707092  -0.15611626 -0.17897783\n",
      "   1.01468187  0.43955915 -0.33524691]\n",
      " [-0.61490142  0.93380611 -0.55426219 -1.00760638  0.08792917  0.10857179\n",
      "  -1.25944491 -0.65144375  0.40880091]\n",
      " [ 0.36475505 -0.23447965  0.60208647 -0.7572844   0.33845754 -0.44578592\n",
      "  -0.71619609  0.44327561  0.51094068]\n",
      " [ 0.82497562 -0.88627699  0.42363851  0.86910974 -0.14427496 -0.0577673\n",
      "   1.18575565  0.12427002 -0.67000931]\n",
      " [-0.81027739  0.88932576 -0.41954817 -0.73359373  0.20740108  0.05854568\n",
      "  -1.18755889 -0.52804232  0.49341177]\n",
      " [-0.00197682  0.2443098   0.41752517 -0.88205851  0.01425121  0.02900705\n",
      "  -0.96818284  0.17460834  0.22022435]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.40940617  1.13486552 -2.169882   -0.12307428  1.23809669 -1.96450398\n",
      "  -0.58820224]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:49 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.7169225]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 49 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.71312504 -0.73732898  0.33331123  0.87710516 -0.15611626 -0.17258188\n",
      "   1.02107782  0.43955915 -0.33524691]\n",
      " [-0.62115275  0.92755478 -0.55426219 -1.01385771  0.08792917  0.10232046\n",
      "  -1.26569624 -0.65144375  0.40880091]\n",
      " [ 0.3584502  -0.2407845   0.60208647 -0.76358925  0.33845754 -0.45209077\n",
      "  -0.72250094  0.44327561  0.51094068]\n",
      " [ 0.83110364 -0.88014897  0.42363851  0.87523776 -0.14427496 -0.05163928\n",
      "   1.19188366  0.12427002 -0.67000931]\n",
      " [-0.81658749  0.88301567 -0.41954817 -0.73990382  0.20740108  0.05223558\n",
      "  -1.19386899 -0.52804232  0.49341177]\n",
      " [-0.00840459  0.23788203  0.41752517 -0.88848628  0.01425121  0.02257929\n",
      "  -0.9746106   0.17460834  0.22022435]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.38068164  1.15903715 -2.16594382 -0.11896103  1.26319876 -1.96037155\n",
      "  -0.58329175]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:49 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.74893789]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 49 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.71822822 -0.73222581  0.33331123  0.88220833 -0.15611626 -0.17258188\n",
      "   1.02618099  0.43955915 -0.33524691]\n",
      " [-0.6261433   0.92256423 -0.55426219 -1.01884826  0.08792917  0.10232046\n",
      "  -1.27068679 -0.65144375  0.40880091]\n",
      " [ 0.35322681 -0.2460079   0.60208647 -0.76881265  0.33845754 -0.45209077\n",
      "  -0.72772433  0.44327561  0.51094068]\n",
      " [ 0.83603634 -0.87521627  0.42363851  0.88017046 -0.14427496 -0.05163928\n",
      "   1.19681636  0.12427002 -0.67000931]\n",
      " [-0.82169739  0.87790577 -0.41954817 -0.74501372  0.20740108  0.05223558\n",
      "  -1.19897888 -0.52804232  0.49341177]\n",
      " [-0.01367398  0.23261264  0.41752517 -0.89375567  0.01425121  0.02257929\n",
      "  -0.97987999  0.17460834  0.22022435]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.35707805  1.17949954 -2.16306299 -0.11417246  1.28403322 -1.95721222\n",
      "  -0.57943645]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:49 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.77416653]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 49 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.7224688  -0.73222581  0.33755182  0.88220833 -0.15611626 -0.16834129\n",
      "   1.03042158  0.43955915 -0.33524691]\n",
      " [-0.62983265  0.92256423 -0.55795153 -1.01884826  0.08792917  0.09863111\n",
      "  -1.27437613 -0.65144375  0.40880091]\n",
      " [ 0.35213265 -0.2460079   0.60099231 -0.76881265  0.33845754 -0.45318492\n",
      "  -0.72881849  0.44327561  0.51094068]\n",
      " [ 0.83964196 -0.87521627  0.42724414  0.88017046 -0.14427496 -0.04803366\n",
      "   1.20042199  0.12427002 -0.67000931]\n",
      " [-0.82532838  0.87790577 -0.42317916 -0.74501372  0.20740108  0.04860459\n",
      "  -1.20260988 -0.52804232  0.49341177]\n",
      " [-0.01620653  0.23261264  0.41499262 -0.89375567  0.01425121  0.02004674\n",
      "  -0.98241254  0.17460834  0.22022435]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.33733651  1.19668399 -2.16134225 -0.10540506  1.3021401  -1.9555521\n",
      "  -0.57222952]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:49 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.2816574]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 50 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.71890544 -0.73578917  0.33755182  0.87864497 -0.15611626 -0.16834129\n",
      "   1.03042158  0.43955915 -0.33881027]\n",
      " [-0.62762826  0.92476862 -0.55795153 -1.01664387  0.08792917  0.09863111\n",
      "  -1.27437613 -0.65144375  0.4110053 ]\n",
      " [ 0.3532074  -0.24493315  0.60099231 -0.7677379   0.33845754 -0.45318492\n",
      "  -0.72881849  0.44327561  0.51201543]\n",
      " [ 0.83840775 -0.87645048  0.42724414  0.87893625 -0.14427496 -0.04803366\n",
      "   1.20042199  0.12427002 -0.67124353]\n",
      " [-0.82392461  0.87930954 -0.42317916 -0.74360994  0.20740108  0.04860459\n",
      "  -1.20260988 -0.52804232  0.49481554]\n",
      " [-0.01311462  0.23570455  0.41499262 -0.89066376  0.01425121  0.02004674\n",
      "  -0.98241254  0.17460834  0.22331626]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.36582989  1.17870006 -2.17334736 -0.11857288  1.28665291 -1.96838573\n",
      "  -0.58327549]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:50 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.78005881]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 50 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.72277176 -0.73578917  0.34141814  0.87864497 -0.15611626 -0.16834129\n",
      "   1.0342879   0.43955915 -0.33881027]\n",
      " [-0.63099401  0.92476862 -0.56131729 -1.01664387  0.08792917  0.09863111\n",
      "  -1.27774188 -0.65144375  0.4110053 ]\n",
      " [ 0.35425709 -0.24493315  0.60204201 -0.7677379   0.33845754 -0.45318492\n",
      "  -0.72776879  0.44327561  0.51201543]\n",
      " [ 0.84176451 -0.87645048  0.4306009   0.87893625 -0.14427496 -0.04803366\n",
      "   1.20377875  0.12427002 -0.67124353]\n",
      " [-0.82730537  0.87930954 -0.42655992 -0.74360994  0.20740108  0.04860459\n",
      "  -1.20599064 -0.52804232  0.49481554]\n",
      " [-0.01563456  0.23570455  0.41247269 -0.89066376  0.01425121  0.02004674\n",
      "  -0.98493248  0.17460834  0.22331626]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.34696256  1.19548484 -2.17186225 -0.1080806   1.30404348 -1.96688654\n",
      "  -0.57650571]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:50 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.66389539]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 50 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.70639332 -0.75216761  0.3250397   0.87864497 -0.15611626 -0.16834129\n",
      "   1.01790946  0.43955915 -0.33881027]\n",
      " [-0.6144118   0.94135083 -0.54473508 -1.01664387  0.08792917  0.09863111\n",
      "  -1.26115968 -0.65144375  0.4110053 ]\n",
      " [ 0.35456081 -0.24462943  0.60234573 -0.7677379   0.33845754 -0.45318492\n",
      "  -0.72746507  0.44327561  0.51201543]\n",
      " [ 0.82520057 -0.89301442  0.41403696  0.87893625 -0.14427496 -0.04803366\n",
      "   1.18721481  0.12427002 -0.67124353]\n",
      " [-0.81073116  0.89588375 -0.40998571 -0.74360994  0.20740108  0.04860459\n",
      "  -1.18941643 -0.52804232  0.49481554]\n",
      " [-0.00930762  0.24203149  0.41879963 -0.89066376  0.01425121  0.02004674\n",
      "  -0.97860554  0.17460834  0.22331626]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.4210328   1.13651038 -2.18488146 -0.14481199  1.24241885 -1.97953167\n",
      "  -0.60708213]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:50 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.59535369]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 50 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.71728428 -0.75216761  0.33593066  0.87864497 -0.15611626 -0.16834129\n",
      "   1.01790946  0.45045011 -0.33881027]\n",
      " [-0.62507313  0.94135083 -0.55539641 -1.01664387  0.08792917  0.09863111\n",
      "  -1.26115968 -0.66210508  0.4110053 ]\n",
      " [ 0.36538932 -0.24462943  0.61317424 -0.7677379   0.33845754 -0.45318492\n",
      "  -0.72746507  0.45410412  0.51201543]\n",
      " [ 0.83597949 -0.89301442  0.42481588  0.87893625 -0.14427496 -0.04803366\n",
      "   1.18721481  0.13504894 -0.67124353]\n",
      " [-0.82149138  0.89588375 -0.42074594 -0.74360994  0.20740108  0.04860459\n",
      "  -1.18941643 -0.53880255  0.49481554]\n",
      " [-0.00276433  0.24203149  0.42534292 -0.89066376  0.01425121  0.02004674\n",
      "  -0.97860554  0.18115163  0.22331626]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.3722916   1.17614712 -2.17803006 -0.10571117  1.28123289 -1.97230804\n",
      "  -0.57578977]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:50 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.56077188]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 50 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.72900903 -0.75216761  0.33593066  0.89036972 -0.15611626 -0.16834129\n",
      "   1.01790946  0.45045011 -0.32708553]\n",
      " [-0.63672028  0.94135083 -0.55539641 -1.02829102  0.08792917  0.09863111\n",
      "  -1.26115968 -0.66210508  0.39935816]\n",
      " [ 0.36686791 -0.24462943  0.61317424 -0.76625931  0.33845754 -0.45318492\n",
      "  -0.72746507  0.45410412  0.51349402]\n",
      " [ 0.84685336 -0.89301442  0.42481588  0.88981012 -0.14427496 -0.04803366\n",
      "   1.18721481  0.13504894 -0.66036966]\n",
      " [-0.83249975  0.89588375 -0.42074594 -0.75461831  0.20740108  0.04860459\n",
      "  -1.18941643 -0.53880255  0.48380718]\n",
      " [-0.01088056  0.24203149  0.42534292 -0.89878     0.01425121  0.02004674\n",
      "  -0.97860554  0.18115163  0.21520002]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.31819917  1.21825969 -2.1658021  -0.07718341  1.32123737 -1.95849561\n",
      "  -0.55748101]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:50 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.20091022]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 50 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.72869716 -0.75216761  0.33593066  0.89036972 -0.15642812 -0.16865316\n",
      "   1.01790946  0.45045011 -0.32739739]\n",
      " [-0.63651558  0.94135083 -0.55539641 -1.02829102  0.08813386  0.09883581\n",
      "  -1.26115968 -0.66210508  0.39956285]\n",
      " [ 0.36419255 -0.24462943  0.61317424 -0.76625931  0.33578218 -0.45586028\n",
      "  -0.72746507  0.45410412  0.51081866]\n",
      " [ 0.84687684 -0.89301442  0.42481588  0.88981012 -0.14425148 -0.04801018\n",
      "   1.18721481  0.13504894 -0.66034618]\n",
      " [-0.83212684  0.89588375 -0.42074594 -0.75461831  0.20777398  0.04897749\n",
      "  -1.18941643 -0.53880255  0.48418008]\n",
      " [-0.01182908  0.24203149  0.42534292 -0.89878     0.0133027   0.01909822\n",
      "  -0.97860554  0.18115163  0.21425151]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.33432677  1.20988372 -2.17366111 -0.08819171  1.31319706 -1.96618597\n",
      "  -0.56650235]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:50 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.05461888]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 50 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.72891019 -0.75195459  0.33593066  0.89036972 -0.1562151  -0.16844014\n",
      "   1.01790946  0.45045011 -0.32718437]\n",
      " [-0.63677483  0.94109158 -0.55539641 -1.02829102  0.08787462  0.09857657\n",
      "  -1.26115968 -0.66210508  0.39930361]\n",
      " [ 0.36402387 -0.2447981   0.61317424 -0.76625931  0.3356135  -0.45602896\n",
      "  -0.72746507  0.45410412  0.51064998]\n",
      " [ 0.84713742 -0.89275384  0.42481588  0.88981012 -0.1439909  -0.0477496\n",
      "   1.18721481  0.13504894 -0.6600856 ]\n",
      " [-0.83236914  0.89564146 -0.42074594 -0.75461831  0.20753169  0.0487352\n",
      "  -1.18941643 -0.53880255  0.48393778]\n",
      " [-0.01198798  0.24187259  0.42534292 -0.89878     0.0131438   0.01893932\n",
      "  -0.97860554  0.18115163  0.21409261]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.33573691  1.20940826 -2.17466114 -0.08907288  1.3127891  -1.96716034\n",
      "  -0.56737241]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:50 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.48173368]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 50 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.71988606 -0.76097871  0.33593066  0.89036972 -0.16523922 -0.17746426\n",
      "   1.00888534  0.45045011 -0.32718437]\n",
      " [-0.62675447  0.95111194 -0.55539641 -1.02829102  0.09789497  0.10859692\n",
      "  -1.25113933 -0.66210508  0.39930361]\n",
      " [ 0.37364409 -0.23517789  0.61317424 -0.76625931  0.34523372 -0.44640874\n",
      "  -0.71784486  0.45410412  0.51064998]\n",
      " [ 0.83564921 -0.90424206  0.42481588  0.88981012 -0.15547911 -0.05923781\n",
      "   1.1757266   0.13504894 -0.6600856 ]\n",
      " [-0.82148371  0.90652688 -0.42074594 -0.75461831  0.21841712  0.05962062\n",
      "  -1.178531   -0.53880255  0.48393778]\n",
      " [-0.00248696  0.25137361  0.42534292 -0.89878     0.02264482  0.02844034\n",
      "  -0.96910451  0.18115163  0.21409261]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.39587325  1.16962506 -2.19368794 -0.10864666  1.26942559 -1.98491739\n",
      "  -0.58710518]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:50 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.82632786]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 50 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.72195227 -0.76097871  0.33593066  0.89243592 -0.16523922 -0.17746426\n",
      "   1.01095154  0.45045011 -0.32718437]\n",
      " [-0.62853489  0.95111194 -0.55539641 -1.03007143  0.09789497  0.10859692\n",
      "  -1.25291974 -0.66210508  0.39930361]\n",
      " [ 0.37106478 -0.23517789  0.61317424 -0.76883862  0.34523372 -0.44640874\n",
      "  -0.72042416  0.45410412  0.51064998]\n",
      " [ 0.83743454 -0.90424206  0.42481588  0.89159545 -0.15547911 -0.05923781\n",
      "   1.17751193  0.13504894 -0.6600856 ]\n",
      " [-0.82341453  0.90652688 -0.42074594 -0.75654913  0.21841712  0.05962062\n",
      "  -1.18046182 -0.53880255  0.48393778]\n",
      " [-0.00518319  0.25137361  0.42534292 -0.90147623  0.02264482  0.02844034\n",
      "  -0.97180074  0.18115163  0.21409261]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.38341139  1.18124055 -2.19304181 -0.10555879  1.28123824 -1.98417185\n",
      "  -0.58544154]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:50 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.69185488]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 50 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.72924137 -0.76097871  0.33593066  0.89243592 -0.15795012 -0.17017515\n",
      "   1.01824064  0.45045011 -0.32718437]\n",
      " [-0.6358453   0.95111194 -0.55539641 -1.03007143  0.09058457  0.10128651\n",
      "  -1.26023015 -0.66210508  0.39930361]\n",
      " [ 0.36754667 -0.23517789  0.61317424 -0.76883862  0.34171561 -0.44992685\n",
      "  -0.72394227  0.45410412  0.51064998]\n",
      " [ 0.84463148 -0.90424206  0.42481588  0.89159545 -0.14828218 -0.05204088\n",
      "   1.18470886  0.13504894 -0.6600856 ]\n",
      " [-0.83068673  0.90652688 -0.42074594 -0.75654913  0.21114492  0.05234843\n",
      "  -1.18773402 -0.53880255  0.48393778]\n",
      " [-0.0113643   0.25137361  0.42534292 -0.90147623  0.01646371  0.02225923\n",
      "  -0.97798185  0.18115163  0.21409261]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.3505644   1.20753864 -2.18785975 -0.09277364  1.30942677 -1.979208\n",
      "  -0.57612096]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:50 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.56434425]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 50 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 7.14805047e-01 -7.60978711e-01  3.35930663e-01  8.77999594e-01\n",
      "  -1.57950120e-01 -1.84611474e-01  1.01824064e+00  4.50450114e-01\n",
      "  -3.41620694e-01]\n",
      " [-6.21196385e-01  9.51111936e-01 -5.55396409e-01 -1.01542252e+00\n",
      "   9.05845653e-02  1.15935424e-01 -1.26023015e+00 -6.62105078e-01\n",
      "   4.13952522e-01]\n",
      " [ 3.73285378e-01 -2.35177888e-01  6.13174236e-01 -7.63099913e-01\n",
      "   3.41715610e-01 -4.44188147e-01 -7.23942269e-01  4.54104117e-01\n",
      "   5.16388683e-01]\n",
      " [ 8.30819117e-01 -9.04242055e-01  4.24815880e-01  8.77783089e-01\n",
      "  -1.48282178e-01 -6.58532356e-02  1.18470886e+00  1.35048941e-01\n",
      "  -6.73897955e-01]\n",
      " [-8.16692522e-01  9.06526884e-01 -4.20745935e-01 -7.42554926e-01\n",
      "   2.11144921e-01  6.63426318e-02 -1.18773402e+00 -5.38802546e-01\n",
      "   4.97931987e-01]\n",
      " [-8.77704890e-04  2.51373611e-01  4.25342918e-01 -8.90989635e-01\n",
      "   1.64637106e-02  3.27458252e-02 -9.77981852e-01  1.81151633e-01\n",
      "   2.24579199e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.41993919  1.15517534 -2.20435019 -0.12161075  1.25838284 -1.99717903\n",
      "  -0.59950341]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:50 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.71824455]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 50 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.72115517 -0.75462859  0.33593066  0.88434971 -0.15795012 -0.17826136\n",
      "   1.02459076  0.45045011 -0.34162069]\n",
      " [-0.62741178  0.94489654 -0.55539641 -1.02163792  0.09058457  0.10972003\n",
      "  -1.26644555 -0.66210508  0.41395252]\n",
      " [ 0.36703179 -0.24143147  0.61317424 -0.7693535   0.34171561 -0.45044173\n",
      "  -0.73019586  0.45410412  0.51638868]\n",
      " [ 0.8369187  -0.89814247  0.42481588  0.88388268 -0.14828218 -0.05975365\n",
      "   1.19080845  0.13504894 -0.67389796]\n",
      " [-0.82296416  0.90025525 -0.42074594 -0.74882656  0.21114492  0.06007099\n",
      "  -1.19400565 -0.53880255  0.49793199]\n",
      " [-0.00725614  0.24499518  0.42534292 -0.89736807  0.01646371  0.02636739\n",
      "  -0.98436028  0.18115163  0.2245792 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.39142986  1.1791508  -2.20040844 -0.11754274  1.28325714 -1.99304428\n",
      "  -0.59465724]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:50 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.75151619]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 50 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.72616977 -0.74961398  0.33593066  0.88936432 -0.15795012 -0.17826136\n",
      "   1.02960537  0.45045011 -0.34162069]\n",
      " [-0.6323203   0.93998802 -0.55539641 -1.02654644  0.09058457  0.10972003\n",
      "  -1.27135407 -0.66210508  0.41395252]\n",
      " [ 0.3618937  -0.24656956  0.61317424 -0.77449159  0.34171561 -0.45044173\n",
      "  -0.73533394  0.45410412  0.51638868]\n",
      " [ 0.84177326 -0.89328791  0.42481588  0.88873723 -0.14828218 -0.05975365\n",
      "   1.195663    0.13504894 -0.67389796]\n",
      " [-0.82798865  0.89523075 -0.42074594 -0.75385106  0.21114492  0.06007099\n",
      "  -1.19903015 -0.53880255  0.49793199]\n",
      " [-0.01243215  0.23981916  0.42534292 -0.90254409  0.01646371  0.02636739\n",
      "  -0.9895363   0.18115163  0.2245792 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.36822898  1.19926804 -2.1975704  -0.11285657  1.30372505 -1.989934\n",
      "  -0.59090048]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:50 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.77675718]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 50 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.73031997 -0.74961398  0.34008086  0.88936432 -0.15795012 -0.17411116\n",
      "   1.03375557  0.45045011 -0.34162069]\n",
      " [-0.63593671  0.93998802 -0.55901282 -1.02654644  0.09058457  0.10610362\n",
      "  -1.27497048 -0.66210508  0.41395252]\n",
      " [ 0.36088534 -0.24656956  0.61216587 -0.77449159  0.34171561 -0.4514501\n",
      "  -0.73634231  0.45410412  0.51638868]\n",
      " [ 0.8453119  -0.89328791  0.42835452  0.88873723 -0.14828218 -0.056215\n",
      "   1.19920165  0.13504894 -0.67389796]\n",
      " [-0.83154913  0.89523075 -0.42430641 -0.75385106  0.21114492  0.05651052\n",
      "  -1.20259062 -0.53880255  0.49793199]\n",
      " [-0.01490302  0.23981916  0.42287205 -0.90254409  0.01646371  0.02389653\n",
      "  -0.99200717  0.18115163  0.2245792 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.34887321  1.21613476 -2.19588418 -0.10419453  1.32147462 -1.98830587\n",
      "  -0.58382006]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:50 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.27446729]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 51 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.72695037 -0.75298358  0.34008086  0.88599472 -0.15795012 -0.17411116\n",
      "   1.03375557  0.45045011 -0.34499029]\n",
      " [-0.63387813  0.9420466  -0.55901282 -1.02448785  0.09058457  0.10610362\n",
      "  -1.27497048 -0.66210508  0.4160111 ]\n",
      " [ 0.36186263 -0.24559227  0.61216587 -0.77351429  0.34171561 -0.4514501\n",
      "  -0.73634231  0.45410412  0.51736598]\n",
      " [ 0.84417979 -0.89442003  0.42835452  0.88760512 -0.14828218 -0.056215\n",
      "   1.19920165  0.13504894 -0.67503007]\n",
      " [-0.83024782  0.89653206 -0.42430641 -0.75254975  0.21114492  0.05651052\n",
      "  -1.20259062 -0.53880255  0.4992333 ]\n",
      " [-0.01196134  0.24276085  0.42287205 -0.8996024   0.01646371  0.02389653\n",
      "  -0.99200717  0.18115163  0.22752088]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.37620123  1.19894212 -2.20745679 -0.11687788  1.30667324 -2.00066055\n",
      "  -0.59444071]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:51 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.78381091]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 51 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.73068809 -0.75298358  0.34381857  0.88599472 -0.15795012 -0.17411116\n",
      "   1.03749328  0.45045011 -0.34499029]\n",
      " [-0.63713439  0.9420466  -0.56226908 -1.02448785  0.09058457  0.10610362\n",
      "  -1.27822674 -0.66210508  0.4160111 ]\n",
      " [ 0.36293581 -0.24559227  0.61323905 -0.77351429  0.34171561 -0.4514501\n",
      "  -0.73526913  0.45410412  0.51736598]\n",
      " [ 0.84743051 -0.89442003  0.43160525  0.88760512 -0.14828218 -0.056215\n",
      "   1.20245237  0.13504894 -0.67503007]\n",
      " [-0.83351936  0.89653206 -0.42757795 -0.75254975  0.21114492  0.05651052\n",
      "  -1.20586216 -0.53880255  0.4992333 ]\n",
      " [-0.01440972  0.24276085  0.42042367 -0.8996024   0.01646371  0.02389653\n",
      "  -0.99445555  0.18115163  0.22752088]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.35788447  1.21526199 -2.20602548 -0.10663618  1.3235638  -1.99921503\n",
      "  -0.58787085]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:51 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.66283297]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 51 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.71431902 -0.76935265  0.3274495   0.88599472 -0.15795012 -0.17411116\n",
      "   1.02112421  0.45045011 -0.34499029]\n",
      " [-0.62055331  0.95862768 -0.545688   -1.02448785  0.09058457  0.10610362\n",
      "  -1.26164566 -0.66210508  0.4160111 ]\n",
      " [ 0.36302259 -0.24550549  0.61332583 -0.77351429  0.34171561 -0.4514501\n",
      "  -0.73518235  0.45410412  0.51736598]\n",
      " [ 0.83086012 -0.91099042  0.41503486  0.88760512 -0.14828218 -0.056215\n",
      "   1.18588198  0.13504894 -0.67503007]\n",
      " [-0.81694217  0.91310924 -0.41100076 -0.75254975  0.21114492  0.05651052\n",
      "  -1.18928497 -0.53880255  0.4992333 ]\n",
      " [-0.00819633  0.24897424  0.42663706 -0.8996024   0.01646371  0.02389653\n",
      "  -0.98824216  0.18115163  0.22752088]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.43195122  1.15633449 -2.21914849 -0.14358278  1.26207334 -2.01196603\n",
      "  -0.61856635]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:51 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.59979922]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 51 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.72506187 -0.76935265  0.33819236  0.88599472 -0.15795012 -0.17411116\n",
      "   1.02112421  0.46119297 -0.34499029]\n",
      " [-0.63102761  0.95862768 -0.5561623  -1.02448785  0.09058457  0.10610362\n",
      "  -1.26164566 -0.67257938  0.4160111 ]\n",
      " [ 0.37372507 -0.24550549  0.62402831 -0.77351429  0.34171561 -0.4514501\n",
      "  -0.73518235  0.46480659  0.51736598]\n",
      " [ 0.84150695 -0.91099042  0.42568168  0.88760512 -0.14828218 -0.056215\n",
      "   1.18588198  0.14569576 -0.67503007]\n",
      " [-0.82751977  0.91310924 -0.42157836 -0.75254975  0.21114492  0.05651052\n",
      "  -1.18928497 -0.54938014  0.4992333 ]\n",
      " [-0.00160663  0.24897424  0.43322676 -0.8996024   0.01646371  0.02389653\n",
      "  -0.98824216  0.18774133  0.22752088]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.3839191   1.19554845 -2.21249914 -0.10482221  1.30045786 -2.00495586\n",
      "  -0.58755859]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:51 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.56327362]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 51 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.73672956 -0.76935265  0.33819236  0.89766241 -0.15795012 -0.17411116\n",
      "   1.02112421  0.46119297 -0.3333226 ]\n",
      " [-0.64262033  0.95862768 -0.5561623  -1.03608057  0.09058457  0.10610362\n",
      "  -1.26164566 -0.67257938  0.40441839]\n",
      " [ 0.37529858 -0.24550549  0.62402831 -0.77194078  0.34171561 -0.4514501\n",
      "  -0.73518235  0.46480659  0.51893949]\n",
      " [ 0.85235847 -0.91099042  0.42568168  0.89845665 -0.14828218 -0.056215\n",
      "   1.18588198  0.14569576 -0.66417854]\n",
      " [-0.83850233  0.91310924 -0.42157836 -0.76353231  0.21114492  0.05651052\n",
      "  -1.18928497 -0.54938014  0.48825073]\n",
      " [-0.0097001   0.24897424  0.43322676 -0.90769588  0.01646371  0.02389653\n",
      "  -0.98824216  0.18774133  0.21942741]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.33020253  1.23745106 -2.20043858 -0.07638679  1.34029181 -1.99134688\n",
      "  -0.56942003]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:51 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.1932114]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 51 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.7364613  -0.76935265  0.33819236  0.89766241 -0.15821838 -0.17437942\n",
      "   1.02112421  0.46119297 -0.33359086]\n",
      " [-0.6424641   0.95862768 -0.5561623  -1.03608057  0.09074079  0.10625984\n",
      "  -1.26164566 -0.67257938  0.40457461]\n",
      " [ 0.3727567  -0.24550549  0.62402831 -0.77194078  0.33917373 -0.45399198\n",
      "  -0.73518235  0.46480659  0.51639761]\n",
      " [ 0.8524199  -0.91099042  0.42568168  0.89845665 -0.14822075 -0.05615358\n",
      "   1.18588198  0.14569576 -0.66411711]\n",
      " [-0.83819191  0.91310924 -0.42157836 -0.76353231  0.21145534  0.05682094\n",
      "  -1.18928497 -0.54938014  0.48856116]\n",
      " [-0.01062705  0.24897424  0.43322676 -0.90769588  0.01553676  0.02296958\n",
      "  -0.98824216  0.18774133  0.21850046]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.3452615   1.22965309 -2.2078118  -0.08672703  1.33282375 -1.99856559\n",
      "  -0.57788616]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:51 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.05007435]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 51 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.73664595 -0.769168    0.33819236  0.89766241 -0.15803373 -0.17419477\n",
      "   1.02112421  0.46119297 -0.33340621]\n",
      " [-0.64268702  0.95840477 -0.5561623  -1.03608057  0.09051787  0.10603692\n",
      "  -1.26164566 -0.67257938  0.40435169]\n",
      " [ 0.37260976 -0.24565243  0.62402831 -0.77194078  0.33902679 -0.45413892\n",
      "  -0.73518235  0.46480659  0.51625067]\n",
      " [ 0.85264418 -0.91076614  0.42568168  0.89845665 -0.14799647 -0.0559293\n",
      "   1.18588198  0.14569576 -0.66389283]\n",
      " [-0.83840122  0.91289994 -0.42157836 -0.76353231  0.21124604  0.05661164\n",
      "  -1.18928497 -0.54938014  0.48835185]\n",
      " [-0.01076585  0.24883544  0.43322676 -0.90769588  0.01539797  0.02283079\n",
      "  -0.98824216  0.18774133  0.21836167]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.34645244  1.22925768 -2.20866282 -0.08747638  1.33248609 -1.99939535\n",
      "  -0.57862617]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:51 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.47224553]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 51 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.72795897 -0.77785498  0.33819236  0.89766241 -0.16672071 -0.18288175\n",
      "   1.01243723  0.46119297 -0.33340621]\n",
      " [-0.63307576  0.96801603 -0.5561623  -1.03608057  0.10012913  0.11564818\n",
      "  -1.25203439 -0.67257938  0.40435169]\n",
      " [ 0.3819726  -0.23628958  0.62402831 -0.77194078  0.34838964 -0.44477608\n",
      "  -0.72581951  0.46480659  0.51625067]\n",
      " [ 0.84158491 -0.92182541  0.42568168  0.89845665 -0.15905575 -0.06698857\n",
      "   1.17482271  0.14569576 -0.66389283]\n",
      " [-0.82793312  0.92336804 -0.42157836 -0.76353231  0.22171414  0.06707974\n",
      "  -1.17881687 -0.54938014  0.48835185]\n",
      " [-0.00151427  0.25808702  0.43322676 -0.90769588  0.02464955  0.03208237\n",
      "  -0.97899058  0.18774133  0.21836167]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.40530124  1.19050896 -2.22755081 -0.10669984  1.29036078 -2.01705456\n",
      "  -0.5979974 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:51 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.83090592]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 51 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.72991054 -0.77785498  0.33819236  0.89961398 -0.16672071 -0.18288175\n",
      "   1.0143888   0.46119297 -0.33340621]\n",
      " [-0.63475885  0.96801603 -0.5561623  -1.03776366  0.10012913  0.11564818\n",
      "  -1.25371748 -0.67257938  0.40435169]\n",
      " [ 0.37950879 -0.23628958  0.62402831 -0.7744046   0.34838964 -0.44477608\n",
      "  -0.72828332  0.46480659  0.51625067]\n",
      " [ 0.84327392 -0.92182541  0.42568168  0.90014566 -0.15905575 -0.06698857\n",
      "   1.17651172  0.14569576 -0.66389283]\n",
      " [-0.82975873  0.92336804 -0.42157836 -0.76535792  0.22171414  0.06707974\n",
      "  -1.18064249 -0.54938014  0.48835185]\n",
      " [-0.0040751   0.25808702  0.43322676 -0.91025671  0.02464955  0.03208237\n",
      "  -0.98155142  0.18774133  0.21836167]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.39342228  1.20159525 -2.22694361 -0.10376819  1.30162889 -2.01635427\n",
      "  -0.59643591]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:51 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.69386473]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 51 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.73712954 -0.77785498  0.33819236  0.89961398 -0.15950171 -0.17566275\n",
      "   1.0216078   0.46119297 -0.33340621]\n",
      " [-0.64199659  0.96801603 -0.5561623  -1.03776366  0.0928914   0.10841045\n",
      "  -1.26095522 -0.67257938  0.40435169]\n",
      " [ 0.3760638  -0.23628958  0.62402831 -0.7744046   0.34494464 -0.44822107\n",
      "  -0.73172831  0.46480659  0.51625067]\n",
      " [ 0.85040526 -0.92182541  0.42568168  0.90014566 -0.1519244  -0.05985722\n",
      "   1.18364306  0.14569576 -0.66389283]\n",
      " [-0.83696079  0.92336804 -0.42157836 -0.76535792  0.21451208  0.05987768\n",
      "  -1.18784454 -0.54938014  0.48835185]\n",
      " [-0.01020541  0.25808702  0.43322676 -0.91025671  0.01851924  0.02595206\n",
      "  -0.98768172  0.18774133  0.21836167]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.36090819  1.22765013 -2.22180428 -0.09107105  1.32950599 -2.01142305\n",
      "  -0.58722958]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:51 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.56513855]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 51 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 7.22660603e-01 -7.77854983e-01  3.38192357e-01  8.85145045e-01\n",
      "  -1.59501709e-01 -1.90131685e-01  1.02160780e+00  4.61192968e-01\n",
      "  -3.47875150e-01]\n",
      " [-6.27325040e-01  9.68016031e-01 -5.56162303e-01 -1.02309211e+00\n",
      "   9.28913954e-02  1.23081994e-01 -1.26095522e+00 -6.72579383e-01\n",
      "   4.19023242e-01]\n",
      " [ 3.81644664e-01 -2.36289581e-01  6.24028305e-01 -7.68823730e-01\n",
      "   3.44944644e-01 -4.42640203e-01 -7.31728312e-01  4.64806594e-01\n",
      "   5.21831535e-01]\n",
      " [ 8.36560452e-01 -9.21825410e-01  4.25681679e-01  8.86300846e-01\n",
      "  -1.51924399e-01 -7.37020334e-02  1.18364306e+00  1.45695763e-01\n",
      "  -6.77737644e-01]\n",
      " [-8.22932170e-01  9.23368038e-01 -4.21578360e-01 -7.51329306e-01\n",
      "   2.14512079e-01  7.39062959e-02 -1.18784454e+00 -5.49380144e-01\n",
      "   5.02380467e-01]\n",
      " [ 2.87435779e-04  2.58087016e-01  4.33226759e-01 -8.99763867e-01\n",
      "   1.85192429e-02  3.64449054e-02 -9.87681721e-01  1.87741331e-01\n",
      "   2.28854511e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.43035156  1.17519195 -2.23829    -0.12010983  1.27837517 -2.02937004\n",
      "  -0.61064043]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:51 with online instance: 10-------------\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.71949975]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 51 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.72896731 -0.77154828  0.33819236  0.89145175 -0.15950171 -0.18382498\n",
      "   1.02791451  0.46119297 -0.34787515]\n",
      " [-0.63350621  0.96183486 -0.5561623  -1.02927328  0.0928914   0.11690082\n",
      "  -1.26713639 -0.67257938  0.41902324]\n",
      " [ 0.37544047 -0.24249378  0.62402831 -0.77502792  0.34494464 -0.4488444\n",
      "  -0.73793251  0.46480659  0.52183153]\n",
      " [ 0.84263286 -0.91575301  0.42568168  0.89237325 -0.1519244  -0.06762963\n",
      "   1.18971547  0.14569576 -0.67773764]\n",
      " [-0.82916722  0.91713299 -0.42157836 -0.75756436  0.21451208  0.06767124\n",
      "  -1.1940796  -0.54938014  0.50238047]\n",
      " [-0.00604395  0.25175563  0.43322676 -0.90609526  0.01851924  0.03011352\n",
      "  -0.99401311  0.18774133  0.22885451]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.4020463   1.19898052 -2.23434421 -0.11608724  1.3030327  -2.0252322\n",
      "  -0.60585642]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:51 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.75403474]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 51 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.73389593 -0.76661965  0.33819236  0.89638038 -0.15950171 -0.18382498\n",
      "   1.03284313  0.46119297 -0.34787515]\n",
      " [-0.63833485  0.95700622 -0.5561623  -1.03410192  0.0928914   0.11690082\n",
      "  -1.27196504 -0.67257938  0.41902324]\n",
      " [ 0.37038518 -0.24754907  0.62402831 -0.78008321  0.34494464 -0.4488444\n",
      "  -0.7429878   0.46480659  0.52183153]\n",
      " [ 0.84741117 -0.91097469  0.42568168  0.89715156 -0.1519244  -0.06762963\n",
      "   1.19449378  0.14569576 -0.67773764]\n",
      " [-0.83410868  0.91219153 -0.42157836 -0.76250581  0.21451208  0.06767124\n",
      "  -1.19902105 -0.54938014  0.50238047]\n",
      " [-0.01112881  0.24667078  0.43322676 -0.91118011  0.01851924  0.03011352\n",
      "  -0.99909796  0.18774133  0.22885451]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.37923716  1.21876155 -2.23154792 -0.11150245  1.32314433 -2.0221695\n",
      "  -0.6021952 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:51 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.77925692]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 51 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.73795984 -0.76661965  0.34225627  0.89638038 -0.15950171 -0.17976107\n",
      "   1.03690704  0.46119297 -0.34787515]\n",
      " [-0.64188181  0.95700622 -0.55970926 -1.03410192  0.0928914   0.11335387\n",
      "  -1.27551199 -0.67257938  0.41902324]\n",
      " [ 0.36945722 -0.24754907  0.62310035 -0.78008321  0.34494464 -0.44977236\n",
      "  -0.74391576  0.46480659  0.52183153]\n",
      " [ 0.85088583 -0.91097469  0.42915634  0.89715156 -0.1519244  -0.06415497\n",
      "   1.19796844  0.14569576 -0.67773764]\n",
      " [-0.83760201  0.91219153 -0.42507169 -0.76250581  0.21451208  0.06417791\n",
      "  -1.20251438 -0.54938014  0.50238047]\n",
      " [-0.01353974  0.24667078  0.43081582 -0.91118011  0.01851924  0.02770258\n",
      "  -1.0015089   0.18774133  0.22885451]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.36025153  1.23532199 -2.22989426 -0.10294364  1.34055081 -2.02057155\n",
      "  -0.59523532]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:51 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.26743873]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 52 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.7347752  -0.76980429  0.34225627  0.89319574 -0.15950171 -0.17976107\n",
      "   1.03690704  0.46119297 -0.35105979]\n",
      " [-0.63996082  0.95892721 -0.55970926 -1.03218093  0.0928914   0.11335387\n",
      "  -1.27551199 -0.67257938  0.42094423]\n",
      " [ 0.37034605 -0.24666023  0.62310035 -0.77919438  0.34494464 -0.44977236\n",
      "  -0.74391576  0.46480659  0.52272037]\n",
      " [ 0.84984894 -0.91201158  0.42915634  0.89611467 -0.1519244  -0.06415497\n",
      "   1.19796844  0.14569576 -0.67877454]\n",
      " [-0.83639726  0.91339629 -0.42507169 -0.76130106  0.21451208  0.06417791\n",
      "  -1.20251438 -0.54938014  0.50358522]\n",
      " [-0.01074133  0.24946918  0.43081582 -0.9083817   0.01851924  0.02770258\n",
      "  -1.0015089   0.18774133  0.23165292]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.3864492   1.21889313 -2.24104317 -0.11515088  1.32641069 -2.0324587\n",
      "  -0.60544068]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:52 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.78744165]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 52 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.73839046 -0.76980429  0.34587152  0.89319574 -0.15950171 -0.17976107\n",
      "   1.0405223   0.46119297 -0.35105979]\n",
      " [-0.64311304  0.95892721 -0.56286148 -1.03218093  0.0928914   0.11335387\n",
      "  -1.27866422 -0.67257938  0.42094423]\n",
      " [ 0.37143867 -0.24666023  0.62419296 -0.77919438  0.34494464 -0.44977236\n",
      "  -0.74282314  0.46480659  0.52272037]\n",
      " [ 0.85299867 -0.91201158  0.43230607  0.89611467 -0.1519244  -0.06415497\n",
      "   1.20111818  0.14569576 -0.67877454]\n",
      " [-0.83956502  0.91339629 -0.42823945 -0.76130106  0.21451208  0.06417791\n",
      "  -1.20568214 -0.54938014  0.50358522]\n",
      " [-0.01312029  0.24946918  0.42843687 -0.9083817   0.01851924  0.02770258\n",
      "  -1.00388786  0.18774133  0.23165292]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.36866047  1.23476512 -2.23966247 -0.10515253  1.342821   -2.03106368\n",
      "  -0.59906159]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:52 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.66164583]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 52 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.72203221 -0.78616254  0.32951327  0.89319574 -0.15950171 -0.17976107\n",
      "   1.02416405  0.46119297 -0.35105979]\n",
      " [-0.62653462  0.97550563 -0.54628306 -1.03218093  0.0928914   0.11335387\n",
      "  -1.2620858  -0.67257938  0.42094423]\n",
      " [ 0.37132483 -0.24677407  0.62407912 -0.77919438  0.34494464 -0.44977236\n",
      "  -0.74293698  0.46480659  0.52272037]\n",
      " [ 0.836424   -0.92858626  0.4157314   0.89611467 -0.1519244  -0.06415497\n",
      "   1.1845435   0.14569576 -0.67877454]\n",
      " [-0.82298637  0.92997493 -0.41166081 -0.76130106  0.21451208  0.06417791\n",
      "  -1.1891035  -0.54938014  0.50358522]\n",
      " [-0.00701877  0.2555707   0.43453838 -0.9083817   0.01851924  0.02770258\n",
      "  -0.99778634  0.18774133  0.23165292]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.44272201  1.17589046 -2.25289149 -0.14229714  1.28146756 -2.04392325\n",
      "  -0.62987324]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:52 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.60415335]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 52 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 7.32625324e-01 -7.86162540e-01  3.40106388e-01  8.93195736e-01\n",
      "  -1.59501709e-01 -1.79761072e-01  1.02416405e+00  4.71786081e-01\n",
      "  -3.51059791e-01]\n",
      " [-6.36824878e-01  9.75505629e-01 -5.56573319e-01 -1.03218093e+00\n",
      "   9.28913954e-02  1.13353867e-01 -1.26208580e+00 -6.82869638e-01\n",
      "   4.20944234e-01]\n",
      " [ 3.81894750e-01 -2.46774069e-01  6.34649043e-01 -7.79194381e-01\n",
      "   3.44944644e-01 -4.49772355e-01 -7.42936980e-01  4.75376515e-01\n",
      "   5.22720368e-01]\n",
      " [ 8.46937110e-01 -9.28586260e-01  4.26244510e-01  8.96114673e-01\n",
      "  -1.51924399e-01 -6.41549699e-02  1.18454350e+00  1.56208877e-01\n",
      "  -6.78774536e-01]\n",
      " [-8.33383734e-01  9.29974931e-01 -4.22058173e-01 -7.61301058e-01\n",
      "   2.14512079e-01  6.41779101e-02 -1.18910350e+00 -5.59777507e-01\n",
      "   5.03585221e-01]\n",
      " [-3.85885136e-04  2.55570699e-01  4.41171270e-01 -9.08381699e-01\n",
      "   1.85192429e-02  2.77025793e-02 -9.97786341e-01  1.94374220e-01\n",
      "   2.31652920e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.39538823  1.21467891 -2.24643449 -0.10388271  1.31942181 -2.03711635\n",
      "  -0.59914699]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:52 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.56579422]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 52 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.74423307 -0.78616254  0.34010639  0.90480348 -0.15950171 -0.17976107\n",
      "   1.02416405  0.47178608 -0.33945205]\n",
      " [-0.64836024  0.97550563 -0.55657332 -1.0437163   0.0928914   0.11335387\n",
      "  -1.2620858  -0.68286964  0.40940887]\n",
      " [ 0.38356055 -0.24677407  0.63464904 -0.77752858  0.34494464 -0.44977236\n",
      "  -0.74293698  0.47537651  0.52438617]\n",
      " [ 0.85776229 -0.92858626  0.42624451  0.90693985 -0.1519244  -0.06415497\n",
      "   1.1845435   0.15620888 -0.66794936]\n",
      " [-0.84433621  0.92997493 -0.42205817 -0.77225353  0.21451208  0.06417791\n",
      "  -1.1891035  -0.55977751  0.49263275]\n",
      " [-0.00845388  0.2555707   0.44117127 -0.91644969  0.01851924  0.02770258\n",
      "  -0.99778634  0.19437422  0.22358493]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.34205232  1.25636395 -2.23453867 -0.07554458  1.35907743 -2.0237071\n",
      "  -0.58117783]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:52 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.18586031]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 52 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.74400299 -0.78616254  0.34010639  0.90480348 -0.15973179 -0.17999115\n",
      "   1.02416405  0.47178608 -0.33968213]\n",
      " [-0.6482453   0.97550563 -0.55657332 -1.0437163   0.09300634  0.11346881\n",
      "  -1.2620858  -0.68286964  0.40952381]\n",
      " [ 0.38114766 -0.24677407  0.63464904 -0.77752858  0.34253176 -0.45218524\n",
      "  -0.74293698  0.47537651  0.52197328]\n",
      " [ 0.85785461 -0.92858626  0.42624451  0.90693985 -0.15183208 -0.06406265\n",
      "   1.1845435   0.15620888 -0.66785704]\n",
      " [-0.84407987  0.92997493 -0.42205817 -0.77225353  0.21476841  0.06443424\n",
      "  -1.1891035  -0.55977751  0.49288908]\n",
      " [-0.00935714  0.2555707   0.44117127 -0.91644969  0.01761598  0.02679932\n",
      "  -0.99778634  0.19437422  0.22268166]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.35611416  1.24910278 -2.24145462 -0.08525627  1.35213884 -2.03048146\n",
      "  -0.58912233]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:52 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.04593713]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 52 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.74416285 -0.78600268  0.34010639  0.90480348 -0.15957193 -0.17983129\n",
      "   1.02416405  0.47178608 -0.33952227]\n",
      " [-0.64843682  0.9753141  -0.55657332 -1.0437163   0.09281482  0.11327729\n",
      "  -1.2620858  -0.68286964  0.40933229]\n",
      " [ 0.38101987 -0.24690186  0.63464904 -0.77752858  0.34240396 -0.45231303\n",
      "  -0.74293698  0.47537651  0.52184549]\n",
      " [ 0.85804745 -0.92839341  0.42624451  0.90693985 -0.15163923 -0.0638698\n",
      "   1.1845435   0.15620888 -0.66766419]\n",
      " [-0.84426047  0.92979434 -0.42205817 -0.77225353  0.21458782  0.06425365\n",
      "  -1.1891035  -0.55977751  0.49270849]\n",
      " [-0.00947817  0.25544967  0.44117127 -0.91644969  0.01749495  0.02667829\n",
      "  -0.99778634  0.19437422  0.22256063]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.35712081  1.24877356 -2.24217916 -0.08589385  1.35185901 -2.03118836\n",
      "  -0.58975207]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:52 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.46272941]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 52 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 7.35810886e-01 -7.94354641e-01  3.40106388e-01  9.04803478e-01\n",
      "  -1.67923889e-01 -1.88183253e-01  1.01581209e+00  4.71786081e-01\n",
      "  -3.39522266e-01]\n",
      " [-6.39230840e-01  9.84520088e-01 -5.56573319e-01 -1.04371630e+00\n",
      "   1.02020800e-01  1.22483272e-01 -1.25287981e+00 -6.82869638e-01\n",
      "   4.09332288e-01]\n",
      " [ 3.90127452e-01 -2.37794278e-01  6.34649043e-01 -7.77528583e-01\n",
      "   3.51511548e-01 -4.43205451e-01 -7.33829397e-01  4.75376515e-01\n",
      "   5.21845486e-01]\n",
      " [ 8.47418781e-01 -9.39022088e-01  4.26244510e-01  9.06939849e-01\n",
      "  -1.62267905e-01 -7.44984756e-02  1.17391483e+00  1.56208877e-01\n",
      "  -6.67664191e-01]\n",
      " [-8.34209183e-01  9.39845620e-01 -4.22058173e-01 -7.72253530e-01\n",
      "   2.24639102e-01  7.43049330e-02 -1.17905222e+00 -5.59777507e-01\n",
      "   4.92708488e-01]\n",
      " [-4.76989168e-04  2.64450851e-01  4.41171270e-01 -9.16449692e-01\n",
      "   2.64961319e-02  3.56794683e-02 -9.88785158e-01  1.94374220e-01\n",
      "   2.22560633e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.41464059  1.21107432 -2.26089545 -0.10474154  1.31098086 -2.04871814\n",
      "  -0.60874051]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:52 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.83532276]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 52 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.73765501 -0.79435464  0.34010639  0.9066476  -0.16792389 -0.18818325\n",
      "   1.01765621  0.47178608 -0.33952227]\n",
      " [-0.64082261  0.98452009 -0.55657332 -1.04530807  0.1020208   0.12248327\n",
      "  -1.25447158 -0.68286964  0.40933229]\n",
      " [ 0.3877733  -0.23779428  0.63464904 -0.77988274  0.35151155 -0.44320545\n",
      "  -0.73618355  0.47537651  0.52184549]\n",
      " [ 0.84901727 -0.93902209  0.42624451  0.90853834 -0.1622679  -0.07449848\n",
      "   1.17551332  0.15620888 -0.66766419]\n",
      " [-0.83593606  0.93984562 -0.42205817 -0.77398041  0.2246391   0.07430493\n",
      "  -1.18077909 -0.55977751  0.49270849]\n",
      " [-0.00290966  0.26445085  0.44117127 -0.91888236  0.02649613  0.03567947\n",
      "  -0.99121782  0.19437422  0.22256063]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.4033142   1.22165776 -2.26032448 -0.1019577   1.32173217 -2.04805995\n",
      "  -0.60727416]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:52 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.69588928]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 52 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.74480325 -0.79435464  0.34010639  0.9066476  -0.16077565 -0.18103501\n",
      "   1.02480445  0.47178608 -0.33952227]\n",
      " [-0.64798694  0.98452009 -0.55657332 -1.04530807  0.09485647  0.11531894\n",
      "  -1.26163592 -0.68286964  0.40933229]\n",
      " [ 0.38439882 -0.23779428  0.63464904 -0.77988274  0.34813707 -0.44657993\n",
      "  -0.73955803  0.47537651  0.52184549]\n",
      " [ 0.85608171 -0.93902209  0.42624451  0.90853834 -0.15520347 -0.06743404\n",
      "   1.18257775  0.15620888 -0.66766419]\n",
      " [-0.84306705  0.93984562 -0.42205817 -0.77398041  0.21750811  0.06717394\n",
      "  -1.18791008 -0.55977751  0.49270849]\n",
      " [-0.00898869  0.26445085  0.44117127 -0.91888236  0.0204171   0.02960044\n",
      "  -0.99729686  0.19437422  0.22256063]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.37113513  1.24746724 -2.25522992 -0.08935264  1.34929847 -2.04316363\n",
      "  -0.59818264]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:52 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.56593824]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 52 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.73030256 -0.79435464  0.34010639  0.89214692 -0.16077565 -0.19553569\n",
      "   1.02480445  0.47178608 -0.35402295]\n",
      " [-0.6332929   0.98452009 -0.55657332 -1.03061403  0.09485647  0.13001298\n",
      "  -1.26163592 -0.68286964  0.42402633]\n",
      " [ 0.38982332 -0.23779428  0.63464904 -0.77445823  0.34813707 -0.44115543\n",
      "  -0.73955803  0.47537651  0.52726999]\n",
      " [ 0.84220437 -0.93902209  0.42624451  0.894661   -0.15520347 -0.08131137\n",
      "   1.18257775  0.15620888 -0.68154153]\n",
      " [-0.82900454  0.93984562 -0.42205817 -0.75991789  0.21750811  0.08123646\n",
      "  -1.18791008 -0.55977751  0.506771  ]\n",
      " [ 0.00150921  0.26445085  0.44117127 -0.90838446  0.0204171   0.04009833\n",
      "  -0.99729686  0.19437422  0.23305853]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.4406471   1.19491592 -2.27171123 -0.11859094  1.29808044 -2.0610874\n",
      "  -0.62162343]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:52 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.72069513]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 52 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.73656799 -0.78808921  0.34010639  0.89841235 -0.16077565 -0.18927026\n",
      "   1.03106988  0.47178608 -0.35402295]\n",
      " [-0.63944132  0.97837167 -0.55657332 -1.03676245  0.09485647  0.12386456\n",
      "  -1.26778433 -0.68286964  0.42402633]\n",
      " [ 0.38366694 -0.24395066  0.63464904 -0.78061462  0.34813707 -0.44731181\n",
      "  -0.74571441  0.47537651  0.52726999]\n",
      " [ 0.84825061 -0.93297585  0.42624451  0.90070724 -0.15520347 -0.07526513\n",
      "   1.18862399  0.15620888 -0.68154153]\n",
      " [-0.83520462  0.93364554 -0.42205817 -0.76611798  0.21750811  0.07503638\n",
      "  -1.19411016 -0.55977751  0.506771  ]\n",
      " [-0.00477716  0.25816449  0.44117127 -0.91467083  0.0204171   0.03381197\n",
      "  -1.00358322  0.19437422  0.23305853]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.41253595  1.21852602 -2.26776116 -0.11461408  1.32253125 -2.05694597\n",
      "  -0.61689979]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:52 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.7564965]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 52 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.74141308 -0.78324413  0.34010639  0.90325743 -0.16077565 -0.18927026\n",
      "   1.03591496  0.47178608 -0.35402295]\n",
      " [-0.64419212  0.97362087 -0.55657332 -1.04151325  0.09485647  0.12386456\n",
      "  -1.27253514 -0.68286964  0.42402633]\n",
      " [ 0.37869209 -0.24892551  0.63464904 -0.78558946  0.34813707 -0.44731181\n",
      "  -0.75068926  0.47537651  0.52726999]\n",
      " [ 0.85295448 -0.92827199  0.42624451  0.90541111 -0.15520347 -0.07526513\n",
      "   1.19332786  0.15620888 -0.68154153]\n",
      " [-0.84006525  0.92878491 -0.42205817 -0.77097861  0.21750811  0.07503638\n",
      "  -1.1989708  -0.55977751  0.506771  ]\n",
      " [-0.00977293  0.25316871  0.44117127 -0.9196666   0.0204171   0.03381197\n",
      "  -1.008579    0.19437422  0.23305853]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.39010811  1.2379794  -2.26500566 -0.11012966  1.34229645 -2.05392954\n",
      "  -0.6133313 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:52 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.78166735]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 52 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.74539467 -0.78324413  0.34408798  0.90325743 -0.16077565 -0.18528867\n",
      "   1.03989655  0.47178608 -0.35402295]\n",
      " [-0.64767295  0.97362087 -0.56005414 -1.04151325  0.09485647  0.12038374\n",
      "  -1.27601596 -0.68286964  0.42402633]\n",
      " [ 0.3778393  -0.24892551  0.63379625 -0.78558946  0.34813707 -0.44816461\n",
      "  -0.75154206  0.47537651  0.52726999]\n",
      " [ 0.85636801 -0.92827199  0.42965804  0.90541111 -0.15520347 -0.0718516\n",
      "   1.19674139  0.15620888 -0.68154153]\n",
      " [-0.84349466  0.92878491 -0.42548759 -0.77097861  0.21750811  0.07160696\n",
      "  -1.20240021 -0.55977751  0.506771  ]\n",
      " [-0.01212578  0.25316871  0.43881842 -0.9196666   0.0204171   0.03145912\n",
      "  -1.01093185  0.19437422  0.23305853]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.37147741  1.25424487 -2.26338273 -0.10167196  1.3593737  -2.05236005\n",
      "  -0.6064863 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:52 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.26056806]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 53 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.74238645 -0.78625234  0.34408798  0.90024921 -0.16077565 -0.18528867\n",
      "   1.03989655  0.47178608 -0.35703116]\n",
      " [-0.64588172  0.9754121  -0.56005414 -1.03972202  0.09485647  0.12038374\n",
      "  -1.27601596 -0.68286964  0.42581756]\n",
      " [ 0.378648   -0.24811681  0.63379625 -0.78478076  0.34813707 -0.44816461\n",
      "  -0.75154206  0.47537651  0.52807869]\n",
      " [ 0.85541983 -0.92922017  0.42965804  0.90446293 -0.15520347 -0.0718516\n",
      "   1.19674139  0.15620888 -0.68248971]\n",
      " [-0.8423808   0.92989877 -0.42548759 -0.76986474  0.21750811  0.07160696\n",
      "  -1.20240021 -0.55977751  0.50788487]\n",
      " [-0.00946394  0.25583055  0.43881842 -0.91700476  0.0204171   0.03145912\n",
      "  -1.01093185  0.19437422  0.23572038]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.39657954  1.23855275 -2.27411708 -0.11341206  1.3458708  -2.0637913\n",
      "  -0.61628656]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:53 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.79095227]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 53 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.74588518 -0.78625234  0.3475867   0.90024921 -0.16077565 -0.18528867\n",
      "   1.04339528  0.47178608 -0.35703116]\n",
      " [-0.64893512  0.9754121  -0.56310755 -1.03972202  0.09485647  0.12038374\n",
      "  -1.27906937 -0.68286964  0.42581756]\n",
      " [ 0.3797563  -0.24811681  0.63490455 -0.78478076  0.34813707 -0.44816461\n",
      "  -0.75043375  0.47537651  0.52807869]\n",
      " [ 0.85847341 -0.92922017  0.43271163  0.90446293 -0.15520347 -0.0718516\n",
      "   1.19979498  0.15620888 -0.68248971]\n",
      " [-0.84544998  0.92989877 -0.42855677 -0.76986474  0.21750811  0.07160696\n",
      "  -1.20546939 -0.55977751  0.50788487]\n",
      " [-0.01177569  0.25583055  0.43650666 -0.91700476  0.0204171   0.03145912\n",
      "  -1.0132436   0.19437422  0.23572038]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.37929685  1.25399379 -2.27278401 -0.1036498   1.36182025 -2.06244379\n",
      "  -0.61008952]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:53 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.6603362]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 53 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.72953927 -0.80259825  0.33124079  0.90024921 -0.16077565 -0.18528867\n",
      "   1.02704937  0.47178608 -0.35703116]\n",
      " [-0.63236106  0.99198616 -0.54653349 -1.03972202  0.09485647  0.12038374\n",
      "  -1.2624953  -0.68286964  0.42581756]\n",
      " [ 0.37945806 -0.24841505  0.63460631 -0.78478076  0.34813707 -0.44816461\n",
      "  -0.75073199  0.47537651  0.52807869]\n",
      " [ 0.84189676 -0.94579682  0.41613498  0.90446293 -0.15520347 -0.0718516\n",
      "   1.18321832  0.15620888 -0.68248971]\n",
      " [-0.82887158  0.94647717 -0.41197837 -0.76986474  0.21750811  0.07160696\n",
      "  -1.18889099 -0.55977751  0.50788487]\n",
      " [-0.00578389  0.26182235  0.44249846 -0.91700476  0.0204171   0.03145912\n",
      "  -1.0072518   0.19437422  0.23572038]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.45335102  1.19517774 -2.28612084 -0.14097513  1.30060665 -2.07541418\n",
      "  -0.64101366]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:53 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.60840829]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 53 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 7.39981971e-01 -8.02598254e-01  3.41683495e-01  9.00249215e-01\n",
      "  -1.60775649e-01 -1.85288674e-01  1.02704937e+00  4.82228785e-01\n",
      "  -3.57031165e-01]\n",
      " [-6.42470787e-01  9.91986162e-01 -5.56643214e-01 -1.03972202e+00\n",
      "   9.48564670e-02  1.20383737e-01 -1.26249530e+00 -6.92979366e-01\n",
      "   4.25817558e-01]\n",
      " [ 3.89890208e-01 -2.48415048e-01  6.45038460e-01 -7.84780764e-01\n",
      "   3.48137068e-01 -4.48164609e-01 -7.50731990e-01  4.85808662e-01\n",
      "   5.28078689e-01]\n",
      " [ 8.52275373e-01 -9.45796820e-01  4.26513588e-01  9.04462926e-01\n",
      "  -1.55203468e-01 -7.18516006e-02  1.18321832e+00  1.66587489e-01\n",
      "  -6.82489707e-01]\n",
      " [-8.39091701e-01  9.46477170e-01 -4.22198488e-01 -7.69864744e-01\n",
      "   2.17508114e-01  7.16069632e-02 -1.18889099e+00 -5.69997628e-01\n",
      "   5.07884866e-01]\n",
      " [ 8.89085583e-04  2.61822354e-01  4.49171442e-01 -9.17004758e-01\n",
      "   2.04171006e-02  3.14591180e-02 -1.00725180e+00  2.01047199e-01\n",
      "   2.35720376e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.40670312  1.23354011 -2.27984681 -0.10291071  1.33813181 -2.06880068\n",
      "  -0.61056488]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:53 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.56832254]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 53 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.75152735 -0.80259825  0.3416835   0.91179459 -0.16077565 -0.18528867\n",
      "   1.02704937  0.48222878 -0.34548579]\n",
      " [-0.65394634  0.99198616 -0.55664321 -1.05119757  0.09485647  0.12038374\n",
      "  -1.2624953  -0.69297937  0.41434201]\n",
      " [ 0.39164556 -0.24841505  0.64503846 -0.78302541  0.34813707 -0.44816461\n",
      "  -0.75073199  0.48580866  0.52983404]\n",
      " [ 0.8630706  -0.94579682  0.42651359  0.91525815 -0.15520347 -0.0718516\n",
      "   1.18321832  0.16658749 -0.67169448]\n",
      " [-0.85001028  0.94647717 -0.42219849 -0.78078332  0.21750811  0.07160696\n",
      "  -1.18889099 -0.56999763  0.49696629]\n",
      " [-0.00715104  0.26182235  0.44917144 -0.92504488  0.0204171   0.03145912\n",
      "  -1.0072518   0.2010472   0.22768025]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.35375096  1.27500133 -2.26811269 -0.07467409  1.37760256 -2.05558705\n",
      "  -0.59276379]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:53 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.17884625]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 53 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.75133061 -0.80259825  0.3416835   0.91179459 -0.16097239 -0.18548541\n",
      "   1.02704937  0.48222878 -0.34568253]\n",
      " [-0.65386636  0.99198616 -0.55664321 -1.05119757  0.09493645  0.12046372\n",
      "  -1.2624953  -0.69297937  0.41442199]\n",
      " [ 0.38935689 -0.24841505  0.64503846 -0.78302541  0.34584839 -0.45045329\n",
      "  -0.75073199  0.48580866  0.52754537]\n",
      " [ 0.8631877  -0.94579682  0.42651359  0.91525815 -0.15508636 -0.0717345\n",
      "   1.18321832  0.16658749 -0.67157738]\n",
      " [-0.8498006   0.94647717 -0.42219849 -0.78078332  0.21771779  0.07181664\n",
      "  -1.18889099 -0.56999763  0.49717597]\n",
      " [-0.008029    0.26182235  0.44917144 -0.92504488  0.01953913  0.03058115\n",
      "  -1.0072518   0.2010472   0.22680229]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.36688367  1.26823812 -2.27459906 -0.08379533  1.37115334 -2.06194358\n",
      "  -0.60021901]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:53 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.04217203]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 53 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.7514689  -0.80245996  0.3416835   0.91179459 -0.16083409 -0.18534712\n",
      "   1.02704937  0.48222878 -0.34554423]\n",
      " [-0.65403084  0.99182168 -0.55664321 -1.05119757  0.09477197  0.12029924\n",
      "  -1.2624953  -0.69297937  0.41425751]\n",
      " [ 0.38924588 -0.24852606  0.64503846 -0.78302541  0.34573738 -0.4505643\n",
      "  -0.75073199  0.48580866  0.52743436]\n",
      " [ 0.86335342 -0.9456311   0.42651359  0.91525815 -0.15492065 -0.07156878\n",
      "   1.18321832  0.16658749 -0.67141166]\n",
      " [-0.8499563   0.94632147 -0.42219849 -0.78078332  0.21756209  0.07166094\n",
      "  -1.18889099 -0.56999763  0.49702027]\n",
      " [-0.00813441  0.26171694  0.44917144 -0.92504488  0.01943372  0.03047574\n",
      "  -1.0072518   0.2010472   0.22669688]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.36773541  1.26796364 -2.27521632 -0.08433819  1.37092107 -2.06254621\n",
      "  -0.60075531]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:53 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.45322656]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 53 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 7.43448030e-01 -8.10480834e-01  3.41683495e-01  9.11794593e-01\n",
      "  -1.68854968e-01 -1.93367993e-01  1.01902849e+00  4.82228785e-01\n",
      "  -3.45544231e-01]\n",
      " [-6.45224064e-01  1.00062846e+00 -5.56643214e-01 -1.05119757e+00\n",
      "   1.03578740e-01  1.29106010e-01 -1.25368853e+00 -6.92979366e-01\n",
      "   4.14257509e-01]\n",
      " [ 3.98101483e-01 -2.39670450e-01  6.45038460e-01 -7.83025411e-01\n",
      "   3.54592989e-01 -4.41708687e-01 -7.41876382e-01  4.85808662e-01\n",
      "   5.27434356e-01]\n",
      " [ 8.53153916e-01 -9.55830604e-01  4.26513588e-01  9.15258151e-01\n",
      "  -1.65120149e-01 -8.17682819e-02  1.17301882e+00  1.66587489e-01\n",
      "  -6.71411661e-01]\n",
      " [-8.40318634e-01  9.55959136e-01 -4.22198488e-01 -7.80783318e-01\n",
      "   2.27199755e-01  8.12986043e-02 -1.17925332e+00 -5.69997628e-01\n",
      "   4.97020267e-01]\n",
      " [ 6.16891623e-04  2.70468251e-01  4.49171442e-01 -9.25044882e-01\n",
      "   2.81850304e-02  3.92270478e-02 -9.98500498e-01  2.01047199e-01\n",
      "   2.26696876e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.42389295  1.231323   -2.29373034 -0.10278765  1.33129232 -2.07991683\n",
      "  -0.61934229]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:53 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.83958329]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 53 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.74519143 -0.81048083  0.3416835   0.91353799 -0.16885497 -0.19336799\n",
      "   1.02077189  0.48222878 -0.34554423]\n",
      " [-0.64673014  1.00062846 -0.55664321 -1.05270365  0.10357874  0.12910601\n",
      "  -1.25519461 -0.69297937  0.41425751]\n",
      " [ 0.39585141 -0.23967045  0.64503846 -0.78527548  0.35459299 -0.44170869\n",
      "  -0.74412645  0.48580866  0.52743436]\n",
      " [ 0.85466734 -0.9558306   0.42651359  0.91677158 -0.16512015 -0.08176828\n",
      "   1.17453224  0.16658749 -0.67141166]\n",
      " [-0.84195285  0.95595914 -0.42219849 -0.78241754  0.22719975  0.0812986\n",
      "  -1.18088754 -0.56999763  0.49702027]\n",
      " [-0.00169451  0.27046825  0.44917144 -0.92735628  0.02818503  0.03922705\n",
      "  -1.0008119   0.2010472   0.22669688]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.41309023  1.24142884 -2.2931931  -0.10014367  1.34155335 -2.07929778\n",
      "  -0.61796456]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:53 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.69793258]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 53 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.75226813 -0.81048083  0.3416835   0.91353799 -0.16177827 -0.18629129\n",
      "   1.02784859  0.48222878 -0.34554423]\n",
      " [-0.65382021  1.00062846 -0.55664321 -1.05270365  0.09648868  0.12201595\n",
      "  -1.26228467 -0.69297937  0.41425751]\n",
      " [ 0.39254476 -0.23967045  0.64503846 -0.78527548  0.35128633 -0.44501534\n",
      "  -0.74743311  0.48580866  0.52743436]\n",
      " [ 0.86166344 -0.9558306   0.42651359  0.91677158 -0.15812405 -0.07477218\n",
      "   1.18152835  0.16658749 -0.67141166]\n",
      " [-0.84901172  0.95595914 -0.42219849 -0.78241754  0.22014089  0.07423974\n",
      "  -1.18794641 -0.56999763  0.49702027]\n",
      " [-0.00772176  0.27046825  0.44917144 -0.92735628  0.02215778  0.03319979\n",
      "  -1.00683915  0.2010472   0.22669688]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.3812489   1.26699033 -2.28814544 -0.08763512  1.36880896 -2.07443875\n",
      "  -0.60898873]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:53 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.5667356]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 53 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.73773672 -0.81048083  0.3416835   0.89900658 -0.16177827 -0.20082271\n",
      "   1.02784859  0.48222878 -0.36007565]\n",
      " [-0.63910402  1.00062846 -0.55664321 -1.03798746  0.09648868  0.13673213\n",
      "  -1.26228467 -0.69297937  0.42897369]\n",
      " [ 0.39781474 -0.23967045  0.64503846 -0.7800055   0.35128633 -0.43974537\n",
      "  -0.74743311  0.48580866  0.53270434]\n",
      " [ 0.84775373 -0.9558306   0.42651359  0.90286186 -0.15812405 -0.08868189\n",
      "   1.18152835  0.16658749 -0.68532138]\n",
      " [-0.834916    0.95595914 -0.42219849 -0.76832182  0.22014089  0.08833545\n",
      "  -1.18794641 -0.56999763  0.51111598]\n",
      " [ 0.00277999  0.27046825  0.44917144 -0.91685453  0.02215778  0.04370154\n",
      "  -1.00683915  0.2010472   0.23719862]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.45082883  1.21434817 -2.30462264 -0.11707012  1.31750405 -2.09234007\n",
      "  -0.63246066]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:53 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.72183635]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 53 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.7439628  -0.80425475  0.3416835   0.90523266 -0.16177827 -0.19459662\n",
      "   1.03407468  0.48222878 -0.36007565]\n",
      " [-0.64522099  0.99451149 -0.55664321 -1.04410443  0.09648868  0.13061516\n",
      "  -1.26840164 -0.69297937  0.42897369]\n",
      " [ 0.3917048  -0.24578038  0.64503846 -0.78611543  0.35128633 -0.4458553\n",
      "  -0.75354304  0.48580866  0.53270434]\n",
      " [ 0.85377466 -0.94980967  0.42651359  0.90888279 -0.15812405 -0.08266096\n",
      "   1.18754928  0.16658749 -0.68532138]\n",
      " [-0.84108253  0.9497926  -0.42219849 -0.77448835  0.22014089  0.08216892\n",
      "  -1.19411294 -0.56999763  0.51111598]\n",
      " [-0.00346315  0.26422511  0.44917144 -0.92309767  0.02215778  0.0374584\n",
      "  -1.01308229  0.2010472   0.23719862]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.42290278  1.23778748 -2.30066815 -0.11313941  1.34175741 -2.08819473\n",
      "  -0.62779586]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:53 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.75890374]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 53 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.74872666 -0.79949089  0.3416835   0.90999652 -0.16177827 -0.19459662\n",
      "   1.03883853  0.48222878 -0.36007565]\n",
      " [-0.6498959   0.98983658 -0.55664321 -1.04877934  0.09648868  0.13061516\n",
      "  -1.27307654 -0.69297937  0.42897369]\n",
      " [ 0.3868082  -0.25067699  0.64503846 -0.79101204  0.35128633 -0.4458553\n",
      "  -0.75843965  0.48580866  0.53270434]\n",
      " [ 0.85840577 -0.94517856  0.42651359  0.9135139  -0.15812405 -0.08266096\n",
      "   1.19218039  0.16658749 -0.68532138]\n",
      " [-0.84586445  0.94501068 -0.42219849 -0.77927027  0.22014089  0.08216892\n",
      "  -1.19889486 -0.56999763  0.51111598]\n",
      " [-0.00837185  0.25931641  0.44917144 -0.92800637  0.02215778  0.0374584\n",
      "  -1.01799099  0.2010472   0.23719862]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.40084623  1.2569215  -2.29795256 -0.1087543   1.36118568 -2.08522337\n",
      "  -0.62431747]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:53 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.78398994]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 53 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.75262976 -0.79949089  0.3455866   0.90999652 -0.16177827 -0.19069352\n",
      "   1.04274164  0.48222878 -0.36007565]\n",
      " [-0.65331378  0.98983658 -0.56006109 -1.04877934  0.09648868  0.12719728\n",
      "  -1.27649443 -0.69297937  0.42897369]\n",
      " [ 0.38602549 -0.25067699  0.64425575 -0.79101204  0.35128633 -0.44663801\n",
      "  -0.75922236  0.48580866  0.53270434]\n",
      " [ 0.86176093 -0.94517856  0.42986875  0.9135139  -0.15812405 -0.07930581\n",
      "   1.19553555  0.16658749 -0.68532138]\n",
      " [-0.84923303  0.94501068 -0.42556706 -0.77927027  0.22014089  0.07880035\n",
      "  -1.20226343 -0.56999763  0.51111598]\n",
      " [-0.01066851  0.25931641  0.44687479 -0.92800637  0.02215778  0.03516175\n",
      "  -1.02028764  0.2010472   0.23719862]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.38255561  1.27290317 -2.29635862 -0.10039559  1.37794725 -2.08368074\n",
      "  -0.61758195]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:53 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.25385126]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 54 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.74978974 -0.80233092  0.3455866   0.90715649 -0.16177827 -0.19069352\n",
      "   1.04274164  0.48222878 -0.36291567]\n",
      " [-0.65164488  0.99150548 -0.56006109 -1.04711044  0.09648868  0.12719728\n",
      "  -1.27649443 -0.69297937  0.43064259]\n",
      " [ 0.38676172 -0.24994076  0.64425575 -0.79027581  0.35128633 -0.44663801\n",
      "  -0.75922236  0.48580866  0.53344057]\n",
      " [ 0.86089534 -0.94604415  0.42986875  0.91264831 -0.15812405 -0.07930581\n",
      "   1.19553555  0.16658749 -0.68618697]\n",
      " [-0.84820466  0.94603906 -0.42556706 -0.7782419   0.22014089  0.07880035\n",
      "  -1.20226343 -0.56999763  0.51214436]\n",
      " [-0.00813678  0.26184814  0.44687479 -0.92547464  0.02215778  0.03516175\n",
      "  -1.02028764  0.2010472   0.23973035]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.40659669  1.2579213  -2.30668785 -0.11167804  1.36505808 -2.0946678\n",
      "  -0.62698746]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:54 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.79434438]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 54 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.75317764 -0.80233092  0.34897451  0.90715649 -0.16177827 -0.19069352\n",
      "   1.04612954  0.48222878 -0.36291567]\n",
      " [-0.65460445  0.99150548 -0.56302067 -1.04711044  0.09648868  0.12719728\n",
      "  -1.279454   -0.69297937  0.43064259]\n",
      " [ 0.3878823  -0.24994076  0.64537633 -0.79027581  0.35128633 -0.44663801\n",
      "  -0.75810177  0.48580866  0.53344057]\n",
      " [ 0.86385741 -0.94604415  0.43283082  0.91264831 -0.15812405 -0.07930581\n",
      "   1.19849762  0.16658749 -0.68618697]\n",
      " [-0.85118022  0.94603906 -0.42854262 -0.7782419   0.22014089  0.07880035\n",
      "  -1.205239   -0.56999763  0.51214436]\n",
      " [-0.01038362  0.26184814  0.44462794 -0.92547464  0.02215778  0.03516175\n",
      "  -1.02253449  0.2010472   0.23973035]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.3897986   1.27294809 -2.30539961 -0.10214456  1.38056568 -2.09336502\n",
      "  -0.62096407]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:54 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.65890587]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 54 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.73684567 -0.81866289  0.33264253  0.90715649 -0.16177827 -0.19069352\n",
      "   1.02979757  0.48222878 -0.36291567]\n",
      " [-0.63803659  1.00807335 -0.54645281 -1.04711044  0.09648868  0.12719728\n",
      "  -1.26288614 -0.69297937  0.43064259]\n",
      " [ 0.3874156  -0.25040746  0.64490963 -0.79027581  0.35128633 -0.44663801\n",
      "  -0.75856848  0.48580866  0.53344057]\n",
      " [ 0.84728121 -0.96262035  0.41625462  0.91264831 -0.15812405 -0.07930581\n",
      "   1.18192142  0.16658749 -0.68618697]\n",
      " [-0.83460394  0.96261533 -0.41196635 -0.7782419   0.22014089  0.07880035\n",
      "  -1.18866272 -0.56999763  0.51214436]\n",
      " [-0.00449901  0.26773275  0.45051256 -0.92547464  0.02215778  0.03516175\n",
      "  -1.01664987  0.2010472   0.23973035]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.46384279  1.21419647 -2.31884572 -0.13963341  1.31949479 -2.10644803\n",
      "  -0.65199648]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:54 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.61255796]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 54 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.74713812 -0.81866289  0.34293499  0.90715649 -0.16177827 -0.19069352\n",
      "   1.02979757  0.49252124 -0.36291567]\n",
      " [-0.64796976  1.00807335 -0.55638598 -1.04711044  0.09648868  0.12719728\n",
      "  -1.26288614 -0.70291254  0.43064259]\n",
      " [ 0.39770596 -0.25040746  0.65519999 -0.79027581  0.35128633 -0.44663801\n",
      "  -0.75856848  0.49609902  0.53344057]\n",
      " [ 0.85752525 -0.96262035  0.42649866  0.91264831 -0.15812405 -0.07930581\n",
      "   1.18192142  0.17683152 -0.68618697]\n",
      " [-0.84465031  0.96261533 -0.42201272 -0.7782419   0.22014089  0.07880035\n",
      "  -1.18866272 -0.580044    0.51214436]\n",
      " [ 0.00221109  0.26773275  0.45722266 -0.92547464  0.02215778  0.03516175\n",
      "  -1.01664987  0.2077573   0.23973035]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.41786685  1.25213406 -2.31274564 -0.10192102  1.35659372 -2.10001847\n",
      "  -0.62182027]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:54 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.57084843]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 54 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.75861915 -0.81866289  0.34293499  0.91863752 -0.16177827 -0.19069352\n",
      "   1.02979757  0.49252124 -0.35143464]\n",
      " [-0.65938344  1.00807335 -0.55638598 -1.05852411  0.09648868  0.12719728\n",
      "  -1.26288614 -0.70291254  0.41922891]\n",
      " [ 0.39954809 -0.25040746  0.65519999 -0.78843368  0.35128633 -0.44663801\n",
      "  -0.75856848  0.49609902  0.5352827 ]\n",
      " [ 0.86828732 -0.96262035  0.42649866  0.92341038 -0.15812405 -0.07930581\n",
      "   1.18192142  0.17683152 -0.6754249 ]\n",
      " [-0.85553164  0.96261533 -0.42201272 -0.78912322  0.22014089  0.07880035\n",
      "  -1.18866272 -0.580044    0.50126303]\n",
      " [-0.00579909  0.26773275  0.45722266 -0.93348482  0.02215778  0.03516175\n",
      "  -1.01664987  0.2077573   0.23172017]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.36529997  1.29336649 -2.30116983 -0.07378935  1.3958742  -2.086996\n",
      "  -0.60418549]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:54 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.17215722]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 54 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.75845147 -0.81866289  0.34293499  0.91863752 -0.16194594 -0.19086119\n",
      "   1.02979757  0.49252124 -0.35160232]\n",
      " [-0.65933293  1.00807335 -0.55638598 -1.05852411  0.09653918  0.12724779\n",
      "  -1.26288614 -0.70291254  0.41927942]\n",
      " [ 0.39737863 -0.25040746  0.65519999 -0.78843368  0.34911688 -0.44880746\n",
      "  -0.75856848  0.49609902  0.53311324]\n",
      " [ 0.86842394 -0.96262035  0.42649866  0.92341038 -0.15798743 -0.07916919\n",
      "   1.18192142  0.17683152 -0.67528828]\n",
      " [-0.85536208  0.96261533 -0.42201272 -0.78912322  0.22031045  0.0789699\n",
      "  -1.18866272 -0.580044    0.50143259]\n",
      " [-0.00665058  0.26773275  0.45722266 -0.93348482  0.02130628  0.03431025\n",
      "  -1.01664987  0.2077573   0.23086868]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.37756781  1.28706481 -2.30725325 -0.08235667  1.38987694 -2.09296028\n",
      "  -0.61118234]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:54 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.03874603]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 54 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.75857105 -0.81854331  0.34293499  0.91863752 -0.16182636 -0.19074161\n",
      "   1.02979757  0.49252124 -0.35148274]\n",
      " [-0.65947417  1.00793211 -0.55638598 -1.05852411  0.09639795  0.12710655\n",
      "  -1.26288614 -0.70291254  0.41913818]\n",
      " [ 0.39728228 -0.25050381  0.65519999 -0.78843368  0.34902053 -0.44890381\n",
      "  -0.75856848  0.49609902  0.53301689]\n",
      " [ 0.8685663  -0.96247799  0.42649866  0.92341038 -0.15784506 -0.07902682\n",
      "   1.18192142  0.17683152 -0.67514591]\n",
      " [-0.85549626  0.96248115 -0.42201272 -0.78912322  0.22017627  0.07883573\n",
      "  -1.18866272 -0.580044    0.50129841]\n",
      " [-0.00674231  0.26764103  0.45722266 -0.93348482  0.02121456  0.03421853\n",
      "  -1.01664987  0.2077573   0.23077695]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.37828936  1.28683562 -2.30777958 -0.08281929  1.38968383 -2.09347444\n",
      "  -0.61163949]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:54 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.44377347]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 54 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.75087578 -0.82623858  0.34293499  0.91863752 -0.16952163 -0.19843688\n",
      "   1.0221023   0.49252124 -0.35148274]\n",
      " [-0.6510586   1.01634768 -0.55638598 -1.05852411  0.10481351  0.13552212\n",
      "  -1.25447057 -0.70291254  0.41913818]\n",
      " [ 0.40589019 -0.2418959   0.65519999 -0.78843368  0.35762843 -0.44029591\n",
      "  -0.74996057  0.49609902  0.53301689]\n",
      " [ 0.85879179 -0.9722525   0.42649866  0.92341038 -0.16761957 -0.08880133\n",
      "   1.17214691  0.17683152 -0.67514591]\n",
      " [-0.84626664  0.97171077 -0.42201272 -0.78912322  0.22940588  0.08806534\n",
      "  -1.17943311 -0.580044    0.50129841]\n",
      " [ 0.00176093  0.27614426  0.45722266 -0.93348482  0.02971779  0.04272176\n",
      "  -1.00814664  0.2077573   0.23077695]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.43305956  1.25125719 -2.32606338 -0.10085124  1.35130055 -2.11065835\n",
      "  -0.62980905]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:54 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.84369232]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 54 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 7.52524772e-01 -8.26238584e-01  3.42934986e-01  9.20286509e-01\n",
      "  -1.69521632e-01 -1.98436885e-01  1.02375129e+00  4.92521242e-01\n",
      "  -3.51482738e-01]\n",
      " [-6.52484272e-01  1.01634768e+00 -5.56385977e-01 -1.05994979e+00\n",
      "   1.04813513e-01  1.35522118e-01 -1.25589624e+00 -7.02912537e-01\n",
      "   4.19138184e-01]\n",
      " [ 4.03738878e-01 -2.41895903e-01  6.55199987e-01 -7.90584988e-01\n",
      "   3.57628435e-01 -4.40295908e-01 -7.52111880e-01  4.96099018e-01\n",
      "   5.33016894e-01]\n",
      " [ 8.60225280e-01 -9.72252499e-01  4.26498656e-01  9.24843872e-01\n",
      "  -1.67619574e-01 -8.88013336e-02  1.17358040e+00  1.76831523e-01\n",
      "  -6.75145912e-01]\n",
      " [-8.47813897e-01  9.71710766e-01 -4.22012720e-01 -7.90670477e-01\n",
      "   2.29405883e-01  8.80653416e-02 -1.18098036e+00 -5.80043996e-01\n",
      "   5.01298410e-01]\n",
      " [-4.35774456e-04  2.76144264e-01  4.57222657e-01 -9.35681523e-01\n",
      "   2.97177903e-02  4.27217598e-02 -1.01034334e+00  2.07757297e-01\n",
      "   2.30776954e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.42275298  1.26090962 -2.32555757 -0.09833954  1.36109664 -2.11007574\n",
      "  -0.6285139 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:54 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.69999708]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 54 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.75952908 -0.82623858  0.34293499  0.92028651 -0.16251732 -0.19143257\n",
      "   1.0307556   0.49252124 -0.35148274]\n",
      " [-0.65949914  1.01634768 -0.55638598 -1.05994979  0.09779865  0.12850725\n",
      "  -1.26291111 -0.70291254  0.41913818]\n",
      " [ 0.4004973  -0.2418959   0.65519999 -0.79058499  0.35438685 -0.44353749\n",
      "  -0.75535346  0.49609902  0.53301689]\n",
      " [ 0.86715159 -0.9722525   0.42649866  0.92484387 -0.16069326 -0.08187502\n",
      "   1.18050671  0.17683152 -0.67514591]\n",
      " [-0.85479953  0.97171077 -0.42201272 -0.79067048  0.22242025  0.08107971\n",
      "  -1.187966   -0.580044    0.50129841]\n",
      " [-0.00641073  0.27614426  0.45722266 -0.93568152  0.02374284  0.03674681\n",
      "  -1.01631829  0.2077573   0.23077695]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.3912525   1.28622027 -2.32055892 -0.08593219  1.38804132 -2.10525641\n",
      "  -0.61965484]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:54 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.56752279]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 54 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.7449681  -0.82623858  0.34293499  0.90572552 -0.16251732 -0.20599356\n",
      "   1.0307556   0.49252124 -0.36604372]\n",
      " [-0.64476137  1.01634768 -0.55638598 -1.04521202  0.09779865  0.14324502\n",
      "  -1.26291111 -0.70291254  0.43387595]\n",
      " [ 0.4056149  -0.2418959   0.65519999 -0.78546739  0.35438685 -0.43841989\n",
      "  -0.75535346  0.49609902  0.53813449]\n",
      " [ 0.85320987 -0.9722525   0.42649866  0.91090215 -0.16069326 -0.09581674\n",
      "   1.18050671  0.17683152 -0.68908763]\n",
      " [-0.8406715   0.97171077 -0.42201272 -0.77654244  0.22242025  0.09520774\n",
      "  -1.187966   -0.580044    0.51542644]\n",
      " [ 0.00409363  0.27614426  0.45722266 -0.92517716  0.02374284  0.04725117\n",
      "  -1.01631829  0.2077573   0.24128131]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.46089909  1.23349013 -2.33703237 -0.11556046  1.33665055 -2.12313601\n",
      "  -0.64315879]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:54 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.72292784]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 54 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.75115658 -0.8200501   0.34293499  0.91191401 -0.16251732 -0.19980507\n",
      "   1.03694408  0.49252124 -0.36604372]\n",
      " [-0.65084806  1.01026099 -0.55638598 -1.05129871  0.09779865  0.13715833\n",
      "  -1.2689978  -0.70291254  0.43387595]\n",
      " [ 0.39955024 -0.24796056  0.65519999 -0.79153205  0.35438685 -0.44448455\n",
      "  -0.76141812  0.49609902  0.53813449]\n",
      " [ 0.85920624 -0.96625613  0.42649866  0.91689853 -0.16069326 -0.08982037\n",
      "   1.18650308  0.17683152 -0.68908763]\n",
      " [-0.84680576  0.9655765  -0.42201272 -0.78267671  0.22242025  0.08907348\n",
      "  -1.19410026 -0.580044    0.51542644]\n",
      " [-0.00210791  0.26994272  0.45722266 -0.9313787   0.02374284  0.04104963\n",
      "  -1.02251983  0.2077573   0.24128131]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.43314987  1.25676577 -2.3330734  -0.11167635  1.36071507 -2.11898657\n",
      "  -0.6385515 ]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:54 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.76125826]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 54 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.75584144 -0.81536524  0.34293499  0.91659886 -0.16251732 -0.19980507\n",
      "   1.04162894  0.49252124 -0.36604372]\n",
      " [-0.65544894  1.00566011 -0.55638598 -1.05589959  0.09779865  0.13715833\n",
      "  -1.27359868 -0.70291254  0.43387595]\n",
      " [ 0.3947298  -0.252781    0.65519999 -0.79635249  0.35438685 -0.44448455\n",
      "  -0.76623856  0.49609902  0.53813449]\n",
      " [ 0.86376624 -0.96169613  0.42649866  0.92145852 -0.16069326 -0.08982037\n",
      "   1.19106308  0.17683152 -0.68908763]\n",
      " [-0.85151099  0.96087127 -0.42201272 -0.78738194  0.22242025  0.08907348\n",
      "  -1.19880549 -0.580044    0.51542644]\n",
      " [-0.00693145  0.26511918  0.45722266 -0.93620225  0.02374284  0.04104963\n",
      "  -1.02734338  0.2077573   0.24128131]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.41145492  1.27558846 -2.33039686 -0.10738942  1.3798156  -2.11605914\n",
      "  -0.63516071]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:54 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.78622612]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 54 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.75966977 -0.81536524  0.34676331  0.91659886 -0.16251732 -0.19597675\n",
      "   1.04545726  0.49252124 -0.36604372]\n",
      " [-0.65880695  1.00566011 -0.55974398 -1.05589959  0.09779865  0.13380032\n",
      "  -1.27695669 -0.70291254  0.43387595]\n",
      " [ 0.39401228 -0.252781    0.65448247 -0.79635249  0.35438685 -0.44520206\n",
      "  -0.76695608  0.49609902  0.53813449]\n",
      " [ 0.86706567 -0.96169613  0.42979809  0.92145852 -0.16069326 -0.08652094\n",
      "   1.19436251  0.17683152 -0.68908763]\n",
      " [-0.85482168  0.96087127 -0.42532341 -0.78738194  0.22242025  0.08576279\n",
      "  -1.20211618 -0.580044    0.51542644]\n",
      " [-0.00917382  0.26511918  0.45498029 -0.93620225  0.02374284  0.03880726\n",
      "  -1.02958575  0.2077573   0.24128131]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.39348994  1.29129737 -2.32883026 -0.09912754  1.39627471 -2.11454185\n",
      "  -0.62852951]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:54 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.24728403]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 55 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.75699002 -0.81804499  0.34676331  0.91391911 -0.16251732 -0.19597675\n",
      "   1.04545726  0.49252124 -0.36872347]\n",
      " [-0.65725334  1.00721371 -0.55974398 -1.05434599  0.09779865  0.13380032\n",
      "  -1.27695669 -0.70291254  0.43542956]\n",
      " [ 0.39468307 -0.25211021  0.65448247 -0.7956817   0.35438685 -0.44520206\n",
      "  -0.76695608  0.49609902  0.53880528]\n",
      " [ 0.86627693 -0.96248487  0.42979809  0.92066978 -0.16069326 -0.08652094\n",
      "   1.19436251  0.17683152 -0.68987637]\n",
      " [-0.85387368  0.96181928 -0.42532341 -0.78643393  0.22242025  0.08576279\n",
      "  -1.20211618 -0.580044    0.51637445]\n",
      " [-0.00676604  0.26752696  0.45498029 -0.93379447  0.02374284  0.03880726\n",
      "  -1.02958575  0.2077573   0.2436891 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.416504    1.27699987 -2.33876401 -0.10996225  1.38397644 -2.12509652\n",
      "  -0.63755068]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:55 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.79761993]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 55 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.76027258 -0.81804499  0.35004588  0.91391911 -0.16251732 -0.19597675\n",
      "   1.04873983  0.49252124 -0.36872347]\n",
      " [-0.66012384  1.00721371 -0.56261448 -1.05434599  0.09779865  0.13380032\n",
      "  -1.27982719 -0.70291254  0.43542956]\n",
      " [ 0.39581285 -0.25211021  0.65561225 -0.7956817   0.35438685 -0.44520206\n",
      "  -0.76582629  0.49609902  0.53880528]\n",
      " [ 0.86915193 -0.96248487  0.43267309  0.92066978 -0.16069326 -0.08652094\n",
      "   1.19723751  0.17683152 -0.68987637]\n",
      " [-0.85676035  0.96181928 -0.42821008 -0.78643393  0.22242025  0.08576279\n",
      "  -1.20500286 -0.580044    0.51637445]\n",
      " [-0.0089503   0.26752696  0.45279603 -0.93379447  0.02374284  0.03880726\n",
      "  -1.03177     0.2077573   0.2436891 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.40016966  1.29162882 -2.33751797 -0.10065024  1.39906069 -2.12383586\n",
      "  -0.63169291]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:55 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.6573563]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 55 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.74395624 -0.83436134  0.33372953  0.91391911 -0.16251732 -0.19597675\n",
      "   1.03242349  0.49252124 -0.36872347]\n",
      " [-0.64356418  1.02377337 -0.54605482 -1.05434599  0.09779865  0.13380032\n",
      "  -1.26326752 -0.70291254  0.43542956]\n",
      " [ 0.39519322 -0.25272984  0.65499263 -0.7956817   0.35438685 -0.44520206\n",
      "  -0.76644592  0.49609902  0.53880528]\n",
      " [ 0.85257874 -0.97905807  0.4160999   0.92066978 -0.16069326 -0.08652094\n",
      "   1.18066432  0.17683152 -0.68987637]\n",
      " [-0.84018826  0.97839137 -0.41163799 -0.78643393  0.22242025  0.08576279\n",
      "  -1.18843076 -0.580044    0.51637445]\n",
      " [-0.00317007  0.27330719  0.45857626 -0.93379447  0.02374284  0.03880726\n",
      "  -1.02598977  0.2077573   0.2436891 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.4742008   1.23294755 -2.35107454 -0.13828555  1.33813553 -2.13703294\n",
      "  -0.66282884]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:55 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.61659787]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 55 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.75409933 -0.83436134  0.34387263  0.91391911 -0.16251732 -0.19597675\n",
      "   1.03242349  0.50266433 -0.36872347]\n",
      " [-0.65332512  1.02377337 -0.55581576 -1.05434599  0.09779865  0.13380032\n",
      "  -1.26326752 -0.71267348  0.43542956]\n",
      " [ 0.40533885 -0.25272984  0.66513825 -0.7956817   0.35438685 -0.44520206\n",
      "  -0.76644592  0.50624464  0.53880528]\n",
      " [ 0.86268874 -0.97905807  0.4262099   0.92066978 -0.16069326 -0.08652094\n",
      "   1.18066432  0.18694152 -0.68987637]\n",
      " [-0.85006477  0.97839137 -0.4215145  -0.78643393  0.22242025  0.08576279\n",
      "  -1.18843076 -0.5899205   0.51637445]\n",
      " [ 0.00357432  0.27330719  0.46532065 -0.93379447  0.02374284  0.03880726\n",
      "  -1.02598977  0.21450169  0.2436891 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.42888172  1.27046328 -2.34513973 -0.10092557  1.37481255 -2.1307782\n",
      "  -0.63291958]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:55 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.57336281]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 55 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.76551443 -0.83436134  0.34387263  0.92533421 -0.16251732 -0.19597675\n",
      "   1.03242349  0.50266433 -0.35730838]\n",
      " [-0.66467527  1.02377337 -0.55581576 -1.06569613  0.09779865  0.13380032\n",
      "  -1.26326752 -0.71267348  0.42407941]\n",
      " [ 0.40726497 -0.25272984  0.66513825 -0.79375558  0.35438685 -0.44520206\n",
      "  -0.76644592  0.50624464  0.5407314 ]\n",
      " [ 0.87341484 -0.97905807  0.4262099   0.93139588 -0.16069326 -0.08652094\n",
      "   1.18066432  0.18694152 -0.67915028]\n",
      " [-0.86090592  0.97839137 -0.4215145  -0.79727509  0.22242025  0.08576279\n",
      "  -1.18843076 -0.5899205   0.50553329]\n",
      " [-0.00440412  0.27330719  0.46532065 -0.94177291  0.02374284  0.03880726\n",
      "  -1.02598977  0.21450169  0.23571065]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.37670017  1.31146317 -2.33371856 -0.0729016   1.41389845 -2.11794215\n",
      "  -0.61544893]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:55 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.16578034]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 55 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.76537205 -0.83436134  0.34387263  0.92533421 -0.1626597  -0.19611913\n",
      "   1.03242349  0.50266433 -0.35745076]\n",
      " [-0.66464948  1.02377337 -0.55581576 -1.06569613  0.09782443  0.13382611\n",
      "  -1.26326752 -0.71267348  0.42410519]\n",
      " [ 0.40520963 -0.25272984  0.66513825 -0.79375558  0.35233152 -0.4472574\n",
      "  -0.76644592  0.50624464  0.53867606]\n",
      " [ 0.87356648 -0.97905807  0.4262099   0.93139588 -0.16054162 -0.0863693\n",
      "   1.18066432  0.18694152 -0.67899864]\n",
      " [-0.86077076  0.97839137 -0.4215145  -0.79727509  0.22255541  0.08589795\n",
      "  -1.18843076 -0.5899205   0.50566846]\n",
      " [-0.00522835  0.27330719  0.46532065 -0.94177291  0.02291861  0.03798303\n",
      "  -1.02598977  0.21450169  0.23488642]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.38816365  1.30558899 -2.33942452 -0.08094978  1.40831842 -2.12353868\n",
      "  -0.62201682]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:55 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.03562835]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 55 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.76547543 -0.83425796  0.34387263  0.92533421 -0.16255632 -0.19601574\n",
      "   1.03242349  0.50266433 -0.35734737]\n",
      " [-0.66477077  1.02365208 -0.55581576 -1.06569613  0.09770314  0.13370482\n",
      "  -1.26326752 -0.71267348  0.4239839 ]\n",
      " [ 0.40512605 -0.25281342  0.66513825 -0.79375558  0.35224793 -0.44734098\n",
      "  -0.76644592  0.50624464  0.53859248]\n",
      " [ 0.87368878 -0.97893576  0.4262099   0.93139588 -0.16041931 -0.08624699\n",
      "   1.18066432  0.18694152 -0.67887633]\n",
      " [-0.86088637  0.97827576 -0.4215145  -0.79727509  0.2224398   0.08578234\n",
      "  -1.18843076 -0.5899205   0.50555284]\n",
      " [-0.00530812  0.27322742  0.46532065 -0.94177291  0.02283883  0.03790325\n",
      "  -1.02598977  0.21450169  0.23480665]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.38877573  1.30539729 -2.33987376 -0.08134444  1.40815756 -2.1239778\n",
      "  -0.62240692]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:55 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.43440182]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 55 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.75809896 -0.84163442  0.34387263  0.92533421 -0.16993278 -0.20339221\n",
      "   1.02504702  0.50266433 -0.35734737]\n",
      " [-0.6567368   1.03168605 -0.55581576 -1.06569613  0.10573711  0.14173879\n",
      "  -1.25523355 -0.71267348  0.4239839 ]\n",
      " [ 0.41349134 -0.24444813  0.66513825 -0.79375558  0.36061323 -0.43897569\n",
      "  -0.75808063  0.50624464  0.53859248]\n",
      " [ 0.86433268 -0.98829187  0.4262099   0.93139588 -0.16977542 -0.09560309\n",
      "   1.17130821  0.18694152 -0.67887633]\n",
      " [-0.85205721  0.98710492 -0.4215145  -0.79727509  0.23126896  0.0946115\n",
      "  -1.1796016  -0.5899205   0.50555284]\n",
      " [ 0.00294996  0.2814855   0.46532065 -0.94177291  0.03109692  0.04616134\n",
      "  -1.01773169  0.21450169  0.23480665]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.44214132  1.27087962 -2.35790227 -0.0989428   1.37101027 -2.1409499\n",
      "  -0.6401459 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:55 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.8476546]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 55 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 7.59659452e-01 -8.41634424e-01  3.43872625e-01  9.26894702e-01\n",
      "  -1.69932784e-01 -2.03392211e-01  1.02660751e+00  5.02664333e-01\n",
      "  -3.57347373e-01]\n",
      " [-6.58087030e-01  1.03168605e+00 -5.55815756e-01 -1.06704636e+00\n",
      "   1.05737109e-01  1.41738786e-01 -1.25658378e+00 -7.12673476e-01\n",
      "   4.23983903e-01]\n",
      " [ 4.11433708e-01 -2.44448134e-01  6.65138253e-01 -7.95813215e-01\n",
      "   3.60613225e-01 -4.38975690e-01 -7.60138259e-01  5.06244642e-01\n",
      "   5.38592478e-01]\n",
      " [ 8.65691058e-01 -9.88291866e-01  4.26209896e-01  9.32754256e-01\n",
      "  -1.69775420e-01 -9.56030942e-02  1.17266659e+00  1.86941523e-01\n",
      "  -6.78876327e-01]\n",
      " [-8.53522846e-01  9.87104919e-01 -4.21514499e-01 -7.98740726e-01\n",
      "   2.31268962e-01  9.46114992e-02 -1.18106723e+00 -5.89920504e-01\n",
      "   5.05552842e-01]\n",
      " [ 8.61694091e-04  2.81485498e-01  4.65320652e-01 -9.43861174e-01\n",
      "   3.10969156e-02  4.61613359e-02 -1.01981996e+00  2.14501687e-01\n",
      "   2.34806648e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.43230466  1.28010174 -2.35742574 -0.09655619  1.38036555 -2.14040121\n",
      "  -0.63892772]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:55 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.70208394]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 55 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.76659049 -0.84163442  0.34387263  0.9268947  -0.16300175 -0.19646117\n",
      "   1.03353855  0.50266433 -0.35734737]\n",
      " [-0.66502576  1.03168605 -0.55581576 -1.06704636  0.09879838  0.13480006\n",
      "  -1.26352251 -0.71267348  0.4239839 ]\n",
      " [ 0.40825442 -0.24444813  0.66513825 -0.79581321  0.35743394 -0.44215497\n",
      "  -0.76331754  0.50624464  0.53859248]\n",
      " [ 0.87254614 -0.98829187  0.4262099   0.93275426 -0.16292034 -0.08874801\n",
      "   1.17952168  0.18694152 -0.67887633]\n",
      " [-0.86043414  0.98710492 -0.4215145  -0.79874073  0.22435767  0.08770021\n",
      "  -1.18797853 -0.5899205   0.50555284]\n",
      " [-0.00506042  0.2814855   0.46532065 -0.94386117  0.0251748   0.04023922\n",
      "  -1.02574207  0.21450169  0.23480665]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.40114829  1.30515861 -2.35247815 -0.08425481  1.40699888 -2.13562396\n",
      "  -0.63018658]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:55 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.56829208]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 55 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.75200125 -0.84163442  0.34387263  0.91230546 -0.16300175 -0.21105041\n",
      "   1.03353855  0.50266433 -0.37193662]\n",
      " [-0.65026716  1.03168605 -0.55581576 -1.05228776  0.09879838  0.14955866\n",
      "  -1.26352251 -0.71267348  0.4387425 ]\n",
      " [ 0.41322204 -0.24444813  0.66513825 -0.7908456   0.35743394 -0.43718736\n",
      "  -0.76331754  0.50624464  0.54356009]\n",
      " [ 0.85857303 -0.98829187  0.4262099   0.91878115 -0.16292034 -0.10272112\n",
      "   1.17952168  0.18694152 -0.69284943]\n",
      " [-0.84627486  0.98710492 -0.4215145  -0.78458145  0.22435767  0.10185948\n",
      "  -1.18797853 -0.5899205   0.51971212]\n",
      " [ 0.00544526  0.2814855   0.46532065 -0.93335549  0.0251748   0.0507449\n",
      "  -1.02574207  0.21450169  0.24531233]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.47085959  1.25234392 -2.36894822 -0.11407239  1.35552396 -2.15348256\n",
      "  -0.65372318]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:55 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.72397308]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 55 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 7.58153753e-01 -8.35481919e-01  3.43872625e-01  9.18457963e-01\n",
      "  -1.63001745e-01 -2.04897910e-01  1.03969105e+00  5.02664333e-01\n",
      "  -3.71936616e-01]\n",
      " [-6.56324669e-01  1.02562854e+00 -5.55815756e-01 -1.05834528e+00\n",
      "   9.87983824e-02  1.43501148e-01 -1.26958002e+00 -7.12673476e-01\n",
      "   4.38742501e-01]\n",
      " [ 4.07201623e-01 -2.50468548e-01  6.65138253e-01 -7.96866016e-01\n",
      "   3.57433941e-01 -4.43207775e-01 -7.69337957e-01  5.06244642e-01\n",
      "   5.43560091e-01]\n",
      " [ 8.64545526e-01 -9.82319373e-01  4.26209896e-01  9.24753641e-01\n",
      "  -1.62920337e-01 -9.67486263e-02  1.18549417e+00  1.86941523e-01\n",
      "  -6.92849435e-01]\n",
      " [-8.52378039e-01  9.81001742e-01 -4.21514499e-01 -7.90684628e-01\n",
      "   2.24357671e-01  9.57563063e-02 -1.19408170e+00 -5.89920504e-01\n",
      "   5.19712116e-01]\n",
      " [-7.16152242e-04  2.75324083e-01  4.65320652e-01 -9.39516903e-01\n",
      "   2.51747980e-02  4.45834895e-02 -1.03190349e+00  2.14501687e-01\n",
      "   2.45312334e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.44327953  1.27546252 -2.36498476 -0.11023531  1.37940771 -2.14932889\n",
      "  -0.64917222]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:55 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.76356149]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 55 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.76276175 -0.83087392  0.34387263  0.92306596 -0.16300175 -0.20489791\n",
      "   1.04429905  0.50266433 -0.37193662]\n",
      " [-0.66085334  1.02109987 -0.55581576 -1.06287395  0.09879838  0.14350115\n",
      "  -1.27410869 -0.71267348  0.4387425 ]\n",
      " [ 0.40245538 -0.25521479  0.66513825 -0.80161226  0.35743394 -0.44320777\n",
      "  -0.7740842   0.50624464  0.54356009]\n",
      " [ 0.869036   -0.9778289   0.4262099   0.92924412 -0.16292034 -0.09674863\n",
      "   1.18998465  0.18694152 -0.69284943]\n",
      " [-0.85700854  0.97637125 -0.4215145  -0.79531512  0.22435767  0.09575631\n",
      "  -1.1987122  -0.5899205   0.51971212]\n",
      " [-0.00545642  0.27058382  0.46532065 -0.94425717  0.0251748   0.04458349\n",
      "  -1.03664375  0.21450169  0.24531233]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.42193677  1.29398174 -2.36234645 -0.10604535  1.39818943 -2.14644435\n",
      "  -0.64586661]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:55 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.78837735]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 55 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.76651888 -0.83087392  0.34762976  0.92306596 -0.16300175 -0.20114078\n",
      "   1.04805618  0.50266433 -0.37193662]\n",
      " [-0.66415442  1.02109987 -0.55911684 -1.06287395  0.09879838  0.14020007\n",
      "  -1.27740977 -0.71267348  0.4387425 ]\n",
      " [ 0.4017984  -0.25521479  0.66448127 -0.80161226  0.35743394 -0.44386476\n",
      "  -0.77474118  0.50624464  0.54356009]\n",
      " [ 0.87228227 -0.9778289   0.42945616  0.92924412 -0.16292034 -0.09350236\n",
      "   1.19323091  0.18694152 -0.69284943]\n",
      " [-0.86026418  0.97637125 -0.42477015 -0.79531512  0.22435767  0.09250066\n",
      "  -1.20196785 -0.5899205   0.51971212]\n",
      " [-0.00764642  0.27058382  0.46313065 -0.94425717  0.0251748   0.04239349\n",
      "  -1.03883376  0.21450169  0.24531233]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.40428337  1.30942871 -2.36080562 -0.09787809  1.41435897 -2.14495094\n",
      "  -0.63933477]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:55 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.24086204]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 56 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.76399181 -0.833401    0.34762976  0.92053888 -0.16300175 -0.20114078\n",
      "   1.04805618  0.50266433 -0.37446369]\n",
      " [-0.66270947  1.02254482 -0.55911684 -1.061429    0.09879838  0.14020007\n",
      "  -1.27740977 -0.71267348  0.44018745]\n",
      " [ 0.40241014 -0.25460305  0.66448127 -0.80100051  0.35743394 -0.44386476\n",
      "  -0.77474118  0.50624464  0.54417184]\n",
      " [ 0.87156501 -0.97854616  0.42945616  0.92852686 -0.16292034 -0.09350236\n",
      "   1.19323091  0.18694152 -0.6935667 ]\n",
      " [-0.8593917   0.97724373 -0.42477015 -0.79444264  0.22435767  0.09250066\n",
      "  -1.20196785 -0.5899205   0.5205846 ]\n",
      " [-0.0053567   0.27287354  0.46313065 -0.94196745  0.0251748   0.04239349\n",
      "  -1.03883376  0.21450169  0.24760206]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.42630388  1.29579039 -2.37035367 -0.10827534  1.40262941 -2.15508501\n",
      "  -0.64798207]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:56 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.8007812]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 56 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.76717428 -0.833401    0.35081223  0.92053888 -0.16300175 -0.20114078\n",
      "   1.05123866  0.50266433 -0.37446369]\n",
      " [-0.66549543  1.02254482 -0.56190279 -1.061429    0.09879838  0.14020007\n",
      "  -1.28019573 -0.71267348  0.44018745]\n",
      " [ 0.40354635 -0.25460305  0.66561747 -0.80100051  0.35743394 -0.44386476\n",
      "  -0.77360498  0.50624464  0.54417184]\n",
      " [ 0.87435719 -0.97854616  0.43224834  0.92852686 -0.16292034 -0.09350236\n",
      "   1.19602309  0.18694152 -0.6935667 ]\n",
      " [-0.86219399  0.97724373 -0.42757244 -0.79444264  0.22435767  0.09250066\n",
      "  -1.20477014 -0.5899205   0.5205846 ]\n",
      " [-0.00748069  0.27287354  0.46100666 -0.94196745  0.0251748   0.04239349\n",
      "  -1.04095775  0.21450169  0.24760206]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.41041313  1.31003755 -2.36914738 -0.09917752  1.4173083  -2.15386402\n",
      "  -0.64228223]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:56 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.65568867]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 56 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.75087537 -0.84969991  0.33451333  0.92053888 -0.16300175 -0.20114078\n",
      "   1.03493975  0.50266433 -0.37446369]\n",
      " [-0.64894612  1.03909413 -0.54535348 -1.061429    0.09879838  0.14020007\n",
      "  -1.26364642 -0.71267348  0.44018745]\n",
      " [ 0.40278883 -0.25536056  0.66485996 -0.80100051  0.35743394 -0.44386476\n",
      "  -0.77436249  0.50624464  0.54417184]\n",
      " [ 0.85778966 -0.99511369  0.41568081  0.92852686 -0.16292034 -0.09350236\n",
      "   1.17945556  0.18694152 -0.6935667 ]\n",
      " [-0.8456283   0.99380942 -0.41100675 -0.79444264  0.22435767  0.09250066\n",
      "  -1.18820445 -0.5899205   0.5205846 ]\n",
      " [-0.00180186  0.27855237  0.46668549 -0.94196745  0.0251748   0.04239349\n",
      "  -1.03527892  0.21450169  0.24760206]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.48442761  1.25143271 -2.38281533 -0.13694249  1.35653214 -2.1671763\n",
      "  -0.67351653]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:56 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.62052498]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 56 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.76087059 -0.84969991  0.34450854  0.92053888 -0.16300175 -0.20114078\n",
      "   1.03493975  0.51265954 -0.37446369]\n",
      " [-0.65853942  1.03909413 -0.55494678 -1.061429    0.09879838  0.14020007\n",
      "  -1.26364642 -0.72226678  0.44018745]\n",
      " [ 0.41278775 -0.25536056  0.67485888 -0.80100051  0.35743394 -0.44386476\n",
      "  -0.77436249  0.51624356  0.54417184]\n",
      " [ 0.86776669 -0.99511369  0.42565785  0.92852686 -0.16292034 -0.09350236\n",
      "   1.17945556  0.19691856 -0.6935667 ]\n",
      " [-0.85533915  0.99380942 -0.4207176  -0.79444264  0.22435767  0.09250066\n",
      "  -1.18820445 -0.59963136  0.5205846 ]\n",
      " [ 0.00497414  0.27855237  0.47346149 -0.94196745  0.0251748   0.04239349\n",
      "  -1.03527892  0.22127768  0.24760206]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.43974941  1.2885309  -2.37703749 -0.09993384  1.39279277 -2.16108766\n",
      "  -0.64386801]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:56 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.57585761]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 56 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.77221855 -0.84969991  0.34450854  0.93188685 -0.16300175 -0.20114078\n",
      "   1.03493975  0.51265954 -0.36311573]\n",
      " [-0.66982474  1.03909413 -0.55494678 -1.07271432  0.09879838  0.14020007\n",
      "  -1.26364642 -0.72226678  0.42890213]\n",
      " [ 0.41479509 -0.25536056  0.67485888 -0.79899317  0.35743394 -0.44386476\n",
      "  -0.77436249  0.51624356  0.54617918]\n",
      " [ 0.87845436 -0.99511369  0.42565785  0.93921452 -0.16292034 -0.09350236\n",
      "   1.17945556  0.19691856 -0.68287903]\n",
      " [-0.86613762  0.99380942 -0.4207176  -0.80524111  0.22435767  0.09250066\n",
      "  -1.18820445 -0.59963136  0.50978613]\n",
      " [-0.00297105  0.27855237  0.47346149 -0.94991263  0.0251748   0.04239349\n",
      "  -1.03527892  0.22127768  0.23965687]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.38795195  1.32929559 -2.36576701 -0.07201962  1.43168079 -2.14843306\n",
      "  -0.62655893]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:56 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.15970209]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 56 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.77209817 -0.84969991  0.34450854  0.93188685 -0.16312213 -0.20126116\n",
      "   1.03493975  0.51265954 -0.36323612]\n",
      " [-0.66981958  1.03909413 -0.55494678 -1.07271432  0.09880354  0.14020522\n",
      "  -1.26364642 -0.72226678  0.42890729]\n",
      " [ 0.41284872 -0.25536056  0.67485888 -0.79899317  0.35548757 -0.44581113\n",
      "  -0.77436249  0.51624356  0.54423281]\n",
      " [ 0.87861722 -0.99511369  0.42565785  0.93921452 -0.16275748 -0.0933395\n",
      "   1.17945556  0.19691856 -0.68271617]\n",
      " [-0.86603186  0.99380942 -0.4207176  -0.80524111  0.22446343  0.09260642\n",
      "  -1.18820445 -0.59963136  0.50989189]\n",
      " [-0.00376755  0.27855237  0.47346149 -0.94991263  0.02437829  0.04159698\n",
      "  -1.03527892  0.22127768  0.23886036]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.39866774  1.32381726 -2.37111975 -0.07958165  1.42648584 -2.15368517\n",
      "  -0.63272568]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:56 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.03279055]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 56 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.77218755 -0.84961052  0.34450854  0.93188685 -0.16303274 -0.20117178\n",
      "   1.03493975  0.51265954 -0.36314673]\n",
      " [-0.66992379  1.03898993 -0.55494678 -1.07271432  0.09869934  0.14010102\n",
      "  -1.26364642 -0.72226678  0.42880309]\n",
      " [ 0.41277623 -0.25543305  0.67485888 -0.79899317  0.35541508 -0.44588362\n",
      "  -0.77436249  0.51624356  0.54416032]\n",
      " [ 0.87872232 -0.99500858  0.42565785  0.93921452 -0.16265238 -0.0932344\n",
      "   1.17945556  0.19691856 -0.68261107]\n",
      " [-0.86613149  0.99370979 -0.4207176  -0.80524111  0.2243638   0.09250679\n",
      "  -1.18820445 -0.59963136  0.50979226]\n",
      " [-0.00383691  0.27848301  0.47346149 -0.94991263  0.02430893  0.04152762\n",
      "  -1.03527892  0.22127768  0.23879101]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.39918773  1.32365663 -2.37150366 -0.07991874  1.42635158 -2.15406063\n",
      "  -0.63305897]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:56 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.42513867]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 56 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.76512201 -0.85667606  0.34450854  0.93188685 -0.17009829 -0.20823732\n",
      "   1.02787421  0.51265954 -0.36314673]\n",
      " [-0.66226052  1.0466532  -0.55494678 -1.07271432  0.1063626   0.14776429\n",
      "  -1.25598315 -0.72226678  0.42880309]\n",
      " [ 0.42090463 -0.24730465  0.67485888 -0.79899317  0.36354348 -0.43775522\n",
      "  -0.7662341   0.51624356  0.54416032]\n",
      " [ 0.86977601 -1.00395489  0.42565785  0.93921452 -0.17159868 -0.1021807\n",
      "   1.17050925  0.19691856 -0.68261107]\n",
      " [-0.85769348  1.0021478  -0.4207176  -0.80524111  0.23280181  0.10094479\n",
      "  -1.17976644 -0.59963136  0.50979226]\n",
      " [ 0.00417988  0.2864998   0.47346149 -0.94991263  0.03232572  0.04954441\n",
      "  -1.02726213  0.22127768  0.23879101]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.45113877  1.29019368 -2.38925472 -0.09707059  1.39042587 -2.1707984\n",
      "  -0.65035696]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:56 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.85147489]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 56 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.76659954 -0.85667606  0.34450854  0.93336438 -0.17009829 -0.20823732\n",
      "   1.02935174  0.51265954 -0.36314673]\n",
      " [-0.66353995  1.0466532  -0.55494678 -1.07399375  0.1063626   0.14776429\n",
      "  -1.25726258 -0.72226678  0.42880309]\n",
      " [ 0.41893585 -0.24730465  0.67485888 -0.80096195  0.36354348 -0.43775522\n",
      "  -0.76820287  0.51624356  0.54416032]\n",
      " [ 0.87106382 -1.00395489  0.42565785  0.94050233 -0.17159868 -0.1021807\n",
      "   1.17179706  0.19691856 -0.68261107]\n",
      " [-0.85908252  1.0021478  -0.4207176  -0.80663014  0.23280181  0.10094479\n",
      "  -1.18115547 -0.59963136  0.50979226]\n",
      " [ 0.00219411  0.2864998   0.47346149 -0.9518984   0.03232572  0.04954441\n",
      "  -1.0292479   0.22127768  0.23879101]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.44174713  1.29900754 -2.38880546 -0.09480225  1.39936332 -2.1702813\n",
      "  -0.64921054]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:56 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.70419326]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 56 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.77345643 -0.85667606  0.34450854  0.93336438 -0.1632414  -0.20138043\n",
      "   1.03620863  0.51265954 -0.36314673]\n",
      " [-0.67040162  1.0466532  -0.55494678 -1.07399375  0.09950094  0.14090262\n",
      "  -1.26412425 -0.72226678  0.42880309]\n",
      " [ 0.41581608 -0.24730465  0.67485888 -0.80096195  0.36042371 -0.44087499\n",
      "  -0.77132264  0.51624356  0.54416032]\n",
      " [ 0.8778463  -1.00395489  0.42565785  0.94050233 -0.1648162  -0.09539822\n",
      "   1.17857954  0.19691856 -0.68261107]\n",
      " [-0.86591838  1.0021478  -0.4207176  -0.80663014  0.22596594  0.09410892\n",
      "  -1.18799134 -0.59963136  0.50979226]\n",
      " [-0.00367466  0.2864998   0.47346149 -0.9518984   0.02645696  0.04367564\n",
      "  -1.03511667  0.22127768  0.23879101]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.4109381   1.32380771 -2.3839109  -0.08261159  1.42568481 -2.16554843\n",
      "  -0.6405885 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:56 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.56903603]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 56 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.75884038 -0.85667606  0.34450854  0.91874833 -0.1632414  -0.21599648\n",
      "   1.03620863  0.51265954 -0.37776278]\n",
      " [-0.65562314  1.0466532  -0.55494678 -1.05921528  0.09950094  0.1556811\n",
      "  -1.26412425 -0.72226678  0.44358156]\n",
      " [ 0.42063631 -0.24730465  0.67485888 -0.79614172  0.36042371 -0.43605476\n",
      "  -0.77132264  0.51624356  0.54898055]\n",
      " [ 0.86384264 -1.00395489  0.42565785  0.92649867 -0.1648162  -0.10940188\n",
      "   1.17857954  0.19691856 -0.69661473]\n",
      " [-0.85172913  1.0021478  -0.4207176  -0.79244089  0.22596594  0.10829818\n",
      "  -1.18799134 -0.59963136  0.52398151]\n",
      " [ 0.00683102  0.2864998   0.47346149 -0.94139273  0.02645696  0.05418132\n",
      "  -1.03511667  0.22127768  0.24929668]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.4807116   1.27091238 -2.40037803 -0.11261404  1.3741281  -2.18338675\n",
      "  -0.6641581 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:56 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.72497477]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 56 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 7.64958428e-01 -8.50558017e-01  3.44508538e-01  9.24866375e-01\n",
      "  -1.63241396e-01 -2.09878434e-01  1.04232668e+00  5.12659545e-01\n",
      "  -3.77762779e-01]\n",
      " [-6.61652504e-01  1.04062384e+00 -5.54946783e-01 -1.06524464e+00\n",
      "   9.95009401e-02  1.49651738e-01 -1.27015361e+00 -7.22266780e-01\n",
      "   4.43581561e-01]\n",
      " [ 4.14659214e-01 -2.53281746e-01  6.74858876e-01 -8.02118819e-01\n",
      "   3.60423713e-01 -4.42031855e-01 -7.77299737e-01  5.16243557e-01\n",
      "   5.48980547e-01]\n",
      " [ 8.69791902e-01 -9.98005627e-01  4.25657847e-01  9.32447932e-01\n",
      "  -1.64816204e-01 -1.03452619e-01  1.18452880e+00  1.96918561e-01\n",
      "  -6.96614725e-01]\n",
      " [-8.57802333e-01  9.96074602e-01 -4.20717598e-01 -7.98514092e-01\n",
      "   2.25965939e-01  1.02224976e-01 -1.19406454e+00 -5.99631355e-01\n",
      "   5.23981510e-01]\n",
      " [ 7.08361921e-04  2.80377140e-01  4.73461487e-01 -9.47515382e-01\n",
      "   2.64569558e-02  4.80586664e-02 -1.04123932e+00  2.21277685e-01\n",
      "   2.49296684e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.45329346  1.29388023 -2.39641007 -0.10882441  1.39783873 -2.17922878\n",
      "  -0.65966241]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:56 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.76581464]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 56 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.76949165 -0.8460248   0.34450854  0.92939959 -0.1632414  -0.20987843\n",
      "   1.0468599   0.51265954 -0.37776278]\n",
      " [-0.66611075  1.03616559 -0.55494678 -1.06970289  0.09950094  0.14965174\n",
      "  -1.27461186 -0.72226678  0.44358156]\n",
      " [ 0.40998531 -0.25795565  0.67485888 -0.80679272  0.36042371 -0.44203185\n",
      "  -0.78197364  0.51624356  0.54898055]\n",
      " [ 0.87421442 -0.99358311  0.42565785  0.93687045 -0.1648162  -0.10345262\n",
      "   1.18895132  0.19691856 -0.69661473]\n",
      " [-0.86236     0.99151694 -0.4207176  -0.80307175  0.22596594  0.10222498\n",
      "  -1.1986222  -0.59963136  0.52398151]\n",
      " [-0.00395044  0.27571834  0.47346149 -0.95217418  0.02645696  0.04805867\n",
      "  -1.04589812  0.22127768  0.24929668]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.43229376  1.31210364 -2.39380917 -0.10473012  1.41631032 -2.17638609\n",
      "  -0.65643966]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:56 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.7904452]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 56 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.77318104 -0.8460248   0.34819793  0.92939959 -0.1632414  -0.20618904\n",
      "   1.05054929  0.51265954 -0.37776278]\n",
      " [-0.66935776  1.03616559 -0.55819379 -1.06970289  0.09950094  0.14640473\n",
      "  -1.27785886 -0.72226678  0.44358156]\n",
      " [ 0.40938442 -0.25795565  0.67425799 -0.80679272  0.36042371 -0.44263274\n",
      "  -0.78257453  0.51624356  0.54898055]\n",
      " [ 0.87741001 -0.99358311  0.42885343  0.93687045 -0.1648162  -0.10025703\n",
      "   1.1921469   0.19691856 -0.69661473]\n",
      " [-0.86556334  0.99151694 -0.42392094 -0.80307175  0.22596594  0.09902163\n",
      "  -1.20182555 -0.59963136  0.52398151]\n",
      " [-0.00608998  0.27571834  0.47132194 -0.95217418  0.02645696  0.04591912\n",
      "  -1.04803766  0.22127768  0.24929668]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.41493826  1.32729931 -2.3922926  -0.09665521  1.43220284 -2.17491519\n",
      "  -0.65000239]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:56 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.234581]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 57 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.77079934 -0.8484065   0.34819793  0.92701789 -0.1632414  -0.20618904\n",
      "   1.05054929  0.51265954 -0.38014448]\n",
      " [-0.66801518  1.03750817 -0.55819379 -1.06836031  0.09950094  0.14640473\n",
      "  -1.27785886 -0.72226678  0.44492414]\n",
      " [ 0.40994294 -0.25739713  0.67425799 -0.8062342   0.36042371 -0.44263274\n",
      "  -0.78257453  0.51624356  0.54953907]\n",
      " [ 0.87675923 -0.99423389  0.42885343  0.93621967 -0.1648162  -0.10025703\n",
      "   1.1921469   0.19691856 -0.69726551]\n",
      " [-0.86476179  0.99231849 -0.42392094 -0.8022702   0.22596594  0.09902163\n",
      "  -1.20182555 -0.59963136  0.52478306]\n",
      " [-0.00391272  0.2778956   0.47132194 -0.94999692  0.02645696  0.04591912\n",
      "  -1.04803766  0.22127768  0.25147394]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.4359981   1.31429565 -2.40146484 -0.10662555  1.42102047 -2.18464042\n",
      "  -0.65828629]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:57 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.80383081]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 57 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.77388675 -0.8484065   0.35128534  0.92701789 -0.1632414  -0.20618904\n",
      "   1.0536367   0.51265954 -0.38014448]\n",
      " [-0.67072091  1.03750817 -0.56089952 -1.06836031  0.09950094  0.14640473\n",
      "  -1.28056459 -0.72226678  0.44492414]\n",
      " [ 0.4110831  -0.25739713  0.67539814 -0.8062342   0.36042371 -0.44263274\n",
      "  -0.78143437  0.51624356  0.54953907]\n",
      " [ 0.87947262 -0.99423389  0.43156683  0.93621967 -0.1648162  -0.10025703\n",
      "   1.1948603   0.19691856 -0.69726551]\n",
      " [-0.86748399  0.99231849 -0.42664314 -0.8022702   0.22596594  0.09902163\n",
      "  -1.20454774 -0.59963136  0.52478306]\n",
      " [-0.00597874  0.2778956   0.46925593 -0.94999692  0.02645696  0.04591912\n",
      "  -1.05010368  0.22127768  0.25147394]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.42053145  1.32817667 -2.40029597 -0.0977347   1.43531141 -2.18345682\n",
      "  -0.65273704]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:57 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.65390401]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 57 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 7.57607204e-01 -8.64686043e-01  3.35005797e-01  9.27017893e-01\n",
      "  -1.63241396e-01 -2.06189039e-01  1.03735716e+00  5.12659545e-01\n",
      "  -3.80144480e-01]\n",
      " [-6.54184247e-01  1.05404483e+00 -5.44362858e-01 -1.06836031e+00\n",
      "   9.95009401e-02  1.46404733e-01 -1.26402793e+00 -7.22266780e-01\n",
      "   4.44924139e-01]\n",
      " [ 4.10202155e-01 -2.58278073e-01  6.74517199e-01 -8.06234202e-01\n",
      "   3.60423713e-01 -4.42632744e-01 -7.82315318e-01  5.16243557e-01\n",
      "   5.49539069e-01]\n",
      " [ 8.62913529e-01 -1.01079298e+00  4.15007735e-01  9.36219671e-01\n",
      "  -1.64816204e-01 -1.00257032e-01  1.17830121e+00  1.96918561e-01\n",
      "  -6.97265507e-01]\n",
      " [-8.50927076e-01  1.00887540e+00 -4.10086228e-01 -8.02270205e-01\n",
      "   2.25965939e-01  9.90216324e-02 -1.18799083e+00 -5.99631355e-01\n",
      "   5.24783060e-01]\n",
      " [-3.98217096e-04  2.83476120e-01  4.74836448e-01 -9.49996923e-01\n",
      "   2.64569558e-02  4.59191247e-02 -1.04452316e+00  2.21277685e-01\n",
      "   2.51473943e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.49452512  1.26965455 -2.41407602 -0.13561281  1.37468781 -2.19688515\n",
      "  -0.68406417]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:57 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.62433758]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 57 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.76745652 -0.86468604  0.34485512  0.92701789 -0.1632414  -0.20618904\n",
      "   1.03735716  0.52250886 -0.38014448]\n",
      " [-0.6636147   1.05404483 -0.55379331 -1.06836031  0.09950094  0.14640473\n",
      "  -1.26402793 -0.73169723  0.44492414]\n",
      " [ 0.42005322 -0.25827807  0.68436826 -0.8062342   0.36042371 -0.44263274\n",
      "  -0.78231532  0.52609462  0.54953907]\n",
      " [ 0.87275912 -1.01079298  0.42485333  0.93621967 -0.1648162  -0.10025703\n",
      "   1.17830121  0.20676415 -0.69726551]\n",
      " [-0.8604767   1.0088754  -0.41963586 -0.8022702   0.22596594  0.09902163\n",
      "  -1.18799083 -0.60918098  0.52478306]\n",
      " [ 0.00640684  0.28347612  0.48164151 -0.94999692  0.02645696  0.04591912\n",
      "  -1.04452316  0.22808274  0.25147394]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.45047115  1.30634066 -2.40844721 -0.09895318  1.41053857 -2.19095425\n",
      "  -0.65466972]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:57 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.57832579]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 57 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.7787365  -0.86468604  0.34485512  0.93829787 -0.1632414  -0.20618904\n",
      "   1.03735716  0.52250886 -0.3688645 ]\n",
      " [-0.67483424  1.05404483 -0.55379331 -1.07957984  0.09950094  0.14640473\n",
      "  -1.26402793 -0.73169723  0.4337046 ]\n",
      " [ 0.42213907 -0.25827807  0.68436826 -0.80414835  0.36042371 -0.44263274\n",
      "  -0.78231532  0.52609462  0.55162492]\n",
      " [ 0.88340623 -1.01079298  0.42485333  0.94686679 -0.1648162  -0.10025703\n",
      "   1.17830121  0.20676415 -0.68661839]\n",
      " [-0.87123036  1.0088754  -0.41963586 -0.81302386  0.22596594  0.09902163\n",
      "  -1.18799083 -0.60918098  0.51402941]\n",
      " [-0.00150381  0.28347612  0.48164151 -0.95790757  0.02645696  0.04591912\n",
      "  -1.04452316  0.22808274  0.24356329]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.39905535  1.34686849 -2.39732327 -0.07115013  1.44922631 -2.17847592\n",
      "  -0.63751934]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:57 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.15390859]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 57 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.77863523 -0.86468604  0.34485512  0.93829787 -0.16334267 -0.20629031\n",
      "   1.03735716  0.52250886 -0.36896578]\n",
      " [-0.6748462   1.05404483 -0.55379331 -1.07957984  0.09948897  0.14639277\n",
      "  -1.26402793 -0.73169723  0.43369264]\n",
      " [ 0.42029654 -0.25827807  0.68436826 -0.80414835  0.35858118 -0.44447527\n",
      "  -0.78231532  0.52609462  0.54978239]\n",
      " [ 0.88357711 -1.01079298  0.42485333  0.94686679 -0.16464533 -0.10008616\n",
      "   1.17830121  0.20676415 -0.68644752]\n",
      " [-0.87114968  1.0088754  -0.41963586 -0.81302386  0.22604662  0.09910231\n",
      "  -1.18799083 -0.60918098  0.51411009]\n",
      " [-0.0022724   0.28347612  0.48164151 -0.95790757  0.02568837  0.04515054\n",
      "  -1.04452316  0.22808274  0.2427947 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.40907639  1.34175667 -2.40234576 -0.07825717  1.44438679 -2.18340575\n",
      "  -0.64331118]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:57 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.03020646]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 57 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.77871253 -0.86460873  0.34485512  0.93829787 -0.16326536 -0.20621301\n",
      "   1.03735716  0.52250886 -0.36888847]\n",
      " [-0.67493577  1.05395526 -0.55379331 -1.07957984  0.0993994   0.1463032\n",
      "  -1.26402793 -0.73169723  0.43360307]\n",
      " [ 0.42023367 -0.25834094  0.68436826 -0.80414835  0.35851831 -0.44453814\n",
      "  -0.78231532  0.52609462  0.54971952]\n",
      " [ 0.88366747 -1.01070262  0.42485333  0.94686679 -0.16455497 -0.0999958\n",
      "   1.17830121  0.20676415 -0.68635716]\n",
      " [-0.87123556  1.00878951 -0.41963586 -0.81302386  0.22596074  0.09901643\n",
      "  -1.18799083 -0.60918098  0.5140242 ]\n",
      " [-0.0023327   0.28341582  0.48164151 -0.95790757  0.02562807  0.04509024\n",
      "  -1.04452316  0.22808274  0.2427344 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.40951883  1.34162182 -2.40267425 -0.07854544  1.4442745  -2.18372718\n",
      "  -0.64359631]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:57 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.41600668]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 57 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.77194918 -0.87137208  0.34485512  0.93829787 -0.17002871 -0.21297635\n",
      "   1.03059381  0.52250886 -0.36888847]\n",
      " [-0.66763132  1.06125971 -0.55379331 -1.07957984  0.10670385  0.15360765\n",
      "  -1.25672348 -0.73169723  0.43360307]\n",
      " [ 0.42813138 -0.25044324  0.68436826 -0.80414835  0.36641602 -0.43664044\n",
      "  -0.77441761  0.52609462  0.54971952]\n",
      " [ 0.87512069 -1.01924939  0.42485333  0.94686679 -0.17310174 -0.10854257\n",
      "   1.16975443  0.20676415 -0.68635716]\n",
      " [-0.86317805  1.01684703 -0.41963586 -0.81302386  0.23401825  0.10707394\n",
      "  -1.17993332 -0.60918098  0.5140242 ]\n",
      " [ 0.00544742  0.29119594  0.48164151 -0.95790757  0.03340819  0.05287036\n",
      "  -1.03674304  0.22808274  0.2427344 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.46005223  1.30920346 -2.42012866 -0.09524094  1.40955171 -2.20021077\n",
      "  -0.66044556]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:57 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.855158]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 57 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.77334894 -0.87137208  0.34485512  0.93969763 -0.17002871 -0.21297635\n",
      "   1.03199356  0.52250886 -0.36888847]\n",
      " [-0.66884433  1.06125971 -0.55379331 -1.08079285  0.10670385  0.15360765\n",
      "  -1.25793649 -0.73169723  0.43360307]\n",
      " [ 0.42624687 -0.25044324  0.68436826 -0.80603286  0.36641602 -0.43664044\n",
      "  -0.77630212  0.52609462  0.54971952]\n",
      " [ 0.87634218 -1.01924939  0.42485333  0.94808827 -0.17310174 -0.10854257\n",
      "   1.17097592  0.20676415 -0.68635716]\n",
      " [-0.86449517  1.01684703 -0.41963586 -0.81434098  0.23401825  0.10707394\n",
      "  -1.18125044 -0.60918098  0.5140242 ]\n",
      " [ 0.0035585   0.29119594  0.48164151 -0.95979649  0.03340819  0.05287036\n",
      "  -1.03863196  0.22808274  0.2427344 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.45108196  1.31763008 -2.41970483 -0.0930844   1.41809319 -2.1997231\n",
      "  -0.65936606]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:57 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.70632435]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 57 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.78013084 -0.87137208  0.34485512  0.93969763 -0.16324682 -0.20619446\n",
      "   1.03877546  0.52250886 -0.36888847]\n",
      " [-0.67562807  1.06125971 -0.55379331 -1.08079285  0.09992012  0.14682391\n",
      "  -1.26472022 -0.73169723  0.43360307]\n",
      " [ 0.42318384 -0.25044324  0.68436826 -0.80603286  0.363353   -0.43970346\n",
      "  -0.77936514  0.52609462  0.54971952]\n",
      " [ 0.88305077 -1.01924939  0.42485333  0.94808827 -0.16639315 -0.10183398\n",
      "   1.17768451  0.20676415 -0.68635716]\n",
      " [-0.87125461  1.01684703 -0.41963586 -0.81434098  0.22725881  0.10031451\n",
      "  -1.18800988 -0.60918098  0.5140242 ]\n",
      " [-0.00225641  0.29119594  0.48164151 -0.95979649  0.02759327  0.04705544\n",
      "  -1.04444687  0.22808274  0.2427344 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.42062335  1.34217072 -2.41486509 -0.08100909  1.44410237 -2.19503678\n",
      "  -0.65086424]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:57 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.56974768]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 57 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.76548956 -0.87137208  0.34485512  0.92505635 -0.16324682 -0.22083573\n",
      "   1.03877546  0.52250886 -0.38352975]\n",
      " [-0.66083084  1.06125971 -0.55379331 -1.06599563  0.09992012  0.16162113\n",
      "  -1.26472022 -0.73169723  0.44840029]\n",
      " [ 0.42785945 -0.25044324  0.68436826 -0.80135725  0.363353   -0.43502785\n",
      "  -0.77936514  0.52609462  0.55439513]\n",
      " [ 0.86901763 -1.01924939  0.42485333  0.93405513 -0.16639315 -0.11586712\n",
      "   1.17768451  0.20676415 -0.7003903 ]\n",
      " [-0.85703682  1.01684703 -0.41963586 -0.80012319  0.22725881  0.1145323\n",
      "  -1.18800988 -0.60918098  0.528242  ]\n",
      " [ 0.00824787  0.29119594  0.48164151 -0.94929221  0.02759327  0.05755972\n",
      "  -1.04444687  0.22808274  0.25323868]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.49045597  1.28919919 -2.43132976 -0.11119157  1.39246684 -2.21285556\n",
      "  -0.67446699]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:57 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.72593503]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 57 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.77157459 -0.86528706  0.34485512  0.93114138 -0.16324682 -0.21475071\n",
      "   1.04486048  0.52250886 -0.38352975]\n",
      " [-0.66683305  1.0552575  -0.55379331 -1.07199784  0.09992012  0.15561892\n",
      "  -1.27072243 -0.73169723  0.44840029]\n",
      " [ 0.42192484 -0.25637785  0.68436826 -0.80729187  0.363353   -0.44096246\n",
      "  -0.78529976  0.52609462  0.55439513]\n",
      " [ 0.87494429 -1.01332273  0.42485333  0.93998179 -0.16639315 -0.10994046\n",
      "   1.18361117  0.20676415 -0.7003903 ]\n",
      " [-0.86308111  1.01080274 -0.41963586 -0.80616748  0.22725881  0.10848801\n",
      "  -1.19405417 -0.60918098  0.528242  ]\n",
      " [ 0.0021627   0.28511077  0.48164151 -0.95537738  0.02759327  0.05147455\n",
      "  -1.05053204  0.22808274  0.25323868]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.4631929   1.31202225 -2.4273573  -0.10744975  1.41601163 -2.20869324\n",
      "  -0.67002556]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:57 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.76801879]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 57 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.77603504 -0.8608266   0.34485512  0.93560183 -0.16324682 -0.21475071\n",
      "   1.04932094  0.52250886 -0.38352975]\n",
      " [-0.67122261  1.05086794 -0.55379331 -1.0763874   0.09992012  0.15561892\n",
      "  -1.27511199 -0.73169723  0.44840029]\n",
      " [ 0.41732151 -0.26098118  0.68436826 -0.8118952   0.363353   -0.44096246\n",
      "  -0.78990309  0.52609462  0.55439513]\n",
      " [ 0.87930039 -1.00896663  0.42485333  0.94433789 -0.16639315 -0.10994046\n",
      "   1.18796727  0.20676415 -0.7003903 ]\n",
      " [-0.86756779  1.00631606 -0.41963586 -0.81065416  0.22725881  0.10848801\n",
      "  -1.19854085 -0.60918098  0.528242  ]\n",
      " [-0.00241641  0.28053166  0.48164151 -0.95995649  0.02759327  0.05147455\n",
      "  -1.05511115  0.22808274  0.25323868]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.44252733  1.32995736 -2.424793   -0.10344974  1.43418154 -2.2058914\n",
      "  -0.66688343]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:57 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.79243134]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 57 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.77966004 -0.8608266   0.34848011  0.93560183 -0.16324682 -0.21112572\n",
      "   1.05294593  0.52250886 -0.38352975]\n",
      " [-0.67441829  1.05086794 -0.55698899 -1.0763874   0.09992012  0.15242325\n",
      "  -1.27830767 -0.73169723  0.44840029]\n",
      " [ 0.41677252 -0.26098118  0.68381927 -0.8118952   0.363353   -0.44151145\n",
      "  -0.79045208  0.52609462  0.55439513]\n",
      " [ 0.8824477  -1.00896663  0.42800063  0.94433789 -0.16639315 -0.10679315\n",
      "   1.19111458  0.20676415 -0.7003903 ]\n",
      " [-0.87072146  1.00631606 -0.42278953 -0.81065416  0.22725881  0.10533434\n",
      "  -1.20169452 -0.60918098  0.528242  ]\n",
      " [-0.00450736  0.28053166  0.47955055 -0.95995649  0.02759327  0.0493836\n",
      "  -1.0572021   0.22808274  0.25323868]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.42545647  1.34491213 -2.42329925 -0.09546483  1.4498092  -2.20444169\n",
      "  -0.66053612]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:57 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.22843678]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 58 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.77741672 -0.86306992  0.34848011  0.93335852 -0.16324682 -0.21112572\n",
      "   1.05294593  0.52250886 -0.38577306]\n",
      " [-0.67317215  1.05211407 -0.55698899 -1.07514126  0.09992012  0.15242325\n",
      "  -1.27830767 -0.73169723  0.44964642]\n",
      " [ 0.41728308 -0.26047061  0.68381927 -0.81138463  0.363353   -0.44151145\n",
      "  -0.79045208  0.52609462  0.55490569]\n",
      " [ 0.88185873 -1.0095556   0.42800063  0.94374893 -0.16639315 -0.10679315\n",
      "   1.19111458  0.20676415 -0.70097927]\n",
      " [-0.86998652  1.007051   -0.42278953 -0.80991922  0.22725881  0.10533434\n",
      "  -1.20169452 -0.60918098  0.52897694]\n",
      " [-0.00243724  0.28260178  0.47955055 -0.95788637  0.02759327  0.0493836\n",
      "  -1.0572021   0.22808274  0.2553088 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.44558785  1.33251929 -2.43210562 -0.10501908  1.43915319 -2.21376979\n",
      "  -0.6684671 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:58 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.80677165]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 58 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.78041384 -0.86306992  0.35147723  0.93335852 -0.16324682 -0.21112572\n",
      "   1.05594305  0.52250886 -0.38577306]\n",
      " [-0.67580175  1.05211407 -0.55961859 -1.07514126  0.09992012  0.15242325\n",
      "  -1.28093727 -0.73169723  0.44964642]\n",
      " [ 0.41842501 -0.26047061  0.6849612  -0.81138463  0.363353   -0.44151145\n",
      "  -0.78931015  0.52609462  0.55490569]\n",
      " [ 0.88449721 -1.0095556   0.43063911  0.94374893 -0.16639315 -0.10679315\n",
      "   1.19375306  0.20676415 -0.70097927]\n",
      " [-0.87263269  1.007051   -0.4254357  -0.80991922  0.22725881  0.10533434\n",
      "  -1.20434069 -0.60918098  0.52897694]\n",
      " [-0.00444755  0.28260178  0.47754024 -0.95788637  0.02759327  0.0493836\n",
      "  -1.05921241  0.22808274  0.2553088 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.43052656  1.34604935 -2.43097201 -0.09632805  1.45307299 -2.21262142\n",
      "  -0.66306141]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:58 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.65200322]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 58 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 7.64155721e-01 -8.79328037e-01  3.35219110e-01  9.33358517e-01\n",
      "  -1.63246816e-01 -2.11125717e-01  1.03968493e+00  5.22508864e-01\n",
      "  -3.85773060e-01]\n",
      " [-6.59280210e-01  1.06863562e+00 -5.43097043e-01 -1.07514126e+00\n",
      "   9.99201181e-02  1.52423250e-01 -1.26441572e+00 -7.31697234e-01\n",
      "   4.49646423e-01]\n",
      " [ 4.17434447e-01 -2.61461177e-01  6.83970637e-01 -8.11384633e-01\n",
      "   3.63352999e-01 -4.41511455e-01 -7.90300713e-01  5.26094622e-01\n",
      "   5.54905691e-01]\n",
      " [ 8.67949434e-01 -1.02610338e+00  4.14091334e-01  9.43748926e-01\n",
      "  -1.66393152e-01 -1.06793153e-01  1.17720528e+00  2.06764152e-01\n",
      "  -7.00979268e-01]\n",
      " [-8.56087112e-01  1.02359658e+00 -4.08890125e-01 -8.09919217e-01\n",
      "   2.27258813e-01  1.05334338e-01 -1.18779512e+00 -6.09180984e-01\n",
      "   5.28976938e-01]\n",
      " [ 1.03780741e-03  2.88087143e-01  4.83025603e-01 -9.57886372e-01\n",
      "   2.75932743e-02  4.93835951e-02 -1.05372705e+00  2.28082744e-01\n",
      "   2.55308800e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.5044947   1.2876165  -2.44486464 -0.13430315  1.39260587 -2.22616638\n",
      "  -0.69447551]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:58 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.62803515]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 58 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.77386154 -0.87932804  0.34492493  0.93335852 -0.16324682 -0.21112572\n",
      "   1.03968493  0.53221468 -0.38577306]\n",
      " [-0.66855272  1.06863562 -0.55236956 -1.07514126  0.09992012  0.15242325\n",
      "  -1.26441572 -0.74096975  0.44964642]\n",
      " [ 0.42713725 -0.26146118  0.69367344 -0.81138463  0.363353   -0.44151145\n",
      "  -0.79030071  0.53579742  0.55490569]\n",
      " [ 0.87766545 -1.02610338  0.42380735  0.94374893 -0.16639315 -0.10679315\n",
      "   1.17720528  0.21648017 -0.70097927]\n",
      " [-0.86548011  1.02359658 -0.41828312 -0.80991922  0.22725881  0.10533434\n",
      "  -1.18779512 -0.61857398  0.52897694]\n",
      " [ 0.00786951  0.28808714  0.48985731 -0.95788637  0.02759327  0.0493836\n",
      "  -1.05372705  0.23491445  0.2553088 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.4610479   1.32389695 -2.4393773  -0.09798915  1.42805406 -2.22038529\n",
      "  -0.66532814]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:58 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.58076133]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 58 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 7.85072991e-01 -8.79328037e-01  3.44924930e-01  9.44569967e-01\n",
      "  -1.63246816e-01 -2.11125717e-01  1.03968493e+00  5.32214684e-01\n",
      "  -3.74561609e-01]\n",
      " [-6.79705827e-01  1.06863562e+00 -5.52369557e-01 -1.08629437e+00\n",
      "   9.99201181e-02  1.52423250e-01 -1.26441572e+00 -7.40969747e-01\n",
      "   4.38493319e-01]\n",
      " [ 4.29298969e-01 -2.61461177e-01  6.93673439e-01 -8.09222913e-01\n",
      "   3.63352999e-01 -4.41511455e-01 -7.90300713e-01  5.35797423e-01\n",
      "   5.57067411e-01]\n",
      " [ 8.88270217e-01 -1.02610338e+00  4.23807354e-01  9.54353688e-01\n",
      "  -1.66393152e-01 -1.06793153e-01  1.17720528e+00  2.16480172e-01\n",
      "  -6.90374506e-01]\n",
      " [-8.76187159e-01  1.02359658e+00 -4.18283122e-01 -8.20626267e-01\n",
      "   2.27258813e-01  1.05334338e-01 -1.18779512e+00 -6.18573981e-01\n",
      "   5.18269889e-01]\n",
      " [-5.54781880e-06  2.88087143e-01  4.89857310e-01 -9.65761434e-01\n",
      "   2.75932743e-02  4.93835951e-02 -1.05372705e+00  2.34914451e-01\n",
      "   2.47433738e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.41001029  1.36418719 -2.42839561 -0.07029812  1.46653997 -2.20807792\n",
      "  -0.6483333 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:58 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.14838586]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 58 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 7.84988313e-01 -8.79328037e-01  3.44924930e-01  9.44569967e-01\n",
      "  -1.63331494e-01 -2.11210395e-01  1.03968493e+00  5.32214684e-01\n",
      "  -3.74646287e-01]\n",
      " [-6.79731916e-01  1.06863562e+00 -5.52369557e-01 -1.08629437e+00\n",
      "   9.98940293e-02  1.52397161e-01 -1.26441572e+00 -7.40969747e-01\n",
      "   4.38467231e-01]\n",
      " [ 4.27555213e-01 -2.61461177e-01  6.93673439e-01 -8.09222913e-01\n",
      "   3.61609242e-01 -4.43255211e-01 -7.90300713e-01  5.35797423e-01\n",
      "   5.55323655e-01]\n",
      " [ 8.88446440e-01 -1.02610338e+00  4.23807354e-01  9.54353688e-01\n",
      "  -1.66216928e-01 -1.06616930e-01  1.17720528e+00  2.16480172e-01\n",
      "  -6.90198282e-01]\n",
      " [-8.76127811e-01  1.02359658e+00 -4.18283122e-01 -8.20626267e-01\n",
      "   2.27318161e-01  1.05393686e-01 -1.18779512e+00 -6.18573981e-01\n",
      "   5.18329236e-01]\n",
      " [-7.46259444e-04  2.88087143e-01  4.89857310e-01 -9.65761434e-01\n",
      "   2.68525626e-02  4.86428835e-02 -1.05372705e+00  2.34914451e-01\n",
      "   2.46693026e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.41938586  1.3594147  -2.43310949 -0.07697947  1.46202857 -2.21270636\n",
      "  -0.65377486]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:58 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.02785216]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 58 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 7.85055208e-01 -8.79261142e-01  3.44924930e-01  9.44569967e-01\n",
      "  -1.63264599e-01 -2.11143500e-01  1.03968493e+00  5.32214684e-01\n",
      "  -3.74579392e-01]\n",
      " [-6.79808970e-01  1.06855856e+00 -5.52369557e-01 -1.08629437e+00\n",
      "   9.98169754e-02  1.52320107e-01 -1.26441572e+00 -7.40969747e-01\n",
      "   4.38390177e-01]\n",
      " [ 4.27500677e-01 -2.61515713e-01  6.93673439e-01 -8.09222913e-01\n",
      "   3.61554707e-01 -4.43309747e-01 -7.90300713e-01  5.35797423e-01\n",
      "   5.55269119e-01]\n",
      " [ 8.88524179e-01 -1.02602564e+00  4.23807354e-01  9.54353688e-01\n",
      "  -1.66139190e-01 -1.06539191e-01  1.17720528e+00  2.16480172e-01\n",
      "  -6.90120544e-01]\n",
      " [-8.76201888e-01  1.02352250e+00 -4.18283122e-01 -8.20626267e-01\n",
      "   2.27244084e-01  1.05319609e-01 -1.18779512e+00 -6.18573981e-01\n",
      "   5.18255160e-01]\n",
      " [-7.98695019e-04  2.88034708e-01  4.89857310e-01 -9.65761434e-01\n",
      "   2.68001271e-02  4.85904479e-02 -1.05372705e+00  2.34914451e-01\n",
      "   2.46640591e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.41976293  1.35930128 -2.43339094 -0.07722633  1.46193445 -2.2129819\n",
      "  -0.65401913]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:58 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.40702446]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 58 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.77858467 -0.88573168  0.34492493  0.94456997 -0.16973514 -0.21761404\n",
      "   1.03321439  0.53221468 -0.37457939]\n",
      " [-0.67285074  1.07551679 -0.55236956 -1.08629437  0.10677521  0.15927834\n",
      "  -1.25745749 -0.74096975  0.43839018]\n",
      " [ 0.43517423 -0.25384216  0.69367344 -0.80922291  0.36922826 -0.43563619\n",
      "  -0.78262716  0.53579742  0.55526912]\n",
      " [ 0.88036535 -1.03418446  0.42380735  0.95435369 -0.17429802 -0.11469802\n",
      "   1.16904645  0.21648017 -0.69012054]\n",
      " [-0.86851314  1.03121125 -0.41828312 -0.82062627  0.23493283  0.11300836\n",
      "  -1.18010637 -0.61857398  0.51825516]\n",
      " [ 0.00675001  0.29558341  0.48985731 -0.96576143  0.03434883  0.05613915\n",
      "  -1.04617835  0.23491445  0.24664059]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.46888174  1.32791386 -2.45053239 -0.09345851  1.42839234 -2.22919409\n",
      "  -0.67041442]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:58 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.85870883]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 58 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.7799115  -0.88573168  0.34492493  0.9458968  -0.16973514 -0.21761404\n",
      "   1.03454122  0.53221468 -0.37457939]\n",
      " [-0.6740014   1.07551679 -0.55236956 -1.08744503  0.10677521  0.15927834\n",
      "  -1.25860815 -0.74096975  0.43839018]\n",
      " [ 0.43336964 -0.25384216  0.69367344 -0.8110275   0.36922826 -0.43563619\n",
      "  -0.78443175  0.53579742  0.55526912]\n",
      " [ 0.88152452 -1.03418446  0.42380735  0.95551285 -0.17429802 -0.11469802\n",
      "   1.17020562  0.21648017 -0.69012054]\n",
      " [-0.86976276  1.03121125 -0.41828312 -0.82187589  0.23493283  0.11300836\n",
      "  -1.18135599 -0.61857398  0.51825516]\n",
      " [ 0.0049526   0.29558341  0.48985731 -0.96755884  0.03434883  0.05613915\n",
      "  -1.04797576  0.23491445  0.24664059]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.46031045  1.3359732  -2.45013227 -0.09140764  1.43655858 -2.22873385\n",
      "  -0.66939736]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:58 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.70847592]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 58 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 7.86617607e-01 -8.85731682e-01  3.44924930e-01  9.45896795e-01\n",
      "  -1.63029028e-01 -2.10907929e-01  1.04124733e+00  5.32214684e-01\n",
      "  -3.74579392e-01]\n",
      " [-6.80706410e-01  1.07551679e+00 -5.52369557e-01 -1.08744503e+00\n",
      "   1.00070194e-01  1.52573326e-01 -1.26531316e+00 -7.40969747e-01\n",
      "   4.38390177e-01]\n",
      " [ 4.30360636e-01 -2.53842159e-01  6.93673439e-01 -8.11027503e-01\n",
      "   3.66219256e-01 -4.38645197e-01 -7.87440754e-01  5.35797423e-01\n",
      "   5.55269119e-01]\n",
      " [ 8.88158049e-01 -1.03418446e+00  4.23807354e-01  9.55512854e-01\n",
      "  -1.67664486e-01 -1.08064487e-01  1.17683915e+00  2.16480172e-01\n",
      "  -6.90120544e-01]\n",
      " [-8.76444846e-01  1.03121125e+00 -4.18283122e-01 -8.21875888e-01\n",
      "   2.28250747e-01  1.06326272e-01 -1.18803808e+00 -6.18573981e-01\n",
      "   5.18255160e-01]\n",
      " [-8.07989815e-04  2.95583411e-01  4.89857310e-01 -9.67558841e-01\n",
      "   2.85882394e-02  5.03785602e-02 -1.05373635e+00  2.34914451e-01\n",
      "   2.46640591e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.43020508  1.36025167 -2.44534901 -0.07945213  1.46225508 -2.2240961\n",
      "  -0.66101677]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:58 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57042063]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 58 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.7719528  -0.88573168  0.34492493  0.93123199 -0.16302903 -0.22557274\n",
      "   1.04124733  0.53221468 -0.3892442 ]\n",
      " [-0.66589173  1.07551679 -0.55236956 -1.07263034  0.10007019  0.16738801\n",
      "  -1.26531316 -0.74096975  0.45320486]\n",
      " [ 0.43489452 -0.25384216  0.69367344 -0.80649362  0.36621926 -0.43411131\n",
      "  -0.78744075  0.53579742  0.55980301]\n",
      " [ 0.87409668 -1.03418446  0.42380735  0.94145148 -0.16766449 -0.12212586\n",
      "   1.17683915  0.21648017 -0.70418192]\n",
      " [-0.86220011  1.03121125 -0.41828312 -0.80763115  0.22825075  0.12057101\n",
      "  -1.18803808 -0.61857398  0.5324999 ]\n",
      " [ 0.00969345  0.29558341  0.48985731 -0.95705741  0.02858824  0.06088\n",
      "  -1.05373635  0.23491445  0.25714203]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.50009328  1.3072088  -2.46181175 -0.10980947  1.4105443  -2.24189611\n",
      "  -0.68465262]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:58 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.72685552]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 58 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.77800618 -0.8796783   0.34492493  0.93728537 -0.16302903 -0.21951935\n",
      "   1.04730071  0.53221468 -0.3892442 ]\n",
      " [-0.67186776  1.06954076 -0.55236956 -1.07860638  0.10007019  0.16141198\n",
      "  -1.27128919 -0.74096975  0.45320486]\n",
      " [ 0.42900162 -0.25973506  0.69367344 -0.81238652  0.36621926 -0.44000421\n",
      "  -0.79333366  0.53579742  0.55980301]\n",
      " [ 0.88000138 -1.02827976  0.42380735  0.94735618 -0.16766449 -0.11622116\n",
      "   1.18274385  0.21648017 -0.70418192]\n",
      " [-0.86821652  1.02519484 -0.41828312 -0.81364756  0.22825075  0.1145546\n",
      "  -1.19405449 -0.61857398  0.5324999 ]\n",
      " [ 0.00364456  0.28953453  0.48985731 -0.96310629  0.02858824  0.05483111\n",
      "  -1.05978523  0.23491445  0.25714203]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.4729787   1.32989279 -2.45783475 -0.10611575  1.43393019 -2.23772936\n",
      "  -0.68026452]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:58 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.770175]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 58 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 7.82395834e-01 -8.75288648e-01  3.44924930e-01  9.41675022e-01\n",
      "  -1.63029028e-01 -2.19519355e-01  1.05169037e+00  5.32214684e-01\n",
      "  -3.89244199e-01]\n",
      " [-6.76190344e-01  1.06521818e+00 -5.52369557e-01 -1.08292896e+00\n",
      "   1.00070194e-01  1.61411975e-01 -1.27561178e+00 -7.40969747e-01\n",
      "   4.53204858e-01]\n",
      " [ 4.24467198e-01 -2.64269485e-01  6.93673439e-01 -8.16920942e-01\n",
      "   3.66219256e-01 -4.40004211e-01 -7.97868080e-01  5.35797423e-01\n",
      "   5.59803007e-01]\n",
      " [ 8.84292578e-01 -1.02398856e+00  4.23807354e-01  9.51647383e-01\n",
      "  -1.67664486e-01 -1.16221158e-01  1.18703505e+00  2.16480172e-01\n",
      "  -7.04181916e-01]\n",
      " [-8.72634023e-01  1.02077733e+00 -4.18283122e-01 -8.18065065e-01\n",
      "   2.28250747e-01  1.14554601e-01 -1.19847199e+00 -6.18573981e-01\n",
      "   5.32499899e-01]\n",
      " [-8.56581006e-04  2.85033384e-01  4.89857310e-01 -9.67607432e-01\n",
      "   2.85882394e-02  5.48311129e-02 -1.06428637e+00  2.34914451e-01\n",
      "   2.57142026e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.45263856  1.34754693 -2.45530624 -0.10220851  1.45180664 -2.23496741\n",
      "  -0.6772008 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:58 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.79433757]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 58 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.78595964 -0.87528865  0.34848874  0.94167502 -0.16302903 -0.21595555\n",
      "   1.05525417  0.53221468 -0.3892442 ]\n",
      " [-0.67933733  1.06521818 -0.55551654 -1.08292896  0.10007019  0.15826499\n",
      "  -1.27875876 -0.74096975  0.45320486]\n",
      " [ 0.42396615 -0.26426948  0.69317239 -0.81692094  0.36621926 -0.44050526\n",
      "  -0.79836913  0.53579742  0.55980301]\n",
      " [ 0.88739393 -1.02398856  0.42690871  0.95164738 -0.16766449 -0.1131198\n",
      "   1.19013641  0.21648017 -0.70418192]\n",
      " [-0.87574056  1.02077733 -0.42138966 -0.81806506  0.22825075  0.11144806\n",
      "  -1.20157853 -0.61857398  0.5324999 ]\n",
      " [-0.00290078  0.28503338  0.48781311 -0.96760743  0.02858824  0.05278691\n",
      "  -1.06633058  0.23491445  0.25714203]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.4358395   1.36227093 -2.45383392 -0.09431123  1.46718123 -2.23353761\n",
      "  -0.67093902]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:58 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.22242548]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 59 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 7.83848010e-01 -8.77400277e-01  3.48488736e-01  9.39563393e-01\n",
      "  -1.63029028e-01 -2.15955550e-01  1.05525417e+00  5.32214684e-01\n",
      "  -3.91355828e-01]\n",
      " [-6.78182037e-01  1.06637347e+00 -5.55516542e-01 -1.08177367e+00\n",
      "   1.00070194e-01  1.58264989e-01 -1.27875876e+00 -7.40969747e-01\n",
      "   4.54360151e-01]\n",
      " [ 4.24433503e-01 -2.63802132e-01  6.93172392e-01 -8.16453589e-01\n",
      "   3.66219256e-01 -4.40505258e-01 -7.98369127e-01  5.35797423e-01\n",
      "   5.60270359e-01]\n",
      " [ 8.86862431e-01 -1.02452007e+00  4.26908710e-01  9.51115881e-01\n",
      "  -1.67664486e-01 -1.13119803e-01  1.19013641e+00  2.16480172e-01\n",
      "  -7.04713418e-01]\n",
      " [-8.75068138e-01  1.02144976e+00 -4.21389661e-01 -8.17392641e-01\n",
      "   2.28250747e-01  1.11448063e-01 -1.20157853e+00 -6.18573981e-01\n",
      "   5.33172322e-01]\n",
      " [-9.32749179e-04  2.87001420e-01  4.87813106e-01 -9.65639396e-01\n",
      "   2.85882394e-02  5.27869090e-02 -1.06633058e+00  2.34914451e-01\n",
      "   2.59110062e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.45507401  1.35046576 -2.4622844  -0.10346039  1.45703138 -2.24248023\n",
      "  -0.67852751]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:59 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.80960686]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 59 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.78675939 -0.87740028  0.35140012  0.93956339 -0.16302903 -0.21595555\n",
      "   1.05816555  0.53221468 -0.39135583]\n",
      " [-0.6807394   1.06637347 -0.55807391 -1.08177367  0.10007019  0.15826499\n",
      "  -1.28131613 -0.74096975  0.45436015]\n",
      " [ 0.42557528 -0.26380213  0.69431417 -0.81645359  0.36621926 -0.44050526\n",
      "  -0.79722735  0.53579742  0.56027036]\n",
      " [ 0.88942967 -1.02452007  0.42947595  0.95111588 -0.16766449 -0.1131198\n",
      "   1.19270365  0.21648017 -0.70471342]\n",
      " [-0.87764215  1.02144976 -0.42396367 -0.81739264  0.22825075  0.11144806\n",
      "  -1.20415254 -0.61857398  0.53317232]\n",
      " [-0.00288955  0.28700142  0.4858563  -0.9656394   0.02858824  0.05278691\n",
      "  -1.06828738  0.23491445  0.25911006]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.44040007  1.36365953 -2.46118401 -0.09496215  1.4705962  -2.24136507\n",
      "  -0.67325869]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:59 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.6499872]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 59 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.77052489 -0.89363478  0.33516562  0.93956339 -0.16302903 -0.21595555\n",
      "   1.04193105  0.53221468 -0.39135583]\n",
      " [-0.66423559  1.08287728 -0.5415701  -1.08177367  0.10007019  0.15826499\n",
      "  -1.26481232 -0.74096975  0.45436015]\n",
      " [ 0.42448824 -0.26488918  0.69322712 -0.81645359  0.36621926 -0.44050526\n",
      "  -0.79831439  0.53579742  0.56027036]\n",
      " [ 0.8728962  -1.04105353  0.41294248  0.95111588 -0.16766449 -0.1131198\n",
      "   1.17617018  0.21648017 -0.70471342]\n",
      " [-0.86111062  1.03798128 -0.40743214 -0.81739264  0.22825075  0.11144806\n",
      "  -1.18762101 -0.61857398  0.53317232]\n",
      " [ 0.00250379  0.29239476  0.49124965 -0.9656394   0.02858824  0.05278691\n",
      "  -1.06289404  0.23491445  0.25911006]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.51433736  1.30532281 -2.47518952 -0.13301847  1.41028991 -2.25502702\n",
      "  -0.70475364]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:59 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.63161821]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 59 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.78008993 -0.89363478  0.34473065  0.93956339 -0.16302903 -0.21595555\n",
      "   1.04193105  0.54177972 -0.39135583]\n",
      " [-0.67335514  1.08287728 -0.55068964 -1.08177367  0.10007019  0.15826499\n",
      "  -1.26481232 -0.75008929  0.45436015]\n",
      " [ 0.43404298 -0.26488918  0.70278187 -0.81645359  0.36621926 -0.44050526\n",
      "  -0.79831439  0.54535217  0.56027036]\n",
      " [ 0.88248482 -1.04105353  0.4225311   0.95111588 -0.16766449 -0.1131198\n",
      "   1.17617018  0.22606879 -0.70471342]\n",
      " [-0.87035167  1.03798128 -0.41667319 -0.81739264  0.22825075  0.11144806\n",
      "  -1.18762101 -0.62781503  0.53317232]\n",
      " [ 0.00935985  0.29239476  0.49810571 -0.9656394   0.02858824  0.05278691\n",
      "  -1.06289404  0.24177051  0.25911006]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.47148044  1.34120475 -2.46983646 -0.09704586  1.44534345 -2.24938818\n",
      "  -0.6758461 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:59 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.58315912]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 59 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.79123259 -0.89363478  0.34473065  0.95070606 -0.16302903 -0.21595555\n",
      "   1.04193105  0.54177972 -0.38021316]\n",
      " [-0.68444144  1.08287728 -0.55068964 -1.09285997  0.10007019  0.15826499\n",
      "  -1.26481232 -0.75008929  0.44327385]\n",
      " [ 0.43627802 -0.26488918  0.70278187 -0.81421855  0.36621926 -0.44050526\n",
      "  -0.79831439  0.54535217  0.5625054 ]\n",
      " [ 0.89304572 -1.04105353  0.4225311   0.96167679 -0.16766449 -0.1131198\n",
      "   1.17617018  0.22606879 -0.69415251]\n",
      " [-0.88101065  1.03798128 -0.41667319 -0.82805163  0.22825075  0.11144806\n",
      "  -1.18762101 -0.62781503  0.52251334]\n",
      " [ 0.00152123  0.29239476  0.49810571 -0.97347802  0.02858824  0.05278691\n",
      "  -1.06289404  0.24177051  0.25127144]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.42081665  1.38125746 -2.4589926  -0.06946712  1.48362671 -2.23724635\n",
      "  -0.65900343]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:59 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.14312]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 59 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 7.91162329e-01 -8.93634775e-01  3.44730651e-01  9.50706061e-01\n",
      "  -1.63099294e-01 -2.16025815e-01  1.04193105e+00  5.41779718e-01\n",
      "  -3.80283425e-01]\n",
      " [-6.84479101e-01  1.08287728e+00 -5.50689641e-01 -1.09285997e+00\n",
      "   1.00032531e-01  1.58227327e-01 -1.26481232e+00 -7.50089289e-01\n",
      "   4.43236185e-01]\n",
      " [ 4.34628083e-01 -2.64889177e-01  7.02781867e-01 -8.14218550e-01\n",
      "   3.64569321e-01 -4.42155193e-01 -7.98314394e-01  5.45352166e-01\n",
      "   5.60855464e-01]\n",
      " [ 8.93225088e-01 -1.04105353e+00  4.22531096e-01  9.61676786e-01\n",
      "  -1.67485120e-01 -1.12940437e-01  1.17617018e+00  2.26068790e-01\n",
      "  -6.93973147e-01]\n",
      " [-8.80969415e-01  1.03798128e+00 -4.16673192e-01 -8.28051627e-01\n",
      "   2.28291987e-01  1.11489302e-01 -1.18762101e+00 -6.27815033e-01\n",
      "   5.22554577e-01]\n",
      " [ 8.08168217e-04  2.92394762e-01  4.98105707e-01 -9.73478018e-01\n",
      "   2.78751777e-02  5.20738473e-02 -1.06289404e+00  2.41770511e-01\n",
      "   2.50558379e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.42959253  1.37679924 -2.4634182  -0.07575028  1.47941833 -2.24159305\n",
      "  -0.66411777]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:59 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.02570581]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 59 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 7.91220250e-01 -8.93576854e-01  3.44730651e-01  9.50706061e-01\n",
      "  -1.63041373e-01 -2.15967894e-01  1.04193105e+00  5.41779718e-01\n",
      "  -3.80225504e-01]\n",
      " [-6.84545444e-01  1.08281094e+00 -5.50689641e-01 -1.09285997e+00\n",
      "   9.99661879e-02  1.58160984e-01 -1.26481232e+00 -7.50089289e-01\n",
      "   4.43169842e-01]\n",
      " [ 4.34580759e-01 -2.64936501e-01  7.02781867e-01 -8.14218550e-01\n",
      "   3.64521997e-01 -4.42202518e-01 -7.98314394e-01  5.45352166e-01\n",
      "   5.60808140e-01]\n",
      " [ 8.93292020e-01 -1.04098660e+00  4.22531096e-01  9.61676786e-01\n",
      "  -1.67418188e-01 -1.12873506e-01  1.17617018e+00  2.26068790e-01\n",
      "  -6.93906215e-01]\n",
      " [-8.81033350e-01  1.03791735e+00 -4.16673192e-01 -8.28051627e-01\n",
      "   2.28228052e-01  1.11425367e-01 -1.18762101e+00 -6.27815033e-01\n",
      "   5.22490641e-01]\n",
      " [ 7.62556497e-04  2.92349150e-01  4.98105707e-01 -9.73478018e-01\n",
      "   2.78295660e-02  5.20282356e-02 -1.06289404e+00  2.41770511e-01\n",
      "   2.50512767e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.42991443  1.37670365 -2.46365971 -0.07596199  1.47933927 -2.24182959\n",
      "  -0.66432735]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:59 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.39820694]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 59 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.78503267 -0.89976444  0.34473065  0.95070606 -0.16922896 -0.22215548\n",
      "   1.03574347  0.54177972 -0.3802255 ]\n",
      " [-0.67792035  1.08943603 -0.55068964 -1.09285997  0.10659128  0.16478608\n",
      "  -1.25818723 -0.75008929  0.44316984]\n",
      " [ 0.44203691 -0.25748035  0.70278187 -0.81421855  0.37197814 -0.43474637\n",
      "  -0.79085825  0.54535217  0.56080814]\n",
      " [ 0.88550856 -1.04877006  0.4225311   0.96167679 -0.17520165 -0.12065697\n",
      "   1.16838672  0.22606879 -0.69390622]\n",
      " [-0.87370086  1.04524984 -0.41667319 -0.82805163  0.23556054  0.11875785\n",
      "  -1.18028852 -0.62781503  0.52249064]\n",
      " [ 0.00808556  0.29967216  0.49810571 -0.97347802  0.03515257  0.05935124\n",
      "  -1.05557103  0.24177051  0.25051277]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.47762722  1.34633049 -2.48047464 -0.09172657  1.44695265 -2.25775575\n",
      "  -0.6802658 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:59 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.86213228]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 59 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.78629109 -0.89976444  0.34473065  0.95196449 -0.16922896 -0.22215548\n",
      "   1.03700189  0.54177972 -0.3802255 ]\n",
      " [-0.67901248  1.08943603 -0.55068964 -1.0939521   0.10659128  0.16478608\n",
      "  -1.25927936 -0.75008929  0.44316984]\n",
      " [ 0.44030812 -0.25748035  0.70278187 -0.81594733  0.37197814 -0.43474637\n",
      "  -0.79258703  0.54535217  0.56080814]\n",
      " [ 0.88660915 -1.04877006  0.4225311   0.96277738 -0.17520165 -0.12065697\n",
      "   1.16948731  0.22606879 -0.69390622]\n",
      " [-0.8748871   1.04524984 -0.41667319 -0.82923786  0.23556054  0.11875785\n",
      "  -1.18147476 -0.62781503  0.52249064]\n",
      " [ 0.00637462  0.29967216  0.49810571 -0.97518896  0.03515257  0.05935124\n",
      "  -1.05728198  0.24177051  0.25051277]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.46943373  1.35404151 -2.48009664 -0.08977557  1.45476328 -2.25732109\n",
      "  -0.679307  ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:59 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.71064627]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 59 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 7.92920697e-01 -8.99764438e-01  3.44730651e-01  9.51964486e-01\n",
      "  -1.62599351e-01 -2.15525872e-01  1.04363150e+00  5.41779718e-01\n",
      "  -3.80225504e-01]\n",
      " [-6.85638072e-01  1.08943603e+00 -5.50689641e-01 -1.09395210e+00\n",
      "   9.99656898e-02  1.58160486e-01 -1.26590495e+00 -7.50089289e-01\n",
      "   4.43169842e-01]\n",
      " [ 4.37350463e-01 -2.57480354e-01  7.02781867e-01 -8.15947332e-01\n",
      "   3.69020483e-01 -4.37704032e-01 -7.95544691e-01  5.45352166e-01\n",
      "   5.60808140e-01]\n",
      " [ 8.93166576e-01 -1.04877006e+00  4.22531096e-01  9.62777379e-01\n",
      "  -1.68644226e-01 -1.14099543e-01  1.17604473e+00  2.26068790e-01\n",
      "  -6.93906215e-01]\n",
      " [-8.81491006e-01  1.04524984e+00 -4.16673192e-01 -8.29237860e-01\n",
      "   2.28956629e-01  1.12153944e-01 -1.18807867e+00 -6.27815033e-01\n",
      "   5.22490641e-01]\n",
      " [ 6.68784920e-04  2.99672157e-01  4.98105707e-01 -9.75188965e-01\n",
      "   2.94467415e-02  5.36454111e-02 -1.06298781e+00  2.41770511e-01\n",
      "   2.50512767e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.43968409  1.37805537 -2.47537132 -0.07794407  1.48014684 -2.25273374\n",
      "  -0.67104854]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:59 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57104907]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 59 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.77823415 -0.89976444  0.34473065  0.93727794 -0.16259935 -0.23021242\n",
      "   1.0436315   0.54177972 -0.39491205]\n",
      " [-0.67080736  1.08943603 -0.55068964 -1.07912139  0.09996569  0.17299119\n",
      "  -1.26590495 -0.75008929  0.45800055]\n",
      " [ 0.44174563 -0.25748035  0.70278187 -0.81155217  0.36902048 -0.43330887\n",
      "  -0.79554469  0.54535217  0.5652033 ]\n",
      " [ 0.87907842 -1.04877006  0.4225311   0.94868922 -0.16864423 -0.1281877\n",
      "   1.17604473  0.22606879 -0.70799438]\n",
      " [-0.86722106  1.04524984 -0.41667319 -0.81496791  0.22895663  0.12642389\n",
      "  -1.18807867 -0.62781503  0.53676059]\n",
      " [ 0.01116589  0.29967216  0.49810571 -0.96469186  0.02944674  0.06414252\n",
      "  -1.06298781  0.24177051  0.26100987]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.50962391  1.32494638 -2.49183272 -0.10847084  1.42836489 -2.27051578\n",
      "  -0.69471728]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:59 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.72773764]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 59 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.78425722 -0.89374137  0.34473065  0.94330101 -0.16259935 -0.22418935\n",
      "   1.04965457  0.54177972 -0.39491205]\n",
      " [-0.67675818  1.08348521 -0.55068964 -1.08507221  0.09996569  0.16704038\n",
      "  -1.27185576 -0.75008929  0.45800055]\n",
      " [ 0.43589372 -0.26333226  0.70278187 -0.81740408  0.36902048 -0.43916078\n",
      "  -0.8013966   0.54535217  0.5652033 ]\n",
      " [ 0.8849618  -1.04288668  0.4225311   0.95457261 -0.16864423 -0.12230432\n",
      "   1.18192812  0.22606879 -0.70799438]\n",
      " [-0.8732106   1.03926029 -0.41667319 -0.82095745  0.22895663  0.12043435\n",
      "  -1.19406821 -0.62781503  0.53676059]\n",
      " [ 0.00515216  0.29365842  0.49810571 -0.97070559  0.02944674  0.05812878\n",
      "  -1.06900154  0.24177051  0.26100987]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.48265148  1.34749683 -2.48785111 -0.10482542  1.45159858 -2.26634452\n",
      "  -0.69038161]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:59 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.77228434]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 59 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 7.88577972e-01 -8.89420617e-01  3.44730651e-01  9.47621762e-01\n",
      "  -1.62599351e-01 -2.24189351e-01  1.05397532e+00  5.41779718e-01\n",
      "  -3.94912050e-01]\n",
      " [-6.81015455e-01  1.07922794e+00 -5.50689641e-01 -1.08932948e+00\n",
      "   9.99656898e-02  1.67040380e-01 -1.27611304e+00 -7.50089289e-01\n",
      "   4.58000550e-01]\n",
      " [ 4.31426614e-01 -2.67799366e-01  7.02781867e-01 -8.21871181e-01\n",
      "   3.69020483e-01 -4.39160775e-01 -8.05863702e-01  5.45352166e-01\n",
      "   5.65203303e-01]\n",
      " [ 8.89189594e-01 -1.03865889e+00  4.22531096e-01  9.58800397e-01\n",
      "  -1.68644226e-01 -1.22304317e-01  1.18615591e+00  2.26068790e-01\n",
      "  -7.07994376e-01]\n",
      " [-8.77560694e-01  1.03491020e+00 -4.16673192e-01 -8.25307548e-01\n",
      "   2.28956629e-01  1.20434350e-01 -1.19841830e+00 -6.27815033e-01\n",
      "   5.36760591e-01]\n",
      " [ 7.27292728e-04  2.89233558e-01  4.98105707e-01 -9.75130457e-01\n",
      "   2.94467415e-02  5.81287847e-02 -1.07342641e+00  2.41770511e-01\n",
      "   2.61009874e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.4626283   1.36487714 -2.48535759 -0.10100938  1.46918953 -2.26362151\n",
      "  -0.68739417]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:59 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.79616587]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 59 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 7.92083677e-01 -8.89420617e-01  3.48236357e-01  9.47621762e-01\n",
      "  -1.62599351e-01 -2.20683646e-01  1.05748103e+00  5.41779718e-01\n",
      "  -3.94912050e-01]\n",
      " [-6.84116295e-01  1.07922794e+00 -5.53790480e-01 -1.08932948e+00\n",
      "   9.99656898e-02  1.63939540e-01 -1.27921388e+00 -7.50089289e-01\n",
      "   4.58000550e-01]\n",
      " [ 4.30969802e-01 -2.67799366e-01  7.02325055e-01 -8.21871181e-01\n",
      "   3.69020483e-01 -4.39617587e-01 -8.06320514e-01  5.45352166e-01\n",
      "   5.65203303e-01]\n",
      " [ 8.92247247e-01 -1.03865889e+00  4.25588748e-01  9.58800397e-01\n",
      "  -1.68644226e-01 -1.19246664e-01  1.18921356e+00  2.26068790e-01\n",
      "  -7.07994376e-01]\n",
      " [-8.80622538e-01  1.03491020e+00 -4.19735036e-01 -8.25307548e-01\n",
      "   2.28956629e-01  1.17372505e-01 -1.20148015e+00 -6.27815033e-01\n",
      "   5.36760591e-01]\n",
      " [-1.27194187e-03  2.89233558e-01  4.96106472e-01 -9.75130457e-01\n",
      "   2.94467415e-02  5.61295501e-02 -1.07542564e+00  2.41770511e-01\n",
      "   2.61009874e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.44608861  1.37938025 -2.48390538 -0.09319728  1.48432244 -2.26221041\n",
      "  -0.68121363]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:59 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.21654349]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 60 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 7.90097309e-01 -8.91406985e-01  3.48236357e-01  9.45635393e-01\n",
      "  -1.62599351e-01 -2.20683646e-01  1.05748103e+00  5.41779718e-01\n",
      "  -3.96898418e-01]\n",
      " [-6.83046544e-01  1.08029769e+00 -5.53790480e-01 -1.08825973e+00\n",
      "   9.99656898e-02  1.63939540e-01 -1.27921388e+00 -7.50089289e-01\n",
      "   4.59070301e-01]\n",
      " [ 4.31398218e-01 -2.67370949e-01  7.02325055e-01 -8.21442764e-01\n",
      "   3.69020483e-01 -4.39617587e-01 -8.06320514e-01  5.45352166e-01\n",
      "   5.65631719e-01]\n",
      " [ 8.91769157e-01 -1.03913698e+00  4.25588748e-01  9.58322308e-01\n",
      "  -1.68644226e-01 -1.19246664e-01  1.18921356e+00  2.26068790e-01\n",
      "  -7.08472466e-01]\n",
      " [-8.80008771e-01  1.03552396e+00 -4.19735036e-01 -8.24693780e-01\n",
      "   2.28956629e-01  1.17372505e-01 -1.20148015e+00 -6.27815033e-01\n",
      "   5.37374359e-01]\n",
      " [ 5.98809197e-04  2.91104309e-01  4.96106472e-01 -9.73259706e-01\n",
      "   2.94467415e-02  5.61295501e-02 -1.07542564e+00  2.41770511e-01\n",
      "   2.62880625e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.46445717  1.36814024 -2.49200993 -0.10195252  1.4746592  -2.27077908\n",
      "  -0.68847002]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:60 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.8123398]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 60 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.79292727 -0.89140699  0.35106631  0.94563539 -0.16259935 -0.22068365\n",
      "   1.06031098  0.54177972 -0.39689842]\n",
      " [-0.68553535  1.08029769 -0.55627929 -1.08825973  0.09996569  0.16393954\n",
      "  -1.28170269 -0.75008929  0.4590703 ]\n",
      " [ 0.43253817 -0.26737095  0.70346501 -0.82144276  0.36902048 -0.43961759\n",
      "  -0.80518056  0.54535217  0.56563172]\n",
      " [ 0.89426864 -1.03913698  0.42808824  0.95832231 -0.16864423 -0.11924666\n",
      "   1.19171305  0.22606879 -0.70847247]\n",
      " [-0.88251427  1.03552396 -0.42224054 -0.82469378  0.22895663  0.11737251\n",
      "  -1.20398565 -0.62781503  0.53737436]\n",
      " [-0.00130663  0.29110431  0.49420104 -0.97325971  0.02944674  0.05612955\n",
      "  -1.07733108  0.24177051  0.26288063]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.45015335  1.38101186 -2.49094086 -0.09364018  1.48788453 -2.26969524\n",
      "  -0.6833317 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:60 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.64785689]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 60 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.77671873 -0.90761552  0.33485778  0.94563539 -0.16259935 -0.22068365\n",
      "   1.04410245  0.54177972 -0.39689842]\n",
      " [-0.66905207  1.09678097 -0.539796   -1.08825973  0.09996569  0.16393954\n",
      "  -1.2652194  -0.75008929  0.4590703 ]\n",
      " [ 0.43136708 -0.26854205  0.70229391 -0.82144276  0.36902048 -0.43961759\n",
      "  -0.80635166  0.54535217  0.56563172]\n",
      " [ 0.87775259 -1.05565303  0.41157219  0.95832231 -0.16864423 -0.11924666\n",
      "   1.175197    0.22606879 -0.70847247]\n",
      " [-0.86599967  1.05203857 -0.40572594 -0.82469378  0.22895663  0.11737251\n",
      "  -1.18747105 -0.62781503  0.53737436]\n",
      " [ 0.0039978   0.29640874  0.49950547 -0.97325971  0.02944674  0.05612955\n",
      "  -1.07202665  0.24177051  0.26288063]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.52405385  1.32277845 -2.50505935 -0.13176231  1.42774386 -2.2834743\n",
      "  -0.71490112]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:60 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.6350882]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 60 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.78614593 -0.90761552  0.34428498  0.94563539 -0.16259935 -0.22068365\n",
      "   1.04410245  0.55120692 -0.39689842]\n",
      " [-0.67802361  1.09678097 -0.54876755 -1.08825973  0.09996569  0.16393954\n",
      "  -1.2652194  -0.75906083  0.4590703 ]\n",
      " [ 0.44077448 -0.26854205  0.71170132 -0.82144276  0.36902048 -0.43961759\n",
      "  -0.80635166  0.55475957  0.56563172]\n",
      " [ 0.8872162  -1.05565303  0.42103579  0.95832231 -0.16864423 -0.11924666\n",
      "   1.175197    0.2355324  -0.70847247]\n",
      " [-0.8750935   1.05203857 -0.41481976 -0.82469378  0.22895663  0.11737251\n",
      "  -1.18747105 -0.63690886  0.53737436]\n",
      " [ 0.01087603  0.29640874  0.50638369 -0.97325971  0.02944674  0.05612955\n",
      "  -1.07202665  0.24864874  0.26288063]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.48176948  1.35826958 -2.49983376 -0.09612615  1.46241109 -2.27797055\n",
      "  -0.68622606]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:60 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.58551497]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 60 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.79721982 -0.90761552  0.34428498  0.95670928 -0.16259935 -0.22068365\n",
      "   1.04410245  0.55120692 -0.38582453]\n",
      " [-0.689043    1.09678097 -0.54876755 -1.09927912  0.09996569  0.16393954\n",
      "  -1.2652194  -0.75906083  0.44805092]\n",
      " [ 0.44308039 -0.26854205  0.71170132 -0.81913685  0.36902048 -0.43961759\n",
      "  -0.80635166  0.55475957  0.56793763]\n",
      " [ 0.89773202 -1.05565303  0.42103579  0.96883812 -0.16864423 -0.11924666\n",
      "   1.175197    0.2355324  -0.69795665]\n",
      " [-0.88570325  1.05203857 -0.41481976 -0.83530354  0.22895663  0.11737251\n",
      "  -1.18747105 -0.63690886  0.5267646 ]\n",
      " [ 0.00307452  0.29640874  0.50638369 -0.98106122  0.02944674  0.05612955\n",
      "  -1.07202665  0.24864874  0.25507911]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.43147437  1.39808556 -2.48912323 -0.06865952  1.50049156 -2.26598879\n",
      "  -0.66953199]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:60 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.13809734]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 60 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.79716207 -0.90761552  0.34428498  0.95670928 -0.1626571  -0.2207414\n",
      "   1.04410245  0.55120692 -0.38588228]\n",
      " [-0.68909007  1.09678097 -0.54876755 -1.09927912  0.09991862  0.16389247\n",
      "  -1.2652194  -0.75906083  0.44800384]\n",
      " [ 0.44151947 -0.26854205  0.71170132 -0.81913685  0.36745956 -0.44117851\n",
      "  -0.80635166  0.55475957  0.5663767 ]\n",
      " [ 0.89791271 -1.05565303  0.42103579  0.96883812 -0.16846353 -0.11906597\n",
      "   1.175197    0.2355324  -0.69777596]\n",
      " [-0.88567735  1.05203857 -0.41481976 -0.83530354  0.22898254  0.11739841\n",
      "  -1.18747105 -0.63690886  0.52679051]\n",
      " [ 0.00238873  0.29640874  0.50638369 -0.98106122  0.02876095  0.05544376\n",
      "  -1.07202665  0.24864874  0.25439332]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.43969299  1.3939185  -2.49327962 -0.07457023  1.49656318 -2.27007219\n",
      "  -0.67434067]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:60 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.02374763]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 60 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.79721226 -0.90756533  0.34428498  0.95670928 -0.16260691 -0.22069121\n",
      "   1.04410245  0.55120692 -0.3858321 ]\n",
      " [-0.68914725  1.09672379 -0.54876755 -1.09927912  0.09986144  0.16383529\n",
      "  -1.2652194  -0.75906083  0.44794667]\n",
      " [ 0.44147838 -0.26858313  0.71170132 -0.81913685  0.36741847 -0.4412196\n",
      "  -0.80635166  0.55475957  0.56633562]\n",
      " [ 0.89797039 -1.05559535  0.42103579  0.96883812 -0.16840585 -0.11900829\n",
      "   1.175197    0.2355324  -0.69771828]\n",
      " [-0.88573257  1.05198334 -0.41481976 -0.83530354  0.22892731  0.11734319\n",
      "  -1.18747105 -0.63690886  0.52673528]\n",
      " [ 0.00234903  0.29636904  0.50638369 -0.98106122  0.02872126  0.05540407\n",
      "  -1.07202665  0.24864874  0.25435363]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.43996827  1.39383777 -2.49348715 -0.07475205  1.49649662 -2.27027555\n",
      "  -0.67452075]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:60 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.38956579]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 60 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.79129747 -0.91348012  0.34428498  0.95670928 -0.1685217  -0.22660599\n",
      "   1.03818766  0.55120692 -0.3858321 ]\n",
      " [-0.68284195  1.1030291  -0.54876755 -1.09927912  0.10616674  0.17014059\n",
      "  -1.2589141  -0.75906083  0.44794667]\n",
      " [ 0.44872397 -0.26133755  0.71170132 -0.81913685  0.37466406 -0.43397401\n",
      "  -0.79910607  0.55475957  0.56633562]\n",
      " [ 0.890549   -1.06301674  0.42103579  0.96883812 -0.17582724 -0.12642968\n",
      "   1.16777561  0.2355324  -0.69771828]\n",
      " [-0.8787433   1.05897261 -0.41481976 -0.83530354  0.23591658  0.12433245\n",
      "  -1.18048178 -0.63690886  0.52673528]\n",
      " [ 0.00945241  0.30347242  0.50638369 -0.98106122  0.03582464  0.06250744\n",
      "  -1.06492327  0.24864874  0.25435363]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.48628848  1.36445963 -2.50996463 -0.09004722  1.4652379  -2.2859035\n",
      "  -0.69000164]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:60 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.86543333]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 60 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.79249172 -0.91348012  0.34428498  0.95790352 -0.1685217  -0.22660599\n",
      "   1.03938191  0.55120692 -0.3858321 ]\n",
      " [-0.68387912  1.1030291  -0.54876755 -1.10031629  0.10616674  0.17014059\n",
      "  -1.25995127 -0.75906083  0.44794667]\n",
      " [ 0.44706711 -0.26133755  0.71170132 -0.82079371  0.37466406 -0.43397401\n",
      "  -0.80076293  0.55475957  0.56633562]\n",
      " [ 0.89159453 -1.06301674  0.42103579  0.96988365 -0.17582724 -0.12642968\n",
      "   1.16882114  0.2355324  -0.69771828]\n",
      " [-0.87987     1.05897261 -0.41481976 -0.83643023  0.23591658  0.12433245\n",
      "  -1.18160847 -0.63690886  0.52673528]\n",
      " [ 0.00782315  0.30347242  0.50638369 -0.98269047  0.03582464  0.06250744\n",
      "  -1.06655253  0.24864874  0.25435363]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.47845276  1.37184029 -2.50960728 -0.08819061  1.4727115  -2.28549272\n",
      "  -0.68909724]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:60 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.71283345]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 60 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.79904417 -0.91348012  0.34428498  0.95790352 -0.16196924 -0.22005354\n",
      "   1.04593436  0.55120692 -0.3858321 ]\n",
      " [-0.69042468  1.1030291  -0.54876755 -1.10031629  0.09962118  0.16359503\n",
      "  -1.26649683 -0.75906083  0.44794667]\n",
      " [ 0.44415819 -0.26133755  0.71170132 -0.82079371  0.37175514 -0.43688293\n",
      "  -0.80367185  0.55475957  0.56633562]\n",
      " [ 0.89807493 -1.06301674  0.42103579  0.96988365 -0.16934684 -0.11994928\n",
      "   1.17530154  0.2355324  -0.69771828]\n",
      " [-0.88639502  1.05897261 -0.41481976 -0.83643023  0.22939156  0.11780743\n",
      "  -1.18813349 -0.63690886  0.52673528]\n",
      " [ 0.00217249  0.30347242  0.50638369 -0.98269047  0.03017397  0.05685678\n",
      "  -1.07220319  0.24864874  0.25435363]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.44906099  1.39558738 -2.5049412  -0.07648706  1.49778206 -2.28095743\n",
      "  -0.68096164]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:60 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57162789]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 60 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.78433776 -0.91348012  0.34428498  0.94319711 -0.16196924 -0.23475995\n",
      "   1.04593436  0.55120692 -0.4005385 ]\n",
      " [-0.6755795   1.1030291  -0.54876755 -1.08547111  0.09962118  0.17844021\n",
      "  -1.26649683 -0.75906083  0.46279185]\n",
      " [ 0.44841769 -0.26133755  0.71170132 -0.8165342   0.37175514 -0.43262343\n",
      "  -0.80367185  0.55475957  0.57059512]\n",
      " [ 0.88396158 -1.06301674  0.42103579  0.9557703  -0.16934684 -0.13406263\n",
      "   1.17530154  0.2355324  -0.71183162]\n",
      " [-0.87210171  1.05897261 -0.41481976 -0.82213693  0.22939156  0.13210074\n",
      "  -1.18813349 -0.63690886  0.54102859]\n",
      " [ 0.01266374  0.30347242  0.50638369 -0.97219922  0.03017397  0.06734803\n",
      "  -1.07220319  0.24864874  0.26484488]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.51904809  1.34241784 -2.5214019  -0.10717759  1.44593346 -2.29872233\n",
      "  -0.70466291]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:60 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.72858256]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 60 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.7903318  -0.90748608  0.34428498  0.94919115 -0.16196924 -0.22876591\n",
      "   1.0519284   0.55120692 -0.4005385 ]\n",
      " [-0.68150604  1.09710255 -0.54876755 -1.09139765  0.09962118  0.17251367\n",
      "  -1.27242338 -0.75906083  0.46279185]\n",
      " [ 0.44260611 -0.26714913  0.71170132 -0.82234579  0.37175514 -0.43843501\n",
      "  -0.80948343  0.55475957  0.57059512]\n",
      " [ 0.88982432 -1.057154    0.42103579  0.96163304 -0.16934684 -0.12819989\n",
      "   1.18116428  0.2355324  -0.71183162]\n",
      " [-0.87806539  1.05300893 -0.41481976 -0.8281006   0.22939156  0.12613706\n",
      "  -1.19409717 -0.63690886  0.54102859]\n",
      " [ 0.00668407  0.29749275  0.50638369 -0.97817889  0.03017397  0.06136836\n",
      "  -1.07818286  0.24864874  0.26484488]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.49221169  1.36484006 -2.51741555 -0.10358059  1.46902136 -2.29454645\n",
      "  -0.70037878]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:60 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.77434795]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 60 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.7945855  -0.90323238  0.34428498  0.95344485 -0.16196924 -0.22876591\n",
      "   1.0561821   0.55120692 -0.4005385 ]\n",
      " [-0.68569965  1.09290894 -0.54876755 -1.09559126  0.09962118  0.17251367\n",
      "  -1.27661699 -0.75906083  0.46279185]\n",
      " [ 0.43820482 -0.27155042  0.71170132 -0.82674708  0.37175514 -0.43843501\n",
      "  -0.81388472  0.55475957  0.57059512]\n",
      " [ 0.89399017 -1.05298815  0.42103579  0.96579889 -0.16934684 -0.12819989\n",
      "   1.18533013  0.2355324  -0.71183162]\n",
      " [-0.88234979  1.04872453 -0.41481976 -0.832385    0.22939156  0.12613706\n",
      "  -1.19838157 -0.63690886  0.54102859]\n",
      " [ 0.00233384  0.29314253  0.50638369 -0.98252912  0.03017397  0.06136836\n",
      "  -1.08253309  0.24864874  0.26484488]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.47249724  1.3819535  -2.51495623 -0.09985411  1.48633454 -2.29186143\n",
      "  -0.69746553]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:60 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: [1.] Net Result: [[0.79791832]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 60 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 7.98036073e-01 -9.03232379e-01  3.47735549e-01  9.53444855e-01\n",
      "  -1.61969241e-01 -2.25315342e-01  1.05963267e+00  5.51206923e-01\n",
      "  -4.00538504e-01]\n",
      " [-6.88756785e-01  1.09290894e+00 -5.51824684e-01 -1.09559126e+00\n",
      "   9.96211810e-02  1.69456532e-01 -1.27967412e+00 -7.59060833e-01\n",
      "   4.62791848e-01]\n",
      " [ 4.37788776e-01 -2.71550417e-01  7.11285272e-01 -8.26747076e-01\n",
      "   3.71755135e-01 -4.38851054e-01 -8.14300767e-01  5.54759572e-01\n",
      "   5.70595124e-01]\n",
      " [ 8.97006294e-01 -1.05298815e+00  4.24051916e-01  9.65798891e-01\n",
      "  -1.69346840e-01 -1.25183771e-01  1.18834625e+00  2.35532398e-01\n",
      "  -7.11831625e-01]\n",
      " [-8.85369283e-01  1.04872453e+00 -4.17839256e-01 -8.32385001e-01\n",
      "   2.29391557e-01  1.23117568e-01 -1.20140106e+00 -6.36908860e-01\n",
      "   5.41028588e-01]\n",
      " [ 3.77854206e-04  2.93142525e-01  5.04427702e-01 -9.82529116e-01\n",
      "   3.01739668e-02  5.94123739e-02 -1.08448908e+00  2.48648737e-01\n",
      "   2.64844881e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.45620494  1.39624529 -2.51352286 -0.09212474  1.50123677 -2.29046787\n",
      "  -0.69136211]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:60 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.21078748]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 61 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.7961688  -0.90509965  0.34773555  0.95157759 -0.16196924 -0.22531534\n",
      "   1.05963267  0.55120692 -0.40240577]\n",
      " [-0.68776756  1.09389817 -0.55182468 -1.09460203  0.09962118  0.16945653\n",
      "  -1.27967412 -0.75906083  0.46378107]\n",
      " [ 0.43818209 -0.2711571   0.71128527 -0.82635376  0.37175514 -0.43885105\n",
      "  -0.81430077  0.55475957  0.57098844]\n",
      " [ 0.89657783 -1.05341661  0.42405192  0.96537043 -0.16934684 -0.12518377\n",
      "   1.18834625  0.2355324  -0.71226009]\n",
      " [-0.88481052  1.0492833  -0.41783926 -0.83182624  0.22939156  0.12311757\n",
      "  -1.20140106 -0.63690886  0.54158735]\n",
      " [ 0.00215588  0.29492055  0.5044277  -0.98075109  0.03017397  0.05941237\n",
      "  -1.08448908  0.24864874  0.26662291]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.47373783  1.38554857 -2.52129143 -0.10049734  1.49204117 -2.29867402\n",
      "  -0.69829674]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:61 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.814974]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 61 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 7.98921431e-01 -9.05099648e-01  3.50488176e-01  9.51577586e-01\n",
      "  -1.61969241e-01 -2.25315342e-01  1.06238530e+00  5.51206923e-01\n",
      "  -4.02405773e-01]\n",
      " [-6.90191294e-01  1.09389817e+00 -5.54248420e-01 -1.09460203e+00\n",
      "   9.96211810e-02  1.69456532e-01 -1.28209786e+00 -7.59060833e-01\n",
      "   4.63781075e-01]\n",
      " [ 4.39318776e-01 -2.71157101e-01  7.12421956e-01 -8.26353760e-01\n",
      "   3.71755135e-01 -4.38851054e-01 -8.13164083e-01  5.54759572e-01\n",
      "   5.70988440e-01]\n",
      " [ 8.99012879e-01 -1.05341661e+00  4.26486965e-01  9.65370428e-01\n",
      "  -1.69346840e-01 -1.25183771e-01  1.19078130e+00  2.35532398e-01\n",
      "  -7.12260088e-01]\n",
      " [-8.87250971e-01  1.04928330e+00 -4.20279710e-01 -8.31826235e-01\n",
      "   2.29391557e-01  1.23117568e-01 -1.20384152e+00 -6.36908860e-01\n",
      "   5.41587354e-01]\n",
      " [ 2.99747377e-04  2.94920551e-01  5.02571570e-01 -9.80751091e-01\n",
      "   3.01739668e-02  5.94123739e-02 -1.08634521e+00  2.48648737e-01\n",
      "   2.66622907e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.45978767  1.39811162 -2.52025189 -0.09236417  1.50494185 -2.29761974\n",
      "  -0.69328284]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:61 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.6456133]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 61 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.78274134 -0.92127974  0.33430808  0.95157759 -0.16196924 -0.22531534\n",
      "   1.0462052   0.55120692 -0.40240577]\n",
      " [-0.67373148  1.11035798 -0.53778861 -1.09460203  0.09962118  0.16945653\n",
      "  -1.26563805 -0.75906083  0.46378107]\n",
      " [ 0.43807534 -0.27240054  0.71117852 -0.82635376  0.37175514 -0.43885105\n",
      "  -0.81440752  0.55475957  0.57098844]\n",
      " [ 0.88251748 -1.06991201  0.40999156  0.96537043 -0.16934684 -0.12518377\n",
      "   1.1742859   0.2355324  -0.71226009]\n",
      " [-0.87075634  1.06577793 -0.40378508 -0.83182624  0.22939156  0.12311757\n",
      "  -1.18734688 -0.63690886  0.54158735]\n",
      " [ 0.00551829  0.3001391   0.50779012 -0.98075109  0.03017397  0.05941237\n",
      "  -1.08112667  0.24864874  0.26662291]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.53364479  1.33998905 -2.53448325 -0.13053711  1.44497205 -2.31151582\n",
      "  -0.72492014]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:61 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.63844732]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 61 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.79203384 -0.92127974  0.34360059  0.95157759 -0.16196924 -0.22531534\n",
      "   1.0462052   0.56049943 -0.40240577]\n",
      " [-0.68255996  1.11035798 -0.54661709 -1.09460203  0.09962118  0.16945653\n",
      "  -1.26563805 -0.76788932  0.46378107]\n",
      " [ 0.44733655 -0.27240054  0.72043973 -0.82635376  0.37175514 -0.43885105\n",
      "  -0.81440752  0.56402079  0.57098844]\n",
      " [ 0.89185864 -1.06991201  0.41933272  0.96537043 -0.16934684 -0.12518377\n",
      "   1.1742859   0.24487356 -0.71226009]\n",
      " [-0.87970765  1.06577793 -0.41273639 -0.83182624  0.22939156  0.12311757\n",
      "  -1.18734688 -0.64586017  0.54158735]\n",
      " [ 0.01241659  0.3001391   0.51468841 -0.98075109  0.03017397  0.05941237\n",
      "  -1.08112667  0.25554703  0.26662291]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.49191576  1.37509748 -2.52937869 -0.09523193  1.47926161 -2.30614037\n",
      "  -0.69647019]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:61 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.58782546]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 61 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.80303916 -0.92127974  0.34360059  0.9625829  -0.16196924 -0.22531534\n",
      "   1.0462052   0.56049943 -0.39140046]\n",
      " [-0.69351254  1.11035798 -0.54661709 -1.10555461  0.09962118  0.16945653\n",
      "  -1.26563805 -0.76788932  0.4528285 ]\n",
      " [ 0.449711   -0.27240054  0.72043973 -0.82397932  0.37175514 -0.43885105\n",
      "  -0.81440752  0.56402079  0.57336288]\n",
      " [ 0.90232837 -1.06991201  0.41933272  0.97584017 -0.16934684 -0.12518377\n",
      "   1.1742859   0.24487356 -0.70179035]\n",
      " [-0.89026728  1.06577793 -0.41273639 -0.84238586  0.22939156  0.12311757\n",
      "  -1.18734688 -0.64586017  0.53102773]\n",
      " [ 0.00465269  0.3001391   0.51468841 -0.98851499  0.03017397  0.05941237\n",
      "  -1.08112667  0.25554703  0.25885901]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.44198356  1.41467815 -2.51879697 -0.06787677  1.51713972 -2.29431319\n",
      "  -0.67992101]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:61 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.13330456]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 61 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.80299228 -0.92127974  0.34360059  0.9625829  -0.16201612 -0.22536222\n",
      "   1.0462052   0.56049943 -0.39144733]\n",
      " [-0.69356719  1.11035798 -0.54661709 -1.10555461  0.09956653  0.16940188\n",
      "  -1.26563805 -0.76788932  0.45277385]\n",
      " [ 0.44823443 -0.27240054  0.72043973 -0.82397932  0.37027857 -0.44032762\n",
      "  -0.81440752  0.56402079  0.57188632]\n",
      " [ 0.90250893 -1.06991201  0.41933272  0.97584017 -0.16916629 -0.12500322\n",
      "   1.1742859   0.24487356 -0.7016098 ]\n",
      " [-0.89025432  1.06577793 -0.41273639 -0.84238586  0.22940451  0.12313053\n",
      "  -1.18734688 -0.64586017  0.53104068]\n",
      " [ 0.00399367  0.3001391   0.51468841 -0.98851499  0.02951495  0.05875336\n",
      "  -1.08112667  0.25554703  0.25819999]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.4496842   1.41078095 -2.52270195 -0.07343906  1.51347022 -2.29815055\n",
      "  -0.68444412]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:61 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.02195967]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 61 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.80303581 -0.92123621  0.34360059  0.9625829  -0.16197259 -0.22531869\n",
      "   1.0462052   0.56049943 -0.39140381]\n",
      " [-0.69361652  1.11030865 -0.54661709 -1.10555461  0.0995172   0.16935255\n",
      "  -1.26563805 -0.76788932  0.45272452]\n",
      " [ 0.44819874 -0.27243624  0.72043973 -0.82397932  0.37024288 -0.44036331\n",
      "  -0.81440752  0.56402079  0.57185062]\n",
      " [ 0.90255868 -1.06986225  0.41933272  0.97584017 -0.16911653 -0.12495346\n",
      "   1.1742859   0.24487356 -0.70156004]\n",
      " [-0.89030207  1.06573018 -0.41273639 -0.84238586  0.22935676  0.12308278\n",
      "  -1.18734688 -0.64586017  0.53099293]\n",
      " [ 0.0039591   0.30010453  0.51468841 -0.98851499  0.02948039  0.05871879\n",
      "  -1.08112667  0.25554703  0.25816542]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.44992001  1.41071264 -2.52288056 -0.07359546  1.51341407 -2.29832565\n",
      "  -0.68459909]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:61 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.38110981]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 61 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.79738349 -0.92688853  0.34360059  0.9625829  -0.16762491 -0.23097101\n",
      "   1.04055289  0.56049943 -0.39140381]\n",
      " [-0.68761756  1.11630761 -0.54661709 -1.10555461  0.10551616  0.17535151\n",
      "  -1.25963908 -0.76788932  0.45272452]\n",
      " [ 0.45524063 -0.26539434  0.72043973 -0.82397932  0.37728477 -0.43332142\n",
      "  -0.80736563  0.56402079  0.57185062]\n",
      " [ 0.8954856  -1.07693533  0.41933272  0.97584017 -0.17618961 -0.13202654\n",
      "   1.16721282  0.24487356 -0.70156004]\n",
      " [-0.88364265  1.07238961 -0.41273639 -0.84238586  0.23601619  0.1297422\n",
      "  -1.18068746 -0.64586017  0.53099293]\n",
      " [ 0.01084916  0.30699459  0.51468841 -0.98851499  0.03637045  0.06560885\n",
      "  -1.07423661  0.25554703  0.25816542]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.49486527  1.38230821 -2.53901209 -0.08842163  1.48325381 -2.31364554\n",
      "  -0.69962366]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:61 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.86861694]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 61 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.7985175  -0.92688853  0.34360059  0.96371691 -0.16762491 -0.23097101\n",
      "   1.04168689  0.56049943 -0.39140381]\n",
      " [-0.6886031   1.11630761 -0.54661709 -1.10654015  0.10551616  0.17535151\n",
      "  -1.26062463 -0.76788932  0.45272452]\n",
      " [ 0.45365203 -0.26539434  0.72043973 -0.82556792  0.37728477 -0.43332142\n",
      "  -0.80895423  0.56402079  0.57185062]\n",
      " [ 0.89647936 -1.07693533  0.41933272  0.97683392 -0.17618961 -0.13202654\n",
      "   1.16820657  0.24487356 -0.70156004]\n",
      " [-0.8847134   1.07238961 -0.41273639 -0.84345661  0.23601619  0.1297422\n",
      "  -1.18175821 -0.64586017  0.53099293]\n",
      " [ 0.00929711  0.30699459  0.51468841 -0.99006705  0.03637045  0.06560885\n",
      "  -1.07578866  0.25554703  0.25816542]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.48736845  1.38937549 -2.53867401 -0.08665423  1.49040791 -2.31325704\n",
      "  -0.69877007]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:61 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.71503535]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 61 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.80499225 -0.92688853  0.34360059  0.96371691 -0.16115016 -0.22449626\n",
      "   1.04816164  0.56049943 -0.39140381]\n",
      " [-0.69506813  1.11630761 -0.54661709 -1.10654015  0.09905113  0.16888648\n",
      "  -1.26708966 -0.76788932  0.45272452]\n",
      " [ 0.45078933 -0.26539434  0.72043973 -0.82556792  0.37442206 -0.43618413\n",
      "  -0.81181693  0.56402079  0.57185062]\n",
      " [ 0.90288195 -1.07693533  0.41933272  0.97683392 -0.16978702 -0.12562395\n",
      "   1.17460916  0.24487356 -0.70156004]\n",
      " [-0.89115893  1.07238961 -0.41273639 -0.84345661  0.22957066  0.12329667\n",
      "  -1.18820374 -0.64586017  0.53099293]\n",
      " [ 0.00370196  0.30699459  0.51468841 -0.99006705  0.0307753   0.06001371\n",
      "  -1.08138381  0.25554703  0.25816542]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.45833628  1.41285391 -2.53406826 -0.07508226  1.51516559 -2.30877528\n",
      "  -0.69075789]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:61 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57215261]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 61 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.79026792 -0.92688853  0.34360059  0.94899258 -0.16115016 -0.23922059\n",
      "   1.04816164  0.56049943 -0.40612814]\n",
      " [-0.68021013  1.11630761 -0.54661709 -1.09168215  0.09905113  0.18374448\n",
      "  -1.26708966 -0.76788932  0.46758252]\n",
      " [ 0.45491629 -0.26539434  0.72043973 -0.82144095  0.37442206 -0.43205716\n",
      "  -0.81181693  0.56402079  0.57597759]\n",
      " [ 0.88874514 -1.07693533  0.41933272  0.96269712 -0.16978702 -0.13976075\n",
      "   1.17460916  0.24487356 -0.71569684]\n",
      " [-0.87684423  1.07238961 -0.41273639 -0.82914191  0.22957066  0.13761137\n",
      "  -1.18820374 -0.64586017  0.54530763]\n",
      " [ 0.01418581  0.30699459  0.51468841 -0.9795832   0.0307753   0.07049756\n",
      "  -1.08138381  0.25554703  0.26864927]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.52836604  1.35962964 -2.55052895 -0.10593074  1.46325527 -2.32652392\n",
      "  -0.7144912 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:61 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.72939135]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 61 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.79623417 -0.92092228  0.34360059  0.95495883 -0.16115016 -0.23325434\n",
      "   1.0541279   0.56049943 -0.40612814]\n",
      " [-0.68611335  1.11040439 -0.54661709 -1.09758537  0.09905113  0.17784126\n",
      "  -1.27299288 -0.76788932  0.46758252]\n",
      " [ 0.44914441 -0.27116623  0.72043973 -0.82721283  0.37442206 -0.43782905\n",
      "  -0.81758882  0.56402079  0.57597759]\n",
      " [ 0.89458791 -1.07109257  0.41933272  0.96853988 -0.16978702 -0.13391799\n",
      "   1.18045192  0.24487356 -0.71569684]\n",
      " [-0.88278302  1.06645081 -0.41273639 -0.8350807   0.22957066  0.13167257\n",
      "  -1.19414253 -0.64586017  0.54530763]\n",
      " [ 0.00823918  0.30104796  0.51468841 -0.98552984  0.0307753   0.06455092\n",
      "  -1.08733044  0.25554703  0.26864927]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.50165973  1.38192877 -2.54653772 -0.10238219  1.48620358 -2.32234328\n",
      "  -0.71025774]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:61 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.77636706]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 61 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.80042261 -0.91673384  0.34360059  0.95914727 -0.16115016 -0.23325434\n",
      "   1.05831634  0.56049943 -0.40612814]\n",
      " [-0.69024488  1.10627286 -0.54661709 -1.1017169   0.09905113  0.17784126\n",
      "  -1.27712441 -0.76788932  0.46758252]\n",
      " [ 0.44480751 -0.27550313  0.72043973 -0.83154973  0.37442206 -0.43782905\n",
      "  -0.82192572  0.56402079  0.57597759]\n",
      " [ 0.89869326 -1.06698721  0.41933272  0.97264523 -0.16978702 -0.13391799\n",
      "   1.18455728  0.24487356 -0.71569684]\n",
      " [-0.88700339  1.06223044 -0.41273639 -0.83930108  0.22957066  0.13167257\n",
      "  -1.19836291 -0.64586017  0.54530763]\n",
      " [ 0.00396199  0.29677077  0.51468841 -0.98980702  0.0307753   0.06455092\n",
      "  -1.09160763  0.25554703  0.26864927]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.48224602  1.39878212 -2.5441118  -0.09874356  1.50324645 -2.31969533\n",
      "  -0.70741667]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:61 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.79959715]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 61 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.80382088 -0.91673384  0.34699886  0.95914727 -0.16115016 -0.22985607\n",
      "   1.06171461  0.56049943 -0.40612814]\n",
      " [-0.69326065  1.10627286 -0.54963286 -1.1017169   0.09905113  0.17482549\n",
      "  -1.28014018 -0.76788932  0.46758252]\n",
      " [ 0.44442899 -0.27550313  0.72006122 -0.83154973  0.37442206 -0.43820756\n",
      "  -0.82230424  0.56402079  0.57597759]\n",
      " [ 0.90166995 -1.06698721  0.42230941  0.97264523 -0.16978702 -0.1309413\n",
      "   1.18753397  0.24487356 -0.71569684]\n",
      " [-0.88998278  1.06223044 -0.41571578 -0.83930108  0.22957066  0.12869318\n",
      "  -1.2013423  -0.64586017  0.54530763]\n",
      " [ 0.00204758  0.29677077  0.51277401 -0.98980702  0.0307753   0.06263652\n",
      "  -1.09352203  0.25554703  0.26864927]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.46618958  1.41287185 -2.54269605 -0.09109443  1.51792855 -2.31831819\n",
      "  -0.7013864 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:61 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.20515446]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 62 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.8020668  -0.91848792  0.34699886  0.95739319 -0.16115016 -0.22985607\n",
      "   1.06171461  0.56049943 -0.40788222]\n",
      " [-0.69234719  1.10718632 -0.54963286 -1.10080344  0.09905113  0.17482549\n",
      "  -1.28014018 -0.76788932  0.46849598]\n",
      " [ 0.44479064 -0.27514148  0.72006122 -0.83118808  0.37442206 -0.43820756\n",
      "  -0.82230424  0.56402079  0.57633924]\n",
      " [ 0.90128758 -1.06736958  0.42230941  0.97226286 -0.16978702 -0.1309413\n",
      "   1.18753397  0.24487356 -0.71607921]\n",
      " [-0.88947556  1.06273767 -0.41571578 -0.83879385  0.22957066  0.12869318\n",
      "  -1.2013423  -0.64586017  0.54581486]\n",
      " [ 0.00373722  0.2984604   0.51277401 -0.98811739  0.0307753   0.06263652\n",
      "  -1.09352203  0.25554703  0.2703389 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.48291645  1.40269715 -2.55013857 -0.09909576  1.50918221 -2.32617315\n",
      "  -0.70800953]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:62 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.8175131]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 62 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.80474596 -0.91848792  0.34967802  0.95739319 -0.16115016 -0.22985607\n",
      "   1.06439377  0.56049943 -0.40788222]\n",
      " [-0.69470914  1.10718632 -0.55199481 -1.10080344  0.09905113  0.17482549\n",
      "  -1.28250213 -0.76788932  0.46849598]\n",
      " [ 0.44592281 -0.27514148  0.72119338 -0.83118808  0.37442206 -0.43820756\n",
      "  -0.82117207  0.56402079  0.57633924]\n",
      " [ 0.90366132 -1.06736958  0.42468315  0.97226286 -0.16978702 -0.1309413\n",
      "   1.18990771  0.24487356 -0.71607921]\n",
      " [-0.89185423  1.06273767 -0.41809445 -0.83879385  0.22957066  0.12869318\n",
      "  -1.20372097 -0.64586017  0.54581486]\n",
      " [ 0.0019284   0.2984604   0.51096519 -0.98811739  0.0307753   0.06263652\n",
      "  -1.09533085  0.25554703  0.2703389 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.46930426  1.41496464 -2.54912688 -0.09113522  1.52177238 -2.32514677\n",
      "  -0.7031143 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:62 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.64325761]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 62 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.78859695 -0.93463694  0.333529    0.95739319 -0.16115016 -0.22985607\n",
      "   1.04824475  0.56049943 -0.40788222]\n",
      " [-0.67827591  1.12361955 -0.53556158 -1.10080344  0.09905113  0.17482549\n",
      "  -1.2660689  -0.76788932  0.46849598]\n",
      " [ 0.44461802 -0.27644626  0.7198886  -0.83118808  0.37442206 -0.43820756\n",
      "  -0.82247686  0.56402079  0.57633924]\n",
      " [ 0.88718992 -1.08384099  0.40821175  0.97226286 -0.16978702 -0.1309413\n",
      "   1.17343631  0.24487356 -0.71607921]\n",
      " [-0.87538277  1.07920912 -0.40162299 -0.83879385  0.22957066  0.12869318\n",
      "  -1.18724951 -0.64586017  0.54581486]\n",
      " [ 0.007064    0.303596    0.51610079 -0.98811739  0.0307753   0.06263652\n",
      "  -1.09019525  0.25554703  0.2703389 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.54311076  1.35696082 -2.56347082 -0.12934435  1.46197921 -2.33915954\n",
      "  -0.73481265]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:62 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.64169845]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 62 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.797758   -0.93463694  0.34269006  0.95739319 -0.16115016 -0.22985607\n",
      "   1.04824475  0.56966048 -0.40788222]\n",
      " [-0.68696619  1.12361955 -0.54425186 -1.10080344  0.09905113  0.17482549\n",
      "  -1.2660689  -0.77657959  0.46849598]\n",
      " [ 0.45373455 -0.27644626  0.72900512 -0.83118808  0.37442206 -0.43820756\n",
      "  -0.82247686  0.57313731  0.57633924]\n",
      " [ 0.8964113  -1.08384099  0.41743313  0.97226286 -0.16978702 -0.1309413\n",
      "   1.17343631  0.25409493 -0.71607921]\n",
      " [-0.88419623  1.07920912 -0.41043645 -0.83879385  0.22957066  0.12869318\n",
      "  -1.18724951 -0.65467362  0.54581486]\n",
      " [ 0.01398034  0.303596    0.52301713 -0.98811739  0.0307753   0.06263652\n",
      "  -1.09019525  0.26246337  0.2703389 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.50192013  1.39169489 -2.55848119 -0.09436428  1.49589989 -2.33390599\n",
      "  -0.70658049]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:62 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.59008796]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 62 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.80869517 -0.93463694  0.34269006  0.96833036 -0.16115016 -0.22985607\n",
      "   1.04824475  0.56966048 -0.39694505]\n",
      " [-0.69785225  1.12361955 -0.54425186 -1.1116895   0.09905113  0.17482549\n",
      "  -1.2660689  -0.77657959  0.45760992]\n",
      " [ 0.4561753  -0.27644626  0.72900512 -0.82874733  0.37442206 -0.43820756\n",
      "  -0.82247686  0.57313731  0.57878   ]\n",
      " [ 0.9068342  -1.08384099  0.41743313  0.98268577 -0.16978702 -0.1309413\n",
      "   1.17343631  0.25409493 -0.70565631]\n",
      " [-0.89470507  1.07920912 -0.41043645 -0.84930269  0.22957066  0.12869318\n",
      "  -1.18724951 -0.65467362  0.53530602]\n",
      " [ 0.00625441  0.303596    0.52301713 -0.99584332  0.0307753   0.06263652\n",
      "  -1.09019525  0.26246337  0.26261297]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.45234452  1.4310422  -2.54802376 -0.0671196   1.53357659 -2.32222793\n",
      "  -0.69017242]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:62 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.12872883]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 62 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.80865774 -0.93463694  0.34269006  0.96833036 -0.16118759 -0.22989351\n",
      "   1.04824475  0.56966048 -0.39698249]\n",
      " [-0.69791293  1.12361955 -0.54425186 -1.1116895   0.09899045  0.17476481\n",
      "  -1.2660689  -0.77657959  0.45754924]\n",
      " [ 0.45477863 -0.27644626  0.72900512 -0.82874733  0.37302539 -0.43960424\n",
      "  -0.82247686  0.57313731  0.57738332]\n",
      " [ 0.90701342 -1.08384099  0.41743313  0.98268577 -0.1696078  -0.13076209\n",
      "   1.17343631  0.25409493 -0.70547709]\n",
      " [-0.89470302  1.07920912 -0.41043645 -0.84930269  0.2295727   0.12869523\n",
      "  -1.18724951 -0.65467362  0.53530807]\n",
      " [ 0.00562158  0.303596    0.52301713 -0.99584332  0.03014247  0.06200369\n",
      "  -1.09019525  0.26246337  0.26198014]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.45956348  1.42739528 -2.55169393 -0.07235589  1.53014662 -2.32583537\n",
      "  -0.69442866]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:62 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.0203257]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 62 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.80869552 -0.93459915  0.34269006  0.96833036 -0.16114981 -0.22985572\n",
      "   1.04824475  0.56966048 -0.3969447 ]\n",
      " [-0.69795555  1.12357694 -0.54425186 -1.1116895   0.09894784  0.17472219\n",
      "  -1.2660689  -0.77657959  0.45750662]\n",
      " [ 0.45474759 -0.2764773   0.72900512 -0.82874733  0.37299436 -0.43963527\n",
      "  -0.82247686  0.57313731  0.57735229]\n",
      " [ 0.9070564  -1.08379801  0.41743313  0.98268577 -0.16956483 -0.13071911\n",
      "   1.17343631  0.25409493 -0.70543412]\n",
      " [-0.89474434  1.0791678  -0.41043645 -0.84930269  0.22953138  0.12865391\n",
      "  -1.18724951 -0.65467362  0.53526674]\n",
      " [ 0.00559146  0.30356588  0.52301713 -0.99584332  0.03011235  0.06197357\n",
      "  -1.09019525  0.26246337  0.26195002]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.45976585  1.42733737 -2.55184789 -0.07249062  1.53009915 -2.32598636\n",
      "  -0.69456223]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:62 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.37284533]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 62 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.80329529 -0.93999939  0.34269006  0.96833036 -0.16655004 -0.23525596\n",
      "   1.04284452  0.56966048 -0.3969447 ]\n",
      " [-0.69224952  1.12928297 -0.54425186 -1.1116895   0.10465387  0.18042822\n",
      "  -1.26036287 -0.77657959  0.45750662]\n",
      " [ 0.46159259 -0.2696323   0.72900512 -0.82874733  0.37983936 -0.43279027\n",
      "  -0.81563186  0.57313731  0.57735229]\n",
      " [ 0.90031761 -1.0905368   0.41743313  0.98268577 -0.17630361 -0.1374579\n",
      "   1.16669753  0.25409493 -0.70543412]\n",
      " [-0.88840125  1.08551089 -0.41043645 -0.84930269  0.23587447  0.134997\n",
      "  -1.18090642 -0.65467362  0.53526674]\n",
      " [ 0.01227465  0.31024907  0.52301713 -0.99584332  0.03679555  0.06865677\n",
      "  -1.08351205  0.26246337  0.26195002]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.50335738  1.39988367 -2.56762719 -0.08685018  1.50100654 -2.34099049\n",
      "  -0.70913341]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:62 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.87168804]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 62 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.80437273 -0.93999939  0.34269006  0.9694078  -0.16655004 -0.23525596\n",
      "   1.04392196  0.56966048 -0.3969447 ]\n",
      " [-0.69318655  1.12928297 -0.54425186 -1.11262654  0.10465387  0.18042822\n",
      "  -1.26129991 -0.77657959  0.45750662]\n",
      " [ 0.4600688  -0.2696323   0.72900512 -0.83027112  0.37983936 -0.43279027\n",
      "  -0.81715565  0.57313731  0.57735229]\n",
      " [ 0.90126265 -1.0905368   0.41743313  0.98363081 -0.17630361 -0.1374579\n",
      "   1.16764257  0.25409493 -0.70543412]\n",
      " [-0.88941941  1.08551089 -0.41043645 -0.85032086  0.23587447  0.134997\n",
      "  -1.18192458 -0.65467362  0.53526674]\n",
      " [ 0.01079555  0.31024907  0.52301713 -0.99732242  0.03679555  0.06865677\n",
      "  -1.08499115  0.26246337  0.26195002]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.49618166  1.40665362 -2.56730712 -0.0851671   1.50785769 -2.3406228\n",
      "  -0.7083273 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:62 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.71724982]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 62 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.8107693  -0.93999939  0.34269006  0.9694078  -0.16015347 -0.22885938\n",
      "   1.05031853  0.56966048 -0.3969447 ]\n",
      " [-0.69957064  1.12928297 -0.54425186 -1.11262654  0.09826977  0.17404413\n",
      "  -1.267684   -0.77657959  0.45750662]\n",
      " [ 0.45724989 -0.2696323   0.72900512 -0.83027112  0.37702044 -0.43560918\n",
      "  -0.81997456  0.57313731  0.57735229]\n",
      " [ 0.90758678 -1.0905368   0.41743313  0.98363081 -0.16997948 -0.13113377\n",
      "   1.1739667   0.25409493 -0.70543412]\n",
      " [-0.89578497  1.08551089 -0.41043645 -0.85032086  0.22950892  0.12863144\n",
      "  -1.18829014 -0.65467362  0.53526674]\n",
      " [ 0.00525627  0.31024907  0.52301713 -0.99732242  0.03125626  0.06311748\n",
      "  -1.09053044  0.26246337  0.26195002]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.46751044  1.42986175 -2.56276263 -0.07373007  1.53230279 -2.33619586\n",
      "  -0.70043896]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:62 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57261942]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 62 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.79602903 -0.93999939  0.34269006  0.95466754 -0.16015347 -0.24359965\n",
      "   1.05031853  0.56966048 -0.41168496]\n",
      " [-0.68470156  1.12928297 -0.54425186 -1.09775746  0.09826977  0.18891321\n",
      "  -1.267684   -0.77657959  0.47237571]\n",
      " [ 0.46124746 -0.2696323   0.72900512 -0.82627354  0.37702044 -0.43161161\n",
      "  -0.81997456  0.57313731  0.58134987]\n",
      " [ 0.89342838 -1.0905368   0.41743313  0.96947241 -0.16997948 -0.14529217\n",
      "   1.1739667   0.25409493 -0.71959252]\n",
      " [-0.88145091  1.08551089 -0.41043645 -0.8359868   0.22950892  0.1429655\n",
      "  -1.18829014 -0.65467362  0.5496008 ]\n",
      " [ 0.01573113  0.31024907  0.52301713 -0.98684755  0.03125626  0.07359235\n",
      "  -1.09053044  0.26246337  0.27242489]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.53757799  1.37658878 -2.57922402 -0.10473053  1.48033602 -2.35392915\n",
      "  -0.72420371]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:62 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.73016506]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 62 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.80196871 -0.93405971  0.34269006  0.96060722 -0.16015347 -0.23765997\n",
      "   1.05625821  0.56966048 -0.41168496]\n",
      " [-0.69058239  1.12340214 -0.54425186 -1.10363829  0.09826977  0.18303238\n",
      "  -1.27356483 -0.77657959  0.47237571]\n",
      " [ 0.45551467 -0.27536509  0.72900512 -0.83200633  0.37702044 -0.4373444\n",
      "  -0.82570736  0.57313731  0.58134987]\n",
      " [ 0.89925186 -1.08471332  0.41743313  0.97529589 -0.16997948 -0.13946869\n",
      "   1.17979018  0.25409493 -0.71959252]\n",
      " [-0.88736579  1.07959601 -0.41043645 -0.84190168  0.22950892  0.13705062\n",
      "  -1.19420501 -0.65467362  0.5496008 ]\n",
      " [ 0.00981655  0.30433448  0.52301713 -0.99276214  0.03125626  0.06767776\n",
      "  -1.09644503  0.26246337  0.27242489]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.510996    1.39876981 -2.57522772 -0.10123039  1.50315068 -2.34974355\n",
      "  -0.72002007]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:62 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.77834299]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 62 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.80609362 -0.9299348   0.34269006  0.96473212 -0.16015347 -0.23765997\n",
      "   1.06038312  0.56966048 -0.41168496]\n",
      " [-0.69465339  1.11933114 -0.54425186 -1.10770929  0.09826977  0.18303238\n",
      "  -1.27763583 -0.77657959  0.47237571]\n",
      " [ 0.45124082 -0.27963894  0.72900512 -0.83628018  0.37702044 -0.4373444\n",
      "  -0.82998121  0.57313731  0.58134987]\n",
      " [ 0.90329813 -1.08066705  0.41743313  0.97934215 -0.16997948 -0.13946869\n",
      "   1.18383645  0.25409493 -0.71959252]\n",
      " [-0.89152376  1.07543804 -0.41043645 -0.84605965  0.22950892  0.13705062\n",
      "  -1.19836298 -0.65467362  0.5496008 ]\n",
      " [ 0.00561085  0.30012879  0.52301713 -0.99696784  0.03125626  0.06767776\n",
      "  -1.10065072  0.26246337  0.27242489]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.49187529  1.41536962 -2.57283443 -0.09767785  1.51993045 -2.34713179\n",
      "  -0.7172492 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:62 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.80120472]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 62 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.8094423  -0.9299348   0.34603874  0.96473212 -0.16015347 -0.23431129\n",
      "   1.0637318   0.56966048 -0.41168496]\n",
      " [-0.69763004  1.11933114 -0.54722851 -1.10770929  0.09826977  0.18005573\n",
      "  -1.28061248 -0.77657959  0.47237571]\n",
      " [ 0.45089682 -0.27963894  0.72866112 -0.83628018  0.37702044 -0.4376884\n",
      "  -0.83032521  0.57313731  0.58134987]\n",
      " [ 0.90623739 -1.08066705  0.42037239  0.97934215 -0.16997948 -0.13652943\n",
      "   1.18677571  0.25409493 -0.71959252]\n",
      " [-0.8944652   1.07543804 -0.41337788 -0.84605965  0.22950892  0.13410918\n",
      "  -1.20130442 -0.65467362  0.5496008 ]\n",
      " [ 0.00373644  0.30012879  0.52114272 -0.99696784  0.03125626  0.06580335\n",
      "  -1.10252513  0.26246337  0.27242489]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.47604366  1.42926619 -2.57143515 -0.09010647  1.53440257 -2.34577001\n",
      "  -0.71128827]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:62 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.19964172]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 63 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.80779573 -0.93158137  0.34603874  0.96308556 -0.16015347 -0.23431129\n",
      "   1.0637318   0.56966048 -0.41333153]\n",
      " [-0.69678783  1.12017334 -0.54722851 -1.10686708  0.09826977  0.18005573\n",
      "  -1.28061248 -0.77657959  0.47321791]\n",
      " [ 0.45122988 -0.27930588  0.72866112 -0.83594713  0.37702044 -0.4376884\n",
      "  -0.83032521  0.57313731  0.58168293]\n",
      " [ 0.9058978  -1.08100664  0.42037239  0.97900256 -0.16997948 -0.13652943\n",
      "   1.18677571  0.25409493 -0.71993211]\n",
      " [-0.89400623  1.07589701 -0.41337788 -0.84560068  0.22950892  0.13410918\n",
      "  -1.20130442 -0.65467362  0.55005976]\n",
      " [ 0.0053418   0.30173415  0.52114272 -0.99536248  0.03125626  0.06580335\n",
      "  -1.10252513  0.26246337  0.27403025]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.49199353  1.41959286 -2.57856145 -0.09774796  1.52608764 -2.35328495\n",
      "  -0.71761008]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:63 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.81996084]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 63 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.81040509 -0.93158137  0.34864809  0.96308556 -0.16015347 -0.23431129\n",
      "   1.06634115  0.56966048 -0.41333153]\n",
      " [-0.6990911   1.12017334 -0.54953177 -1.10686708  0.09826977  0.18005573\n",
      "  -1.28291574 -0.77657959  0.47321791]\n",
      " [ 0.45235647 -0.27930588  0.72978771 -0.83594713  0.37702044 -0.4376884\n",
      "  -0.82919862  0.57313731  0.58168293]\n",
      " [ 0.90821321 -1.08100664  0.4226878   0.97900256 -0.16997948 -0.13652943\n",
      "   1.18909112  0.25409493 -0.71993211]\n",
      " [-0.8963262   1.07589701 -0.41569785 -0.84560068  0.22950892  0.13410918\n",
      "  -1.20362439 -0.65467362  0.55005976]\n",
      " [ 0.0035784   0.30173415  0.51937932 -0.99536248  0.03125626  0.06580335\n",
      "  -1.10428853  0.26246337  0.27403025]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.47870438  1.43157723 -2.57757607 -0.08995371  1.53838078 -2.35228493\n",
      "  -0.71282805]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:63 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.64079119]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 63 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.79428992 -0.94769653  0.33253293  0.96308556 -0.16015347 -0.23431129\n",
      "   1.05022599  0.56966048 -0.41333153]\n",
      " [-0.68268773  1.13657671 -0.5331284  -1.10686708  0.09826977  0.18005573\n",
      "  -1.26651237 -0.77657959  0.47321791]\n",
      " [ 0.45100061 -0.28066174  0.72843185 -0.83594713  0.37702044 -0.4376884\n",
      "  -0.83055447  0.57313731  0.58168293]\n",
      " [ 0.89176928 -1.09745057  0.40624388  0.97900256 -0.16997948 -0.13652943\n",
      "   1.17264719  0.25409493 -0.71993211]\n",
      " [-0.87988129  1.09234192 -0.39925294 -0.84560068  0.22950892  0.13410918\n",
      "  -1.18717948 -0.65467362  0.55005976]\n",
      " [ 0.00863388  0.30678962  0.5244348  -0.99536248  0.03125626  0.06580335\n",
      "  -1.09923305  0.26246337  0.27403025]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.55245235  1.37370042 -2.59203206 -0.12818477  1.47877048 -2.36641385\n",
      "  -0.74458044]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:63 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.64484498]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 63 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.80332284 -0.94769653  0.34156584  0.96308556 -0.16015347 -0.23431129\n",
      "   1.05022599  0.5786934  -0.41333153]\n",
      " [-0.69124454  1.13657671 -0.54168521 -1.10686708  0.09826977  0.18005573\n",
      "  -1.26651237 -0.78513641  0.47321791]\n",
      " [ 0.45997421 -0.28066174  0.73740544 -0.83594713  0.37702044 -0.4376884\n",
      "  -0.83055447  0.5821109   0.58168293]\n",
      " [ 0.90087362 -1.09745057  0.41534822  0.97900256 -0.16997948 -0.13652943\n",
      "   1.17264719  0.26319928 -0.71993211]\n",
      " [-0.88856145  1.09234192 -0.4079331  -0.84560068  0.22950892  0.13410918\n",
      "  -1.18717948 -0.66335378  0.55005976]\n",
      " [ 0.0155663   0.30678962  0.53136721 -0.99536248  0.03125626  0.06580335\n",
      "  -1.09923305  0.26939578  0.27403025]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.51178356  1.40806859 -2.58715165 -0.09352371  1.5123311  -2.36127617\n",
      "  -0.71655891]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:63 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.59230046]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 63 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.81419243 -0.94769653  0.34156584  0.97395515 -0.16015347 -0.23431129\n",
      "   1.05022599  0.5786934  -0.40246193]\n",
      " [-0.70206456  1.13657671 -0.54168521 -1.1176871   0.09826977  0.18005573\n",
      "  -1.26651237 -0.78513641  0.4623979 ]\n",
      " [ 0.46247917 -0.28066174  0.73740544 -0.83344216  0.37702044 -0.4376884\n",
      "  -0.83055447  0.5821109   0.58418789]\n",
      " [ 0.91124914 -1.09745057  0.41534822  0.98937808 -0.16997948 -0.13652943\n",
      "   1.17264719  0.26319928 -0.7095566 ]\n",
      " [-0.89901906  1.09234192 -0.4079331  -0.85605829  0.22950892  0.13410918\n",
      "  -1.18717948 -0.66335378  0.53960215]\n",
      " [ 0.00787856  0.30678962  0.53136721 -1.00305021  0.03125626  0.06580335\n",
      "  -1.09923305  0.26939578  0.26634251]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.46255779  1.44718498 -2.57681398 -0.06638816  1.54980779 -2.34974182\n",
      "  -0.70028807]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:63 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.12435781]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 63 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.81416321 -0.94769653  0.34156584  0.97395515 -0.1601827  -0.23434051\n",
      "   1.05022599  0.5786934  -0.40249116]\n",
      " [-0.70212997  1.13657671 -0.54168521 -1.1176871   0.09820436  0.17999032\n",
      "  -1.26651237 -0.78513641  0.46233248]\n",
      " [ 0.46115811 -0.28066174  0.73740544 -0.83344216  0.37569938 -0.43900946\n",
      "  -0.83055447  0.5821109   0.58286683]\n",
      " [ 0.91142608 -1.09745057  0.41534822  0.98937808 -0.16980254 -0.13635249\n",
      "   1.17264719  0.26319928 -0.70937966]\n",
      " [-0.89902617  1.09234192 -0.4079331  -0.85605829  0.2295018   0.13410207\n",
      "  -1.18717948 -0.66335378  0.53959504]\n",
      " [ 0.00727126  0.30678962  0.53136721 -1.00305021  0.03064896  0.06519605\n",
      "  -1.09923305  0.26939578  0.26573521]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.46932863  1.44377033 -2.58026483 -0.07131935  1.54659963 -2.35313435\n",
      "  -0.70429484]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:63 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.01883109]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 63 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.81419605 -0.9476637   0.34156584  0.97395515 -0.16014986 -0.23430768\n",
      "   1.05022599  0.5786934  -0.40245832]\n",
      " [-0.70216682  1.13653986 -0.54168521 -1.1176871   0.09816751  0.17995347\n",
      "  -1.26651237 -0.78513641  0.46229563]\n",
      " [ 0.46113111 -0.28068874  0.73740544 -0.83344216  0.37567238 -0.43903646\n",
      "  -0.83055447  0.5821109   0.58283983]\n",
      " [ 0.91146323 -1.09741341  0.41534822  0.98937808 -0.16976539 -0.13631534\n",
      "   1.17264719  0.26319928 -0.70934251]\n",
      " [-0.89906198  1.09230611 -0.4079331  -0.85605829  0.229466    0.13406627\n",
      "  -1.18717948 -0.66335378  0.53955923]\n",
      " [ 0.00724499  0.30676335  0.53136721 -1.00305021  0.03062269  0.06516978\n",
      "  -1.09923305  0.26939578  0.26570894]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.4695026   1.44372113 -2.58039775 -0.07143559  1.54655941 -2.35326475\n",
      "  -0.70441014]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:63 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.3647766]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 63 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.80903755 -0.95282219  0.34156584  0.97395515 -0.16530836 -0.23946617\n",
      "   1.04506749  0.5786934  -0.40245832]\n",
      " [-0.69674048  1.1419662  -0.54168521 -1.1176871   0.10359385  0.18537981\n",
      "  -1.26108603 -0.78513641  0.46229563]\n",
      " [ 0.46778591 -0.27403394  0.73740544 -0.83344216  0.38232718 -0.43238166\n",
      "  -0.82389968  0.5821109   0.58283983]\n",
      " [ 0.90504465 -1.10383199  0.41534822  0.98937808 -0.17618397 -0.14273392\n",
      "   1.16622861  0.26319928 -0.70934251]\n",
      " [-0.8930217   1.09834639 -0.4079331  -0.85605829  0.23550628  0.14010654\n",
      "  -1.1811392  -0.66335378  0.53955923]\n",
      " [ 0.01372784  0.3132462   0.53136721 -1.00305021  0.03710554  0.07165263\n",
      "  -1.0927502   0.26939578  0.26570894]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.51176464  1.41719386 -2.59582055 -0.08533267  1.51850264 -2.3679474\n",
      "  -0.7185324 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:63 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.87465153]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 63 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.81006184 -0.95282219  0.34156584  0.97497945 -0.16530836 -0.23946617\n",
      "   1.04609178  0.5786934  -0.40245832]\n",
      " [-0.6976319   1.1419662  -0.54168521 -1.11857851  0.10359385  0.18537981\n",
      "  -1.26197745 -0.78513641  0.46229563]\n",
      " [ 0.46632367 -0.27403394  0.73740544 -0.8349044   0.38232718 -0.43238166\n",
      "  -0.82536192  0.5821109   0.58283983]\n",
      " [ 0.90594385 -1.10383199  0.41534822  0.99027728 -0.17618397 -0.14273392\n",
      "   1.16712781  0.26319928 -0.70934251]\n",
      " [-0.8939904   1.09834639 -0.4079331  -0.857027    0.23550628  0.14010654\n",
      "  -1.18210791 -0.66335378  0.53955923]\n",
      " [ 0.01231772  0.3132462   0.53136721 -1.00446033  0.03710554  0.07165263\n",
      "  -1.09416032  0.26939578  0.26570894]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.50489327  1.42368162 -2.59551732 -0.08372931  1.52506645 -2.36759916\n",
      "  -0.71777069]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:63 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.71947472]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 63 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.81637985 -0.95282219  0.34156584  0.97497945 -0.15899035 -0.23314816\n",
      "   1.05240979  0.5786934  -0.40245832]\n",
      " [-0.70393476  1.1419662  -0.54168521 -1.11857851  0.09729099  0.17907695\n",
      "  -1.2682803  -0.78513641  0.46229563]\n",
      " [ 0.46354622 -0.27403394  0.73740544 -0.8349044   0.37954973 -0.43515911\n",
      "  -0.82813936  0.5821109   0.58283983]\n",
      " [ 0.91218899 -1.10383199  0.41534822  0.99027728 -0.16993883 -0.13648878\n",
      "   1.17337295  0.26319928 -0.70934251]\n",
      " [-0.9002756   1.09834639 -0.4079331  -0.857027    0.22922108  0.13382134\n",
      "  -1.18839311 -0.66335378  0.53955923]\n",
      " [ 0.00683458  0.3132462   0.53136721 -1.00446033  0.03162239  0.06616949\n",
      "  -1.09964346  0.26939578  0.26570894]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.47658394  1.44661813 -2.59103484 -0.07243022  1.54919947 -2.36322814\n",
      "  -0.7100064 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:63 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57302511]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 63 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.80162569 -0.95282219  0.34156584  0.96022528 -0.15899035 -0.24790233\n",
      "   1.05240979  0.5786934  -0.41721249]\n",
      " [-0.68905641  1.1419662  -0.54168521 -1.10370016  0.09729099  0.1939553\n",
      "  -1.2682803  -0.78513641  0.47717398]\n",
      " [ 0.46741756 -0.27403394  0.73740544 -0.83103306  0.37954973 -0.43128778\n",
      "  -0.82813936  0.5821109   0.58671117]\n",
      " [ 0.89801094 -1.10383199  0.41534822  0.97609922 -0.16993883 -0.15066683\n",
      "   1.17337295  0.26319928 -0.72352056]\n",
      " [-0.8859243   1.09834639 -0.4079331  -0.8426757   0.22922108  0.14817264\n",
      "  -1.18839311 -0.66335378  0.55391054]\n",
      " [ 0.01729888  0.3132462   0.53136721 -0.99399602  0.03162239  0.07663379\n",
      "  -1.09964346  0.26939578  0.27617325]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.54668421  1.39330267 -2.60749769 -0.10357665  1.49718178 -2.380947\n",
      "  -0.73380192]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:63 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.73090471]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 63 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.80753996 -0.94690792  0.34156584  0.96613956 -0.15899035 -0.24198805\n",
      "   1.05832407  0.5786934  -0.41721249]\n",
      " [-0.69491576  1.13610685 -0.54168521 -1.10955952  0.09729099  0.18809595\n",
      "  -1.27413966 -0.78513641  0.47717398]\n",
      " [ 0.4617233  -0.2797282   0.73740544 -0.83672732  0.37954973 -0.43698204\n",
      "  -0.83383362  0.5821109   0.58671117]\n",
      " [ 0.90381583 -1.0980271   0.41534822  0.98190412 -0.16993883 -0.14486194\n",
      "   1.17917785  0.26319928 -0.72352056]\n",
      " [-0.89181622  1.09245447 -0.4079331  -0.84856761  0.22922108  0.14228073\n",
      "  -1.19428502 -0.66335378  0.55391054]\n",
      " [ 0.0114154   0.30736272  0.53136721 -0.99987951  0.03162239  0.07075031\n",
      "  -1.10552695  0.26939578  0.27617325]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.52022097  1.41537041 -2.60349609 -0.10012479  1.51986852 -2.37675626\n",
      "  -0.72966725]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:63 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.78027717]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 63 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.811603   -0.94284488  0.34156584  0.9702026  -0.15899035 -0.24198805\n",
      "   1.06238711  0.5786934  -0.41721249]\n",
      " [-0.69892773  1.13209488 -0.54168521 -1.11357149  0.09729099  0.18809595\n",
      "  -1.27815163 -0.78513641  0.47717398]\n",
      " [ 0.45751122 -0.28394027  0.73740544 -0.84093939  0.37954973 -0.43698204\n",
      "  -0.8380457   0.5821109   0.58671117]\n",
      " [ 0.90780438 -1.09403855  0.41534822  0.98589267 -0.16993883 -0.14486194\n",
      "   1.1831664   0.26319928 -0.72352056]\n",
      " [-0.89591334  1.08835734 -0.4079331  -0.85266474  0.22922108  0.14228073\n",
      "  -1.19838215 -0.66335378  0.55391054]\n",
      " [ 0.0072797   0.30322702  0.53136721 -1.00401521  0.03162239  0.07075031\n",
      "  -1.10966265  0.26939578  0.27617325]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.50138581  1.43172301 -2.60113468 -0.09665654  1.53639213 -2.37417981\n",
      "  -0.72696466]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:63 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.80274344]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 63 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.81490468 -0.94284488  0.34486752  0.9702026  -0.15899035 -0.23868637\n",
      "   1.06568879  0.5786934  -0.41721249]\n",
      " [-0.7018674   1.13209488 -0.54462488 -1.11357149  0.09729099  0.18515628\n",
      "  -1.2810913  -0.78513641  0.47717398]\n",
      " [ 0.45719894 -0.28394027  0.73709316 -0.84093939  0.37954973 -0.43729432\n",
      "  -0.83835798  0.5821109   0.58671117]\n",
      " [ 0.91070816 -1.09403855  0.41825199  0.98589267 -0.16993883 -0.14195816\n",
      "   1.18607018  0.26319928 -0.72352056]\n",
      " [-0.89881889  1.08835734 -0.41083865 -0.85266474  0.22922108  0.13937518\n",
      "  -1.20128769 -0.66335378  0.55391054]\n",
      " [ 0.00544377  0.30322702  0.52953128 -1.00401521  0.03162239  0.06891438\n",
      "  -1.11149858  0.26939578  0.27617325]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.48576838  1.44543502 -2.59975075 -0.08916044  1.55066397 -2.37283236\n",
      "  -0.72106942]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:63 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.19424685]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 64 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.81336017 -0.94438939  0.34486752  0.96865809 -0.15899035 -0.23868637\n",
      "   1.06568879  0.5786934  -0.418757  ]\n",
      " [-0.70109215  1.13287012 -0.54462488 -1.11279625  0.09729099  0.18515628\n",
      "  -1.2810913  -0.78513641  0.47794923]\n",
      " [ 0.45750615 -0.28363307  0.73709316 -0.84063219  0.37954973 -0.43729432\n",
      "  -0.83835798  0.5821109   0.58701838]\n",
      " [ 0.91040824 -1.09433846  0.41825199  0.98559276 -0.16993883 -0.14195816\n",
      "   1.18607018  0.26319928 -0.72382048]\n",
      " [-0.89840507  1.08877116 -0.41083865 -0.85225092  0.22922108  0.13937518\n",
      "  -1.20128769 -0.66335378  0.55432436]\n",
      " [ 0.00696878  0.30475203  0.52953128 -1.0024902   0.03162239  0.06891438\n",
      "  -1.11149858  0.26939578  0.27769826]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.50096965  1.43624294 -2.60657063 -0.09645354  1.54276311 -2.38001835\n",
      "  -0.72709999]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:64 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.82232101]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 64 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.81590316 -0.94438939  0.3474105   0.96865809 -0.15899035 -0.23868637\n",
      "   1.06823177  0.5786934  -0.418757  ]\n",
      " [-0.70333965  1.13287012 -0.54687237 -1.11279625  0.09729099  0.18515628\n",
      "  -1.28333879 -0.78513641  0.47794923]\n",
      " [ 0.45862626 -0.28363307  0.73821328 -0.84063219  0.37954973 -0.43729432\n",
      "  -0.83723787  0.5821109   0.58701838]\n",
      " [ 0.91266811 -1.09433846  0.42051186  0.98559276 -0.16993883 -0.14195816\n",
      "   1.18833004  0.26319928 -0.72382048]\n",
      " [-0.90066923  1.08877116 -0.41310281 -0.85225092  0.22922108  0.13937518\n",
      "  -1.20355185 -0.66335378  0.55432436]\n",
      " [ 0.00524898  0.30475203  0.52781148 -1.0024902   0.03162239  0.06891438\n",
      "  -1.11321838  0.26939578  0.27769826]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.48798939  1.44795603 -2.60561009 -0.08881945  1.55477204 -2.37904324\n",
      "  -0.72242601]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:64 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.6382156]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 64 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.79982477 -0.96046777  0.33133212  0.96865809 -0.15899035 -0.23868637\n",
      "   1.05215338  0.5786934  -0.418757  ]\n",
      " [-0.68696957  1.1492402  -0.5305023  -1.11279625  0.09729099  0.18515628\n",
      "  -1.26696872 -0.78513641  0.47794923]\n",
      " [ 0.45722892 -0.28503041  0.73681593 -0.84063219  0.37954973 -0.43729432\n",
      "  -0.83863521  0.5821109   0.58701838]\n",
      " [ 0.89625526 -1.11075131  0.40409901  0.98559276 -0.16993883 -0.14195816\n",
      "   1.17191719  0.26319928 -0.72382048]\n",
      " [-0.8842544   1.10518599 -0.39668798 -0.85225092  0.22922108  0.13937518\n",
      "  -1.18713703 -0.66335378  0.55432436]\n",
      " [ 0.01022702  0.30973007  0.53278953 -1.0024902   0.03162239  0.06891438\n",
      "  -1.10824033  0.26939578  0.27769826]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.56167024  1.39021489 -2.6201774  -0.12705856  1.49535136 -2.39328753\n",
      "  -0.75422522]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:64 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.64789074]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 64 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.80873287 -0.96046777  0.34024022  0.96865809 -0.15899035 -0.23868637\n",
      "   1.05215338  0.5876015  -0.418757  ]\n",
      " [-0.69539753  1.1492402  -0.53893026 -1.11279625  0.09729099  0.18515628\n",
      "  -1.26696872 -0.79356437  0.47794923]\n",
      " [ 0.46606156 -0.28503041  0.74564857 -0.84063219  0.37954973 -0.43729432\n",
      "  -0.83863521  0.59094354  0.58701838]\n",
      " [ 0.90524534 -1.11075131  0.41308909  0.98559276 -0.16993883 -0.14195816\n",
      "   1.17191719  0.27218936 -0.72382048]\n",
      " [-0.89280571  1.10518599 -0.40523929 -0.85225092  0.22922108  0.13937518\n",
      "  -1.18713703 -0.67190509  0.55432436]\n",
      " [ 0.01717361  0.30973007  0.53973611 -1.0024902   0.03162239  0.06891438\n",
      "  -1.10824033  0.27634237  0.27769826]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.5215072   1.42422564 -2.61540083 -0.09271026  1.52856068 -2.38826006\n",
      "  -0.72640736]]\n",
      "\n",
      "Target: \n",
      "[1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:64 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.59446156]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 64 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.81953562 -0.96046777  0.34024022  0.97946084 -0.15899035 -0.23868637\n",
      "   1.05215338  0.5876015  -0.40795424]\n",
      " [-0.70615212  1.1492402  -0.53893026 -1.12355084  0.09729099  0.18515628\n",
      "  -1.26696872 -0.79356437  0.46719464]\n",
      " [ 0.46862875 -0.28503041  0.74564857 -0.83806499  0.37954973 -0.43729432\n",
      "  -0.83863521  0.59094354  0.58958557]\n",
      " [ 0.91557309 -1.11075131  0.41308909  0.99592051 -0.16993883 -0.14195816\n",
      "   1.17191719  0.27218936 -0.71349273]\n",
      " [-0.90321184  1.10518599 -0.40523929 -0.86265705  0.22922108  0.13937518\n",
      "  -1.18713703 -0.67190509  0.54391823]\n",
      " [ 0.00952417  0.30973007  0.53973611 -1.01013963  0.03162239  0.06891438\n",
      "  -1.10824033  0.27634237  0.27004882]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.4726242   1.4631139  -2.60517847 -0.06568221  1.56583912 -2.37686406\n",
      "  -0.71026984]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:64 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.12017974]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 64 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.81951354 -0.96046777  0.34024022  0.97946084 -0.15901243 -0.23870846\n",
      "   1.05215338  0.5876015  -0.40797633]\n",
      " [-0.70622117  1.1492402  -0.53893026 -1.12355084  0.09722195  0.18508723\n",
      "  -1.26696872 -0.79356437  0.46712559]\n",
      " [ 0.46737922 -0.28503041  0.74564857 -0.83806499  0.3783002  -0.43854385\n",
      "  -0.83863521  0.59094354  0.58833604]\n",
      " [ 0.915747   -1.11075131  0.41308909  0.99592051 -0.16976492 -0.14178425\n",
      "   1.17191719  0.27218936 -0.71331881]\n",
      " [-0.90322662  1.10518599 -0.40523929 -0.86265705  0.2292063   0.1393604\n",
      "  -1.18713703 -0.67190509  0.54390345]\n",
      " [ 0.00894169  0.30973007  0.53973611 -1.01013963  0.03103991  0.06833189\n",
      "  -1.10824033  0.27634237  0.26946634]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.4789779   1.45991497 -2.60842439 -0.07032772  1.56283654 -2.38005569\n",
      "  -0.7140433 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:64 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.01746265]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 64 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.8195421  -0.96043921  0.34024022  0.97946084 -0.15898387 -0.2386799\n",
      "   1.05215338  0.5876015  -0.40794777]\n",
      " [-0.70625308  1.14920829 -0.53893026 -1.12355084  0.09719004  0.18505533\n",
      "  -1.26696872 -0.79356437  0.46709368]\n",
      " [ 0.46735571 -0.28505392  0.74564857 -0.83806499  0.37827669 -0.43856736\n",
      "  -0.83863521  0.59094354  0.58831253]\n",
      " [ 0.91577916 -1.11071915  0.41308909  0.99592051 -0.16973276 -0.14175209\n",
      "   1.17191719  0.27218936 -0.71328665]\n",
      " [-0.90325768  1.10515494 -0.40523929 -0.86265705  0.22917525  0.13932935\n",
      "  -1.18713703 -0.67190509  0.54387239]\n",
      " [ 0.00891876  0.30970714  0.53973611 -1.01013963  0.03101698  0.06830896\n",
      "  -1.10824033  0.27634237  0.26944341]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.4791277   1.45987308 -2.60853932 -0.07042817  1.56280239 -2.38016849\n",
      "  -0.71414299]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:64 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.35690606]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 64 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.81461512 -0.96536619  0.34024022  0.97946084 -0.16391085 -0.24360688\n",
      "   1.0472264   0.5876015  -0.40794777]\n",
      " [-0.70109343  1.15436794 -0.53893026 -1.12355084  0.10234969  0.19021498\n",
      "  -1.26180907 -0.79356437  0.46709368]\n",
      " [ 0.47382683 -0.2785828   0.74564857 -0.83806499  0.38474781 -0.43209624\n",
      "  -0.83216409  0.59094354  0.58831253]\n",
      " [ 0.90966676 -1.11683156  0.41308909  0.99592051 -0.17584517 -0.1478645\n",
      "   1.16580478  0.27218936 -0.71328665]\n",
      " [-0.89750683  1.11090579 -0.40523929 -0.86265705  0.2349261   0.1450802\n",
      "  -1.18138618 -0.67190509  0.54387239]\n",
      " [ 0.01520779  0.31599617  0.53973611 -1.01013963  0.03730601  0.07459799\n",
      "  -1.10195131  0.27634237  0.26944341]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.52008698  1.43424699 -2.62360318 -0.08386841  1.53574907 -2.39452571\n",
      "  -0.72782211]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:64 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.87751219]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 64 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.81558945 -0.96536619  0.34024022  0.98043517 -0.16391085 -0.24360688\n",
      "   1.04820073  0.5876015  -0.40794777]\n",
      " [-0.70194193  1.15436794 -0.53893026 -1.12439934  0.10234969  0.19021498\n",
      "  -1.26265757 -0.79356437  0.46709368]\n",
      " [ 0.47242309 -0.2785828   0.74564857 -0.83946873  0.38474781 -0.43209624\n",
      "  -0.83356783  0.59094354  0.58831253]\n",
      " [ 0.9105228  -1.11683156  0.41308909  0.99677655 -0.17584517 -0.1478645\n",
      "   1.16666083  0.27218936 -0.71328665]\n",
      " [-0.89842899  1.11090579 -0.40523929 -0.86357921  0.2349261   0.1450802\n",
      "  -1.18230834 -0.67190509  0.54387239]\n",
      " [ 0.01386291  0.31599617  0.53973611 -1.01148451  0.03730601  0.07459799\n",
      "  -1.10329619  0.27634237  0.26944341]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.51350421  1.44046683 -2.62331571 -0.08234045  1.54204021 -2.39419567\n",
      "  -0.72710195]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:64 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.721708]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 64 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.8218286  -0.96536619  0.34024022  0.98043517 -0.1576717  -0.23736773\n",
      "   1.05443988  0.5876015  -0.40794777]\n",
      " [-0.70816335  1.15436794 -0.53893026 -1.12439934  0.09612827  0.18399356\n",
      "  -1.26887898 -0.79356437  0.46709368]\n",
      " [ 0.46968489 -0.2785828   0.74564857 -0.83946873  0.38200961 -0.43483444\n",
      "  -0.83630603  0.59094354  0.58831253]\n",
      " [ 0.91668854 -1.11683156  0.41308909  0.99677655 -0.16967942 -0.14169875\n",
      "   1.17282657  0.27218936 -0.71328665]\n",
      " [-0.90463356  1.11090579 -0.40523929 -0.86357921  0.22872153  0.13887563\n",
      "  -1.18851291 -0.67190509  0.54387239]\n",
      " [ 0.00843617  0.31599617  0.53973611 -1.01148451  0.03187927  0.06917125\n",
      "  -1.10872293  0.27634237  0.26944341]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.48555735  1.46313067 -2.61889583 -0.07118201  1.56586184 -2.38988152\n",
      "  -0.71946178]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:64 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57336703]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 64 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.80706259 -0.96536619  0.34024022  0.96566916 -0.1576717  -0.25213374\n",
      "   1.05443988  0.5876015  -0.42271378]\n",
      " [-0.69327759  1.15436794 -0.53893026 -1.10951358  0.09612827  0.19887932\n",
      "  -1.26887898 -0.79356437  0.48197944]\n",
      " [ 0.47343313 -0.2785828   0.74564857 -0.83572049  0.38200961 -0.43108619\n",
      "  -0.83630603  0.59094354  0.59206077]\n",
      " [ 0.90249286 -1.11683156  0.41308909  0.98258087 -0.16967942 -0.15589444\n",
      "   1.17282657  0.27218936 -0.72748234]\n",
      " [-0.89026716  1.11090579 -0.40523929 -0.84921282  0.22872153  0.15324202\n",
      "  -1.18851291 -0.67190509  0.55823878]\n",
      " [ 0.01888832  0.31599617  0.53973611 -1.00103236  0.03187927  0.0796234\n",
      "  -1.10872293  0.27634237  0.27989557]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.55568509  1.40977904 -2.63536091 -0.10246834  1.51379901 -2.4075869\n",
      "  -0.74328728]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:64 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.7316114]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 64 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.81295259 -0.95947619  0.34024022  0.97155917 -0.1576717  -0.24624373\n",
      "   1.06032989  0.5876015  -0.42271378]\n",
      " [-0.69911637  1.14852915 -0.53893026 -1.11535237  0.09612827  0.19304053\n",
      "  -1.27471777 -0.79356437  0.48197944]\n",
      " [ 0.46777687 -0.28423906  0.74564857 -0.84137675  0.38200961 -0.43674246\n",
      "  -0.84196229  0.59094354  0.59206077]\n",
      " [ 0.90827988 -1.11104454  0.41308909  0.98836789 -0.16967942 -0.15010742\n",
      "   1.17861359  0.27218936 -0.72748234]\n",
      " [-0.89613706  1.10503589 -0.40523929 -0.85508271  0.22872153  0.14737213\n",
      "  -1.1943828  -0.67190509  0.55823878]\n",
      " [ 0.01303504  0.31014289  0.53973611 -1.00688563  0.03187927  0.07377013\n",
      "  -1.1145762   0.27634237  0.27989557]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.52933521  1.43173813 -2.63135377 -0.09906456  1.53636332 -2.40339077\n",
      "  -0.73920076]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:64 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.78217115]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 64 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.81695536 -0.95547342  0.34024022  0.97556193 -0.1576717  -0.24624373\n",
      "   1.06433265  0.5876015  -0.42271378]\n",
      " [-0.70307076  1.14457477 -0.53893026 -1.11930676  0.09612827  0.19304053\n",
      "  -1.27867216 -0.79356437  0.48197944]\n",
      " [ 0.46362538 -0.28839055  0.74564857 -0.84552824  0.38200961 -0.43674246\n",
      "  -0.84611378  0.59094354  0.59206077]\n",
      " [ 0.91221204 -1.10711238  0.41308909  0.99230005 -0.16967942 -0.15010742\n",
      "   1.18254575  0.27218936 -0.72748234]\n",
      " [-0.90017484  1.10099811 -0.40523929 -0.8591205   0.22872153  0.14737213\n",
      "  -1.19842059 -0.67190509  0.55823878]\n",
      " [ 0.0089679   0.30607574  0.53973611 -1.01095278  0.03187927  0.07377013\n",
      "  -1.11864335  0.27634237  0.27989557]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.51077844  1.44784961 -2.62902349 -0.09567878  1.55263741 -2.40084878\n",
      "  -0.73656458]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:64 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.80421583]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 64 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.8202125  -0.95547342  0.34349736  0.97556193 -0.1576717  -0.24298659\n",
      "   1.06758979  0.5876015  -0.42271378]\n",
      " [-0.70597548  1.14457477 -0.54183498 -1.11930676  0.09612827  0.19013581\n",
      "  -1.28157688 -0.79356437  0.48197944]\n",
      " [ 0.46334223 -0.28839055  0.74536542 -0.84552824  0.38200961 -0.43702561\n",
      "  -0.84639693  0.59094354  0.59206077]\n",
      " [ 0.91508218 -1.10711238  0.41595922  0.99230005 -0.16967942 -0.14723728\n",
      "   1.18541589  0.27218936 -0.72748234]\n",
      " [-0.90304646  1.10099811 -0.4081109  -0.8591205   0.22872153  0.14450052\n",
      "  -1.2012922  -0.67190509  0.55823878]\n",
      " [ 0.00716899  0.30607574  0.53793721 -1.01095278  0.03187927  0.07197122\n",
      "  -1.12044226  0.27634237  0.27989557]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.49536506  1.46138529 -2.62765385 -0.0882555   1.56671826 -2.3995147\n",
      "  -0.73073152]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:64 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.18896771]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 65 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.81876482 -0.95692111  0.34349736  0.97411425 -0.1576717  -0.24298659\n",
      "   1.06758979  0.5876015  -0.42416146]\n",
      " [-0.70526312  1.14528713 -0.54183498 -1.11859439  0.09612827  0.19013581\n",
      "  -1.28157688 -0.79356437  0.4826918 ]\n",
      " [ 0.46362603 -0.28810675  0.74536542 -0.84524444  0.38200961 -0.43702561\n",
      "  -0.84639693  0.59094354  0.59234457]\n",
      " [ 0.91481903 -1.10737553  0.41595922  0.9920369  -0.16967942 -0.14723728\n",
      "   1.18541589  0.27218936 -0.72774549]\n",
      " [-0.90267482  1.10136974 -0.4081109  -0.85874887  0.22872153  0.14450052\n",
      "  -1.2012922  -0.67190509  0.55861042]\n",
      " [ 0.00861739  0.30752414  0.53793721 -1.00950438  0.03187927  0.07197122\n",
      "  -1.12044226  0.27634237  0.28134396]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.50984555  1.45265492 -2.63417704 -0.09521166  1.55921463 -2.40638266\n",
      "  -0.73648086]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:65 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.8245974]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 65 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.82124467 -0.95692111  0.34597721  0.97411425 -0.1576717  -0.24298659\n",
      "   1.07006964  0.5876015  -0.42416146]\n",
      " [-0.70745759  1.14528713 -0.54402945 -1.11859439  0.09612827  0.19013581\n",
      "  -1.28377135 -0.79356437  0.4826918 ]\n",
      " [ 0.46473891 -0.28810675  0.7464783  -0.84524444  0.38200961 -0.43702561\n",
      "  -0.84528405  0.59094354  0.59234457]\n",
      " [ 0.91702599 -1.10737553  0.41816619  0.9920369  -0.16967942 -0.14723728\n",
      "   1.18762285  0.27218936 -0.72774549]\n",
      " [-0.9048859   1.10136974 -0.41032198 -0.85874887  0.22872153  0.14450052\n",
      "  -1.20350328 -0.67190509  0.55861042]\n",
      " [ 0.00693946  0.30752414  0.53625928 -1.00950438  0.03187927  0.07197122\n",
      "  -1.12212018  0.27634237  0.28134396]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.49716074  1.46410802 -2.63323998 -0.08773183  1.57095149 -2.4054311\n",
      "  -0.73191001]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:65 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.63553265]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 65 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.80520612 -0.97295966  0.32993866  0.97411425 -0.1576717  -0.24298659\n",
      "   1.05403109  0.5876015  -0.42416146]\n",
      " [-0.69112439  1.16162032 -0.52769625 -1.11859439  0.09612827  0.19013581\n",
      "  -1.26743815 -0.79356437  0.4826918 ]\n",
      " [ 0.46330898 -0.28953668  0.74504837 -0.84524444  0.38200961 -0.43702561\n",
      "  -0.84671398  0.59094354  0.59234457]\n",
      " [ 0.90064793 -1.12375359  0.40178813  0.9920369  -0.16967942 -0.14723728\n",
      "   1.17124479  0.27218936 -0.72774549]\n",
      " [-0.88850484  1.1177508  -0.39394092 -0.85874887  0.22872153  0.14450052\n",
      "  -1.18712221 -0.67190509  0.55861042]\n",
      " [ 0.01184264  0.31242731  0.54116245 -1.00950438  0.03187927  0.07197122\n",
      "  -1.11721701  0.27634237  0.28134396]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.57076524  1.40651155 -2.64791763 -0.12596545  1.51172766 -2.41978977\n",
      "  -0.76374868]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:65 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.65083987]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 65 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.81399272 -0.97295966  0.33872527  0.97411425 -0.1576717  -0.24298659\n",
      "   1.05403109  0.59638811 -0.42416146]\n",
      " [-0.69942794  1.16162032 -0.5359998  -1.11859439  0.09612827  0.19013581\n",
      "  -1.26743815 -0.80186792  0.4826918 ]\n",
      " [ 0.47200279 -0.28953668  0.75374217 -0.84524444  0.38200961 -0.43702561\n",
      "  -0.84671398  0.59963735  0.59234457]\n",
      " [ 0.90952652 -1.12375359  0.41066671  0.9920369  -0.16967942 -0.14723728\n",
      "   1.17124479  0.28106794 -0.72774549]\n",
      " [-0.89693161  1.1177508  -0.40236769 -0.85874887  0.22872153  0.14450052\n",
      "  -1.18712221 -0.68033187  0.55861042]\n",
      " [ 0.0188015   0.31242731  0.54812131 -1.00950438  0.03187927  0.07197122\n",
      "  -1.11721701  0.28330123  0.28134396]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.53109239  1.44017326 -2.64323985 -0.09192361  1.54459432 -2.41486717\n",
      "  -0.73612776]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:65 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.59657039]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 65 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.82472948 -0.97295966  0.33872527  0.984851   -0.1576717  -0.24298659\n",
      "   1.05403109  0.59638811 -0.41342471]\n",
      " [-0.71011784  1.16162032 -0.5359998  -1.12928429  0.09612827  0.19013581\n",
      "  -1.26743815 -0.80186792  0.4720019 ]\n",
      " [ 0.47463035 -0.28953668  0.75374217 -0.84261687  0.38200961 -0.43702561\n",
      "  -0.84671398  0.59963735  0.59497214]\n",
      " [ 0.9198063  -1.12375359  0.41066671  1.00231668 -0.16967942 -0.14723728\n",
      "   1.17124479  0.28106794 -0.71746571]\n",
      " [-0.90728618  1.1177508  -0.40236769 -0.86910343  0.22872153  0.14450052\n",
      "  -1.18712221 -0.68033187  0.54825585]\n",
      " [ 0.01119038  0.31242731  0.54812131 -1.0171155   0.03187927  0.07197122\n",
      "  -1.11721701  0.28330123  0.27373284]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.48254485  1.47883653 -2.63312842 -0.0650012   1.5816766  -2.40360429\n",
      "  -0.72011967]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:65 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.11618346]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 65 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.8247136  -0.97295966  0.33872527  0.984851   -0.15768758 -0.24300247\n",
      "   1.05403109  0.59638811 -0.41344058]\n",
      " [-0.71018961  1.16162032 -0.5359998  -1.12928429  0.09605651  0.19006405\n",
      "  -1.26743815 -0.80186792  0.47193014]\n",
      " [ 0.47344847 -0.28953668  0.75374217 -0.84261687  0.38082772 -0.43820749\n",
      "  -0.84671398  0.59963735  0.59379025]\n",
      " [ 0.9199766  -1.12375359  0.41066671  1.00231668 -0.16950912 -0.14706698\n",
      "   1.17124479  0.28106794 -0.7172954 ]\n",
      " [-0.90730734  1.1177508  -0.40236769 -0.86910343  0.22870037  0.14447936\n",
      "  -1.18712221 -0.68033187  0.54823469]\n",
      " [ 0.01063197  0.31242731  0.54812131 -1.0171155   0.03132086  0.07141282\n",
      "  -1.11721701  0.28330123  0.27317444]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.48850999  1.47583809 -2.63618278 -0.06937912  1.57886471 -2.40660803\n",
      "  -0.72367482]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:65 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.0162085]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 65 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.82473847 -0.97293478  0.33872527  0.984851   -0.15766271 -0.24297759\n",
      "   1.05403109  0.59638811 -0.41341571]\n",
      " [-0.71021727  1.16159266 -0.5359998  -1.12928429  0.09602885  0.19003639\n",
      "  -1.26743815 -0.80186792  0.47190248]\n",
      " [ 0.47342797 -0.28955718  0.75374217 -0.84261687  0.38080723 -0.43822799\n",
      "  -0.84671398  0.59963735  0.59376976]\n",
      " [ 0.92000448 -1.12372571  0.41066671  1.00231668 -0.16948124 -0.1470391\n",
      "   1.17124479  0.28106794 -0.71726753]\n",
      " [-0.90733431  1.11772384 -0.40236769 -0.86910343  0.2286734   0.14445239\n",
      "  -1.18712221 -0.68033187  0.54820772]\n",
      " [ 0.01061194  0.31240728  0.54812131 -1.0171155   0.03130083  0.07139278\n",
      "  -1.11721701  0.28330123  0.2731544 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.48863922  1.47580237 -2.63628231 -0.06946604  1.57883565 -2.40670575\n",
      "  -0.72376114]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:65 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.34923471]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 65 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.82003296 -0.9776403   0.33872527  0.984851   -0.16236822 -0.24768311\n",
      "   1.04932558  0.59638811 -0.41341571]\n",
      " [-0.70531163  1.1664983  -0.5359998  -1.12928429  0.10093449  0.19494203\n",
      "  -1.26253251 -0.80186792  0.47190248]\n",
      " [ 0.47972176 -0.28326339  0.75374217 -0.84261687  0.38710102 -0.4319342\n",
      "  -0.84042019  0.59963735  0.59376976]\n",
      " [ 0.9141844  -1.12954579  0.41066671  1.00231668 -0.17530132 -0.15285917\n",
      "   1.16542472  0.28106794 -0.71726753]\n",
      " [-0.90185971  1.12319843 -0.40236769 -0.86910343  0.23414799  0.14992698\n",
      "  -1.18164762 -0.68033187  0.54820772]\n",
      " [ 0.01671361  0.31850895  0.54812131 -1.0171155   0.0374025   0.07749446\n",
      "  -1.11111533  0.28330123  0.2731544 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.52832447  1.4510515  -2.65098639 -0.08245641  1.55275305 -2.42073524\n",
      "  -0.73700405]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:65 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.88027473]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 65 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.82096029 -0.9776403   0.33872527  0.98577833 -0.16236822 -0.24768311\n",
      "   1.05025291  0.59638811 -0.41341571]\n",
      " [-0.70611974  1.1664983  -0.5359998  -1.13009241  0.10093449  0.19494203\n",
      "  -1.26334063 -0.80186792  0.47190248]\n",
      " [ 0.47837364 -0.28326339  0.75374217 -0.84396499  0.38710102 -0.4319342\n",
      "  -0.84176831  0.59963735  0.59376976]\n",
      " [ 0.91499979 -1.12954579  0.41066671  1.00313206 -0.17530132 -0.15285917\n",
      "   1.1662401   0.28106794 -0.71726753]\n",
      " [-0.90273806  1.12319843 -0.40236769 -0.86998178  0.23414799  0.14992698\n",
      "  -1.18252596 -0.68033187  0.54820772]\n",
      " [ 0.01543045  0.31850895  0.54812131 -1.01839866  0.0374025   0.07749446\n",
      "  -1.11239849  0.28330123  0.2731544 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.52201548  1.45701687 -2.65071368 -0.08099976  1.55878534 -2.42042223\n",
      "  -0.73632279]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:65 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.72394768]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 65 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.82712036 -0.9776403   0.33872527  0.98577833 -0.15620815 -0.24152303\n",
      "   1.05641298  0.59638811 -0.41341571]\n",
      " [-0.7122596   1.1664983  -0.5359998  -1.13009241  0.09479463  0.18880217\n",
      "  -1.26948048 -0.80186792  0.47190248]\n",
      " [ 0.47567259 -0.28326339  0.75374217 -0.84396499  0.38439996 -0.43463525\n",
      "  -0.84446936  0.59963735  0.59376976]\n",
      " [ 0.92108584 -1.12954579  0.41066671  1.00313206 -0.16921527 -0.14677313\n",
      "   1.17232615  0.28106794 -0.71726753]\n",
      " [-0.90886181  1.12319843 -0.40236769 -0.86998178  0.22802424  0.14380323\n",
      "  -1.18864972 -0.68033187  0.54820772]\n",
      " [ 0.01006034  0.31850895  0.54812131 -1.01839866  0.03203239  0.07212434\n",
      "  -1.11776861  0.28330123  0.2731544 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.49443131  1.47940723 -2.64635685 -0.06998437  1.58229645 -2.41616571\n",
      "  -0.72880663]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:65 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57364307]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 65 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.81234458 -0.9776403   0.33872527  0.97100255 -0.15620815 -0.25629882\n",
      "   1.05641298  0.59638811 -0.42819149]\n",
      " [-0.69736834  1.1664983  -0.5359998  -1.11520115  0.09479463  0.20369343\n",
      "  -1.26948048 -0.80186792  0.48679374]\n",
      " [ 0.47930086 -0.28326339  0.75374217 -0.84033672  0.38439996 -0.43100698\n",
      "  -0.84446936  0.59963735  0.59739803]\n",
      " [ 0.90687461 -1.12954579  0.41066671  0.98892084 -0.16921527 -0.16098435\n",
      "   1.17232615  0.28106794 -0.73147875]\n",
      " [-0.89448253  1.12319843 -0.40236769 -0.85560249  0.22802424  0.15818251\n",
      "  -1.18864972 -0.68033187  0.56258701]\n",
      " [ 0.02049876  0.31850895  0.54812131 -1.00796024  0.03203239  0.08256277\n",
      "  -1.11776861  0.28330123  0.28359283]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.56458117  1.42602586 -2.66282496 -0.10140452  1.53019444 -2.43385857\n",
      "  -0.75266128]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:65 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.73228626]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 65 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.81821141 -0.97177347  0.33872527  0.97686938 -0.15620815 -0.25043199\n",
      "   1.06227981  0.59638811 -0.42819149]\n",
      " [-0.70318743  1.1606792  -0.5359998  -1.12102024  0.09479463  0.19787434\n",
      "  -1.27529958 -0.80186792  0.48679374]\n",
      " [ 0.4736821  -0.28888215  0.75374217 -0.84595548  0.38439996 -0.43662574\n",
      "  -0.85008812  0.59963735  0.59739803]\n",
      " [ 0.91264445 -1.12377595  0.41066671  0.99469068 -0.16921527 -0.15521451\n",
      "   1.17809599  0.28106794 -0.73147875]\n",
      " [-0.9003313   1.11734966 -0.40236769 -0.86145126  0.22802424  0.15233374\n",
      "  -1.19449849 -0.68033187  0.56258701]\n",
      " [ 0.01467484  0.31268503  0.54812131 -1.01378416  0.03203239  0.07673885\n",
      "  -1.12359253  0.28330123  0.28359283]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.53833946  1.44788076 -2.658812   -0.09804855  1.55264158 -2.42965682\n",
      "  -0.74862208]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:65 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.78402655]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 65 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.82215543 -0.96782945  0.33872527  0.9808134  -0.15620815 -0.25043199\n",
      "   1.06622383  0.59638811 -0.42819149]\n",
      " [-0.70708562  1.15678101 -0.5359998  -1.12491843  0.09479463  0.19787434\n",
      "  -1.27919777 -0.80186792  0.48679374]\n",
      " [ 0.46959007 -0.29297419  0.75374217 -0.85004751  0.38439996 -0.43662574\n",
      "  -0.85418016  0.59963735  0.59739803]\n",
      " [ 0.9165215  -1.11989889  0.41066671  0.99856773 -0.16921527 -0.15521451\n",
      "   1.18197304  0.28106794 -0.73147875]\n",
      " [-0.9043112   1.11336976 -0.40236769 -0.86543116  0.22802424  0.15233374\n",
      "  -1.19847839 -0.68033187  0.56258701]\n",
      " [ 0.01067486  0.30868505  0.54812131 -1.01778414  0.03203239  0.07673885\n",
      "  -1.12759251  0.28330123  0.28359283]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.52005418  1.46375698 -2.65651214 -0.09474341  1.5686725  -2.42714848\n",
      "  -0.74605051]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:65 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.80562444]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 65 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.82537038 -0.96782945  0.34194022  0.9808134  -0.15620815 -0.24721704\n",
      "   1.06943878  0.59638811 -0.42819149]\n",
      " [-0.70995734  1.15678101 -0.53887152 -1.12491843  0.09479463  0.19500262\n",
      "  -1.28206948 -0.80186792  0.48679374]\n",
      " [ 0.46933364 -0.29297419  0.75348575 -0.85004751  0.38439996 -0.43688217\n",
      "  -0.85443659  0.59963735  0.59739803]\n",
      " [ 0.91935977 -1.11989889  0.41350498  0.99856773 -0.16921527 -0.15237625\n",
      "   1.18481131  0.28106794 -0.73147875]\n",
      " [-0.90715075  1.11336976 -0.40520724 -0.86543116  0.22802424  0.14949419\n",
      "  -1.20131794 -0.68033187  0.56258701]\n",
      " [ 0.0089116   0.30868505  0.54635806 -1.01778414  0.03203239  0.07497559\n",
      "  -1.12935576  0.28330123  0.28359283]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.50483519  1.47712422 -2.6551558  -0.08739053  1.58257121 -2.42582684\n",
      "  -0.74027628]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:65 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.18380241]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 66 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.82401448 -0.96918534  0.34194022  0.97945751 -0.15620815 -0.24721704\n",
      "   1.06943878  0.59638811 -0.42954738]\n",
      " [-0.70930397  1.15743438 -0.53887152 -1.12426506  0.09479463  0.19500262\n",
      "  -1.28206948 -0.80186792  0.48744711]\n",
      " [ 0.46959621 -0.29271162  0.75348575 -0.84978495  0.38439996 -0.43688217\n",
      "  -0.85443659  0.59963735  0.5976606 ]\n",
      " [ 0.91913064 -1.12012802  0.41350498  0.9983386  -0.16921527 -0.15237625\n",
      "   1.18481131  0.28106794 -0.73170788]\n",
      " [-0.90681848  1.11370202 -0.40520724 -0.8650989   0.22802424  0.14949419\n",
      "  -1.20131794 -0.68033187  0.56291928]\n",
      " [ 0.01028695  0.3100604   0.54635806 -1.01640879  0.03203239  0.07497559\n",
      "  -1.12935576  0.28330123  0.28496817]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.51862212  1.46883652 -2.66139191 -0.09402118  1.57544844 -2.43238753\n",
      "  -0.74575427]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:66 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.82679383]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 66 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.82643425 -0.96918534  0.34435998  0.97945751 -0.15620815 -0.24721704\n",
      "   1.07185854  0.59638811 -0.42954738]\n",
      " [-0.71144799  1.15743438 -0.54101553 -1.12426506  0.09479463  0.19500262\n",
      "  -1.2842135  -0.80186792  0.48744711]\n",
      " [ 0.47070123 -0.29271162  0.75459077 -0.84978495  0.38439996 -0.43688217\n",
      "  -0.85333157  0.59963735  0.5976606 ]\n",
      " [ 0.92128719 -1.12012802  0.41566153  0.9983386  -0.16921527 -0.15237625\n",
      "   1.18696786  0.28106794 -0.73170788]\n",
      " [-0.90897904  1.11370202 -0.4073678  -0.8650989   0.22802424  0.14949419\n",
      "  -1.2034785  -0.68033187  0.56291928]\n",
      " [ 0.00864926  0.3100604   0.54472036 -1.01640879  0.03203239  0.07497559\n",
      "  -1.13099346  0.28330123  0.28496817]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.50622006  1.48004033 -2.66047707 -0.08668992  1.58692477 -2.43145826\n",
      "  -0.74128194]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:66 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.63274442]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 66 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.81043873 -0.98518086  0.32836446  0.97945751 -0.15620815 -0.24721704\n",
      "   1.05586302  0.59638811 -0.42954738]\n",
      " [-0.69515541  1.17372696 -0.52472296 -1.12426506  0.09479463  0.19500262\n",
      "  -1.26792092 -0.80186792  0.48744711]\n",
      " [ 0.46924696 -0.29416589  0.75313649 -0.84978495  0.38439996 -0.43688217\n",
      "  -0.85478584  0.59963735  0.5976606 ]\n",
      " [ 0.90494776 -1.13646745  0.39932209  0.9983386  -0.16921527 -0.15237625\n",
      "   1.17062843  0.28106794 -0.73170788]\n",
      " [-0.89263557  1.13004549 -0.39102433 -0.8650989   0.22802424  0.14949419\n",
      "  -1.18713503 -0.68033187  0.56291928]\n",
      " [ 0.01347998  0.31489112  0.54955108 -1.01640879  0.03203239  0.07497559\n",
      "  -1.12616274  0.28330123  0.28496817]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.57973829  1.42259791 -2.67526384 -0.12490484  1.52790548 -2.44593005\n",
      "  -0.77315249]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:66 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.65369678]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 66 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.8191071  -0.98518086  0.33703283  0.97945751 -0.15620815 -0.24721704\n",
      "   1.05586302  0.60505648 -0.42954738]\n",
      " [-0.70333883  1.17372696 -0.53290637 -1.12426506  0.09479463  0.19500262\n",
      "  -1.26792092 -0.81005133  0.48744711]\n",
      " [ 0.47780417 -0.29416589  0.7616937  -0.84978495  0.38439996 -0.43688217\n",
      "  -0.85478584  0.60819456  0.5976606 ]\n",
      " [ 0.91371759 -1.13646745  0.40809193  0.9983386  -0.16921527 -0.15237625\n",
      "   1.17062843  0.28983777 -0.73170788]\n",
      " [-0.90094195  1.13004549 -0.39933071 -0.8650989   0.22802424  0.14949419\n",
      "  -1.18713503 -0.68863825  0.56291928]\n",
      " [ 0.02044925  0.31489112  0.55652035 -1.01640879  0.03203239  0.07497559\n",
      "  -1.12616274  0.2902705   0.28496817]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.5405407   1.45591882 -2.67068013 -0.09116322  1.56043788 -2.44110735\n",
      "  -0.74572213]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:66 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.59862649]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 66 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.82977879 -0.98518086  0.33703283  0.9901292  -0.15620815 -0.24721704\n",
      "   1.05586302  0.60505648 -0.41887569]\n",
      " [-0.71396488  1.17372696 -0.53290637 -1.13489112  0.09479463  0.19500262\n",
      "  -1.26792092 -0.81005133  0.47682105]\n",
      " [ 0.48049036 -0.29416589  0.7616937  -0.84709875  0.38439996 -0.43688217\n",
      "  -0.85478584  0.60819456  0.6003468 ]\n",
      " [ 0.92394933 -1.13646745  0.40809193  1.00857034 -0.16921527 -0.15237625\n",
      "   1.17062843  0.28983777 -0.72147614]\n",
      " [-0.91124502  1.13004549 -0.39933071 -0.87540196  0.22802424  0.14949419\n",
      "  -1.18713503 -0.68863825  0.55261621]\n",
      " [ 0.01287636  0.31489112  0.55652035 -1.02398168  0.03203239  0.07497559\n",
      "  -1.12616274  0.2902705   0.27739529]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.49232112  1.49436049 -2.66067533 -0.06434437  1.59732635 -2.42997247\n",
      "  -0.72983957]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:66 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.11235839]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 66 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.82976832 -0.98518086  0.33703283  0.9901292  -0.15621862 -0.24722751\n",
      "   1.05586302  0.60505648 -0.41888616]\n",
      " [-0.71403858  1.17372696 -0.53290637 -1.13489112  0.09472093  0.19492892\n",
      "  -1.26792092 -0.81005133  0.47674735]\n",
      " [ 0.47937244 -0.29416589  0.7616937  -0.84709875  0.38328204 -0.4380001\n",
      "  -0.85478584  0.60819456  0.59922887]\n",
      " [ 0.92411559 -1.13646745  0.40809193  1.00857034 -0.169049   -0.15220998\n",
      "   1.17062843  0.28983777 -0.72130987]\n",
      " [-0.91127148  1.13004549 -0.39933071 -0.87540196  0.22799778  0.14946773\n",
      "  -1.18713503 -0.68863825  0.55258975]\n",
      " [ 0.01234127  0.31489112  0.55652035 -1.02398168  0.03149729  0.0744405\n",
      "  -1.12616274  0.2902705   0.2768602 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.4979241   1.49154852 -2.66355055 -0.06847151  1.59469152 -2.43280041\n",
      "  -0.73319034]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:66 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.01505795]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 66 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.82979001 -0.98515918  0.33703283  0.9901292  -0.15619694 -0.24720583\n",
      "   1.05586302  0.60505648 -0.41886448]\n",
      " [-0.7140626   1.17370294 -0.53290637 -1.13489112  0.09469692  0.19490491\n",
      "  -1.26792092 -0.81005133  0.47672334]\n",
      " [ 0.47935456 -0.29418377  0.7616937  -0.84709875  0.38326416 -0.43801798\n",
      "  -0.85478584  0.60819456  0.59921099]\n",
      " [ 0.92413978 -1.13644326  0.40809193  1.00857034 -0.16902481 -0.15218579\n",
      "   1.17062843  0.28983777 -0.72128568]\n",
      " [-0.91129492  1.13002205 -0.39933071 -0.87540196  0.22797433  0.14944429\n",
      "  -1.18713503 -0.68863825  0.55256631]\n",
      " [ 0.01232375  0.31487361  0.55652035 -1.02398168  0.03147977  0.07442298\n",
      "  -1.12616274  0.2902705   0.27684268]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.49803576  1.491518   -2.66363688 -0.06854685  1.59466675 -2.4328852\n",
      "  -0.7332652 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:66 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.34176234]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 66 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.82529614 -0.98965305  0.33703283  0.9901292  -0.16069081 -0.2516997\n",
      "   1.05136915  0.60505648 -0.41886448]\n",
      " [-0.70939867  1.17836687 -0.53290637 -1.13489112  0.09936084  0.19956884\n",
      "  -1.26325699 -0.81005133  0.47672334]\n",
      " [ 0.48547714 -0.28806119  0.7616937  -0.84709875  0.38938674 -0.43189539\n",
      "  -0.84866325  0.60819456  0.59921099]\n",
      " [ 0.91859847 -1.14198457  0.40809193  1.00857034 -0.17456613 -0.15772711\n",
      "   1.16508711  0.28983777 -0.72128568]\n",
      " [-0.90608371  1.13523326 -0.39933071 -0.87540196  0.23318555  0.1546555\n",
      "  -1.18192381 -0.68863825  0.55256631]\n",
      " [ 0.01824445  0.32079431  0.55652035 -1.02398168  0.03740048  0.08034368\n",
      "  -1.12024204  0.2902705   0.27684268]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.53647734  1.46761598 -2.67798179 -0.0810954   1.56952209 -2.44658608\n",
      "  -0.74607978]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:66 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.8829437]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 66 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.82617922 -0.98965305  0.33703283  0.99101229 -0.16069081 -0.2516997\n",
      "   1.05225224  0.60505648 -0.41886448]\n",
      " [-0.71016873  1.17836687 -0.53290637 -1.13566118  0.09936084  0.19956884\n",
      "  -1.26402706 -0.81005133  0.47672334]\n",
      " [ 0.48418195 -0.28806119  0.7616937  -0.84839394  0.38938674 -0.43189539\n",
      "  -0.84995845  0.60819456  0.59921099]\n",
      " [ 0.91937553 -1.14198457  0.40809193  1.0093474  -0.17456613 -0.15772711\n",
      "   1.16586417  0.28983777 -0.72128568]\n",
      " [-0.90692077  1.13523326 -0.39933071 -0.87623902  0.23318555  0.1546555\n",
      "  -1.18276087 -0.68863825  0.55256631]\n",
      " [ 0.01701971  0.32079431  0.55652035 -1.02520642  0.03740048  0.08034368\n",
      "  -1.12146678  0.2902705   0.27684268]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.53042821  1.47333954 -2.6777229  -0.07970621  1.57530851 -2.44628903\n",
      "  -0.74543496]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:66 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.72619193]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 66 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.83226008 -0.98965305  0.33703283  0.99101229 -0.15460994 -0.24561884\n",
      "   1.0583331   0.60505648 -0.41886448]\n",
      " [-0.716227    1.17836687 -0.53290637 -1.13566118  0.09330258  0.19351057\n",
      "  -1.27008533 -0.81005133  0.47672334]\n",
      " [ 0.48151605 -0.28806119  0.7616937  -0.84839394  0.38672085 -0.43456129\n",
      "  -0.85262434  0.60819456  0.59921099]\n",
      " [ 0.92538169 -1.14198457  0.40809193  1.0093474  -0.16855996 -0.15172094\n",
      "   1.17187033  0.28983777 -0.72128568]\n",
      " [-0.91296363  1.13523326 -0.39933071 -0.87623902  0.22714269  0.14861264\n",
      "  -1.18880373 -0.68863825  0.55256631]\n",
      " [ 0.01170642  0.32079431  0.55652035 -1.02520642  0.03208719  0.07503039\n",
      "  -1.12678007  0.2902705   0.27684268]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.50320659  1.49545587 -2.67342942 -0.06883598  1.59851015 -2.44209077\n",
      "  -0.73804255]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:66 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.5738516]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 66 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.81747661 -0.98965305  0.33703283  0.97622881 -0.15460994 -0.26040231\n",
      "   1.0583331   0.60505648 -0.43364795]\n",
      " [-0.70133217  1.17836687 -0.53290637 -1.12076635  0.09330258  0.20840541\n",
      "  -1.27008533 -0.81005133  0.49161817]\n",
      " [ 0.48502744 -0.28806119  0.7616937  -0.84488255  0.38672085 -0.4310499\n",
      "  -0.85262434  0.60819456  0.60272238]\n",
      " [ 0.91115705 -1.14198457  0.40809193  0.99512276 -0.16855996 -0.16594559\n",
      "   1.17187033  0.28983777 -0.73551033]\n",
      " [-0.89857366  1.13523326 -0.39933071 -0.86184905  0.22714269  0.16300261\n",
      "  -1.18880373 -0.68863825  0.56695628]\n",
      " [ 0.02212953  0.32079431  0.55652035 -1.01478331  0.03208719  0.08545351\n",
      "  -1.12678007  0.2902705   0.28726579]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.57337313  1.44205124 -2.68990136 -0.10038388  1.54637504 -2.45977208\n",
      "  -0.76192544]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:66 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.73293052]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 66 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.8233213  -0.98380835  0.33703283  0.98207351 -0.15460994 -0.25455762\n",
      "   1.06417779  0.60505648 -0.43364795]\n",
      " [-0.70713242  1.17256662 -0.53290637 -1.1265666   0.09330258  0.20260515\n",
      "  -1.27588558 -0.81005133  0.49161817]\n",
      " [ 0.47944571 -0.29364292  0.7616937  -0.85046428  0.38672085 -0.43663163\n",
      "  -0.85820608  0.60819456  0.60272238]\n",
      " [ 0.9169104  -1.13623122  0.40809193  1.00087611 -0.16855996 -0.16019224\n",
      "   1.17762368  0.28983777 -0.73551033]\n",
      " [-0.90440219  1.12940472 -0.39933071 -0.86767759  0.22714269  0.15717407\n",
      "  -1.19463227 -0.68863825  0.56695628]\n",
      " [ 0.01633417  0.31499894  0.55652035 -1.02057867  0.03208719  0.07965814\n",
      "  -1.13257544  0.2902705   0.28726579]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.54723459  1.46380622 -2.6858823  -0.0970754   1.56871003 -2.45556443\n",
      "  -0.75793275]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:66 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.78584509]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 66 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.82720804 -0.97992162  0.33703283  0.98596024 -0.15460994 -0.25455762\n",
      "   1.06806453  0.60505648 -0.43364795]\n",
      " [-0.71097575  1.16872329 -0.53290637 -1.13040993  0.09330258  0.20260515\n",
      "  -1.27972891 -0.81005133  0.49161817]\n",
      " [ 0.47541208 -0.29767655  0.7616937  -0.85449792  0.38672085 -0.43663163\n",
      "  -0.86223971  0.60819456  0.60272238]\n",
      " [ 0.92073358 -1.13240804  0.40809193  1.00469929 -0.16855996 -0.16019224\n",
      "   1.18144686  0.28983777 -0.73551033]\n",
      " [-0.90832558  1.12548133 -0.39933071 -0.87160098  0.22714269  0.15717407\n",
      "  -1.19855566 -0.68863825  0.56695628]\n",
      " [ 0.01240002  0.3110648   0.55652035 -1.02451282  0.03208719  0.07965814\n",
      "  -1.13650959  0.2902705   0.28726579]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.52921425  1.47945278 -2.68361217 -0.09384905  1.58450385 -2.45308897\n",
      "  -0.75542404]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:66 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.80697187]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 66 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.83038301 -0.97992162  0.34020781  0.98596024 -0.15460994 -0.25138264\n",
      "   1.07123951  0.60505648 -0.43364795]\n",
      " [-0.7138163   1.16872329 -0.53574693 -1.13040993  0.09330258  0.1997646\n",
      "  -1.28256946 -0.81005133  0.49161817]\n",
      " [ 0.47518015 -0.29767655  0.76146178 -0.85449792  0.38672085 -0.43686355\n",
      "  -0.86247163  0.60819456  0.60272238]\n",
      " [ 0.92354166 -1.13240804  0.41090001  1.00469929 -0.16855996 -0.15738415\n",
      "   1.18425495  0.28983777 -0.73551033]\n",
      " [-0.91113484  1.12548133 -0.40213997 -0.87160098  0.22714269  0.15436481\n",
      "  -1.20136492 -0.68863825  0.56695628]\n",
      " [ 0.01067111  0.3110648   0.55479145 -1.02451282  0.03208719  0.07792923\n",
      "  -1.13823849  0.2902705   0.28726579]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.51418042  1.49265911 -2.68226818 -0.0865642   1.59822884 -2.45177889\n",
      "  -0.74970546]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:66 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.17874926]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 67 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.82911408 -0.98119056  0.34020781  0.98469131 -0.15460994 -0.25138264\n",
      "   1.07123951  0.60505648 -0.43491689]\n",
      " [-0.71321823  1.16932136 -0.53574693 -1.12981186  0.09330258  0.1997646\n",
      "  -1.28256946 -0.81005133  0.49221624]\n",
      " [ 0.47542343 -0.29743328  0.76146178 -0.85425464  0.38672085 -0.43686355\n",
      "  -0.86247163  0.60819456  0.60296566]\n",
      " [ 0.92334399 -1.13260572  0.41090001  1.00450161 -0.16855996 -0.15738415\n",
      "   1.18425495  0.28983777 -0.73570801]\n",
      " [-0.91083927  1.12577691 -0.40213997 -0.8713054   0.22714269  0.15436481\n",
      "  -1.20136492 -0.68863825  0.56725185]\n",
      " [ 0.01197682  0.3123705   0.55479145 -1.02320712  0.03208719  0.07792923\n",
      "  -1.13823849  0.2902705   0.2885715 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.52730044  1.48479557 -2.68822673 -0.09288071  1.59147104 -2.45804292\n",
      "  -0.75492186]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:67 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.82891403]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 67 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.83147661 -0.98119056  0.34257034  0.98469131 -0.15460994 -0.25138264\n",
      "   1.07360204  0.60505648 -0.43491689]\n",
      " [-0.7153142   1.16932136 -0.5378429  -1.12981186  0.09330258  0.1997646\n",
      "  -1.28466543 -0.81005133  0.49221624]\n",
      " [ 0.47652008 -0.29743328  0.76255843 -0.85425464  0.38672085 -0.43686355\n",
      "  -0.86137498  0.60819456  0.60296566]\n",
      " [ 0.92545245 -1.13260572  0.41300847  1.00450161 -0.16855996 -0.15738415\n",
      "   1.18636341  0.28983777 -0.73570801]\n",
      " [-0.9129517   1.12577691 -0.4042524  -0.8713054   0.22714269  0.15436481\n",
      "  -1.20347735 -0.68863825  0.56725185]\n",
      " [ 0.01037779  0.3123705   0.55319242 -1.02320712  0.03208719  0.07792923\n",
      "  -1.13983752  0.2902705   0.2885715 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.51516911  1.49576027 -2.68733294 -0.0856926   1.60269771 -2.45713478\n",
      "  -0.75054369]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:67 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.62985326]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 67 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.81552744 -0.99713973  0.32662117  0.98469131 -0.15460994 -0.25138264\n",
      "   1.05765287  0.60505648 -0.43491689]\n",
      " [-0.69906611  1.18556945 -0.52159481 -1.12981186  0.09330258  0.1997646\n",
      "  -1.26841734 -0.81005133  0.49221624]\n",
      " [ 0.47504906 -0.29890429  0.76108741 -0.85425464  0.38672085 -0.43686355\n",
      "  -0.862846    0.60819456  0.60296566]\n",
      " [ 0.90915559 -1.14890258  0.39671161  1.00450161 -0.16855996 -0.15738415\n",
      "   1.17006655  0.28983777 -0.73570801]\n",
      " [-0.89664979  1.14207882 -0.3879505  -0.8713054   0.22714269  0.15436481\n",
      "  -1.18717544 -0.68863825  0.56725185]\n",
      " [ 0.01513834  0.31713105  0.55795297 -1.02320712  0.03208719  0.07792923\n",
      "  -1.13507697  0.2902705   0.2885715 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.58859052  1.4384816  -2.70222736 -0.1238759   1.54389112 -2.47171818\n",
      "  -0.78243839]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:67 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.65646603]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 67 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.82408077 -0.99713973  0.3351745   0.98469131 -0.15460994 -0.25138264\n",
      "   1.05765287  0.61360981 -0.43491689]\n",
      " [-0.70713348  1.18556945 -0.52966218 -1.12981186  0.09330258  0.1997646\n",
      "  -1.26841734 -0.8181187   0.49221624]\n",
      " [ 0.48347197 -0.29890429  0.76951033 -0.85425464  0.38672085 -0.43686355\n",
      "  -0.862846    0.61661747  0.60296566]\n",
      " [ 0.91781935 -1.14890258  0.40537537  1.00450161 -0.16855996 -0.15738415\n",
      "   1.17006655  0.29850153 -0.73570801]\n",
      " [-0.90483977  1.14207882 -0.39614047 -0.8713054   0.22714269  0.15436481\n",
      "  -1.18717544 -0.69682823  0.56725185]\n",
      " [ 0.02211616  0.31713105  0.56493079 -1.02320712  0.03208719  0.07792923\n",
      "  -1.13507697  0.29724832  0.2885715 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.5498539   1.47146969 -2.6977333  -0.09042838  1.5760974  -2.46699071\n",
      "  -0.75519251]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:67 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.6006298]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 67 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.83468843 -0.99713973  0.3351745   0.99529897 -0.15460994 -0.25138264\n",
      "   1.05765287  0.61360981 -0.42430923]\n",
      " [-0.71769661  1.18556945 -0.52966218 -1.14037499  0.09330258  0.1997646\n",
      "  -1.26841734 -0.8181187   0.48165311]\n",
      " [ 0.48621518 -0.29890429  0.76951033 -0.85151144  0.38672085 -0.43686355\n",
      "  -0.862846    0.61661747  0.60570887]\n",
      " [ 0.92800311 -1.14890258  0.40537537  1.01468537 -0.16855996 -0.15738415\n",
      "   1.17006655  0.29850153 -0.72552425]\n",
      " [-0.91509153  1.14207882 -0.39614047 -0.88155716  0.22714269  0.15436481\n",
      "  -1.18717544 -0.69682823  0.5570001 ]\n",
      " [ 0.01458136  0.31713105  0.56493079 -1.03074192  0.03208719  0.07792923\n",
      "  -1.13507697  0.29724832  0.2810367 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.50195471  1.50969336 -2.68783093 -0.06371086  1.61279464 -2.45597883\n",
      "  -0.73943165]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:67 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.10869457]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 67 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.83468266 -0.99713973  0.3351745   0.99529897 -0.15461572 -0.25138842\n",
      "   1.05765287  0.61360981 -0.424315  ]\n",
      " [-0.71777161  1.18556945 -0.52966218 -1.14037499  0.09322758  0.1996896\n",
      "  -1.26841734 -0.8181187   0.48157811]\n",
      " [ 0.48515772 -0.29890429  0.76951033 -0.85151144  0.38566338 -0.43792101\n",
      "  -0.862846    0.61661747  0.6046514 ]\n",
      " [ 0.928165   -1.14890258  0.40537537  1.01468537 -0.16839807 -0.15722225\n",
      "   1.17006655  0.29850153 -0.72536235]\n",
      " [-0.91512234  1.14207882 -0.39614047 -0.88155716  0.22711187  0.154334\n",
      "  -1.18717544 -0.69682823  0.55696928]\n",
      " [ 0.0140688   0.31713105  0.56493079 -1.03074192  0.03157463  0.07741667\n",
      "  -1.13507697  0.29724832  0.28052414]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.50721988  1.507055   -2.69053855 -0.06760284  1.61032437 -2.45864223\n",
      "  -0.74259096]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:67 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.01400137]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 67 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.83470158 -0.9971208   0.3351745   0.99529897 -0.15459679 -0.25136949\n",
      "   1.05765287  0.61360981 -0.42429607]\n",
      " [-0.71779249  1.18554858 -0.52966218 -1.14037499  0.09320671  0.19966873\n",
      "  -1.26841734 -0.8181187   0.48155724]\n",
      " [ 0.4851421  -0.29891991  0.76951033 -0.85151144  0.38564777 -0.43793663\n",
      "  -0.862846    0.61661747  0.60463578]\n",
      " [ 0.92818602 -1.14888156  0.40537537  1.01468537 -0.16837705 -0.15720123\n",
      "   1.17006655  0.29850153 -0.72534133]\n",
      " [-0.91514275  1.14205841 -0.39614047 -0.88155716  0.22709146  0.15431359\n",
      "  -1.18717544 -0.69682823  0.55694887]\n",
      " [ 0.01405347  0.31711572  0.56493079 -1.03074192  0.03155929  0.07740134\n",
      "  -1.13507697  0.29724832  0.2805088 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.50731653  1.50702887 -2.69061355 -0.06766824  1.6103032  -2.45871591\n",
      "  -0.74265598]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:67 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.33448777]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 67 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.8304098  -1.00141258  0.3351745   0.99529897 -0.15888857 -0.25566127\n",
      "   1.05336109  0.61360981 -0.42429607]\n",
      " [-0.71335837  1.1899827  -0.52966218 -1.14037499  0.09764083  0.20410285\n",
      "  -1.26398322 -0.8181187   0.48155724]\n",
      " [ 0.49109939 -0.29296262  0.76951033 -0.85151144  0.39160506 -0.43197934\n",
      "  -0.85688871  0.61661747  0.60463578]\n",
      " [ 0.92291024 -1.15415734  0.40537537  1.01468537 -0.17365283 -0.16247701\n",
      "   1.16479077  0.29850153 -0.72534133]\n",
      " [-0.91018239  1.14701877 -0.39614047 -0.88155716  0.23205182  0.15927395\n",
      "  -1.18221509 -0.69682823  0.55694887]\n",
      " [ 0.01979946  0.32286171  0.56493079 -1.03074192  0.03730528  0.08314733\n",
      "  -1.12933098  0.29724832  0.2805088 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.54454597  1.48394908 -2.70460112 -0.07978394  1.58606383 -2.47208858\n",
      "  -0.75505091]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:67 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.8855235]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 67 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.83125122 -1.00141258  0.3351745   0.99614038 -0.15888857 -0.25566127\n",
      "   1.0542025   0.61360981 -0.42429607]\n",
      " [-0.71409257  1.1899827  -0.52966218 -1.1411092   0.09764083  0.20410285\n",
      "  -1.26471743 -0.8181187   0.48155724]\n",
      " [ 0.48985458 -0.29296262  0.76951033 -0.85275625  0.39160506 -0.43197934\n",
      "  -0.85813352  0.61661747  0.60463578]\n",
      " [ 0.92365115 -1.15415734  0.40537537  1.01542628 -0.17365283 -0.16247701\n",
      "   1.16553168  0.29850153 -0.72534133]\n",
      " [-0.91098053  1.14701877 -0.39614047 -0.88235529  0.23205182  0.15927395\n",
      "  -1.18301322 -0.69682823  0.55694887]\n",
      " [ 0.01863003  0.32286171  0.56493079 -1.03191134  0.03730528  0.08314733\n",
      "  -1.1305004   0.29724832  0.2805088 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.53874363  1.48944274 -2.70435521 -0.07845862  1.59161662 -2.47180649\n",
      "  -0.75444025]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:67 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.72843904]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 67 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.8372528  -1.00141258  0.3351745   0.99614038 -0.15288699 -0.24965969\n",
      "   1.06020408  0.61360981 -0.42429607]\n",
      " [-0.7200693   1.1899827  -0.52966218 -1.1411092   0.0916641   0.19812612\n",
      "  -1.27069415 -0.8181187   0.48155724]\n",
      " [ 0.48722197 -0.29296262  0.76951033 -0.85275625  0.38897245 -0.43461194\n",
      "  -0.86076613  0.61661747  0.60463578]\n",
      " [ 0.92957733 -1.15415734  0.40537537  1.01542628 -0.16772665 -0.15655083\n",
      "   1.17145786  0.29850153 -0.72534133]\n",
      " [-0.91694249  1.14701877 -0.39614047 -0.88235529  0.22608986  0.15331199\n",
      "  -1.18897518 -0.69682823  0.55694887]\n",
      " [ 0.01337373  0.32286171  0.56493079 -1.03191134  0.03204898  0.07789103\n",
      "  -1.13575671  0.29724832  0.2805088 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.51188414  1.51128474 -2.70012526 -0.06773537  1.61450997 -2.46766698\n",
      "  -0.74717121]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:67 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57399137]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 67 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.82246371 -1.00141258  0.3351745   0.98135129 -0.15288699 -0.26444878\n",
      "   1.06020408  0.61360981 -0.43908516]\n",
      " [-0.70517284  1.1899827  -0.52966218 -1.12621274  0.0916641   0.21302258\n",
      "  -1.27069415 -0.8181187   0.49645369]\n",
      " [ 0.49061952 -0.29296262  0.76951033 -0.8493587   0.38897245 -0.4312144\n",
      "  -0.86076613  0.61661747  0.60803333]\n",
      " [ 0.91534144 -1.15415734  0.40537537  1.00119038 -0.16772665 -0.17078673\n",
      "   1.17145786  0.29850153 -0.73957723]\n",
      " [-0.90254407  1.14701877 -0.39614047 -0.86795687  0.22608986  0.1677104\n",
      "  -1.18897518 -0.69682823  0.57134729]\n",
      " [ 0.02377998  0.32286171  0.56493079 -1.02150509  0.03204898  0.08829728\n",
      "  -1.13575671  0.29724832  0.29091505]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.58206184  1.45786334 -2.71660182 -0.09940498  1.56234796 -2.48533771\n",
      "  -0.77108136]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:67 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.73354549]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 67 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.82828727 -0.99558902  0.3351745   0.98717485 -0.15288699 -0.25862522\n",
      "   1.06602764  0.61360981 -0.43908516]\n",
      " [-0.71095508  1.18420047 -0.52966218 -1.13199498  0.0916641   0.20724034\n",
      "  -1.27647639 -0.8181187   0.49645369]\n",
      " [ 0.48507438 -0.29850775  0.76951033 -0.85490384  0.38897245 -0.43675953\n",
      "  -0.86631126  0.61661747  0.60803333]\n",
      " [ 0.92107897 -1.14841981  0.40537537  1.00692792 -0.16772665 -0.1650492\n",
      "   1.17719539  0.29850153 -0.73957723]\n",
      " [-0.90835321  1.14120962 -0.39614047 -0.87376602  0.22608986  0.16190126\n",
      "  -1.19478432 -0.69682823  0.57134729]\n",
      " [ 0.01801241  0.31709415  0.56493079 -1.02727265  0.03204898  0.08252971\n",
      "  -1.14152427  0.29724832  0.29091505]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.5560217   1.4795225  -2.71257638 -0.09614362  1.58457558 -2.48112389\n",
      "  -0.7671344 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:67 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.78762855]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 67 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.8321181  -0.99175819  0.3351745   0.99100568 -0.15288699 -0.25862522\n",
      "   1.06985848  0.61360981 -0.43908516]\n",
      " [-0.7147448   1.18041074 -0.52966218 -1.1357847   0.0916641   0.20724034\n",
      "  -1.28026612 -0.8181187   0.49645369]\n",
      " [ 0.48109816 -0.30248398  0.76951033 -0.85888006  0.38897245 -0.43675953\n",
      "  -0.87028749  0.61661747  0.60803333]\n",
      " [ 0.92484945 -1.14464932  0.40537537  1.0106984  -0.16772665 -0.1650492\n",
      "   1.18096588  0.29850153 -0.73957723]\n",
      " [-0.91222141  1.13734142 -0.39614047 -0.87763421  0.22608986  0.16190126\n",
      "  -1.19865252 -0.69682823  0.57134729]\n",
      " [ 0.01414283  0.31322456  0.56493079 -1.03114224  0.03204898  0.08252971\n",
      "  -1.14539386  0.29724832  0.29091505]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.53826004  1.49494473 -2.71033533 -0.09299421  1.60013807 -2.47868057\n",
      "  -0.76468685]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:67 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.80826072]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 67 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.83525521 -0.99175819  0.33831161  0.99100568 -0.15288699 -0.25548811\n",
      "   1.07299559  0.61360981 -0.43908516]\n",
      " [-0.71755594  1.18041074 -0.53247331 -1.1357847   0.0916641   0.20442921\n",
      "  -1.28307725 -0.8181187   0.49645369]\n",
      " [ 0.48088869 -0.30248398  0.76930086 -0.85888006  0.38897245 -0.436969\n",
      "  -0.87049695  0.61661747  0.60803333]\n",
      " [ 0.92762896 -1.14464932  0.40815488  1.0106984  -0.16772665 -0.16226969\n",
      "   1.18374539  0.29850153 -0.73957723]\n",
      " [-0.91500206  1.13734142 -0.39892112 -0.87763421  0.22608986  0.15912061\n",
      "  -1.20143317 -0.69682823  0.57134729]\n",
      " [ 0.01244703  0.31322456  0.563235   -1.03114224  0.03204898  0.08083392\n",
      "  -1.14708965  0.29724832  0.29091505]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.52340261  1.50799734 -2.70900277 -0.08577508  1.61369736 -2.47738121\n",
      "  -0.75902086]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:67 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.17380677]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 68 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.83406859 -0.99294482  0.33831161  0.98981906 -0.15288699 -0.25548811\n",
      "   1.07299559  0.61360981 -0.44027179]\n",
      " [-0.71700963  1.18095704 -0.53247331 -1.1352384   0.0916641   0.20442921\n",
      "  -1.28307725 -0.8181187   0.497     ]\n",
      " [ 0.4811144  -0.30225827  0.76930086 -0.85865435  0.38897245 -0.436969\n",
      "  -0.87049695  0.61661747  0.60825904]\n",
      " [ 0.9274603  -1.14481798  0.40815488  1.01052974 -0.16772665 -0.16226969\n",
      "   1.18374539  0.29850153 -0.73974589]\n",
      " [-0.91474062  1.13760286 -0.39892112 -0.87737278  0.22608986  0.15912061\n",
      "  -1.20143317 -0.69682823  0.57160873]\n",
      " [ 0.01368634  0.31446387  0.563235   -1.02990293  0.03204898  0.08083392\n",
      "  -1.14708965  0.29724832  0.29215437]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.53588176  1.50053997 -2.71469321 -0.09178875  1.60728905 -2.48335904\n",
      "  -0.76398532]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:68 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.83096172]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 68 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.83637658 -0.99294482  0.3406196   0.98981906 -0.15288699 -0.25548811\n",
      "   1.07530358  0.61360981 -0.44027179]\n",
      " [-0.71905983  1.18095704 -0.53452351 -1.1352384   0.0916641   0.20442921\n",
      "  -1.28512744 -0.8181187   0.497     ]\n",
      " [ 0.48220226 -0.30225827  0.77038872 -0.85865435  0.38897245 -0.436969\n",
      "  -0.86940909  0.61661747  0.60825904]\n",
      " [ 0.92952287 -1.14481798  0.41021745  1.01052974 -0.16772665 -0.16226969\n",
      "   1.18580796  0.29850153 -0.73974589]\n",
      " [-0.91680719  1.13760286 -0.40098769 -0.87737278  0.22608986  0.15912061\n",
      "  -1.20349974 -0.69682823  0.57160873]\n",
      " [ 0.01212451  0.31446387  0.56167316 -1.02990293  0.03204898  0.08083392\n",
      "  -1.14865149  0.29724832  0.29215437]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.52400983  1.51127517 -2.71381935 -0.08473856  1.61827637 -2.48247094\n",
      "  -0.75969721]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:68 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.62686181]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 68 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.82047717 -1.00884422  0.3247202   0.98981906 -0.15288699 -0.25548811\n",
      "   1.05940417  0.61360981 -0.44027179]\n",
      " [-0.70286022  1.19715665 -0.5183239  -1.1352384   0.0916641   0.20442921\n",
      "  -1.26892784 -0.8181187   0.497     ]\n",
      " [ 0.48072149 -0.30373904  0.76890795 -0.85865435  0.38897245 -0.436969\n",
      "  -0.87088986  0.61661747  0.60825904]\n",
      " [ 0.91327263 -1.16106823  0.39396721  1.01052974 -0.16772665 -0.16226969\n",
      "   1.16955771  0.29850153 -0.73974589]\n",
      " [-0.90055094  1.15385911 -0.38473144 -0.87737278  0.22608986  0.15912061\n",
      "  -1.18724349 -0.69682823  0.57160873]\n",
      " [ 0.01681703  0.3191564   0.56636568 -1.02990293  0.03204898  0.08083392\n",
      "  -1.14395897  0.29724832  0.29215437]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.59732323  1.45417027 -2.72881968 -0.12287765  1.55969103 -2.49716417\n",
      "  -0.79160817]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:68 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.65915226]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 68 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.82891856 -1.00884422  0.33316158  0.98981906 -0.15288699 -0.25548811\n",
      "   1.05940417  0.6220512  -0.44027179]\n",
      " [-0.71081543  1.19715665 -0.52627911 -1.1352384   0.0916641   0.20442921\n",
      "  -1.26892784 -0.82607391  0.497     ]\n",
      " [ 0.48901244 -0.30373904  0.7771989  -0.85865435  0.38897245 -0.436969\n",
      "  -0.87088986  0.62490842  0.60825904]\n",
      " [ 0.92183293 -1.16106823  0.40252751  1.01052974 -0.16772665 -0.16226969\n",
      "   1.16955771  0.30706184 -0.73974589]\n",
      " [-0.9086283   1.15385911 -0.3928088  -0.87737278  0.22608986  0.15912061\n",
      "  -1.18724349 -0.70490559  0.57160873]\n",
      " [ 0.02380154  0.3191564   0.57335019 -1.02990293  0.03204898  0.08083392\n",
      "  -1.14395897  0.30423282  0.29215437]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.559034    1.48683328 -2.72441115 -0.08971828  1.59157904 -2.49252758\n",
      "  -0.76454107]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:68 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.60258061]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 68 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.83946328 -1.00884422  0.33316158  1.00036377 -0.15288699 -0.25548811\n",
      "   1.05940417  0.6220512  -0.42972707]\n",
      " [-0.72131665  1.19715665 -0.52627911 -1.14573961  0.0916641   0.20442921\n",
      "  -1.26892784 -0.82607391  0.48649878]\n",
      " [ 0.49181115 -0.30373904  0.7771989  -0.85585565  0.38897245 -0.436969\n",
      "  -0.87088986  0.62490842  0.61105775]\n",
      " [ 0.93196888 -1.16106823  0.40252751  1.02066569 -0.16772665 -0.16226969\n",
      "   1.16955771  0.30706184 -0.72960994]\n",
      " [-0.91882904  1.15385911 -0.3928088  -0.88757352  0.22608986  0.15912061\n",
      "  -1.18724349 -0.70490559  0.56140798]\n",
      " [ 0.0163046   0.3191564   0.57335019 -1.03739987  0.03204898  0.08083392\n",
      "  -1.14395897  0.30423282  0.28465743]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.51144755  1.52484269 -2.71460715 -0.06309972  1.62808777 -2.48163386\n",
      "  -0.74889811]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:68 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.10518263]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 68 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.8394616  -1.00884422  0.33316158  1.00036377 -0.15288867 -0.25548979\n",
      "   1.05940417  0.6220512  -0.42972876]\n",
      " [-0.7213924   1.19715665 -0.52627911 -1.14573961  0.09158834  0.20435345\n",
      "  -1.26892784 -0.82607391  0.48642303]\n",
      " [ 0.49081084 -0.30373904  0.7771989  -0.85585565  0.38797214 -0.43796931\n",
      "  -0.87088986  0.62490842  0.61005744]\n",
      " [ 0.93212619 -1.16106823  0.40252751  1.02066569 -0.16756934 -0.16211238\n",
      "   1.16955771  0.30706184 -0.72945263]\n",
      " [-0.91886343  1.15385911 -0.3928088  -0.88757352  0.22605548  0.15908623\n",
      "  -1.18724349 -0.70490559  0.5613736 ]\n",
      " [ 0.0158138   0.3191564   0.57335019 -1.03739987  0.03155818  0.08034312\n",
      "  -1.14395897  0.30423282  0.28416662]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.51639741  1.52236608 -2.71715788 -0.06677107  1.62577058 -2.48414317\n",
      "  -0.75187797]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:68 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.0130301]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 68 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.83947814 -1.00882768  0.33316158  1.00036377 -0.15287213 -0.25547325\n",
      "   1.05940417  0.6220512  -0.42971222]\n",
      " [-0.72141057  1.19713848 -0.52627911 -1.14573961  0.09157018  0.20433529\n",
      "  -1.26892784 -0.82607391  0.48640486]\n",
      " [ 0.49079718 -0.30375269  0.7771989  -0.85585565  0.38795849 -0.43798296\n",
      "  -0.87088986  0.62490842  0.61004378]\n",
      " [ 0.93214447 -1.16104994  0.40252751  1.02066569 -0.16755106 -0.1620941\n",
      "   1.16955771  0.30706184 -0.72943435]\n",
      " [-0.91888121  1.15384133 -0.3928088  -0.88757352  0.22603769  0.15906844\n",
      "  -1.18724349 -0.70490559  0.56135582]\n",
      " [ 0.01580036  0.31914296  0.57335019 -1.03739987  0.03154474  0.08032968\n",
      "  -1.14395897  0.30423282  0.28415319]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.5164812   1.52234367 -2.71722312 -0.06682792  1.62575247 -2.48420729\n",
      "  -0.75193453]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:68 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.32740904]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 68 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.83537918 -1.01292663  0.33316158  1.00036377 -0.15697109 -0.25957221\n",
      "   1.05530522  0.6220512  -0.42971222]\n",
      " [-0.71719479  1.20135426 -0.52627911 -1.14573961  0.09578596  0.20855106\n",
      "  -1.26471206 -0.82607391  0.48640486]\n",
      " [ 0.49659487 -0.29795501  0.7771989  -0.85585565  0.39375617 -0.43218528\n",
      "  -0.86509218  0.62490842  0.61004378]\n",
      " [ 0.92712139 -1.16607302  0.40252751  1.02066569 -0.17257414 -0.16711718\n",
      "   1.16453463  0.30706184 -0.72943435]\n",
      " [-0.91415958  1.15856296 -0.3928088  -0.88757352  0.23075933  0.16379008\n",
      "  -1.18252185 -0.70490559  0.56135582]\n",
      " [ 0.02137775  0.32472035  0.57335019 -1.03739987  0.03712213  0.08590707\n",
      "  -1.13838158  0.30423282  0.28415319]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.55253095  1.50005946 -2.7308563  -0.07852052  1.60238608 -2.49725328\n",
      "  -0.76391914]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:68 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.88801838]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 68 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.83618131 -1.01292663  0.33316158  1.0011659  -0.15697109 -0.25957221\n",
      "   1.05610734  0.6220512  -0.42971222]\n",
      " [-0.71789516  1.20135426 -0.52627911 -1.14643999  0.09578596  0.20855106\n",
      "  -1.26541244 -0.82607391  0.48640486]\n",
      " [ 0.49539805 -0.29795501  0.7771989  -0.85705247  0.39375617 -0.43218528\n",
      "  -0.866289    0.62490842  0.61004378]\n",
      " [ 0.92782818 -1.16607302  0.40252751  1.02137248 -0.17257414 -0.16711718\n",
      "   1.16524142  0.30706184 -0.72943435]\n",
      " [-0.91492099  1.15856296 -0.3928088  -0.88833493  0.23075933  0.16379008\n",
      "  -1.18328326 -0.70490559  0.56135582]\n",
      " [ 0.02026073  0.32472035  0.57335019 -1.03851689  0.03712213  0.08590707\n",
      "  -1.13949859  0.30423282  0.28415319]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.54696313  1.50533443 -2.73062255 -0.07725567  1.6077167  -2.49698523\n",
      "  -0.76334053]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:68 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.73068746]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 68 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.84210362 -1.01292663  0.33316158  1.0011659  -0.15104878 -0.2536499\n",
      "   1.06202966  0.6220512  -0.42971222]\n",
      " [-0.72379047  1.20135426 -0.52627911 -1.14643999  0.08989065  0.20265576\n",
      "  -1.27130775 -0.82607391  0.48640486]\n",
      " [ 0.49279697 -0.29795501  0.7771989  -0.85705247  0.39115509 -0.43478635\n",
      "  -0.86889008  0.62490842  0.61004378]\n",
      " [ 0.93367437 -1.16607302  0.40252751  1.02137248 -0.16672795 -0.16127099\n",
      "   1.17108761  0.30706184 -0.72943435]\n",
      " [-0.92080212  1.15856296 -0.3928088  -0.88833493  0.22487819  0.15790895\n",
      "  -1.1891644  -0.70490559  0.56135582]\n",
      " [ 0.01506155  0.32472035  0.57335019 -1.03851689  0.03192295  0.08070789\n",
      "  -1.14469777  0.30423282  0.28415319]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.52046503  1.52690201 -2.72645621 -0.06668094  1.63030311 -2.49290483\n",
      "  -0.75619432]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:68 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57406153]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 68 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.82731098 -1.01292663  0.33316158  0.98637327 -0.15104878 -0.26844253\n",
      "   1.06202966  0.6220512  -0.44450485]\n",
      " [-0.70889436  1.20135426 -0.52627911 -1.13154388  0.08989065  0.21755187\n",
      "  -1.27130775 -0.82607391  0.50130097]\n",
      " [ 0.49608366 -0.29795501  0.7771989  -0.85376578  0.39115509 -0.43149966\n",
      "  -0.86889008  0.62490842  0.61333047]\n",
      " [ 0.9194294  -1.16607302  0.40252751  1.00712751 -0.16672795 -0.17551596\n",
      "   1.17108761  0.30706184 -0.74367932]\n",
      " [-0.90639749  1.15856296 -0.3928088  -0.8739303   0.22487819  0.17231358\n",
      "  -1.1891644  -0.70490559  0.57576045]\n",
      " [ 0.0254494   0.32472035  0.57335019 -1.02812904  0.03192295  0.09109574\n",
      "  -1.14469777  0.30423282  0.29454104]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.59064832  1.47347035 -2.74293819 -0.09846629  1.57812047 -2.51056591\n",
      "  -0.78013072]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:68 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.73413255]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 68 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.83311436 -1.00712326  0.33316158  0.99217664 -0.15104878 -0.26263915\n",
      "   1.06783303  0.6220512  -0.44450485]\n",
      " [-0.71465936  1.19558926 -0.52627911 -1.13730887  0.08989065  0.21178687\n",
      "  -1.27707274 -0.82607391  0.50130097]\n",
      " [ 0.49057473 -0.30346395  0.7771989  -0.85927471  0.39115509 -0.4370086\n",
      "  -0.87439901  0.62490842  0.61333047]\n",
      " [ 0.92515176 -1.16035065  0.40252751  1.01284987 -0.16672795 -0.1697936\n",
      "   1.17680998  0.30706184 -0.74367932]\n",
      " [-0.91218805  1.1527724  -0.3928088  -0.87972086  0.22487819  0.16652302\n",
      "  -1.19495496 -0.70490559  0.57576045]\n",
      " [ 0.01970894  0.31897989  0.57335019 -1.0338695   0.03192295  0.08535528\n",
      "  -1.15043823  0.30423282  0.29454104]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.56470206  1.49503757 -2.73890607 -0.09525161  1.60024524 -2.50634566\n",
      "  -0.77622871]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:68 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.78937877]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 68 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.83689061 -1.00334701  0.33316158  0.99595289 -0.15104878 -0.26263915\n",
      "   1.07160928  0.6220512  -0.44450485]\n",
      " [-0.71839669  1.19185193 -0.52627911 -1.14104621  0.08989065  0.21178687\n",
      "  -1.28081007 -0.82607391  0.50130097]\n",
      " [ 0.48665499 -0.30738369  0.7771989  -0.86319445  0.39115509 -0.4370086\n",
      "  -0.87831875  0.62490842  0.61333047]\n",
      " [ 0.92887068 -1.15663174  0.40252751  1.01656879 -0.16672795 -0.1697936\n",
      "   1.18052889  0.30706184 -0.74367932]\n",
      " [-0.9160023   1.14895815 -0.3928088  -0.88353511  0.22487819  0.16652302\n",
      "  -1.19876921 -0.70490559  0.57576045]\n",
      " [ 0.0159027   0.31517365  0.57335019 -1.03767574  0.03192295  0.08535528\n",
      "  -1.15424448  0.30423282  0.29454104]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.54719312  1.51024056 -2.73669348 -0.09217733  1.61558188 -2.50393378\n",
      "  -0.7738407 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:68 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.80949359]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 68 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.83999184 -1.00334701  0.33626282  0.99595289 -0.15104878 -0.25953792\n",
      "   1.07471051  0.6220512  -0.44450485]\n",
      " [-0.72118006  1.19185193 -0.52906248 -1.14104621  0.08989065  0.2090035\n",
      "  -1.28359344 -0.82607391  0.50130097]\n",
      " [ 0.48646608 -0.30738369  0.77701    -0.86319445  0.39115509 -0.4371975\n",
      "  -0.87850765  0.62490842  0.61333047]\n",
      " [ 0.93162313 -1.15663174  0.40527997  1.01656879 -0.16672795 -0.16704114\n",
      "   1.18328135  0.30706184 -0.74367932]\n",
      " [-0.91875594  1.14895815 -0.39556243 -0.88353511  0.22487819  0.16376939\n",
      "  -1.20152285 -0.70490559  0.57576045]\n",
      " [ 0.01423885  0.31517365  0.57168634 -1.03767574  0.03192295  0.08369144\n",
      "  -1.15590832  0.30423282  0.29454104]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.53250377  1.52314629 -2.73537152 -0.08502165  1.62898308 -2.50264436\n",
      "  -0.7682244 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:68 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.16897359]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 69 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.83888306 -1.0044558   0.33626282  0.99484411 -0.15104878 -0.25953792\n",
      "   1.07471051  0.6220512  -0.44561364]\n",
      " [-0.72068216  1.19234982 -0.52906248 -1.14054831  0.08989065  0.2090035\n",
      "  -1.28359344 -0.82607391  0.50179887]\n",
      " [ 0.48667576 -0.30717402  0.77701    -0.86298478  0.39115509 -0.4371975\n",
      "  -0.87850765  0.62490842  0.61354014]\n",
      " [ 0.9314812  -1.15677367  0.40527997  1.01642686 -0.16672795 -0.16704114\n",
      "   1.18328135  0.30706184 -0.74382125]\n",
      " [-0.91852621  1.14918788 -0.39556243 -0.88330538  0.22487819  0.16376939\n",
      "  -1.20152285 -0.70490559  0.57599018]\n",
      " [ 0.01541488  0.31634968  0.57168634 -1.03649971  0.03192295  0.08369144\n",
      "  -1.15590832  0.30423282  0.29571707]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.54436754  1.51607758 -2.74080313 -0.09074368  1.62290921 -2.50834628\n",
      "  -0.77294642]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:69 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.8329405]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 69 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.84113901 -1.0044558   0.33851877  0.99484411 -0.15104878 -0.25953792\n",
      "   1.07696647  0.6220512  -0.44561364]\n",
      " [-0.72268869  1.19234982 -0.53106901 -1.14054831  0.08989065  0.2090035\n",
      "  -1.28559997 -0.82607391  0.50179887]\n",
      " [ 0.4877545  -0.30717402  0.77808874 -0.86298478  0.39115509 -0.4371975\n",
      "  -0.87742891  0.62490842  0.61354014]\n",
      " [ 0.93349994 -1.15677367  0.4072987   1.01642686 -0.16672795 -0.16704114\n",
      "   1.18530008  0.30706184 -0.74382125]\n",
      " [-0.920549    1.14918788 -0.39758523 -0.88330538  0.22487819  0.16376939\n",
      "  -1.20354564 -0.70490559  0.57599018]\n",
      " [ 0.01388884  0.31634968  0.5701603  -1.03649971  0.03192295  0.08369144\n",
      "  -1.15743436  0.30423282  0.29571707]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.53274432  1.52659238 -2.7399482  -0.08382646  1.63366691 -2.5074772\n",
      "  -0.76874448]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:69 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.62377298]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 69 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.82529292 -1.02030189  0.32267268  0.99484411 -0.15104878 -0.25953792\n",
      "   1.06112037  0.6220512  -0.44561364]\n",
      " [-0.70654167  1.20849684 -0.51492199 -1.14054831  0.08989065  0.2090035\n",
      "  -1.26945295 -0.82607391  0.50179887]\n",
      " [ 0.48627038 -0.30865813  0.77660462 -0.86298478  0.39115509 -0.4371975\n",
      "  -0.87891303  0.62490842  0.61354014]\n",
      " [ 0.91730045 -1.17297316  0.39109921  1.01642686 -0.16672795 -0.16704114\n",
      "   1.16910059  0.30706184 -0.74382125]\n",
      " [-0.90434262  1.16539426 -0.38137884 -0.88330538  0.22487819  0.16376939\n",
      "  -1.18733926 -0.70490559  0.57599018]\n",
      " [ 0.01851534  0.32097618  0.5747868  -1.03649971  0.03192295  0.08369144\n",
      "  -1.15280786  0.30423282  0.29571707]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.60593792  1.46967155 -2.75505243 -0.12190901  1.57531177 -2.52227823\n",
      "  -0.80066367]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:69 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.66176017]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 69 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.83362536 -1.02030189  0.33100512  0.99484411 -0.15104878 -0.25953792\n",
      "   1.06112037  0.63038364 -0.44561364]\n",
      " [-0.71438842  1.20849684 -0.52276874 -1.14054831  0.08989065  0.2090035\n",
      "  -1.26945295 -0.83392066  0.50179887]\n",
      " [ 0.49443171 -0.30865813  0.78476595 -0.86298478  0.39115509 -0.4371975\n",
      "  -0.87891303  0.63306974  0.61354014]\n",
      " [ 0.92575982 -1.17297316  0.39955858  1.01642686 -0.16672795 -0.16704114\n",
      "   1.16910059  0.31552121 -0.74382125]\n",
      " [-0.91231098  1.16539426 -0.3893472  -0.88330538  0.22487819  0.16376939\n",
      "  -1.18733926 -0.71287395  0.57599018]\n",
      " [ 0.02550466  0.32097618  0.58177613 -1.03649971  0.03192295  0.08369144\n",
      "  -1.15280786  0.31122215  0.29571707]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.56808319  1.50201689 -2.75072558 -0.08903208  1.60688898 -2.51772845\n",
      "  -0.77377006]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:69 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.60447944]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 69 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.84410826 -1.02030189  0.33100512  1.00532701 -0.15104878 -0.25953792\n",
      "   1.06112037  0.63038364 -0.43513073]\n",
      " [-0.72482876  1.20849684 -0.52276874 -1.15098865  0.08989065  0.2090035\n",
      "  -1.26945295 -0.83392066  0.49135853]\n",
      " [ 0.49728451 -0.30865813  0.78476595 -0.86013197  0.39115509 -0.4371975\n",
      "  -0.87891303  0.63306974  0.61639295]\n",
      " [ 0.93584822 -1.17297316  0.39955858  1.02651526 -0.16672795 -0.16704114\n",
      "   1.16910059  0.31552121 -0.73373285]\n",
      " [-0.92246111  1.16539426 -0.3893472  -0.89345551  0.22487819  0.16376939\n",
      "  -1.18733926 -0.71287395  0.56584005]\n",
      " [ 0.01804531  0.32097618  0.58177613 -1.04395907  0.03192295  0.08369144\n",
      "  -1.15280786  0.31122215  0.28825771]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.52080186  1.53981591 -2.74101599 -0.06250998  1.6432121  -2.5069482\n",
      "  -0.75824128]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:69 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.10181377]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 69 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.84411014 -1.02030189  0.33100512  1.00532701 -0.1510469  -0.25953605\n",
      "   1.06112037  0.63038364 -0.43512886]\n",
      " [-0.72490482  1.20849684 -0.52276874 -1.15098865  0.08981459  0.20892744\n",
      "  -1.26945295 -0.83392066  0.49128247]\n",
      " [ 0.49633823 -0.30865813  0.78476595 -0.86013197  0.39020881 -0.43814379\n",
      "  -0.87891303  0.63306974  0.61544667]\n",
      " [ 0.93600078 -1.17297316  0.39955858  1.02651526 -0.16657539 -0.16688858\n",
      "   1.16910059  0.31552121 -0.73358029]\n",
      " [-0.92249837  1.16539426 -0.3893472  -0.89345551  0.22484093  0.16373212\n",
      "  -1.18733926 -0.71287395  0.56580279]\n",
      " [ 0.01757549  0.32097618  0.58177613 -1.04395907  0.03145313  0.08322162\n",
      "  -1.15280786  0.31122215  0.28778789]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.52545718  1.53749013 -2.74341976 -0.06597422  1.64103744 -2.50931313\n",
      "  -0.76105282]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:69 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.01213636]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 69 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.84412461 -1.02028742  0.33100512  1.00532701 -0.15103244 -0.25952158\n",
      "   1.06112037  0.63038364 -0.43511439]\n",
      " [-0.72492065  1.20848101 -0.52276874 -1.15098865  0.08979876  0.20891161\n",
      "  -1.26945295 -0.83392066  0.49126664]\n",
      " [ 0.49632628 -0.30867008  0.78476595 -0.86013197  0.39019686 -0.43815574\n",
      "  -0.87891303  0.63306974  0.61543472]\n",
      " [ 0.93601671 -1.17295723  0.39955858  1.02651526 -0.16655946 -0.16687265\n",
      "   1.16910059  0.31552121 -0.73356436]\n",
      " [-0.92251389  1.16537874 -0.3893472  -0.89345551  0.22482541  0.1637166\n",
      "  -1.18733926 -0.71287395  0.56578727]\n",
      " [ 0.01756371  0.32096439  0.58177613 -1.04395907  0.03144135  0.08320983\n",
      "  -1.15280786  0.31122215  0.28777611]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.52552993  1.53747087 -2.74347661 -0.06602372  1.64102191 -2.50936901\n",
      "  -0.76110209]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:69 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.32052356]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 69 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.84020951 -1.02420252  0.33100512  1.00532701 -0.15494753 -0.26343668\n",
      "   1.05720527  0.63038364 -0.43511439]\n",
      " [-0.72091219  1.21248947 -0.52276874 -1.15098865  0.09380722  0.21292007\n",
      "  -1.2654445  -0.83392066  0.49126664]\n",
      " [ 0.5019698  -0.30302656  0.78476595 -0.86013197  0.39584038 -0.43251221\n",
      "  -0.87326951  0.63306974  0.61543472]\n",
      " [ 0.93123393 -1.17774002  0.39955858  1.02651526 -0.17134224 -0.17165543\n",
      "   1.16431781  0.31552121 -0.73356436]\n",
      " [-0.91801926  1.16987337 -0.3893472  -0.89345551  0.22932004  0.16821123\n",
      "  -1.18284463 -0.71287395  0.56578727]\n",
      " [ 0.02297844  0.32637913  0.58177613 -1.04395907  0.03685608  0.08862457\n",
      "  -1.14739313  0.31122215  0.28777611]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.56043306  1.5159557  -2.75675926 -0.07730356  1.61849664 -2.52209082\n",
      "  -0.77268625]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:69 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.89043239]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 69 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.84097457 -1.02420252  0.33100512  1.00609208 -0.15494753 -0.26343668\n",
      "   1.05797034  0.63038364 -0.43511439]\n",
      " [-0.72158064  1.21248947 -0.52276874 -1.1516571   0.09380722  0.21292007\n",
      "  -1.26611294 -0.83392066  0.49126664]\n",
      " [ 0.50081873 -0.30302656  0.78476595 -0.86128304  0.39584038 -0.43251221\n",
      "  -0.87442057  0.63306974  0.61543472]\n",
      " [ 0.93190849 -1.17774002  0.39955858  1.02718983 -0.17134224 -0.17165543\n",
      "   1.16499238  0.31552121 -0.73356436]\n",
      " [-0.918746    1.16987337 -0.3893472  -0.89418225  0.22932004  0.16821123\n",
      "  -1.18357137 -0.71287395  0.56578727]\n",
      " [ 0.0219111   0.32637913  0.58177613 -1.04502641  0.03685608  0.08862457\n",
      "  -1.14846047  0.31122215  0.28777611]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.55508821  1.52102251 -2.75653695 -0.07609599  1.6236159  -2.52183596\n",
      "  -0.77213772]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:69 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.73293575]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 69 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.84681769 -1.02420252  0.33100512  1.00609208 -0.14910442 -0.25759357\n",
      "   1.06381345  0.63038364 -0.43511439]\n",
      " [-0.72739472  1.21248947 -0.52276874 -1.1516571   0.08799314  0.20710599\n",
      "  -1.27192702 -0.83392066  0.49126664]\n",
      " [ 0.49824755 -0.30302656  0.78476595 -0.86128304  0.3932692  -0.43508339\n",
      "  -0.87699175  0.63306974  0.61543472]\n",
      " [ 0.93767476 -1.17774002  0.39955858  1.02718983 -0.16557597 -0.16588917\n",
      "   1.17075864  0.31552121 -0.73356436]\n",
      " [-0.92454645  1.16987337 -0.3893472  -0.89418225  0.22351959  0.16241078\n",
      "  -1.18937182 -0.71287395  0.56578727]\n",
      " [ 0.01676917  0.32637913  0.58177613 -1.04502641  0.03171414  0.08348263\n",
      "  -1.15360241  0.31122215  0.28777611]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.52895051  1.54231578 -2.75243418 -0.06567106  1.64589684 -2.5178149\n",
      "  -0.7651137 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:69 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57406153]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 69 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.83202357 -1.02420252  0.33100512  0.99129797 -0.14910442 -0.27238768\n",
      "   1.06381345  0.63038364 -0.4499085 ]\n",
      " [-0.71250092  1.21248947 -0.52276874 -1.1367633   0.08799314  0.22199979\n",
      "  -1.27192702 -0.83392066  0.50616044]\n",
      " [ 0.5014263  -0.30302656  0.78476595 -0.85810429  0.3932692  -0.43190464\n",
      "  -0.87699175  0.63306974  0.61861347]\n",
      " [ 0.9234229  -1.17774002  0.39955858  1.01293797 -0.16557597 -0.18014103\n",
      "   1.17075864  0.31552121 -0.74781622]\n",
      " [-0.91013783  1.16987337 -0.3893472  -0.87977363  0.22351959  0.1768194\n",
      "  -1.18937182 -0.71287395  0.58019589]\n",
      " [ 0.0271371   0.32637913  0.58177613 -1.03465848  0.03171414  0.09385056\n",
      "  -1.15360241  0.31122215  0.29814404]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.5991338   1.48888037 -2.76892235 -0.09756625  1.5936999  -2.53546729\n",
      "  -0.78907527]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:69 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.73469318]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 69 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.83780766 -1.01841843  0.33100512  0.99708205 -0.14910442 -0.26660359\n",
      "   1.06959754  0.63038364 -0.4499085 ]\n",
      " [-0.71824942  1.20674097 -0.52276874 -1.1425118   0.08799314  0.21625129\n",
      "  -1.27767552 -0.83392066  0.50616044]\n",
      " [ 0.4959532  -0.30849966  0.78476595 -0.86357739  0.3932692  -0.43737774\n",
      "  -0.88246485  0.63306974  0.61861347]\n",
      " [ 0.92913072 -1.17203219  0.39955858  1.01864579 -0.16557597 -0.1744332\n",
      "   1.17646646  0.31552121 -0.74781622]\n",
      " [-0.91591057  1.16410063 -0.3893472  -0.88554637  0.22351959  0.17104666\n",
      "  -1.19514456 -0.71287395  0.58019589]\n",
      " [ 0.0214231   0.32066513  0.58177613 -1.04037247  0.03171414  0.08813656\n",
      "  -1.1593164   0.31122215  0.29814404]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.57327712  1.51035933 -2.76488327 -0.09439779  1.61572609 -2.53124035\n",
      "  -0.78521747]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:69 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.79109762]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 69 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.84153056 -1.01469553  0.33100512  1.00080495 -0.14910442 -0.26660359\n",
      "   1.07332044  0.63038364 -0.4499085 ]\n",
      " [-0.72193549  1.20305489 -0.52276874 -1.14619787  0.08799314  0.21625129\n",
      "  -1.28136159 -0.83392066  0.50616044]\n",
      " [ 0.49208908 -0.31236378  0.78476595 -0.86744151  0.3932692  -0.43737774\n",
      "  -0.88632897  0.63306974  0.61861347]\n",
      " [ 0.93279913 -1.16836379  0.39955858  1.02231419 -0.16557597 -0.1744332\n",
      "   1.18013487  0.31552121 -0.74781622]\n",
      " [-0.91967206  1.16033914 -0.3893472  -0.88930787  0.22351959  0.17104666\n",
      "  -1.19890606 -0.71287395  0.58019589]\n",
      " [ 0.01767904  0.31692107  0.58177613 -1.04411654  0.03171414  0.08813656\n",
      "  -1.16306046  0.31122215  0.29814404]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.55601529  1.52534789 -2.76269856 -0.09139683  1.63084206 -2.52885924\n",
      "  -0.78288742]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:69 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.81067306]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 69 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.8445978  -1.01469553  0.33407235  1.00080495 -0.14910442 -0.26353635\n",
      "   1.07638767  0.63038364 -0.4499085 ]\n",
      " [-0.72469265  1.20305489 -0.5255259  -1.14619787  0.08799314  0.21349413\n",
      "  -1.28411875 -0.83392066  0.50616044]\n",
      " [ 0.491919   -0.31236378  0.78459586 -0.86744151  0.3932692  -0.43754783\n",
      "  -0.88649906  0.63306974  0.61861347]\n",
      " [ 0.93552598 -1.16836379  0.40228543  1.02231419 -0.16557597 -0.17170636\n",
      "   1.18286172  0.31552121 -0.74781622]\n",
      " [-0.92240019  1.16033914 -0.39207533 -0.88930787  0.22351959  0.16831854\n",
      "  -1.20163418 -0.71287395  0.58019589]\n",
      " [ 0.01604604  0.31692107  0.58014313 -1.04411654  0.03171414  0.08650357\n",
      "  -1.16469346  0.31122215  0.29814404]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.54148613  1.53811324 -2.76138638 -0.0843024   1.64409237 -2.52757901\n",
      "  -0.77731806]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:69 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.16424853]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 70 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.84356257 -1.01573076  0.33407235  0.99976973 -0.14910442 -0.26353635\n",
      "   1.07638767  0.63038364 -0.45094373]\n",
      " [-0.72423996  1.20350758 -0.5255259  -1.14574518  0.08799314  0.21349413\n",
      "  -1.28411875 -0.83392066  0.50661312]\n",
      " [ 0.492114   -0.31216878  0.78459586 -0.86724651  0.3932692  -0.43754783\n",
      "  -0.88649906  0.63306974  0.61880847]\n",
      " [ 0.93540862 -1.16848115  0.40228543  1.02219683 -0.16557597 -0.17170636\n",
      "   1.18286172  0.31552121 -0.74793358]\n",
      " [-0.92219985  1.16053947 -0.39207533 -0.88910753  0.22351959  0.16831854\n",
      "  -1.20163418 -0.71287395  0.58039623]\n",
      " [ 0.01716176  0.31803679  0.58014313 -1.04300081  0.03171414  0.08650357\n",
      "  -1.16469346  0.31122215  0.29925976]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.5527594   1.53141617 -2.76656836 -0.08974388  1.63833834 -2.53301515\n",
      "  -0.78180699]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:70 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.83485392]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 70 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.84576885 -1.01573076  0.33627863  0.99976973 -0.14910442 -0.26353635\n",
      "   1.07859395  0.63038364 -0.45094373]\n",
      " [-0.7262048   1.20350758 -0.52749073 -1.14574518  0.08799314  0.21349413\n",
      "  -1.28608359 -0.83392066  0.50661312]\n",
      " [ 0.49318337 -0.31216878  0.78566523 -0.86724651  0.3932692  -0.43754783\n",
      "  -0.88542969  0.63306974  0.61880847]\n",
      " [ 0.93738544 -1.16848115  0.40426226  1.02219683 -0.16557597 -0.17170636\n",
      "   1.18483855  0.31552121 -0.74793358]\n",
      " [-0.92418085  1.16053947 -0.39405633 -0.88910753  0.22351959  0.16831854\n",
      "  -1.20361518 -0.71287395  0.58039623]\n",
      " [ 0.01567019  0.31803679  0.57865156 -1.04300081  0.03171414  0.08650357\n",
      "  -1.16618503  0.31122215  0.29925976]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.54137482  1.54171918 -2.7657314  -0.08295488  1.64887557 -2.53216415\n",
      "  -0.7776876 ]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:70 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.62058999]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 70 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.82997967 -1.03151994  0.32048945  0.99976973 -0.14910442 -0.26353635\n",
      "   1.06280478  0.63038364 -0.45094373]\n",
      " [-0.71011457  1.21959781 -0.51140051 -1.14574518  0.08799314  0.21349413\n",
      "  -1.26999336 -0.83392066  0.50661312]\n",
      " [ 0.49170175 -0.3136504   0.78418361 -0.86724651  0.3932692  -0.43754783\n",
      "  -0.8869113   0.63306974  0.61880847]\n",
      " [ 0.92124092 -1.18462567  0.38811773  1.02219683 -0.16557597 -0.17170636\n",
      "   1.16869402  0.31552121 -0.74793358]\n",
      " [-0.90802863  1.1766917  -0.3779041  -0.88910753  0.22351959  0.16831854\n",
      "  -1.18746296 -0.71287395  0.58039623]\n",
      " [ 0.02023255  0.32259914  0.58321391 -1.04300081  0.03171414  0.08650357\n",
      "  -1.16162268  0.31122215  0.29925976]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.61443628  1.48499298 -2.78093724 -0.12096885  1.59075991 -2.54707066\n",
      "  -0.80960682]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:70 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.66429445]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 70 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.83820603 -1.03151994  0.32871582  0.99976973 -0.14910442 -0.26353635\n",
      "   1.06280478  0.63861    -0.45094373]\n",
      " [-0.71785635  1.21959781 -0.51914228 -1.14574518  0.08799314  0.21349413\n",
      "  -1.26999336 -0.84166243  0.50661312]\n",
      " [ 0.49973577 -0.3136504   0.79221764 -0.86724651  0.3932692  -0.43754783\n",
      "  -0.8869113   0.64110376  0.61880847]\n",
      " [ 0.92960178 -1.18462567  0.3964786   1.02219683 -0.16557597 -0.17170636\n",
      "   1.16869402  0.32388207 -0.74793358]\n",
      " [-0.91589141  1.1766917  -0.38576688 -0.88910753  0.22351959  0.16831854\n",
      "  -1.18746296 -0.72073673  0.58039623]\n",
      " [ 0.0272248   0.32259914  0.59020616 -1.04300081  0.03171414  0.08650357\n",
      "  -1.16162268  0.3182144   0.29925976]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.57700388  1.51702771 -2.7766885  -0.08836891  1.62203344 -2.54260391\n",
      "  -0.78288179]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:70 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.60632704]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 70 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.8486283  -1.03151994  0.32871582  1.010192   -0.14910442 -0.26353635\n",
      "   1.06280478  0.63861    -0.44052146]\n",
      " [-0.7282369   1.21959781 -0.51914228 -1.15612574  0.08799314  0.21349413\n",
      "  -1.26999336 -0.84166243  0.49623257]\n",
      " [ 0.50264139 -0.3136504   0.79221764 -0.8643409   0.3932692  -0.43754783\n",
      "  -0.8869113   0.64110376  0.62171408]\n",
      " [ 0.93964298 -1.18462567  0.3964786   1.03223803 -0.16557597 -0.17170636\n",
      "   1.16869402  0.32388207 -0.73789238]\n",
      " [-0.92599139  1.1766917  -0.38576688 -0.89920752  0.22351959  0.16831854\n",
      "  -1.18746296 -0.72073673  0.57029624]\n",
      " [ 0.01980271  0.32259914  0.59020616 -1.0504229   0.03171414  0.08650357\n",
      "  -1.16162268  0.3182144   0.29183768]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.53002008  1.55462029 -2.7670695  -0.06194072  1.65817393 -2.5319326\n",
      "  -0.76746356]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:70 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.09857977]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 70 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.84863327 -1.03151994  0.32871582  1.010192   -0.14909946 -0.26353139\n",
      "   1.06280478  0.63861    -0.44051649]\n",
      " [-0.72831289  1.21959781 -0.51914228 -1.15612574  0.08791715  0.21341814\n",
      "  -1.26999336 -0.84166243  0.49615658]\n",
      " [ 0.50174618 -0.3136504   0.79221764 -0.8643409   0.39237399 -0.43844304\n",
      "  -0.8869113   0.64110376  0.62081887]\n",
      " [ 0.93979071 -1.18462567  0.3964786   1.03223803 -0.16542825 -0.17155863\n",
      "   1.16869402  0.32388207 -0.73774465]\n",
      " [-0.92603096  1.1766917  -0.38576688 -0.89920752  0.22348002  0.16827898\n",
      "  -1.18746296 -0.72073673  0.57025668]\n",
      " [ 0.01935311  0.32259914  0.59020616 -1.0504229   0.03126454  0.08605397\n",
      "  -1.16162268  0.3182144   0.29138807]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.53440007  1.55243526 -2.76933554 -0.06521042  1.65613211 -2.53416217\n",
      "  -0.77011713]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:70 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.01131312]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 70 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.84864594 -1.03150727  0.32871582  1.010192   -0.14908679 -0.26351872\n",
      "   1.06280478  0.63861    -0.44050382]\n",
      " [-0.72832671  1.219584   -0.51914228 -1.15612574  0.08790334  0.21340433\n",
      "  -1.26999336 -0.84166243  0.49614276]\n",
      " [ 0.50173571 -0.31366087  0.79221764 -0.8643409   0.39236352 -0.43845351\n",
      "  -0.8869113   0.64110376  0.6208084 ]\n",
      " [ 0.9398046  -1.18461178  0.3964786   1.03223803 -0.16541435 -0.17154474\n",
      "   1.16869402  0.32388207 -0.73773076]\n",
      " [-0.92604451  1.17667814 -0.38576688 -0.89920752  0.22346647  0.16826542\n",
      "  -1.18746296 -0.72073673  0.57024312]\n",
      " [ 0.01934276  0.3225888   0.59020616 -1.0504229   0.0312542   0.08604362\n",
      "  -1.16162268  0.3182144   0.29137773]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.53446334  1.55241869 -2.76938514 -0.06525357  1.65611877 -2.53421093\n",
      "  -0.77016011]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:70 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.3138283]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 70 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.84490606 -1.03524715  0.32871582  1.010192   -0.15282667 -0.2672586\n",
      "   1.05906489  0.63861    -0.44050382]\n",
      " [-0.72451501  1.2233957  -0.51914228 -1.15612574  0.09171504  0.21721603\n",
      "  -1.26618166 -0.84166243  0.49614276]\n",
      " [ 0.50723029 -0.30816628  0.79221764 -0.8643409   0.39785811 -0.43295893\n",
      "  -0.88141672  0.64110376  0.6208084 ]\n",
      " [ 0.93525017 -1.18916622  0.3964786   1.03223803 -0.16996879 -0.17609917\n",
      "   1.16413958  0.32388207 -0.73773076]\n",
      " [-0.92176561  1.18095704 -0.38576688 -0.89920752  0.22774537  0.17254432\n",
      "  -1.18318406 -0.72073673  0.57024312]\n",
      " [ 0.02460061  0.32784665  0.59020616 -1.0504229   0.03651205  0.09130147\n",
      "  -1.15636483  0.3182144   0.29137773]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.56825325  1.53164622 -2.78232193 -0.07613148  1.63440335 -2.54661192\n",
      "  -0.78135409]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:70 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.89276941]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 70 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.84563614 -1.03524715  0.32871582  1.01092207 -0.15282667 -0.2672586\n",
      "   1.05979497  0.63861    -0.44050382]\n",
      " [-0.72515329  1.2233957  -0.51914228 -1.15676402  0.09171504  0.21721603\n",
      "  -1.26681994 -0.84166243  0.49614276]\n",
      " [ 0.50612287 -0.30816628  0.79221764 -0.86544832  0.39785811 -0.43295893\n",
      "  -0.88252415  0.64110376  0.6208084 ]\n",
      " [ 0.93589428 -1.18916622  0.3964786   1.03288215 -0.16996879 -0.17609917\n",
      "   1.1647837   0.32388207 -0.73773076]\n",
      " [-0.9224596   1.18095704 -0.38576688 -0.8999015   0.22774537  0.17254432\n",
      "  -1.18387804 -0.72073673  0.57024312]\n",
      " [ 0.02358038  0.32784665  0.59020616 -1.05144313  0.03651205  0.09130147\n",
      "  -1.15738505  0.3182144   0.29137773]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.56312054  1.53651481 -2.78211038 -0.0749782   1.6393214  -2.54636946\n",
      "  -0.78083383]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:70 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.7351826]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 70 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.85140018 -1.03524715  0.32871582  1.01092207 -0.14706263 -0.26149456\n",
      "   1.06555901  0.63861    -0.44050382]\n",
      " [-0.73088638  1.2233957  -0.51914228 -1.15676402  0.08598195  0.21148294\n",
      "  -1.27255303 -0.84166243  0.49614276]\n",
      " [ 0.50358006 -0.30816628  0.79221764 -0.86544832  0.3953153  -0.43550173\n",
      "  -0.88506695  0.64110376  0.6208084 ]\n",
      " [ 0.94158077 -1.18916622  0.3964786   1.03288215 -0.1642823  -0.17041268\n",
      "   1.17047018  0.32388207 -0.73773076]\n",
      " [-0.92817958  1.18095704 -0.38576688 -0.8999015   0.22202539  0.16682434\n",
      "  -1.18959802 -0.72073673  0.57024312]\n",
      " [ 0.01849578  0.32784665  0.59020616 -1.05144313  0.03142744  0.08621687\n",
      "  -1.16246966  0.3182144   0.29137773]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.537342    1.55753408 -2.77807106 -0.06470411  1.66129846 -2.54240789\n",
      "  -0.77393124]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:70 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57399111]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 70 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.83660664 -1.03524715  0.32871582  0.99612854 -0.14706263 -0.2762881\n",
      "   1.06555901  0.63861    -0.45529736]\n",
      " [-0.71599686  1.2233957  -0.51914228 -1.14187451  0.08598195  0.22637246\n",
      "  -1.27255303 -0.84166243  0.51103228]\n",
      " [ 0.50665371 -0.30816628  0.79221764 -0.86237467  0.3953153  -0.43242808\n",
      "  -0.88506695  0.64110376  0.62388205]\n",
      " [ 0.92732422 -1.18916622  0.3964786   1.01862559 -0.1642823  -0.18466924\n",
      "   1.17047018  0.32388207 -0.75198731]\n",
      " [-0.91376919  1.18095704 -0.38576688 -0.88549111  0.22202539  0.18123473\n",
      "  -1.18959802 -0.72073673  0.58465351]\n",
      " [ 0.02884231  0.32784665  0.59020616 -1.0410966   0.03142744  0.09656339\n",
      "  -1.16246966  0.3182144   0.30172425]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.60751968  1.50410139 -2.79456617 -0.0967033   1.60909354 -2.56005248\n",
      "  -0.79791687]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:70 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.73522893]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 70 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.84237227 -1.02948151  0.32871582  1.00189417 -0.14706263 -0.27052246\n",
      "   1.07132465  0.63861    -0.45529736]\n",
      " [-0.72172956  1.217663   -0.51914228 -1.14760721  0.08598195  0.22063976\n",
      "  -1.27828573 -0.84166243  0.51103228]\n",
      " [ 0.50121612 -0.31360388  0.79221764 -0.86781226  0.3953153  -0.43786567\n",
      "  -0.89050455  0.64110376  0.62388205]\n",
      " [ 0.93301808 -1.18347235  0.3964786   1.02431946 -0.1642823  -0.17897537\n",
      "   1.17616405  0.32388207 -0.75198731]\n",
      " [-0.91952483  1.1752014  -0.38576688 -0.89124675  0.22202539  0.17547909\n",
      "  -1.19535366 -0.72073673  0.58465351]\n",
      " [ 0.02315418  0.32215852  0.59020616 -1.04678473  0.03142744  0.09087526\n",
      "  -1.16815778  0.3182144   0.30172425]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.58174854  1.52549555 -2.79051988 -0.09358057  1.63102517 -2.55581861\n",
      "  -0.79410255]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:70 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.79278701]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 70 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.84604299 -1.02581079  0.32871582  1.00556489 -0.14706263 -0.27052246\n",
      "   1.07499537  0.63861    -0.45529736]\n",
      " [-0.72536544  1.21402712 -0.51914228 -1.15124309  0.08598195  0.22063976\n",
      "  -1.28192161 -0.84166243  0.51103228]\n",
      " [ 0.49740682 -0.31741318  0.79221764 -0.87162157  0.3953153  -0.43786567\n",
      "  -0.89431385  0.64110376  0.62388205]\n",
      " [ 0.93663699 -1.17985345  0.3964786   1.02793836 -0.1642823  -0.17897537\n",
      "   1.17978295  0.32388207 -0.75198731]\n",
      " [-0.92323467  1.17149155 -0.38576688 -0.8949566   0.22202539  0.17547909\n",
      "  -1.19906351 -0.72073673  0.58465351]\n",
      " [ 0.01947121  0.31847554  0.59020616 -1.0504677   0.03142744  0.09087526\n",
      "  -1.17184076  0.3182144   0.30172425]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.5647285   1.54027424 -2.7883625  -0.09065115  1.64592536 -2.55346766\n",
      "  -0.79182893]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:70 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.81180169]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 70 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.849078   -1.02581079  0.33175083  1.00556489 -0.14706263 -0.26748745\n",
      "   1.07803038  0.63861    -0.45529736]\n",
      " [-0.72809786  1.21402712 -0.5218747  -1.15124309  0.08598195  0.21790734\n",
      "  -1.28465403 -0.84166243  0.51103228]\n",
      " [ 0.49725394 -0.31741318  0.79206476 -0.87162157  0.3953153  -0.43801855\n",
      "  -0.89446673  0.64110376  0.62388205]\n",
      " [ 0.93933959 -1.17985345  0.3991812   1.02793836 -0.1642823  -0.17627277\n",
      "   1.18248556  0.32388207 -0.75198731]\n",
      " [-0.9259387   1.17149155 -0.38847091 -0.8949566   0.22202539  0.17277506\n",
      "  -1.20176754 -0.72073673  0.58465351]\n",
      " [ 0.01786803  0.31847554  0.58860298 -1.0504677   0.03142744  0.08927208\n",
      "  -1.17344394  0.3182144   0.30172425]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.55035206  1.55290537 -2.78705935 -0.08361585  1.65903161 -2.55219592\n",
      "  -0.7863039 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:70 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.15963048]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 71 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.84811222 -1.02677658  0.33175083  1.0045991  -0.14706263 -0.26748745\n",
      "   1.07803038  0.63861    -0.45626314]\n",
      " [-0.72768733  1.21443764 -0.5218747  -1.15083256  0.08598195  0.21790734\n",
      "  -1.28465403 -0.84166243  0.51144281]\n",
      " [ 0.49743548 -0.31723164  0.79206476 -0.87144003  0.3953153  -0.43801855\n",
      "  -0.89446673  0.64110376  0.6240636 ]\n",
      " [ 0.93924476 -1.17994827  0.3991812   1.02784354 -0.1642823  -0.17627277\n",
      "   1.18248556  0.32388207 -0.75208214]\n",
      " [-0.92576556  1.17166469 -0.38847091 -0.89478346  0.22202539  0.17277506\n",
      "  -1.20176754 -0.72073673  0.58482665]\n",
      " [ 0.01892629  0.31953381  0.58860298 -1.04940944  0.03142744  0.08927208\n",
      "  -1.17344394  0.3182144   0.30278252]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.56105916  1.54656342 -2.79200075 -0.08878772  1.65358321 -2.55737621\n",
      "  -0.79056893]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:71 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.8367054]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 71 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.85027102 -1.02677658  0.33390963  1.0045991  -0.14706263 -0.26748745\n",
      "   1.08018918  0.63861    -0.45626314]\n",
      " [-0.72961233  1.21443764 -0.52379969 -1.15083256  0.08598195  0.21790734\n",
      "  -1.28657903 -0.84166243  0.51144281]\n",
      " [ 0.49849528 -0.31723164  0.79312456 -0.87144003  0.3953153  -0.43801855\n",
      "  -0.89340692  0.64110376  0.6240636 ]\n",
      " [ 0.94118148 -1.17994827  0.40111792  1.02784354 -0.1642823  -0.17627277\n",
      "   1.18442228  0.32388207 -0.75208214]\n",
      " [-0.9277066   1.17166469 -0.39041195 -0.89478346  0.22202539  0.17277506\n",
      "  -1.20370858 -0.72073673  0.58482665]\n",
      " [ 0.01746795  0.31953381  0.58714464 -1.04940944  0.03142744  0.08927208\n",
      "  -1.17490228  0.3182144   0.30278252]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.54990373  1.55666275 -2.79118089 -0.08212243  1.66390862 -2.55654241\n",
      "  -0.78652866]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:71 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.61731634]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 71 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.83454245 -1.04250514  0.31818106  1.0045991  -0.14706263 -0.26748745\n",
      "   1.06446061  0.63861    -0.45626314]\n",
      " [-0.71358316  1.23046681 -0.50777053 -1.15083256  0.08598195  0.21790734\n",
      "  -1.27054986 -0.84166243  0.51144281]\n",
      " [ 0.49702148 -0.31870544  0.79165076 -0.87144003  0.3953153  -0.43801855\n",
      "  -0.89488072  0.64110376  0.6240636 ]\n",
      " [ 0.9250962  -1.19603356  0.38503264  1.02784354 -0.1642823  -0.17627277\n",
      "   1.16833699  0.32388207 -0.75208214]\n",
      " [-0.91161293  1.18775837 -0.37431828 -0.89478346  0.22202539  0.17277506\n",
      "  -1.1876149  -0.72073673  0.58482665]\n",
      " [ 0.0219679   0.32403376  0.59164459 -1.04940944  0.03142744  0.08927208\n",
      "  -1.17040233  0.3182144   0.30278252]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.62282017  1.50014195 -2.80648575 -0.12005606  1.60604201 -2.57155181\n",
      "  -0.81843963]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:71 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.66675971]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 71 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.84266548 -1.04250514  0.32630409  1.0045991  -0.14706263 -0.26748745\n",
      "   1.06446061  0.64673303 -0.45626314]\n",
      " [-0.72122326  1.23046681 -0.51541062 -1.15083256  0.08598195  0.21790734\n",
      "  -1.27054986 -0.84930252  0.51144281]\n",
      " [ 0.50493049 -0.31870544  0.79955977 -0.87144003  0.3953153  -0.43801855\n",
      "  -0.89488072  0.64901277  0.6240636 ]\n",
      " [ 0.93336088 -1.19603356  0.39329731  1.02784354 -0.1642823  -0.17627277\n",
      "   1.16833699  0.33214675 -0.75208214]\n",
      " [-0.91937335  1.18775837 -0.38207869 -0.89478346  0.22202539  0.17277506\n",
      "  -1.1876149  -0.72849715  0.58482665]\n",
      " [ 0.02896117  0.32403376  0.59863786 -1.04940944  0.03142744  0.08927208\n",
      "  -1.17040233  0.32520767  0.30278252]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.58579864  1.53187276 -2.8023118  -0.08772794  1.63701861 -2.56716459\n",
      "  -0.79187869]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:71 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.60812436]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 71 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.85302831 -1.04250514  0.32630409  1.01496194 -0.14706263 -0.26748745\n",
      "   1.06446061  0.64673303 -0.44590031]\n",
      " [-0.73154515  1.23046681 -0.51541062 -1.16115446  0.08598195  0.21790734\n",
      "  -1.27054986 -0.84930252  0.50112091]\n",
      " [ 0.50788772 -0.31870544  0.79955977 -0.8684828   0.3953153  -0.43801855\n",
      "  -0.89488072  0.64901277  0.62702082]\n",
      " [ 0.94335529 -1.19603356  0.39329731  1.03783795 -0.1642823  -0.17627277\n",
      "   1.16833699  0.33214675 -0.74208772]\n",
      " [-0.92942373  1.18775837 -0.38207869 -0.90483385  0.22202539  0.17277506\n",
      "  -1.1876149  -0.72849715  0.57477626]\n",
      " [ 0.02157598  0.32403376  0.59863786 -1.05679462  0.03142744  0.08927208\n",
      "  -1.17040233  0.32520767  0.29539733]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.53910487  1.5692629  -2.7927797  -0.06139103  1.67297952 -2.55659785\n",
      "  -0.77656746]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:71 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.09547293]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 71 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.85303596 -1.04250514  0.32630409  1.01496194 -0.14705498 -0.2674798\n",
      "   1.06446061  0.64673303 -0.44589267]\n",
      " [-0.73162076  1.23046681 -0.51541062 -1.16115446  0.08590634  0.21783173\n",
      "  -1.27054986 -0.84930252  0.5010453 ]\n",
      " [ 0.5070408  -0.31870544  0.79955977 -0.8684828   0.39446838 -0.43886548\n",
      "  -0.89488072  0.64901277  0.6261739 ]\n",
      " [ 0.94349815 -1.19603356  0.39329731  1.03783795 -0.16413944 -0.17612991\n",
      "   1.16833699  0.33214675 -0.74194486]\n",
      " [-0.9294651   1.18775837 -0.38207869 -0.90483385  0.22198402  0.1727337\n",
      "  -1.1876149  -0.72849715  0.5747349 ]\n",
      " [ 0.02114585  0.32403376  0.59863786 -1.05679462  0.03099731  0.08884195\n",
      "  -1.17040233  0.32520767  0.29496719]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.54322729  1.56720934 -2.79491659 -0.06447788  1.67106163 -2.55870043\n",
      "  -0.77907267]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:71 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.01055408]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 71 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.85304707 -1.04249403  0.32630409  1.01496194 -0.14704387 -0.26746869\n",
      "   1.06446061  0.64673303 -0.44588155]\n",
      " [-0.73163283  1.23045474 -0.51541062 -1.16115446  0.08589427  0.21781967\n",
      "  -1.27054986 -0.84930252  0.50103324]\n",
      " [ 0.50703162 -0.31871462  0.79955977 -0.8684828   0.3944592  -0.43887466\n",
      "  -0.89488072  0.64901277  0.62616472]\n",
      " [ 0.94351028 -1.19602143  0.39329731  1.03783795 -0.16412731 -0.17611778\n",
      "   1.16833699  0.33214675 -0.74193273]\n",
      " [-0.92947696  1.18774651 -0.38207869 -0.90483385  0.22197217  0.17272184\n",
      "  -1.1876149  -0.72849715  0.57472304]\n",
      " [ 0.02113675  0.32402466  0.59863786 -1.05679462  0.03098821  0.08883285\n",
      "  -1.17040233  0.32520767  0.2949581 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.5432824   1.56719505 -2.79495993 -0.06451555  1.67105014 -2.55874305\n",
      "  -0.77911022]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:71 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.30731983]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 71 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.84947409 -1.04606702  0.32630409  1.01496194 -0.15061685 -0.27104167\n",
      "   1.06088763  0.64673303 -0.44588155]\n",
      " [-0.72800777  1.2340798  -0.51541062 -1.16115446  0.08951933  0.22144473\n",
      "  -1.2669248  -0.84930252  0.50103324]\n",
      " [ 0.51238227 -0.31336397  0.79955977 -0.8684828   0.39980985 -0.43352401\n",
      "  -0.88953007  0.64901277  0.62616472]\n",
      " [ 0.93917271 -1.200359    0.39329731  1.03783795 -0.16846488 -0.18045535\n",
      "   1.16399943  0.33214675 -0.74193273]\n",
      " [-0.92540295  1.19182052 -0.38207869 -0.90483385  0.22604617  0.17679584\n",
      "  -1.1835409  -0.72849715  0.57472304]\n",
      " [ 0.02624331  0.32913122  0.59863786 -1.05679462  0.03609477  0.09393941\n",
      "  -1.16529578  0.32520767  0.2949581 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.57599265  1.54713931 -2.8075562  -0.07500272  1.65011395 -2.57082729\n",
      "  -0.78992461]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:71 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.89503314]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 71 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.85017111 -1.04606702  0.32630409  1.01565895 -0.15061685 -0.27104167\n",
      "   1.06158465  0.64673303 -0.44588155]\n",
      " [-0.72861753  1.2340798  -0.51541062 -1.16176422  0.08951933  0.22144473\n",
      "  -1.26753457 -0.84930252  0.50103324]\n",
      " [ 0.5113165  -0.31336397  0.79955977 -0.86954856  0.39980985 -0.43352401\n",
      "  -0.89059584  0.64901277  0.62616472]\n",
      " [ 0.93978802 -1.200359    0.39329731  1.03845326 -0.16846488 -0.18045535\n",
      "   1.16461473  0.33214675 -0.74193273]\n",
      " [-0.92606596  1.19182052 -0.38207869 -0.90549686  0.22604617  0.17679584\n",
      "  -1.18420391 -0.72849715  0.57472304]\n",
      " [ 0.02526779  0.32913122  0.59863786 -1.05777014  0.03609477  0.09393941\n",
      "  -1.1662713   0.32520767  0.2949581 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.5710619   1.55181901 -2.80735477 -0.0739009   1.65484034 -2.57059649\n",
      "  -0.78943093]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:71 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.73742684]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 71 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.85585626 -1.04606702  0.32630409  1.01565895 -0.14493169 -0.26535652\n",
      "   1.0672698   0.64673303 -0.44588155]\n",
      " [-0.73426994  1.2340798  -0.51541062 -1.16176422  0.08386693  0.21579232\n",
      "  -1.27318697 -0.84930252  0.50103324]\n",
      " [ 0.50880065 -0.31336397  0.79955977 -0.86954856  0.397294   -0.43603985\n",
      "  -0.89311169  0.64901277  0.62616472]\n",
      " [ 0.94539494 -1.200359    0.39329731  1.03845326 -0.16285796 -0.17484843\n",
      "   1.17022165  0.33214675 -0.74193273]\n",
      " [-0.93170574  1.19182052 -0.38207869 -0.90549686  0.2204064   0.17115607\n",
      "  -1.18984368 -0.72849715  0.57472304]\n",
      " [ 0.02024059  0.32913122  0.59863786 -1.05777014  0.03106757  0.08891221\n",
      "  -1.17129849  0.32520767  0.2949581 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.54564107  1.57256477 -2.80337871 -0.06377847  1.67651523 -2.56669446\n",
      "  -0.7826489 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:71 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57385024]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 71 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.84106533 -1.04606702  0.32630409  1.00086802 -0.14493169 -0.28014745\n",
      "   1.0672698   0.64673303 -0.46067249]\n",
      " [-0.71938667  1.2340798  -0.51541062 -1.14688096  0.08386693  0.23067559\n",
      "  -1.27318697 -0.84930252  0.5159165 ]\n",
      " [ 0.51177198 -0.31336397  0.79955977 -0.86657724  0.397294   -0.43306853\n",
      "  -0.89311169  0.64901277  0.62913604]\n",
      " [ 0.93113588 -1.200359    0.39329731  1.0241942  -0.16285796 -0.18910749\n",
      "   1.17022165  0.33214675 -0.75619179]\n",
      " [-0.91729578  1.19182052 -0.38207869 -0.8910869   0.2204064   0.18556602\n",
      "  -1.18984368 -0.72849715  0.589133  ]\n",
      " [ 0.03056425  0.32913122  0.59863786 -1.04744649  0.03106757  0.09923587\n",
      "  -1.17129849  0.32520767  0.30528176]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.61580751  1.51914123 -2.81988147 -0.09587592  1.62430868 -2.58433213\n",
      "  -0.80665743]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:71 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.73574143]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 71 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.84681329 -1.04031905  0.32630409  1.00661599 -0.14493169 -0.27439949\n",
      "   1.07301777  0.64673303 -0.46067249]\n",
      " [-0.72510422  1.22836226 -0.51541062 -1.1525985   0.08386693  0.22495804\n",
      "  -1.27890452 -0.84930252  0.5159165 ]\n",
      " [ 0.5063696  -0.31876635  0.79955977 -0.87197962  0.397294   -0.43847091\n",
      "  -0.89851406  0.64901277  0.62913604]\n",
      " [ 0.93681634 -1.19467854  0.39329731  1.02987466 -0.16285796 -0.18342703\n",
      "   1.17590211  0.33214675 -0.75619179]\n",
      " [-0.92303499  1.18608131 -0.38207869 -0.89682611  0.2204064   0.17982681\n",
      "  -1.19558289 -0.72849715  0.589133  ]\n",
      " [ 0.02490146  0.32346843  0.59863786 -1.05310928  0.03106757  0.09357308\n",
      "  -1.17696128  0.32520767  0.30528176]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.59011814  1.54045383 -2.81582772 -0.0927984   1.6461495  -2.5800911\n",
      "  -0.80288588]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:71 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.79444883]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 71 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.85043293 -1.03669942  0.32630409  1.01023562 -0.14493169 -0.27439949\n",
      "   1.07663741  0.64673303 -0.46067249]\n",
      " [-0.72869092  1.22477556 -0.51541062 -1.1561852   0.08386693  0.22495804\n",
      "  -1.28249122 -0.84930252  0.5159165 ]\n",
      " [ 0.50261436 -0.32252159  0.79955977 -0.87573486  0.397294   -0.43847091\n",
      "  -0.9022693   0.64901277  0.62913604]\n",
      " [ 0.94038669 -1.19110819  0.39329731  1.03344501 -0.16285796 -0.18342703\n",
      "   1.17947246  0.33214675 -0.75619179]\n",
      " [-0.92669424  1.18242206 -0.38207869 -0.90048535  0.2204064   0.17982681\n",
      "  -1.19924213 -0.72849715  0.589133  ]\n",
      " [ 0.02127853  0.3198455   0.59863786 -1.05673221  0.03106757  0.09357308\n",
      "  -1.18058421  0.32520767  0.30528176]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.5733349   1.55502695 -2.81369717 -0.08993876  1.66083851 -2.57776974\n",
      "  -0.80066724]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:71 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.81288197]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 71 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.85343739 -1.03669942  0.32930855  1.01023562 -0.14493169 -0.27139503\n",
      "   1.07964186  0.64673303 -0.46067249]\n",
      " [-0.73139997  1.22477556 -0.51811967 -1.1561852   0.08386693  0.22224899\n",
      "  -1.28520027 -0.84930252  0.5159165 ]\n",
      " [ 0.50247721 -0.32252159  0.79942262 -0.87573486  0.397294   -0.43860806\n",
      "  -0.90240646  0.64901277  0.62913604]\n",
      " [ 0.94306633 -1.19110819  0.39597696  1.03344501 -0.16285796 -0.18074739\n",
      "   1.1821521   0.33214675 -0.75619179]\n",
      " [-0.92937551  1.18242206 -0.38475996 -0.90048535  0.2204064   0.17714554\n",
      "  -1.2019234  -0.72849715  0.589133  ]\n",
      " [ 0.01970419  0.3198455   0.59706353 -1.05673221  0.03106757  0.09199874\n",
      "  -1.18215855  0.32520767  0.30528176]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.55910412  1.56752971 -2.81240235 -0.08296056  1.67380716 -2.57650583\n",
      "  -0.79518406]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:71 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.15511841]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 72 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.85253709 -1.03759971  0.32930855  1.00933533 -0.14493169 -0.27139503\n",
      "   1.07964186  0.64673303 -0.46157278]\n",
      " [-0.7310287   1.22514683 -0.51811967 -1.15581393  0.08386693  0.22224899\n",
      "  -1.28520027 -0.84930252  0.51628777]\n",
      " [ 0.50264637 -0.32235242  0.79942262 -0.87556569  0.397294   -0.43860806\n",
      "  -0.90240646  0.64901277  0.62930521]\n",
      " [ 0.94299212 -1.19118241  0.39597696  1.03337079 -0.16285796 -0.18074739\n",
      "   1.1821521   0.33214675 -0.756266  ]\n",
      " [-0.92922747  1.1825701  -0.38475996 -0.90033732  0.2204064   0.17714554\n",
      "  -1.2019234  -0.72849715  0.58928103]\n",
      " [ 0.02070773  0.32084904  0.59706353 -1.05572867  0.03106757  0.09199874\n",
      "  -1.18215855  0.32520767  0.30628529]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.56926877  1.56152682 -2.81711206 -0.0878736   1.66865061 -2.58144003\n",
      "  -0.79923423]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:72 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.83849826]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 72 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.85465047 -1.03759971  0.33142193  1.00933533 -0.14493169 -0.27139503\n",
      "   1.08175524  0.64673303 -0.46157278]\n",
      " [-0.73291558  1.22514683 -0.52000655 -1.15581393  0.08386693  0.22224899\n",
      "  -1.28708715 -0.84930252  0.51628777]\n",
      " [ 0.50369647 -0.32235242  0.80047272 -0.87556569  0.397294   -0.43860806\n",
      "  -0.90135636  0.64901277  0.62930521]\n",
      " [ 0.94489042 -1.19118241  0.39787527  1.03337079 -0.16285796 -0.18074739\n",
      "   1.18405041  0.33214675 -0.756266  ]\n",
      " [-0.93113027  1.1825701  -0.38666277 -0.90033732  0.2204064   0.17714554\n",
      "  -1.20382621 -0.72849715  0.58928103]\n",
      " [ 0.01928144  0.32084904  0.59563723 -1.05572867  0.03106757  0.09199874\n",
      "  -1.18358484  0.32520767  0.30628529]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.55833357  1.57143014 -2.81630849 -0.08132772  1.67877233 -2.58062263\n",
      "  -0.79526986]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:72 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.61395581]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 72 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.83898626 -1.05326393  0.31575771  1.00933533 -0.14493169 -0.27139503\n",
      "   1.06609102  0.64673303 -0.46157278]\n",
      " [-0.71695181  1.2411106  -0.50404279 -1.15581393  0.08386693  0.22224899\n",
      "  -1.27112338 -0.84930252  0.51628777]\n",
      " [ 0.5022353  -0.32381359  0.79901155 -0.87556569  0.397294   -0.43860806\n",
      "  -0.90281753  0.64901277  0.62930521]\n",
      " [ 0.92886869 -1.20720414  0.38185354  1.03337079 -0.16285796 -0.18074739\n",
      "   1.16802868  0.33214675 -0.756266  ]\n",
      " [-0.91509958  1.19860079 -0.37063208 -0.90033732  0.2204064   0.17714554\n",
      "  -1.18779552 -0.72849715  0.58928103]\n",
      " [ 0.02372061  0.3252882   0.6000764  -1.05572867  0.03106757  0.09199874\n",
      "  -1.17914567  0.32520767  0.30628529]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.63109166  1.51512568 -2.83170951 -0.11916951  1.62116459 -2.59573204\n",
      "  -0.82716416]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:72 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.66916048]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 72 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.84700857 -1.05326393  0.32378002  1.00933533 -0.14493169 -0.27139503\n",
      "   1.06609102  0.65475534 -0.46157278]\n",
      " [-0.72449331  1.2411106  -0.51158429 -1.15581393  0.08386693  0.22224899\n",
      "  -1.27112338 -0.85684403  0.51628777]\n",
      " [ 0.51002154 -0.32381359  0.80679779 -0.87556569  0.397294   -0.43860806\n",
      "  -0.90281753  0.65679902  0.62930521]\n",
      " [ 0.93703939 -1.20720414  0.39002424  1.03337079 -0.16285796 -0.18074739\n",
      "   1.16802868  0.34031745 -0.756266  ]\n",
      " [-0.92276068  1.19860079 -0.37829318 -0.90033732  0.2204064   0.17714554\n",
      "  -1.18779552 -0.73615825  0.58928103]\n",
      " [ 0.03071297  0.3252882   0.60706876 -1.05572867  0.03106757  0.09199874\n",
      "  -1.17914567  0.33220003  0.30628529]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.59447025  1.54655889 -2.82760727 -0.08710838  1.65185058 -2.59142107\n",
      "  -0.80076323]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:72 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.60987248]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 72 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.85731317 -1.05326393  0.32378002  1.01963993 -0.14493169 -0.27139503\n",
      "   1.06609102  0.65475534 -0.45126818]\n",
      " [-0.73475769  1.2411106  -0.51158429 -1.1660783   0.08386693  0.22224899\n",
      "  -1.27112338 -0.85684403  0.5060234 ]\n",
      " [ 0.51302929 -0.32381359  0.80679779 -0.87255795  0.397294   -0.43860806\n",
      "  -0.90281753  0.65679902  0.63231295]\n",
      " [ 0.9469875  -1.20720414  0.39002424  1.0433189  -0.16285796 -0.18074739\n",
      "   1.16802868  0.34031745 -0.7463179 ]\n",
      " [-0.93276206  1.19860079 -0.37829318 -0.9103387   0.2204064   0.17714554\n",
      "  -1.18779552 -0.73615825  0.57927965]\n",
      " [ 0.02336429  0.3252882   0.60706876 -1.06307735  0.03106757  0.09199874\n",
      "  -1.17914567  0.33220003  0.29893662]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.54805911  1.58375058 -2.81815853 -0.06086007  1.68763502 -2.58095471\n",
      "  -0.78555558]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:72 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.09248603]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 72 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.85732315 -1.05326393  0.32378002  1.01963993 -0.14492172 -0.27138506\n",
      "   1.06609102  0.65475534 -0.4512582 ]\n",
      " [-0.73483266  1.2411106  -0.51158429 -1.1660783   0.08379195  0.22217402\n",
      "  -1.27112338 -0.85684403  0.50594843]\n",
      " [ 0.51222802 -0.32381359  0.80679779 -0.87255795  0.39649273 -0.43940933\n",
      "  -0.90281753  0.65679902  0.63151168]\n",
      " [ 0.94712549 -1.20720414  0.39002424  1.0433189  -0.16271997 -0.1806094\n",
      "   1.16802868  0.34031745 -0.74617991]\n",
      " [-0.93280481  1.19860079 -0.37829318 -0.9103387   0.22036365  0.1771028\n",
      "  -1.18779552 -0.73615825  0.57923691]\n",
      " [ 0.02295288  0.3252882   0.60706876 -1.06307735  0.03065616  0.09158733\n",
      "  -1.17914567  0.33220003  0.29852521]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.5519404   1.58181992 -2.82017422 -0.06377497  1.68583284 -2.58293811\n",
      "  -0.78792136]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:72 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.00985353]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 72 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.8573329  -1.05325417  0.32378002  1.01963993 -0.14491196 -0.2713753\n",
      "   1.06609102  0.65475534 -0.45124845]\n",
      " [-0.73484321  1.24110005 -0.51158429 -1.1660783   0.0837814   0.22216346\n",
      "  -1.27112338 -0.85684403  0.50593788]\n",
      " [ 0.51221996 -0.32382165  0.80679779 -0.87255795  0.39648467 -0.43941739\n",
      "  -0.90281753  0.65679902  0.63150363]\n",
      " [ 0.9471361  -1.20719353  0.39002424  1.0433189  -0.16270937 -0.18059879\n",
      "   1.16802868  0.34031745 -0.7461693 ]\n",
      " [-0.93281519  1.19859041 -0.37829318 -0.9103387   0.22035327  0.17709242\n",
      "  -1.18779552 -0.73615825  0.57922653]\n",
      " [ 0.02294488  0.32528021  0.60706876 -1.06307735  0.03064817  0.09157934\n",
      "  -1.17914567  0.33220003  0.29851721]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.55198846  1.58180758 -2.82021214 -0.0638079   1.68582294 -2.58297541\n",
      "  -0.78795421]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:72 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.30099447]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 72 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.85391882 -1.05666825  0.32378002  1.01963993 -0.14832604 -0.27478938\n",
      "   1.06267694  0.65475534 -0.45124845]\n",
      " [-0.73139513  1.24454813 -0.51158429 -1.1660783   0.08722948  0.22561155\n",
      "  -1.2676753  -0.85684403  0.50593788]\n",
      " [ 0.51743147 -0.31861014  0.80679779 -0.87255795  0.40169618 -0.43420588\n",
      "  -0.89760602  0.65679902  0.63150363]\n",
      " [ 0.94300439 -1.21132523  0.39002424  1.0433189  -0.16684107 -0.1847305\n",
      "   1.16389697  0.34031745 -0.7461693 ]\n",
      " [-0.9289357   1.2024699  -0.37829318 -0.9103387   0.22423276  0.18097191\n",
      "  -1.18391603 -0.73615825  0.57922653]\n",
      " [ 0.02790555  0.33024087  0.60706876 -1.06307735  0.03560883  0.09654\n",
      "  -1.17418501  0.33220003  0.29851721]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.5836526   1.562443   -2.83247381 -0.07391577  1.66563608 -2.5947476\n",
      "  -0.79839982]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:72 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.89722707]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 72 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.85458458 -1.05666825  0.32378002  1.0203057  -0.14832604 -0.27478938\n",
      "   1.06334271  0.65475534 -0.45124845]\n",
      " [-0.73197792  1.24454813 -0.51158429 -1.16666109  0.08722948  0.22561155\n",
      "  -1.26825809 -0.85684403  0.50593788]\n",
      " [ 0.51640549 -0.31861014  0.80679779 -0.87358393  0.40169618 -0.43420588\n",
      "  -0.898632    0.65679902  0.63150363]\n",
      " [ 0.94359244 -1.21132523  0.39002424  1.04390694 -0.16684107 -0.1847305\n",
      "   1.16448502  0.34031745 -0.7461693 ]\n",
      " [-0.9295694   1.2024699  -0.37829318 -0.9109724   0.22423276  0.18097191\n",
      "  -1.18454973 -0.73615825  0.57922653]\n",
      " [ 0.02697247  0.33024087  0.60706876 -1.06401042  0.03560883  0.09654\n",
      "  -1.17511809  0.33220003  0.29851721]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.57891422  1.56694261 -2.83228191 -0.07286277  1.67017979 -2.59452779\n",
      "  -0.79793113]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:72 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.73966739]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 72 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.86019109 -1.05666825  0.32378002  1.0203057  -0.14271953 -0.26918287\n",
      "   1.06894921  0.65475534 -0.45124845]\n",
      " [-0.73754999  1.24454813 -0.51158429 -1.16666109  0.08165741  0.22003947\n",
      "  -1.27383016 -0.85684403  0.50593788]\n",
      " [ 0.5139153  -0.31861014  0.80679779 -0.87358393  0.39920599 -0.43669607\n",
      "  -0.90112219  0.65679902  0.63150363]\n",
      " [ 0.94912005 -1.21132523  0.39002424  1.04390694 -0.16131346 -0.17920288\n",
      "   1.17001263  0.34031745 -0.7461693 ]\n",
      " [-0.93512929  1.2024699  -0.37829318 -0.9109724   0.21867287  0.17541202\n",
      "  -1.19010962 -0.73615825  0.57922653]\n",
      " [ 0.02200273  0.33024087  0.60706876 -1.06401042  0.03063909  0.09157026\n",
      "  -1.18008783  0.33220003  0.29851721]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.55384946  1.58741549 -2.82836883 -0.06289259  1.69155431 -2.59068525\n",
      "  -0.79126872]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:72 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57363909]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 72 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.84540478 -1.05666825  0.32378002  1.00551938 -0.14271953 -0.28396919\n",
      "   1.06894921  0.65475534 -0.46603476]\n",
      " [-0.72267493  1.24454813 -0.51158429 -1.15178603  0.08165741  0.23491453\n",
      "  -1.27383016 -0.85684403  0.52081293]\n",
      " [ 0.51678698 -0.31861014  0.80679779 -0.87071225  0.39920599 -0.4338244\n",
      "  -0.90112219  0.65679902  0.6343753 ]\n",
      " [ 0.93486066 -1.21132523  0.39002424  1.02964756 -0.16131346 -0.19346227\n",
      "   1.17001263  0.34031745 -0.76042869]\n",
      " [-0.92072196  1.2024699  -0.37829318 -0.89656507  0.21867287  0.18981935\n",
      "  -1.19010962 -0.73615825  0.59363386]\n",
      " [ 0.03230209  0.33024087  0.60706876 -1.05371106  0.03063909  0.10186962\n",
      "  -1.18008783  0.33220003  0.30881657]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.623999    1.53400751 -2.84487993 -0.09508266  1.63935248 -2.60831684\n",
      "  -0.81529893]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:72 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.73623235]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 72 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.85113579 -1.05093724  0.32378002  1.0112504  -0.14271953 -0.27823817\n",
      "   1.07468023  0.65475534 -0.46603476]\n",
      " [-0.72837791  1.23884515 -0.51158429 -1.15748901  0.08165741  0.22921155\n",
      "  -1.27953314 -0.85684403  0.52081293]\n",
      " [ 0.51141956 -0.32397756  0.80679779 -0.87607966  0.39920599 -0.43919181\n",
      "  -0.9064896   0.65679902  0.6343753 ]\n",
      " [ 0.94052822 -1.20565768  0.39002424  1.03531511 -0.16131346 -0.18779471\n",
      "   1.17568019  0.34031745 -0.76042869]\n",
      " [-0.92644535  1.1967465  -0.37829318 -0.90228846  0.21867287  0.18409595\n",
      "  -1.19583301 -0.73615825  0.59363386]\n",
      " [ 0.02666416  0.32460294  0.60706876 -1.05934899  0.03063909  0.09623169\n",
      "  -1.18572576  0.33220003  0.30881657]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.59838792  1.55524157 -2.84081851 -0.0920498   1.66110598 -2.60406845\n",
      "  -0.81156948]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:72 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.79608501]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 72 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.85470537 -1.04736766  0.32378002  1.01481998 -0.14271953 -0.27823817\n",
      "   1.07824981  0.65475534 -0.46603476]\n",
      " [-0.73191637  1.23530669 -0.51158429 -1.16102747  0.08165741  0.22921155\n",
      "  -1.28307159 -0.85684403  0.52081293]\n",
      " [ 0.50771769 -0.32767943  0.80679779 -0.87978154  0.39920599 -0.43919181\n",
      "  -0.91019148  0.65679902  0.6343753 ]\n",
      " [ 0.9440509  -1.202135    0.39002424  1.03883779 -0.16131346 -0.18779471\n",
      "   1.17920287  0.34031745 -0.76042869]\n",
      " [-0.93005497  1.19313688 -0.37829318 -0.90589808  0.21867287  0.18409595\n",
      "  -1.19944263 -0.73615825  0.59363386]\n",
      " [ 0.02310029  0.32103907  0.60706876 -1.06291286  0.03063909  0.09623169\n",
      "  -1.18928963  0.33220003  0.30881657]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.58183678  1.56961314 -2.83871432 -0.08925823  1.67558814 -2.60177615\n",
      "  -0.80940441]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:72 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.81391637]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 72 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.85768084 -1.04736766  0.32675549  1.01481998 -0.14271953 -0.2752627\n",
      "   1.08122527  0.65475534 -0.46603476]\n",
      " [-0.73460335  1.23530669 -0.51427127 -1.16102747  0.08165741  0.22652457\n",
      "  -1.28575858 -0.85684403  0.52081293]\n",
      " [ 0.50759489 -0.32767943  0.80667499 -0.87978154  0.39920599 -0.43931461\n",
      "  -0.91031428  0.65679902  0.6343753 ]\n",
      " [ 0.9467088  -1.202135    0.39268214  1.03883779 -0.16131346 -0.18513681\n",
      "   1.18186077  0.34031745 -0.76042869]\n",
      " [-0.93271474  1.19313688 -0.38095294 -0.90589808  0.21867287  0.18143619\n",
      "  -1.20210239 -0.73615825  0.59363386]\n",
      " [ 0.02155389  0.32103907  0.60552236 -1.06291286  0.03063909  0.09468528\n",
      "  -1.19083603  0.33220003  0.30881657]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.567745    1.58199307 -2.83742716 -0.08233516  1.68842527 -2.60051945\n",
      "  -0.80396074]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:72 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.15071135]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 73 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.85684226 -1.04820624  0.32675549  1.0139814  -0.14271953 -0.2752627\n",
      "   1.08122527  0.65475534 -0.46687334]\n",
      " [-0.73426857  1.23564147 -0.51427127 -1.16069269  0.08165741  0.22652457\n",
      "  -1.28575858 -0.85684403  0.52114771]\n",
      " [ 0.50775264 -0.32752168  0.80667499 -0.87962379  0.39920599 -0.43931461\n",
      "  -0.91031428  0.65679902  0.63453305]\n",
      " [ 0.94665339 -1.20219041  0.39268214  1.03878238 -0.16131346 -0.18513681\n",
      "   1.18186077  0.34031745 -0.7604841 ]\n",
      " [-0.93258981  1.19326181 -0.38095294 -0.90577316  0.21867287  0.18143619\n",
      "  -1.20210239 -0.73615825  0.59375878]\n",
      " [ 0.02250531  0.32199049  0.60552236 -1.06196144  0.03063909  0.09468528\n",
      "  -1.19083603  0.33220003  0.30976799]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.57739033  1.57631369 -2.84191396 -0.08699996  1.68354719 -2.60521714\n",
      "  -0.80780491]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:73 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.84023571]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 73 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.85891214 -1.04820624  0.32882537  1.0139814  -0.14271953 -0.2752627\n",
      "   1.08329516  0.65475534 -0.46687334]\n",
      " [-0.73611894  1.23564147 -0.51612164 -1.16069269  0.08165741  0.22652457\n",
      "  -1.28760895 -0.85684403  0.52114771]\n",
      " [ 0.50879295 -0.32752168  0.8077153  -0.87962379  0.39920599 -0.43931461\n",
      "  -0.90927397  0.65679902  0.63453305]\n",
      " [ 0.94851486 -1.20219041  0.39454361  1.03878238 -0.16131346 -0.18513681\n",
      "   1.18372224  0.34031745 -0.7604841 ]\n",
      " [-0.93445598  1.19326181 -0.3828191  -0.90577316  0.21867287  0.18143619\n",
      "  -1.20396856 -0.73615825  0.59375878]\n",
      " [ 0.02110995  0.32199049  0.604127   -1.06196144  0.03063909  0.09468528\n",
      "  -1.19223139  0.33220003  0.30976799]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.56666698  1.5860282  -2.84112592 -0.08056943  1.69347288 -2.60441537\n",
      "  -0.80391342]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:73 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.61051247]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 73 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.84331605 -1.06380233  0.31322928  1.0139814  -0.14271953 -0.2752627\n",
      "   1.06769907  0.65475534 -0.46687334]\n",
      " [-0.72022494  1.25153548 -0.50022764 -1.16069269  0.08165741  0.22652457\n",
      "  -1.27171494 -0.85684403  0.52114771]\n",
      " [ 0.50734874 -0.32896589  0.80627109 -0.87962379  0.39920599 -0.43931461\n",
      "  -0.91071818  0.65679902  0.63453305]\n",
      " [ 0.93256102 -1.21814424  0.37858978  1.03878238 -0.16131346 -0.18513681\n",
      "   1.16776841  0.34031745 -0.7604841 ]\n",
      " [-0.91849275  1.20922503 -0.36685588 -0.90577316  0.21867287  0.18143619\n",
      "  -1.18800533 -0.73615825  0.59375878]\n",
      " [ 0.02548986  0.3263704   0.60850691 -1.06196144  0.03063909  0.09468528\n",
      "  -1.18785148  0.33220003  0.30976799]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.63925294  1.52995115 -2.85661995 -0.11830815  1.63613401 -2.61962165\n",
      "  -0.83578255]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:73 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.67150117]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 73 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.85124012 -1.06380233  0.32115334  1.0139814  -0.14271953 -0.2752627\n",
      "   1.06769907  0.66267941 -0.46687334]\n",
      " [-0.72767075  1.25153548 -0.50767345 -1.16069269  0.08165741  0.22652457\n",
      "  -1.27171494 -0.86428984  0.52114771]\n",
      " [ 0.5150144  -0.32896589  0.81393676 -0.87962379  0.39920599 -0.43931461\n",
      "  -0.91071818  0.66446468  0.63453305]\n",
      " [ 0.94063984 -1.21814424  0.38666859  1.03878238 -0.16131346 -0.18513681\n",
      "   1.16776841  0.34839626 -0.7604841 ]\n",
      " [-0.92605739  1.20922503 -0.37442051 -0.90577316  0.21867287  0.18143619\n",
      "  -1.18800533 -0.74372288  0.59375878]\n",
      " [ 0.03247936  0.3263704   0.6154964  -1.06196144  0.03063909  0.09468528\n",
      "  -1.18785148  0.33918953  0.30976799]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.6030216   1.56109268 -2.85258656 -0.08650949  1.66653534 -2.61538391\n",
      "  -0.80953798]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:73 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.61157257]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 73 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.86148772 -1.06380233  0.32115334  1.02422899 -0.14271953 -0.2752627\n",
      "   1.06769907  0.66267941 -0.45662574]\n",
      " [-0.73787876  1.25153548 -0.50767345 -1.17090069  0.08165741  0.22652457\n",
      "  -1.27171494 -0.86428984  0.5109397 ]\n",
      " [ 0.51807166 -0.32896589  0.81393676 -0.87656653  0.39920599 -0.43931461\n",
      "  -0.91071818  0.66446468  0.63759031]\n",
      " [ 0.95054216 -1.21814424  0.38666859  1.0486847  -0.16131346 -0.18513681\n",
      "   1.16776841  0.34839626 -0.75058178]\n",
      " [-0.93601041  1.20922503 -0.37442051 -0.91572618  0.21867287  0.18143619\n",
      "  -1.18800533 -0.74372288  0.58380576]\n",
      " [ 0.02516676  0.3263704   0.6154964  -1.06927403  0.03063909  0.09468528\n",
      "  -1.18785148  0.33918953  0.3024554 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.55688583  1.59808995 -2.84321777 -0.06034708  1.70214647 -2.60501389\n",
      "  -0.79443055]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:73 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.08961239]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 73 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.8614997  -1.06380233  0.32115334  1.02422899 -0.14270754 -0.27525071\n",
      "   1.06769907  0.66267941 -0.45661375]\n",
      " [-0.73795288  1.25153548 -0.50767345 -1.17090069  0.08158329  0.22645044\n",
      "  -1.27171494 -0.86428984  0.51086558]\n",
      " [ 0.51731357 -0.32896589  0.81393676 -0.87656653  0.39844789 -0.4400727\n",
      "  -0.91071818  0.66446468  0.63683222]\n",
      " [ 0.95067532 -1.21814424  0.38666859  1.0486847  -0.1611803  -0.18500366\n",
      "   1.16776841  0.34839626 -0.75044862]\n",
      " [-0.93605416  1.20922503 -0.37442051 -0.91572618  0.21862912  0.18139244\n",
      "  -1.18800533 -0.74372288  0.58376201]\n",
      " [ 0.02477336  0.3263704   0.6154964  -1.06927403  0.03024569  0.09429189\n",
      "  -1.18785148  0.33918953  0.302062  ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.56054121  1.59627425 -2.84511966 -0.06310018  1.70045241 -2.60688535\n",
      "  -0.79666523]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:73 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.00920633]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 73 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.86150827 -1.06379376  0.32115334  1.02422899 -0.14269897 -0.27524214\n",
      "   1.06769907  0.66267941 -0.45660518]\n",
      " [-0.73796212  1.25152623 -0.50767345 -1.17090069  0.08157405  0.2264412\n",
      "  -1.27171494 -0.86428984  0.51085634]\n",
      " [ 0.51730649 -0.32897297  0.81393676 -0.87656653  0.39844081 -0.44007979\n",
      "  -0.91071818  0.66446468  0.63682514]\n",
      " [ 0.9506846  -1.21813496  0.38666859  1.0486847  -0.16117102 -0.18499437\n",
      "   1.16776841  0.34839626 -0.75043934]\n",
      " [-0.93606326  1.20921593 -0.37442051 -0.91572618  0.21862002  0.18138334\n",
      "  -1.18800533 -0.74372288  0.58375291]\n",
      " [ 0.02476632  0.32636335  0.6154964  -1.06927403  0.03023865  0.09428484\n",
      "  -1.18785148  0.33918953  0.30205496]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.56058319  1.59626358 -2.84515288 -0.063129    1.70044386 -2.60691804\n",
      "  -0.796694  ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:73 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.29484835]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 73 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.85824544 -1.0670566   0.32115334  1.02422899 -0.14596181 -0.27850498\n",
      "   1.06443623  0.66267941 -0.45660518]\n",
      " [-0.73468179  1.25480656 -0.50767345 -1.17090069  0.08485437  0.22972153\n",
      "  -1.26843461 -0.86428984  0.51085634]\n",
      " [ 0.52238344 -0.32389602  0.81393676 -0.87656653  0.40351776 -0.43500284\n",
      "  -0.90564123  0.66446468  0.63682514]\n",
      " [ 0.94674823 -1.22207133  0.38666859  1.0486847  -0.16510739 -0.18893074\n",
      "   1.16383203  0.34839626 -0.75043934]\n",
      " [-0.93236836  1.21291083 -0.37442051 -0.91572618  0.22231492  0.18507824\n",
      "  -1.18431043 -0.74372288  0.58375291]\n",
      " [ 0.0295863   0.33118334  0.6154964  -1.06927403  0.03505863  0.09910482\n",
      "  -1.1830315   0.33918953  0.30205496]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.59123457  1.57756507 -2.85708633 -0.07286919  1.68097722 -2.61838341\n",
      "  -0.80678177]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:73 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.89935452]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 73 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.85888162 -1.0670566   0.32115334  1.02486518 -0.14596181 -0.27850498\n",
      "   1.06507242  0.66267941 -0.45660518]\n",
      " [-0.73523903  1.25480656 -0.50767345 -1.17145793  0.08485437  0.22972153\n",
      "  -1.26899185 -0.86428984  0.51085634]\n",
      " [ 0.52139549 -0.32389602  0.81393676 -0.87755448  0.40351776 -0.43500284\n",
      "  -0.90662918  0.66446468  0.63682514]\n",
      " [ 0.94731045 -1.22207133  0.38666859  1.04924692 -0.16510739 -0.18893074\n",
      "   1.16439426  0.34839626 -0.75043934]\n",
      " [-0.9329743   1.21291083 -0.37442051 -0.91633212  0.22231492  0.18507824\n",
      "  -1.18491638 -0.74372288  0.58375291]\n",
      " [ 0.02869355  0.33118334  0.6154964  -1.07016679  0.03505863  0.09910482\n",
      "  -1.18392426  0.33918953  0.30205496]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.58667955  1.58189287 -2.85690343 -0.07186252  1.68534671 -2.61817396\n",
      "  -0.80633662]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:73 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.74190328]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 73 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.86440977 -1.0670566   0.32115334  1.02486518 -0.14043367 -0.27297684\n",
      "   1.07060056  0.66267941 -0.45660518]\n",
      " [-0.74073117  1.25480656 -0.50767345 -1.17145793  0.07936224  0.22422939\n",
      "  -1.27448399 -0.86428984  0.51085634]\n",
      " [ 0.51892975 -0.32389602  0.81393676 -0.87755448  0.40105203 -0.43746857\n",
      "  -0.90909491  0.66446468  0.63682514]\n",
      " [ 0.95275909 -1.22207133  0.38666859  1.04924692 -0.15965876 -0.18348211\n",
      "   1.16984289  0.34839626 -0.75043934]\n",
      " [-0.93845468  1.21291083 -0.37442051 -0.91633212  0.21683454  0.17959787\n",
      "  -1.19039675 -0.74372288  0.58375291]\n",
      " [ 0.02378129  0.33118334  0.6154964  -1.07016679  0.03014638  0.09419257\n",
      "  -1.18883651  0.33918953  0.30205496]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.56196901  1.60209369 -2.853053   -0.06204496  1.70642276 -2.61439079\n",
      "  -0.79979278]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:73 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57335799]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 73 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.84963007 -1.0670566   0.32115334  1.01008549 -0.14043367 -0.28775653\n",
      "   1.07060056  0.66267941 -0.47138488]\n",
      " [-0.72586627  1.25480656 -0.50767345 -1.15659303  0.07936224  0.2390943\n",
      "  -1.27448399 -0.86428984  0.52572124]\n",
      " [ 0.52170437 -0.32389602  0.81393676 -0.87477986  0.40105203 -0.43469395\n",
      "  -0.90909491  0.66446468  0.63959975]\n",
      " [ 0.93850153 -1.22207133  0.38666859  1.03498937 -0.15965876 -0.19773966\n",
      "   1.16984289  0.34839626 -0.76469689]\n",
      " [-0.92405214  1.21291083 -0.37442051 -0.90192958  0.21683454  0.1940004\n",
      "  -1.19039675 -0.74372288  0.59815544]\n",
      " [ 0.03405495  0.33118334  0.6154964  -1.05989313  0.03014638  0.10446623\n",
      "  -1.18883651  0.33918953  0.31232862]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.63209603  1.54870761 -2.86957306 -0.09432211  1.65423196 -2.63201707\n",
      "  -0.82384344]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:73 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.73670343]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 73 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.85534479 -1.06134188  0.32115334  1.01580021 -0.14043367 -0.28204181\n",
      "   1.07631528  0.66267941 -0.47138488]\n",
      " [-0.73155522  1.2491176  -0.50767345 -1.16228199  0.07936224  0.23340534\n",
      "  -1.28017295 -0.86428984  0.52572124]\n",
      " [ 0.5163717  -0.32922869  0.81393676 -0.88011253  0.40105203 -0.44002662\n",
      "  -0.91442758  0.66446468  0.63959975]\n",
      " [ 0.94415665 -1.21641621  0.38666859  1.04064449 -0.15965876 -0.19208455\n",
      "   1.175498    0.34839626 -0.76469689]\n",
      " [-0.92976027  1.2072027  -0.37442051 -0.90763772  0.21683454  0.18829227\n",
      "  -1.19610489 -0.74372288  0.59815544]\n",
      " [ 0.02844148  0.32556986  0.6154964  -1.06550661  0.03014638  0.09885276\n",
      "  -1.19444999  0.33918953  0.31232862]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.60656002  1.5698659  -2.8655038  -0.09133336  1.67590136 -2.62776118\n",
      "  -0.82015543]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:73 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.79769742]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 73 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.85886527 -1.0578214   0.32115334  1.01932068 -0.14043367 -0.28204181\n",
      "   1.07983575  0.66267941 -0.47138488]\n",
      " [-0.73504631  1.24562652 -0.50767345 -1.16577307  0.07936224  0.23340534\n",
      "  -1.28366403 -0.86428984  0.52572124]\n",
      " [ 0.51272255 -0.33287784  0.81393676 -0.88376168  0.40105203 -0.44002662\n",
      "  -0.91807673  0.66446468  0.63959975]\n",
      " [ 0.94763248 -1.21294038  0.38666859  1.04412032 -0.15965876 -0.19208455\n",
      "   1.17897384  0.34839626 -0.76469689]\n",
      " [-0.93332118  1.20364179 -0.37442051 -0.91119863  0.21683454  0.18829227\n",
      "  -1.19966579 -0.74372288  0.59815544]\n",
      " [ 0.02493573  0.32206411  0.6154964  -1.06901235  0.03014638  0.09885276\n",
      "  -1.19795573  0.33918953  0.31232862]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.5902366   1.58403972 -2.86342555 -0.08860817  1.69018072 -2.62549745\n",
      "  -0.81804258]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:73 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.81490729]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 73 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.86181322 -1.0578214   0.3241013   1.01932068 -0.14043367 -0.27909386\n",
      "   1.08278371  0.66267941 -0.47138488]\n",
      " [-0.73771244  1.24562652 -0.51033958 -1.16577307  0.07936224  0.23073921\n",
      "  -1.28633016 -0.86428984  0.52572124]\n",
      " [ 0.51261284 -0.33287784  0.81382705 -0.88376168  0.40105203 -0.44013632\n",
      "  -0.91818644  0.66446468  0.63959975]\n",
      " [ 0.95026978 -1.21294038  0.38930589  1.04412032 -0.15965876 -0.18944726\n",
      "   1.18161114  0.34839626 -0.76469689]\n",
      " [-0.93596061  1.20364179 -0.37705995 -0.91119863  0.21683454  0.18565284\n",
      "  -1.20230522 -0.74372288  0.59815544]\n",
      " [ 0.0234164   0.32206411  0.61397707 -1.06901235  0.03014638  0.09733342\n",
      "  -1.19947507  0.33918953  0.31232862]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.57627752  1.59630205 -2.86214541 -0.08173835  1.7028921  -2.62424739\n",
      "  -0.81263621]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:73 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.14640839]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 74 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.86103273 -1.05860189  0.3241013   1.01854019 -0.14043367 -0.27909386\n",
      "   1.08278371  0.66267941 -0.47216537]\n",
      " [-0.73741154  1.24592742 -0.51033958 -1.16547217  0.07936224  0.23073921\n",
      "  -1.28633016 -0.86428984  0.52602214]\n",
      " [ 0.51276004 -0.33273064  0.81382705 -0.88361448  0.40105203 -0.44013632\n",
      "  -0.91818644  0.66446468  0.63974695]\n",
      " [ 0.95023146 -1.21297869  0.38930589  1.04408201 -0.15965876 -0.18944726\n",
      "   1.18161114  0.34839626 -0.7647352 ]\n",
      " [-0.93585692  1.20374549 -0.37705995 -0.91109493  0.21683454  0.18565284\n",
      "  -1.20230522 -0.74372288  0.59825914]\n",
      " [ 0.02431821  0.32296593  0.61397707 -1.06811053  0.03014638  0.09733342\n",
      "  -1.19947507  0.33918953  0.31323043]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.58542606  1.59093107 -2.86641791 -0.08616532  1.69827951 -2.62871793\n",
      "  -0.81628304]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:74 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.84192081]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 74 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.86306092 -1.05860189  0.32612948  1.01854019 -0.14043367 -0.27909386\n",
      "   1.08481189  0.66267941 -0.47216537]\n",
      " [-0.73922689  1.24592742 -0.51215494 -1.16547217  0.07936224  0.23073921\n",
      "  -1.28814552 -0.86428984  0.52602214]\n",
      " [ 0.51379051 -0.33273064  0.81485751 -0.88361448  0.40105203 -0.44013632\n",
      "  -0.91715598  0.66446468  0.63974695]\n",
      " [ 0.95205757 -1.21297869  0.39113199  1.04408201 -0.15965876 -0.18944726\n",
      "   1.18343724  0.34839626 -0.7647352 ]\n",
      " [-0.93768794  1.20374549 -0.37889097 -0.91109493  0.21683454  0.18565284\n",
      "  -1.20413624 -0.74372288  0.59825914]\n",
      " [ 0.02295275  0.32296593  0.61261161 -1.06811053  0.03014638  0.09733342\n",
      "  -1.20084053  0.33918953  0.31323043]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.57490667  1.60046359 -2.8656447  -0.07984627  1.70801638 -2.6279311\n",
      "  -0.81246161]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:74 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.60699061]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 74 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.84753675 -1.07412606  0.31060531  1.01854019 -0.14043367 -0.27909386\n",
      "   1.06928772  0.66267941 -0.47216537]\n",
      " [-0.72340703  1.26174729 -0.49633508 -1.16547217  0.07936224  0.23073921\n",
      "  -1.27232566 -0.86428984  0.52602214]\n",
      " [ 0.51236714 -0.33415401  0.81343415 -0.88361448  0.40105203 -0.44013632\n",
      "  -0.91857934  0.66446468  0.63974695]\n",
      " [ 0.93617599 -1.22886027  0.37525041  1.04408201 -0.15965876 -0.18944726\n",
      "   1.16755566  0.34839626 -0.7647352 ]\n",
      " [-0.92179667  1.21963675 -0.3629997  -0.91109493  0.21683454  0.18565284\n",
      "  -1.18824498 -0.74372288  0.59825914]\n",
      " [ 0.02727479  0.32728797  0.61693365 -1.06811053  0.03014638  0.09733342\n",
      "  -1.19651849  0.33918953  0.31323043]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.64730639  1.54462508 -2.88122831 -0.11747097  1.6509565  -2.64323082\n",
      "  -0.844297  ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:74 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.67378602]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 74 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.85536491 -1.07412606  0.31843347  1.01854019 -0.14043367 -0.27909386\n",
      "   1.06928772  0.67050757 -0.47216537]\n",
      " [-0.73075987  1.26174729 -0.50368791 -1.16547217  0.07936224  0.23073921\n",
      "  -1.27232566 -0.87164268  0.52602214]\n",
      " [ 0.51991435 -0.33415401  0.82098135 -0.88361448  0.40105203 -0.44013632\n",
      "  -0.91857934  0.67201189  0.63974695]\n",
      " [ 0.94416489 -1.22886027  0.38323931  1.04408201 -0.15965876 -0.18944726\n",
      "   1.16755566  0.35638516 -0.7647352 ]\n",
      " [-0.92926751  1.21963675 -0.37047054 -0.91109493  0.21683454  0.18565284\n",
      "  -1.18824498 -0.75119372  0.59825914]\n",
      " [ 0.03425945  0.32728797  0.6239183  -1.06811053  0.03014638  0.09733342\n",
      "  -1.19651849  0.34617418  0.31323043]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.61145573  1.57548048 -2.87726113 -0.08593059  1.68107873 -2.6390635\n",
      "  -0.81820554]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:74 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.61322591]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 74 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.86555672 -1.07412606  0.31843347  1.028732   -0.14043367 -0.27909386\n",
      "   1.06928772  0.67050757 -0.46197356]\n",
      " [-0.74091266  1.26174729 -0.50368791 -1.17562496  0.07936224  0.23073921\n",
      "  -1.27232566 -0.87164268  0.51586935]\n",
      " [ 0.5230202  -0.33415401  0.82098135 -0.88050863  0.40105203 -0.44013632\n",
      "  -0.91857934  0.67201189  0.64285281]\n",
      " [ 0.95402199 -1.22886027  0.38323931  1.05393911 -0.15965876 -0.18944726\n",
      "   1.16755566  0.35638516 -0.7548781 ]\n",
      " [-0.93917285  1.21963675 -0.37047054 -0.92100027  0.21683454  0.18565284\n",
      "  -1.18824498 -0.75119372  0.5883538 ]\n",
      " [ 0.02698249  0.32728797  0.6239183  -1.07538749  0.03014638  0.09733342\n",
      "  -1.19651849  0.34617418  0.30595348]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.56558821  1.61228732 -2.86796902 -0.05985139  1.71651969 -2.62878596\n",
      "  -0.80319511]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:74 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.08684574]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 74 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.86557044 -1.07412606  0.31843347  1.028732   -0.14041994 -0.27908013\n",
      "   1.06928772  0.67050757 -0.46195983]\n",
      " [-0.74098576  1.26174729 -0.50368791 -1.17562496  0.07928914  0.23066611\n",
      "  -1.27232566 -0.87164268  0.51579626]\n",
      " [ 0.52230294 -0.33415401  0.82098135 -0.88050863  0.40033478 -0.44085358\n",
      "  -0.91857934  0.67201189  0.64213555]\n",
      " [ 0.95415037 -1.22886027  0.38323931  1.05393911 -0.15953038 -0.18931888\n",
      "   1.16755566  0.35638516 -0.75474972]\n",
      " [-0.9392173   1.21963675 -0.37047054 -0.92100027  0.21679009  0.18560839\n",
      "  -1.18824498 -0.75119372  0.58830935]\n",
      " [ 0.02660641  0.32728797  0.6239183  -1.07538749  0.02977029  0.09695733\n",
      "  -1.19651849  0.34617418  0.3055774 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.5690318   1.61057925 -2.86976399 -0.06245214  1.71492675 -2.63055223\n",
      "  -0.80530642]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:74 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.00860787]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 74 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.86557798 -1.07411852  0.31843347  1.028732   -0.1404124  -0.27907259\n",
      "   1.06928772  0.67050757 -0.46195229]\n",
      " [-0.74099386  1.26173918 -0.50368791 -1.17562496  0.07928104  0.23065801\n",
      "  -1.27232566 -0.87164268  0.51578816]\n",
      " [ 0.52229672 -0.33416024  0.82098135 -0.88050863  0.40032855 -0.4408598\n",
      "  -0.91857934  0.67201189  0.64212932]\n",
      " [ 0.95415851 -1.22885213  0.38323931  1.05393911 -0.15952224 -0.18931074\n",
      "   1.16755566  0.35638516 -0.75474158]\n",
      " [-0.93922529  1.21962876 -0.37047054 -0.92100027  0.2167821   0.1856004\n",
      "  -1.18824498 -0.75119372  0.58830136]\n",
      " [ 0.0266002   0.32728177  0.6239183  -1.07538749  0.02976408  0.09695112\n",
      "  -1.19651849  0.34617418  0.30557119]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.56906853  1.61057001 -2.86979314 -0.0624774   1.71491936 -2.63058091\n",
      "  -0.80533165]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:74 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.28887745]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 74 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.86245904 -1.07723746  0.31843347  1.028732   -0.14353134 -0.28219153\n",
      "   1.06616878  0.67050757 -0.46195229]\n",
      " [-0.73787249  1.26486055 -0.50368791 -1.17562496  0.08240241  0.23377937\n",
      "  -1.26920429 -0.87164268  0.51578816]\n",
      " [ 0.5272435  -0.32921345  0.82098135 -0.88050863  0.40527533 -0.43591302\n",
      "  -0.91363256  0.67201189  0.64212932]\n",
      " [ 0.95040741 -1.23260323  0.38323931  1.05393911 -0.16327333 -0.19306183\n",
      "   1.16380456  0.35638516 -0.75474158]\n",
      " [-0.93570548  1.22314857 -0.37047054 -0.92100027  0.22030191  0.1891202\n",
      "  -1.18472517 -0.75119372  0.58830136]\n",
      " [ 0.03128453  0.3319661   0.6239183  -1.07538749  0.03444841  0.10163546\n",
      "  -1.19183415  0.34617418  0.30557119]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.59874018  1.59251303 -2.88140514 -0.07186161  1.69614465 -2.64174513\n",
      "  -0.81507261]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:74 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.9014186]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 74 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.86306722 -1.07723746  0.31843347  1.02934018 -0.14353134 -0.28219153\n",
      "   1.06677696  0.67050757 -0.46195229]\n",
      " [-0.73840553  1.26486055 -0.50368791 -1.176158    0.08240241  0.23377937\n",
      "  -1.26973733 -0.87164268  0.51578816]\n",
      " [ 0.52629192 -0.32921345  0.82098135 -0.88146021  0.40527533 -0.43591302\n",
      "  -0.91458414  0.67201189  0.64212932]\n",
      " [ 0.95094516 -1.23260323  0.38323931  1.05447686 -0.16327333 -0.19306183\n",
      "   1.16434231  0.35638516 -0.75474158]\n",
      " [-0.93628512  1.22314857 -0.37047054 -0.92157991  0.22030191  0.1891202\n",
      "  -1.18530481 -0.75119372  0.58830136]\n",
      " [ 0.03043009  0.3319661   0.6239183  -1.07624193  0.03444841  0.10163546\n",
      "  -1.19268859  0.34617418  0.30557119]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.59436006  1.59667682 -2.88123071 -0.07089891  1.70034789 -2.64154544\n",
      "  -0.81464962]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:74 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.74413361]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 74 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.86851733 -1.07723746  0.31843347  1.02934018 -0.13808123 -0.27674142\n",
      "   1.07222707  0.67050757 -0.46195229]\n",
      " [-0.74381817  1.26486055 -0.50368791 -1.176158    0.07698977  0.22836674\n",
      "  -1.27514997 -0.87164268  0.51578816]\n",
      " [ 0.52384955 -0.32921345  0.82098135 -0.88146021  0.40283296 -0.43835539\n",
      "  -0.91702651  0.67201189  0.64212932]\n",
      " [ 0.95631518 -1.23260323  0.38323931  1.05447686 -0.15790331 -0.18769181\n",
      "   1.16971233  0.35638516 -0.75474158]\n",
      " [-0.94168639  1.22314857 -0.37047054 -0.92157991  0.21490064  0.18371894\n",
      "  -1.19070608 -0.75119372  0.58830136]\n",
      " [ 0.02557534  0.3319661   0.6239183  -1.07624193  0.02959366  0.0967807\n",
      "  -1.19754335  0.34617418  0.30557119]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.57000173  1.61660652 -2.87744255 -0.06123419  1.72112745 -2.63782147\n",
      "  -0.80822325]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:74 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57300741]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 74 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.85374623 -1.07723746  0.31843347  1.01456908 -0.13808123 -0.29151252\n",
      "   1.07222707  0.67050757 -0.47672339]\n",
      " [-0.72896536  1.26486055 -0.50368791 -1.16130519  0.07698977  0.24321954\n",
      "  -1.27514997 -0.87164268  0.53064096]\n",
      " [ 0.5265296  -0.32921345  0.82098135 -0.87878016  0.40283296 -0.43567533\n",
      "  -0.91702651  0.67201189  0.64480938]\n",
      " [ 0.94206161 -1.23260323  0.38323931  1.04022329 -0.15790331 -0.20194538\n",
      "   1.16971233  0.35638516 -0.76899515]\n",
      " [-0.92729079  1.22314857 -0.37047054 -0.90718431  0.21490064  0.19811454\n",
      "  -1.19070608 -0.75119372  0.60269697]\n",
      " [ 0.03582193  0.3319661   0.6239183  -1.06599533  0.02959366  0.10702729\n",
      "  -1.19754335  0.34617418  0.31581778]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.64010057  1.56324864 -2.89397217 -0.09359297  1.66895396 -2.65544318\n",
      "  -0.83229305]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:74 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.73715642]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 74 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.85944525 -1.07153844  0.31843347  1.0202681  -0.13808123 -0.2858135\n",
      "   1.07792609  0.67050757 -0.47672339]\n",
      " [-0.73464078  1.25918513 -0.50368791 -1.16698061  0.07698977  0.23754413\n",
      "  -1.28082538 -0.87164268  0.53064096]\n",
      " [ 0.5212315  -0.33451156  0.82098135 -0.88407826  0.40283296 -0.44097343\n",
      "  -0.92232461  0.67201189  0.64480938]\n",
      " [ 0.94770469 -1.22696015  0.38323931  1.04586637 -0.15790331 -0.1963023\n",
      "   1.17535541  0.35638516 -0.76899515]\n",
      " [-0.93298417  1.21745519 -0.37047054 -0.91287769  0.21490064  0.19242116\n",
      "  -1.19639947 -0.75119372  0.60269697]\n",
      " [ 0.03023254  0.32637671  0.6239183  -1.07158472  0.02959366  0.10143791\n",
      "  -1.20313273  0.34617418  0.31581778]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.6146367   1.58433371 -2.88989491 -0.09064776  1.69054223 -2.65117965\n",
      "  -0.82864588]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:74 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.79928793]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 74 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.86291751 -1.06806618  0.31843347  1.02374036 -0.13808123 -0.2858135\n",
      "   1.08139835  0.67050757 -0.47672339]\n",
      " [-0.7380853   1.25574061 -0.50368791 -1.17042513  0.07698977  0.23754413\n",
      "  -1.2842699  -0.87164268  0.53064096]\n",
      " [ 0.51763446 -0.33810859  0.82098135 -0.8876753   0.40283296 -0.44097343\n",
      "  -0.92592165  0.67201189  0.64480938]\n",
      " [ 0.95113445 -1.22353039  0.38323931  1.04929613 -0.15790331 -0.1963023\n",
      "   1.17878517  0.35638516 -0.76899515]\n",
      " [-0.93649721  1.21394215 -0.37047054 -0.91639073  0.21490064  0.19242116\n",
      "  -1.19991251 -0.75119372  0.60269697]\n",
      " [ 0.02678404  0.32292821  0.6239183  -1.07503322  0.02959366  0.10143791\n",
      "  -1.20658123  0.34617418  0.31581778]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.59853691  1.59831333 -2.88784221 -0.08798729  1.70462259 -2.64894406\n",
      "  -0.82658393]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:74 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.81585704]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 74 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.86583933 -1.06806618  0.3213553   1.02374036 -0.13808123 -0.28289167\n",
      "   1.08432017  0.67050757 -0.47672339]\n",
      " [-0.74073171  1.25574061 -0.50633433 -1.17042513  0.07698977  0.23489772\n",
      "  -1.28691632 -0.87164268  0.53064096]\n",
      " [ 0.51753669 -0.33810859  0.82088358 -0.8876753   0.40283296 -0.44107121\n",
      "  -0.92601942  0.67201189  0.64480938]\n",
      " [ 0.95375221 -1.22353039  0.38585707  1.04929613 -0.15790331 -0.19368454\n",
      "   1.18140293  0.35638516 -0.76899515]\n",
      " [-0.93911741  1.21394215 -0.37309074 -0.91639073  0.21490064  0.18980096\n",
      "  -1.2025327  -0.75119372  0.60269697]\n",
      " [ 0.02529098  0.32292821  0.62242523 -1.07503322  0.02959366  0.09994484\n",
      "  -1.2080743   0.34617418  0.31581778]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.58470461  1.61046298 -2.88656853 -0.08116893  1.71721363 -2.64770008\n",
      "  -0.82121278]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:74 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.14220861]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 75 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.86511347 -1.06879205  0.3213553   1.0230145  -0.13808123 -0.28289167\n",
      "   1.08432017  0.67050757 -0.47744925]\n",
      " [-0.7404622   1.25601012 -0.50633433 -1.17015562  0.07698977  0.23489772\n",
      "  -1.28691632 -0.87164268  0.53091048]\n",
      " [ 0.51767411 -0.33797117  0.82088358 -0.88753788  0.40283296 -0.44107121\n",
      "  -0.92601942  0.67201189  0.6449468 ]\n",
      " [ 0.9537294  -1.2235532   0.38585707  1.04927331 -0.15790331 -0.19368454\n",
      "   1.18140293  0.35638516 -0.76901797]\n",
      " [-0.93903315  1.2140264  -0.37309074 -0.91630648  0.21490064  0.18980096\n",
      "  -1.2025327  -0.75119372  0.60278122]\n",
      " [ 0.02614559  0.32378282  0.62242523 -1.07417861  0.02959366  0.09994484\n",
      "  -1.2080743   0.34617418  0.31667239]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.5933783   1.60538581 -2.89063516 -0.08536826  1.71285397 -2.65195265\n",
      "  -0.82467074]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:75 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.84355653]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 75 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.86710164 -1.06879205  0.32334346  1.0230145  -0.13808123 -0.28289167\n",
      "   1.08630834  0.67050757 -0.47744925]\n",
      " [-0.74224393  1.25601012 -0.50811606 -1.17015562  0.07698977  0.23489772\n",
      "  -1.28869805 -0.87164268  0.53091048]\n",
      " [ 0.51869471 -0.33797117  0.82190418 -0.88753788  0.40283296 -0.44107121\n",
      "  -0.92499882  0.67201189  0.6449468 ]\n",
      " [ 0.95552151 -1.2235532   0.38764918  1.04927331 -0.15790331 -0.19368454\n",
      "   1.18319505  0.35638516 -0.76901797]\n",
      " [-0.94083042  1.2140264  -0.37488801 -0.91630648  0.21490064  0.18980096\n",
      "  -1.20432997 -0.75119372  0.60278122]\n",
      " [ 0.02480903  0.32378282  0.62108867 -1.07417861  0.02959366  0.09994484\n",
      "  -1.20941086  0.34617418  0.31667239]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.58305546  1.61474273 -2.88987614 -0.07915704  1.72240879 -2.65118012\n",
      "  -0.82091672]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:75 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.60339478]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 75 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.85165318 -1.08424051  0.307895    1.0230145  -0.13808123 -0.28289167\n",
      "   1.07085988  0.67050757 -0.47744925]\n",
      " [-0.72650259  1.27175147 -0.49237471 -1.17015562  0.07698977  0.23489772\n",
      "  -1.2729567  -0.87164268  0.53091048]\n",
      " [ 0.51729564 -0.33937025  0.82050511 -0.88753788  0.40283296 -0.44107121\n",
      "  -0.9263979   0.67201189  0.6449468 ]\n",
      " [ 0.93971651 -1.2393582   0.37184418  1.04927331 -0.15790331 -0.19368454\n",
      "   1.16739005  0.35638516 -0.76901797]\n",
      " [-0.92501561  1.22984122 -0.35907319 -0.91630648  0.21490064  0.18980096\n",
      "  -1.18851515 -0.75119372  0.60278122]\n",
      " [ 0.02907453  0.32804832  0.62535417 -1.07417861  0.02959366  0.09994484\n",
      "  -1.20514536  0.34617418  0.31667239]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.65525452  1.55915393 -2.90554563 -0.11665705  1.6656381  -2.66656959\n",
      "  -0.85270977]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:75 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.67601912]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 75 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.85938764 -1.08424051  0.31562946  1.0230145  -0.13808123 -0.28289167\n",
      "   1.07085988  0.67824203 -0.47744925]\n",
      " [-0.73376498  1.27175147 -0.49963711 -1.17015562  0.07698977  0.23489772\n",
      "  -1.2729567  -0.87890507  0.53091048]\n",
      " [ 0.52472645 -0.33937025  0.82793592 -0.88753788  0.40283296 -0.44107121\n",
      "  -0.9263979   0.6794427   0.6449468 ]\n",
      " [ 0.94761735 -1.2393582   0.37974503  1.04927331 -0.15790331 -0.19368454\n",
      "   1.16739005  0.36428601 -0.76901797]\n",
      " [-0.93239514  1.22984122 -0.36645273 -0.91630648  0.21490064  0.18980096\n",
      "  -1.18851515 -0.75857326  0.60278122]\n",
      " [ 0.03605232  0.32804832  0.63233197 -1.07417861  0.02959366  0.09994484\n",
      "  -1.20514536  0.35315198  0.31667239]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.61977581  1.58972835 -2.90164221 -0.0853711   1.69548639 -2.66247009\n",
      "  -0.82676856]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:75 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.61483382]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 75 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.86952488 -1.08424051  0.31562946  1.03315174 -0.13808123 -0.28289167\n",
      "   1.07085988  0.67824203 -0.46731201]\n",
      " [-0.74386371  1.27175147 -0.49963711 -1.18025435  0.07698977  0.23489772\n",
      "  -1.2729567  -0.87890507  0.52081174]\n",
      " [ 0.52788005 -0.33937025  0.82793592 -0.88438427  0.40283296 -0.44107121\n",
      "  -0.9263979   0.6794427   0.64810041]\n",
      " [ 0.95742984 -1.2393582   0.37974503  1.0590858  -0.15790331 -0.19368454\n",
      "   1.16739005  0.36428601 -0.75920548]\n",
      " [-0.9422535   1.22984122 -0.36645273 -0.92616484  0.21490064  0.18980096\n",
      "  -1.18851515 -0.75857326  0.59292286]\n",
      " [ 0.02881054  0.32804832  0.63233197 -1.08142039  0.02959366  0.09994484\n",
      "  -1.20514536  0.35315198  0.30943061]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.5741696   1.62634871 -2.89242364 -0.05937238  1.73076034 -2.65228134\n",
      "  -0.811852  ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:75 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.08418026]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 75 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.86954009 -1.08424051  0.31562946  1.03315174 -0.13806602 -0.28287646\n",
      "   1.07085988  0.67824203 -0.46729679]\n",
      " [-0.74393564  1.27175147 -0.49963711 -1.18025435  0.07691784  0.23482579\n",
      "  -1.2729567  -0.87890507  0.52073982]\n",
      " [ 0.52720144 -0.33937025  0.82793592 -0.88438427  0.40215435 -0.44174982\n",
      "  -0.9263979   0.6794427   0.64742179]\n",
      " [ 0.95755352 -1.2393582   0.37974503  1.0590858  -0.15777963 -0.19356086\n",
      "   1.16739005  0.36428601 -0.7590818 ]\n",
      " [-0.94229839  1.22984122 -0.36645273 -0.92616484  0.21485576  0.18975608\n",
      "  -1.18851515 -0.75857326  0.59287797]\n",
      " [ 0.0284511   0.32804832  0.63233197 -1.08142039  0.02923421  0.09958539\n",
      "  -1.20514536  0.35315198  0.30907116]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.57741449  1.62474149 -2.89411811 -0.06182962  1.72926206 -2.65394869\n",
      "  -0.81384715]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:75 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.00805394]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 75 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.86954673 -1.08423387  0.31562946  1.03315174 -0.13805938 -0.28286982\n",
      "   1.07085988  0.67824203 -0.46729016]\n",
      " [-0.74394275  1.27174436 -0.49963711 -1.18025435  0.07691073  0.23481868\n",
      "  -1.2729567  -0.87890507  0.52073271]\n",
      " [ 0.52719595 -0.33937573  0.82793592 -0.88438427  0.40214886 -0.44175531\n",
      "  -0.9263979   0.6794427   0.64741631]\n",
      " [ 0.95756067 -1.23935106  0.37974503  1.0590858  -0.15777249 -0.19355372\n",
      "   1.16739005  0.36428601 -0.75907466]\n",
      " [-0.9423054   1.2298342  -0.36645273 -0.92616484  0.21484874  0.18974906\n",
      "  -1.18851515 -0.75857326  0.59287095]\n",
      " [ 0.02844562  0.32804284  0.63233197 -1.08142039  0.02922873  0.09957991\n",
      "  -1.20514536  0.35315198  0.30906568]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.57744666  1.62473347 -2.89414371 -0.06185179  1.72925566 -2.65397389\n",
      "  -0.81386931]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:75 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.28307765]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 75 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.86656466 -1.08721594  0.31562946  1.03315174 -0.14104145 -0.28585189\n",
      "   1.0678778   0.67824203 -0.46729016]\n",
      " [-0.74097198  1.27471513 -0.49963711 -1.18025435  0.0798815   0.23778945\n",
      "  -1.26998593 -0.87890507  0.52073271]\n",
      " [ 0.53201678 -0.3345549   0.82793592 -0.88438427  0.40696969 -0.43693448\n",
      "  -0.92157707  0.6794427   0.64741631]\n",
      " [ 0.95398525 -1.24292648  0.37974503  1.0590858  -0.1613479  -0.19712913\n",
      "   1.16381463  0.36428601 -0.75907466]\n",
      " [-0.93895164  1.23318796 -0.36645273 -0.92616484  0.21820251  0.19310282\n",
      "  -1.18516139 -0.75857326  0.59287095]\n",
      " [ 0.03299914  0.33259637  0.63233197 -1.08142039  0.03378226  0.10413344\n",
      "  -1.20059184  0.35315198  0.30906568]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.60617122  1.60729407 -2.90544133 -0.07089173  1.71114542 -2.66484297\n",
      "  -0.82327449]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:75 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.90342227]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 75 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.8671463  -1.08721594  0.31562946  1.03373337 -0.14104145 -0.28585189\n",
      "   1.06845944  0.67824203 -0.46729016]\n",
      " [-0.74148206  1.27471513 -0.49963711 -1.18076444  0.0798815   0.23778945\n",
      "  -1.27049602 -0.87890507  0.52073271]\n",
      " [ 0.5311     -0.3345549   0.82793592 -0.88530105  0.40696969 -0.43693448\n",
      "  -0.92249385  0.6794427   0.64741631]\n",
      " [ 0.95449978 -1.24292648  0.37974503  1.05960034 -0.1613479  -0.19712913\n",
      "   1.16432917  0.36428601 -0.75907466]\n",
      " [-0.93950634  1.23318796 -0.36645273 -0.92671953  0.21820251  0.19310282\n",
      "  -1.18571609 -0.75857326  0.59287095]\n",
      " [ 0.03218114  0.33259637  0.63233197 -1.08223838  0.03378226  0.10413344\n",
      "  -1.20140983  0.35315198  0.30906568]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.60195799  1.61130122 -2.90527491 -0.06997079  1.71518993 -2.66465251\n",
      "  -0.8228724 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:75 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.74635757]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 75 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.87251874 -1.08721594  0.31562946  1.03373337 -0.135669   -0.28047945\n",
      "   1.07383189  0.67824203 -0.46729016]\n",
      " [-0.74681568  1.27471513 -0.49963711 -1.18076444  0.07454788  0.23245583\n",
      "  -1.27582963 -0.87890507  0.52073271]\n",
      " [ 0.52867999 -0.3345549   0.82793592 -0.88530105  0.40454968 -0.43935449\n",
      "  -0.92491386  0.6794427   0.64741631]\n",
      " [ 0.95979161 -1.24292648  0.37974503  1.05960034 -0.15605608 -0.19183731\n",
      "   1.16962099  0.36428601 -0.75907466]\n",
      " [-0.94482895  1.23318796 -0.36645273 -0.92671953  0.21287989  0.18778021\n",
      "  -1.1910387  -0.75857326  0.59287095]\n",
      " [ 0.02738388  0.33259637  0.63233197 -1.08223838  0.028985    0.09933618\n",
      "  -1.2062071   0.35315198  0.30906568]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.57794973  1.63096088 -2.90154858 -0.06045892  1.73567507 -2.6609875\n",
      "  -0.81656232]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:75 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57258795]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 75 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.8577582  -1.08721594  0.31562946  1.01897283 -0.135669   -0.29523999\n",
      "   1.07383189  0.67824203 -0.4820507 ]\n",
      " [-0.73197689  1.27471513 -0.49963711 -1.16592565  0.07454788  0.24729462\n",
      "  -1.27582963 -0.87890507  0.53557149]\n",
      " [ 0.53126789 -0.3345549   0.82793592 -0.88271315  0.40454968 -0.43676659\n",
      "  -0.92491386  0.6794427   0.6500042 ]\n",
      " [ 0.94554414 -1.24292648  0.37974503  1.04535287 -0.15605608 -0.20608478\n",
      "   1.16962099  0.36428601 -0.77332213]\n",
      " [-0.9304424   1.23318796 -0.36645273 -0.91233298  0.21287989  0.20216676\n",
      "  -1.1910387  -0.75857326  0.60725751]\n",
      " [ 0.03760206  0.33259637  0.63233197 -1.0720202   0.028985    0.10955436\n",
      "  -1.2062071   0.35315198  0.31928386]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.64801473  1.57763742 -2.91808828 -0.092894    1.68352514 -2.6786053\n",
      "  -0.84064992]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:75 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.73759312]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 75 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.86344205 -1.08153208  0.31562946  1.02465669 -0.135669   -0.28955613\n",
      "   1.07951574  0.67824203 -0.4820507 ]\n",
      " [-0.73763918  1.26905284 -0.49963711 -1.17158794  0.07454788  0.24163233\n",
      "  -1.28149193 -0.87890507  0.53557149]\n",
      " [ 0.52600421 -0.33981858  0.82793592 -0.88797683  0.40454968 -0.44203027\n",
      "  -0.93017753  0.6794427   0.6500042 ]\n",
      " [ 0.95117555 -1.23729507  0.37974503  1.05098428 -0.15605608 -0.20045337\n",
      "   1.1752524   0.36428601 -0.77332213]\n",
      " [-0.93612147  1.22750889 -0.36645273 -0.91801206  0.21287989  0.19648768\n",
      "  -1.19671778 -0.75857326  0.60725751]\n",
      " [ 0.03203647  0.32703078  0.63233197 -1.07758579  0.028985    0.10398877\n",
      "  -1.21177269  0.35315198  0.31928386]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.62262037  1.5986516  -2.91400293 -0.08999175  1.70503497 -2.67433404\n",
      "  -0.83704299]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:75 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.80085834]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 75 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.86686692 -1.07810722  0.31562946  1.02808155 -0.135669   -0.28955613\n",
      "   1.0829406   0.67824203 -0.4820507 ]\n",
      " [-0.7410379   1.26565413 -0.49963711 -1.17498666  0.07454788  0.24163233\n",
      "  -1.28489064 -0.87890507  0.53557149]\n",
      " [ 0.52245873 -0.34336406  0.82793592 -0.89152231  0.40454968 -0.44203027\n",
      "  -0.93372302  0.6794427   0.6500042 ]\n",
      " [ 0.95455995 -1.23391067  0.37974503  1.05436868 -0.15605608 -0.20045337\n",
      "   1.1786368   0.36428601 -0.77332213]\n",
      " [-0.93958743  1.22404293 -0.36645273 -0.92147802  0.21287989  0.19648768\n",
      "  -1.20018374 -0.75857326  0.60725751]\n",
      " [ 0.02864439  0.32363869  0.63233197 -1.08097788  0.028985    0.10398877\n",
      "  -1.21516477  0.35315198  0.31928386]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.60674039  1.61244032 -2.91197544 -0.08739441  1.71891987 -2.67212619\n",
      "  -0.8350307 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:75 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.81676788]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 75 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.8697639  -1.07810722  0.31852645  1.02808155 -0.135669   -0.28665915\n",
      "   1.08583759  0.67824203 -0.4820507 ]\n",
      " [-0.74366565  1.26565413 -0.50226486 -1.17498666  0.07454788  0.23900457\n",
      "  -1.2875184  -0.87890507  0.53557149]\n",
      " [ 0.52237182 -0.34336406  0.82784901 -0.89152231  0.40454968 -0.44211718\n",
      "  -0.93380993  0.6794427   0.6500042 ]\n",
      " [ 0.95715917 -1.23391067  0.38234425  1.05436868 -0.15605608 -0.19785414\n",
      "   1.18123602  0.36428601 -0.77332213]\n",
      " [-0.94218942  1.22404293 -0.36905472 -0.92147802  0.21287989  0.19388569\n",
      "  -1.20278573 -0.75857326  0.60725751]\n",
      " [ 0.02717683  0.32363869  0.63086441 -1.08097788  0.028985    0.10252121\n",
      "  -1.21663233  0.35315198  0.31928386]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.59302931  1.62448196 -2.91070768 -0.08062579  1.73139567 -2.67088781\n",
      "  -0.82969279]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:75 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.13811109]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 76 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.86908936 -1.07878177  0.31852645  1.02740701 -0.135669   -0.28665915\n",
      "   1.08583759  0.67824203 -0.48272524]\n",
      " [-0.74342517  1.26589461 -0.50226486 -1.17474617  0.07454788  0.23900457\n",
      "  -1.2875184  -0.87890507  0.53581198]\n",
      " [ 0.52250015 -0.34323573  0.82784901 -0.89139398  0.40454968 -0.44211718\n",
      "  -0.93380993  0.6794427   0.65013254]\n",
      " [ 0.95715035 -1.2339195   0.38234425  1.05435985 -0.15605608 -0.19785414\n",
      "   1.18123602  0.36428601 -0.77333095]\n",
      " [-0.94212293  1.22410943 -0.36905472 -0.92141152  0.21287989  0.19388569\n",
      "  -1.20278573 -0.75857326  0.60732401]\n",
      " [ 0.02798654  0.3244484   0.63086441 -1.08016817  0.028985    0.10252121\n",
      "  -1.21663233  0.35315198  0.32009358]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.60124943  1.61968446 -2.9145767  -0.08460743  1.72727678 -2.67493136\n",
      "  -0.83297017]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:76 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.84514567]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 76 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.87103907 -1.07878177  0.32047616  1.02740701 -0.135669   -0.28665915\n",
      "   1.0877873   0.67824203 -0.48272524]\n",
      " [-0.74517458  1.26589461 -0.50401428 -1.17474617  0.07454788  0.23900457\n",
      "  -1.28926781 -0.87890507  0.53581198]\n",
      " [ 0.5235109  -0.34323573  0.82885975 -0.89139398  0.40454968 -0.44211718\n",
      "  -0.93279918  0.6794427   0.65013254]\n",
      " [ 0.95890975 -1.2339195   0.38410365  1.05435985 -0.15605608 -0.19785414\n",
      "   1.18299543  0.36428601 -0.77333095]\n",
      " [-0.94388774  1.22410943 -0.37081953 -0.92141152  0.21287989  0.19388569\n",
      "  -1.20455054 -0.75857326  0.60732401]\n",
      " [ 0.02667796  0.3244484   0.62955583 -1.08016817  0.028985    0.10252121\n",
      "  -1.21794091  0.35315198  0.32009358]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.59111619  1.6288718  -2.91383127 -0.07850059  1.73665593 -2.67417254\n",
      "  -0.82928105]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:76 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.59972976]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 76 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.85567009 -1.09415075  0.30510717  1.02740701 -0.135669   -0.28665915\n",
      "   1.07241832  0.67824203 -0.48272524]\n",
      " [-0.72951608  1.28155311 -0.48835578 -1.17474617  0.07454788  0.23900457\n",
      "  -1.27360931 -0.87890507  0.53581198]\n",
      " [ 0.52213916 -0.34460746  0.82748802 -0.89139398  0.40454968 -0.44211718\n",
      "  -0.93417092  0.6794427   0.65013254]\n",
      " [ 0.94318563 -1.24964362  0.36837953  1.05435985 -0.15605608 -0.19785414\n",
      "   1.1672713   0.36428601 -0.77333095]\n",
      " [-0.92815383  1.23984334 -0.35508562 -0.92141152  0.21287989  0.19388569\n",
      "  -1.18881663 -0.75857326  0.60732401]\n",
      " [ 0.03088812  0.32865857  0.63376599 -1.08016817  0.028985    0.10252121\n",
      "  -1.21373075  0.35315198  0.32009358]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.66309995  1.57354384 -2.92958269 -0.11586553  1.68018462 -2.6896478\n",
      "  -0.86102315]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:76 with online instance: 2-------------\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.67820437]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 76 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.86331292 -1.09415075  0.31275001  1.02740701 -0.135669   -0.28665915\n",
      "   1.07241832  0.68588486 -0.48272524]\n",
      " [-0.73669038  1.28155311 -0.49553008 -1.17474617  0.07454788  0.23900457\n",
      "  -1.27360931 -0.88607937  0.53581198]\n",
      " [ 0.52945556 -0.34460746  0.83480441 -0.89139398  0.40454968 -0.44211718\n",
      "  -0.93417092  0.68675909  0.65013254]\n",
      " [ 0.95100016 -1.24964362  0.37619407  1.05435985 -0.15605608 -0.19785414\n",
      "   1.1672713   0.37210054 -0.77333095]\n",
      " [-0.93544438  1.23984334 -0.36237618 -0.92141152  0.21287989  0.19388569\n",
      "  -1.18881663 -0.76586382  0.60732401]\n",
      " [ 0.03785704  0.32865857  0.64073491 -1.08016817  0.028985    0.10252121\n",
      "  -1.21373075  0.36012089  0.32009358]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.62798509  1.60384208 -2.92574075 -0.08483047  1.70976378 -2.68561372\n",
      "  -0.83522971]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:76 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.61639766]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 76 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.87339679 -1.09415075  0.31275001  1.03749088 -0.135669   -0.28665915\n",
      "   1.07241832  0.68588486 -0.47264137]\n",
      " [-0.74673621  1.28155311 -0.49553008 -1.184792    0.07454788  0.23900457\n",
      "  -1.27360931 -0.88607937  0.52576615]\n",
      " [ 0.53265617 -0.34460746  0.83480441 -0.88819337  0.40454968 -0.44211718\n",
      "  -0.93417092  0.68675909  0.65333314]\n",
      " [ 0.96076866 -1.24964362  0.37619407  1.06412834 -0.15605608 -0.19785414\n",
      "   1.1672713   0.37210054 -0.76356246]\n",
      " [-0.9452565   1.23984334 -0.36237618 -0.93122363  0.21287989  0.19388569\n",
      "  -1.18881663 -0.76586382  0.59751189]\n",
      " [ 0.03064996  0.32865857  0.64073491 -1.08737525  0.028985    0.10252121\n",
      "  -1.21373075  0.36012089  0.3128865 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.5826334   1.64027987 -2.91659274 -0.05890953  1.74487385 -2.67551022\n",
      "  -0.82040402]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:76 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.08161051]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 76 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.87341328 -1.09415075  0.31275001  1.03749088 -0.13565251 -0.28664265\n",
      "   1.07241832  0.68588486 -0.47262488]\n",
      " [-0.74680685  1.28155311 -0.49553008 -1.184792    0.07447725  0.23893394\n",
      "  -1.27360931 -0.88607937  0.52569552]\n",
      " [ 0.53201411 -0.34460746  0.83480441 -0.88819337  0.40390762 -0.44275924\n",
      "  -0.93417092  0.68675909  0.65269108]\n",
      " [ 0.96088774 -1.24964362  0.37619407  1.06412834 -0.155937   -0.19773506\n",
      "   1.1672713   0.37210054 -0.76344338]\n",
      " [-0.94530159  1.23984334 -0.36237618 -0.93122363  0.2128348   0.1938406\n",
      "  -1.18881663 -0.76586382  0.5974668 ]\n",
      " [ 0.03030649  0.32865857  0.64073491 -1.08737525  0.02864153  0.10217774\n",
      "  -1.21373075  0.36012089  0.31254303]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.58569177  1.63876718 -2.91819266 -0.06123151  1.74346424 -2.67708451\n",
      "  -0.82228974]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:76 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.00754078]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 76 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.87341914 -1.09414489  0.31275001  1.03749088 -0.13564666 -0.2866368\n",
      "   1.07241832  0.68588486 -0.47261903]\n",
      " [-0.7468131   1.28154686 -0.49553008 -1.184792    0.074471    0.23892769\n",
      "  -1.27360931 -0.88607937  0.52568927]\n",
      " [ 0.53200927 -0.3446123   0.83480441 -0.88819337  0.40390279 -0.44276407\n",
      "  -0.93417092  0.68675909  0.65268625]\n",
      " [ 0.96089401 -1.24963734  0.37619407  1.06412834 -0.15593072 -0.19772879\n",
      "   1.1672713   0.37210054 -0.76343711]\n",
      " [-0.94530776  1.23983717 -0.36237618 -0.93122363  0.21282863  0.19383443\n",
      "  -1.18881663 -0.76586382  0.59746063]\n",
      " [ 0.03030165  0.32865373  0.64073491 -1.08737525  0.02863669  0.1021729\n",
      "  -1.21373075  0.36012089  0.31253819]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.58571999  1.63876022 -2.91821518 -0.06125099  1.74345869 -2.67710668\n",
      "  -0.82230922]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:76 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.27744481]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 76 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.87056722 -1.09699681  0.31275001  1.03749088 -0.13849858 -0.28948872\n",
      "   1.0695664   0.68588486 -0.47261903]\n",
      " [-0.74398495  1.28437501 -0.49553008 -1.184792    0.07729914  0.24175583\n",
      "  -1.27078117 -0.88607937  0.52568927]\n",
      " [ 0.53670818 -0.33991339  0.83480441 -0.88819337  0.40860169 -0.43806517\n",
      "  -0.92947201  0.68675909  0.65268625]\n",
      " [ 0.95748514 -1.25304622  0.37619407  1.06412834 -0.1593396  -0.20113766\n",
      "   1.16386243  0.37210054 -0.76343711]\n",
      " [-0.94211141  1.24303352 -0.36237618 -0.93122363  0.21602499  0.19703079\n",
      "  -1.18562027 -0.76586382  0.59746063]\n",
      " [ 0.03472903  0.33308111  0.64073491 -1.08737525  0.03306407  0.10660028\n",
      "  -1.20930336  0.36012089  0.31253819]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.61352955  1.62191506 -2.92920571 -0.06995835  1.72598634 -2.68768693\n",
      "  -0.83138961]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:76 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.90536829]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 76 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.87112368 -1.09699681  0.31275001  1.03804734 -0.13849858 -0.28948872\n",
      "   1.07012286  0.68588486 -0.47261903]\n",
      " [-0.74447326  1.28437501 -0.49553008 -1.1852803   0.07729914  0.24175583\n",
      "  -1.27126947 -0.88607937  0.52568927]\n",
      " [ 0.53582472 -0.33991339  0.83480441 -0.88907683  0.40860169 -0.43806517\n",
      "  -0.93035547  0.68675909  0.65268625]\n",
      " [ 0.95797764 -1.25304622  0.37619407  1.06462084 -0.1593396  -0.20113766\n",
      "   1.16435493  0.37210054 -0.76343711]\n",
      " [-0.94264243  1.24303352 -0.36237618 -0.93175466  0.21602499  0.19703079\n",
      "  -1.1861513  -0.76586382  0.59746063]\n",
      " [ 0.0339457   0.33308111  0.64073491 -1.08815858  0.03306407  0.10660028\n",
      "  -1.2100867   0.36012089  0.31253819]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.60947569  1.62577252 -2.92904687 -0.06907709  1.7298792  -2.68750518\n",
      "  -0.83100723]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:76 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.7485744]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 76 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.87641887 -1.09699681  0.31275001  1.03804734 -0.13320339 -0.28419353\n",
      "   1.07541805  0.68588486 -0.47261903]\n",
      " [-0.74972837  1.28437501 -0.49553008 -1.1852803   0.07204404  0.23650072\n",
      "  -1.27652458 -0.88607937  0.52568927]\n",
      " [ 0.53342617 -0.33991339  0.83480441 -0.88907683  0.40620314 -0.44046373\n",
      "  -0.93275402  0.68675909  0.65268625]\n",
      " [ 0.96319172 -1.25304622  0.37619407  1.06462084 -0.15412551 -0.19592358\n",
      "   1.16956901  0.37210054 -0.76343711]\n",
      " [-0.94788688  1.24303352 -0.36237618 -0.93175466  0.21078054  0.19178634\n",
      "  -1.19139574 -0.76586382  0.59746063]\n",
      " [ 0.02920591  0.33308111  0.64073491 -1.08815858  0.02832427  0.10186049\n",
      "  -1.21482649  0.36012089  0.31253819]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.58581519  1.64516334 -2.9253819  -0.05971791  1.75007206 -2.68389883\n",
      "  -0.8248122 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:76 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57210031]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 76 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.86167082 -1.09699681  0.31275001  1.02329929 -0.13320339 -0.29894158\n",
      "   1.07541805  0.68588486 -0.48736708]\n",
      " [-0.73490551  1.28437501 -0.49553008 -1.17045744  0.07204404  0.25132358\n",
      "  -1.27652458 -0.88607937  0.54051212]\n",
      " [ 0.53592421 -0.33991339  0.83480441 -0.88657879  0.40620314 -0.43796569\n",
      "  -0.93275402  0.68675909  0.65518429]\n",
      " [ 0.94895246 -1.25304622  0.37619407  1.05038159 -0.15412551 -0.21016284\n",
      "   1.16956901  0.37210054 -0.77767636]\n",
      " [-0.93351147  1.24303352 -0.36237618 -0.91737925  0.21078054  0.20616175\n",
      "  -1.19139574 -0.76586382  0.61183604]\n",
      " [ 0.03939437  0.33308111  0.64073491 -1.07797012  0.02832427  0.11204895\n",
      "  -1.21482649  0.36012089  0.32272665]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.65584071  1.59188049 -2.94193214 -0.09222406  1.6979519  -2.70151332\n",
      "  -0.84891623]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:76 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.73801532]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 76 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.86733998 -1.09132766  0.31275001  1.02896845 -0.13320339 -0.29327243\n",
      "   1.08108721  0.68588486 -0.48736708]\n",
      " [-0.74055505  1.27872547 -0.49553008 -1.17610698  0.07204404  0.24567404\n",
      "  -1.28217412 -0.88607937  0.54051212]\n",
      " [ 0.53069484 -0.34514275  0.83480441 -0.89180815  0.40620314 -0.44319505\n",
      "  -0.93798338  0.68675909  0.65518429]\n",
      " [ 0.95457251 -1.24742618  0.37619407  1.05600163 -0.15412551 -0.20454279\n",
      "   1.17518906  0.37210054 -0.77767636]\n",
      " [-0.93917662  1.23736836 -0.36237618 -0.9230444   0.21078054  0.20049659\n",
      "  -1.1970609  -0.76586382  0.61183604]\n",
      " [ 0.03385233  0.32753908  0.64073491 -1.08351215  0.02832427  0.10650691\n",
      "  -1.22036853  0.36012089  0.32272665]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.63051351  1.61282586 -2.93783864 -0.08936419  1.71938572 -2.69723431\n",
      "  -0.84534897]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:76 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.80241044]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 76 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.87071821 -1.08794943  0.31275001  1.03234668 -0.13320339 -0.29327243\n",
      "   1.08446544  0.68588486 -0.48736708]\n",
      " [-0.74390864  1.27537187 -0.49553008 -1.17946058  0.07204404  0.24567404\n",
      "  -1.28552771 -0.88607937  0.54051212]\n",
      " [ 0.5272004  -0.3486372   0.83480441 -0.8953026   0.40620314 -0.44319505\n",
      "  -0.94147783  0.68675909  0.65518429]\n",
      " [ 0.9579122  -1.24408649  0.37619407  1.05934132 -0.15412551 -0.20454279\n",
      "   1.17852875  0.37210054 -0.77767636]\n",
      " [-0.94259623  1.23394876 -0.36237618 -0.926464    0.21078054  0.20049659\n",
      "  -1.2004805  -0.76586382  0.61183604]\n",
      " [ 0.03051588  0.32420262  0.64073491 -1.08684861  0.02832427  0.10650691\n",
      "  -1.22370498  0.36012089  0.32272665]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.6148498   1.62642676 -2.93583605 -0.08682841  1.73307846 -2.69505385\n",
      "  -0.84338513]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:76 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.81764199]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 76 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.87359157 -1.08794943  0.31562337  1.03234668 -0.13320339 -0.29039907\n",
      "   1.0873388   0.68588486 -0.48736708]\n",
      " [-0.74651873  1.27537187 -0.49814016 -1.17946058  0.07204404  0.24306396\n",
      "  -1.2881378  -0.88607937  0.54051212]\n",
      " [ 0.52712336 -0.3486372   0.83472738 -0.8953026   0.40620314 -0.44327208\n",
      "  -0.94155487  0.68675909  0.65518429]\n",
      " [ 0.96049382 -1.24408649  0.37877569  1.05934132 -0.15412551 -0.20196117\n",
      "   1.18111037  0.37210054 -0.77767636]\n",
      " [-0.94518097  1.23394876 -0.36496092 -0.926464    0.21078054  0.19791185\n",
      "  -1.20306525 -0.76586382  0.61183604]\n",
      " [ 0.02907313  0.32420262  0.63929216 -1.08684861  0.02832427  0.10506416\n",
      "  -1.22514773  0.36012089  0.32272665]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.60125469  1.63836477 -2.93457371 -0.08010789  1.74544382 -2.69382059\n",
      "  -0.83807862]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:76 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.13411494]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 77 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.87296518 -1.08857581  0.31562337  1.0317203  -0.13320339 -0.29039907\n",
      "   1.0873388   0.68588486 -0.48799346]\n",
      " [-0.74630504  1.27558556 -0.49814016 -1.17924689  0.07204404  0.24306396\n",
      "  -1.2881378  -0.88607937  0.54072581]\n",
      " [ 0.52724324 -0.34851733  0.83472738 -0.89518272  0.40620314 -0.44327208\n",
      "  -0.94155487  0.68675909  0.65530416]\n",
      " [ 0.96049757 -1.24408273  0.37877569  1.05934507 -0.15412551 -0.20196117\n",
      "   1.18111037  0.37210054 -0.77767261]\n",
      " [-0.94513063  1.2339991  -0.36496092 -0.92641367  0.21078054  0.19791185\n",
      "  -1.20306525 -0.76586382  0.61188638]\n",
      " [ 0.02984015  0.32496964  0.63929216 -1.08608159  0.02832427  0.10506416\n",
      "  -1.22514773  0.36012089  0.32349367]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.60904195  1.63383329 -2.93825322 -0.08388158  1.74155395 -2.69766388\n",
      "  -0.84118348]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:77 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.84669095]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 77 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.8748779  -1.08857581  0.31753608  1.0317203  -0.13320339 -0.29039907\n",
      "   1.08925151  0.68588486 -0.48799346]\n",
      " [-0.74802334  1.27558556 -0.49985846 -1.17924689  0.07204404  0.24306396\n",
      "  -1.2898561  -0.88607937  0.54072581]\n",
      " [ 0.52824415 -0.34851733  0.8357283  -0.89518272  0.40620314 -0.44327208\n",
      "  -0.94055395  0.68675909  0.65530416]\n",
      " [ 0.96222546 -1.24408273  0.38050358  1.05934507 -0.15412551 -0.20196117\n",
      "   1.18283826  0.37210054 -0.77767261]\n",
      " [-0.94686419  1.2339991  -0.36669448 -0.92641367  0.21078054  0.19791185\n",
      "  -1.20479881 -0.76586382  0.61188638]\n",
      " [ 0.02855866  0.32496964  0.63801067 -1.08608159  0.02832427  0.10506416\n",
      "  -1.22642922  0.36012089  0.32349367]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.59909178  1.64285673 -2.93752083 -0.07787582  1.75076341 -2.6969182\n",
      "  -0.83755694]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:77 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.5960005]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 77 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.85959212 -1.10386159  0.3022503   1.0317203  -0.13320339 -0.29039907\n",
      "   1.07396573  0.68588486 -0.48799346]\n",
      " [-0.73245197  1.29115693 -0.48428709 -1.17924689  0.07204404  0.24306396\n",
      "  -1.27428473 -0.88607937  0.54072581]\n",
      " [ 0.52690243 -0.34985905  0.83438657 -0.89518272  0.40620314 -0.44327208\n",
      "  -0.94189567  0.68675909  0.65530416]\n",
      " [ 0.94658646 -1.25972174  0.36486457  1.05934507 -0.15412551 -0.20196117\n",
      "   1.16719926  0.37210054 -0.77767261]\n",
      " [-0.93121559  1.24964771 -0.35104587 -0.92641367  0.21078054  0.19791185\n",
      "  -1.1891502  -0.76586382  0.61188638]\n",
      " [ 0.03271463  0.32912561  0.64216664 -1.08608159  0.02832427  0.10506416\n",
      "  -1.22227325  0.36012089  0.32349367]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.67084544  1.58780064 -2.95334995 -0.11509564  1.69460165 -2.71247506\n",
      "  -0.86923948]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:77 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.68034544]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 77 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.86714527 -1.10386159  0.30980345  1.0317203  -0.13320339 -0.29039907\n",
      "   1.07396573  0.69343801 -0.48799346]\n",
      " [-0.73954037  1.29115693 -0.4913755  -1.17924689  0.07204404  0.24306396\n",
      "  -1.27428473 -0.89316778  0.54072581]\n",
      " [ 0.53410633 -0.34985905  0.84159048 -0.89518272  0.40620314 -0.44327208\n",
      "  -0.94189567  0.693963    0.65530416]\n",
      " [ 0.95431631 -1.25972174  0.37259443  1.05934507 -0.15412551 -0.20196117\n",
      "   1.16719926  0.3798304  -0.77767261]\n",
      " [-0.93841933  1.24964771 -0.35824962 -0.92641367  0.21078054  0.19791185\n",
      "  -1.1891502  -0.77306756  0.61188638]\n",
      " [ 0.03967261  0.32912561  0.64912463 -1.08608159  0.02832427  0.10506416\n",
      "  -1.22227325  0.36707888  0.32349367]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.63608692  1.61782713 -2.94956742 -0.08430827  1.72391609 -2.70850418\n",
      "  -0.8435917 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:77 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.61791881]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 77 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.87717696 -1.10386159  0.30980345  1.04175199 -0.13320339 -0.29039907\n",
      "   1.07396573  0.69343801 -0.47796177]\n",
      " [-0.74953443  1.29115693 -0.4913755  -1.18924095  0.07204404  0.24306396\n",
      "  -1.27428473 -0.89316778  0.53073175]\n",
      " [ 0.53735325 -0.34985905  0.84159048 -0.89193581  0.40620314 -0.44327208\n",
      "  -0.94189567  0.693963    0.65855107]\n",
      " [ 0.96404146 -1.25972174  0.37259443  1.06907022 -0.15412551 -0.20196117\n",
      "   1.16719926  0.3798304  -0.76794746]\n",
      " [-0.94818595  1.24964771 -0.35824962 -0.93618028  0.21078054  0.19791185\n",
      "  -1.1891502  -0.77306756  0.60211976]\n",
      " [ 0.03249974  0.32912561  0.64912463 -1.09325446  0.02832427  0.10506416\n",
      "  -1.22227325  0.36707888  0.31632079]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.59098316  1.6540862  -2.9404871  -0.05846242  1.75886542 -2.69848255\n",
      "  -0.82885399]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:77 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.07913147]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 77 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.87719453 -1.10386159  0.30980345  1.04175199 -0.13318582 -0.29038149\n",
      "   1.07396573  0.69343801 -0.47794419]\n",
      " [-0.74960369  1.29115693 -0.4913755  -1.18924095  0.07197478  0.2429947\n",
      "  -1.27428473 -0.89316778  0.5306625 ]\n",
      " [ 0.53674579 -0.34985905  0.84159048 -0.89193581  0.40559567 -0.44387954\n",
      "  -0.94189567  0.693963    0.65794361]\n",
      " [ 0.96415605 -1.25972174  0.37259443  1.06907022 -0.15401093 -0.20184658\n",
      "   1.16719926  0.3798304  -0.76783288]\n",
      " [-0.94823104  1.24964771 -0.35824962 -0.93618028  0.21073544  0.19786675\n",
      "  -1.1891502  -0.77306756  0.60207467]\n",
      " [ 0.03217162  0.32912561  0.64912463 -1.09325446  0.02799615  0.10473604\n",
      "  -1.22227325  0.36707888  0.31599267]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.5938663   1.65266221 -2.94199804 -0.06065684  1.75753892 -2.69996925\n",
      "  -0.83063655]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:77 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.00706494]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 77 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.8771997  -1.10385643  0.30980345  1.04175199 -0.13318065 -0.29037633\n",
      "   1.07396573  0.69343801 -0.47793903]\n",
      " [-0.74960918  1.29115143 -0.4913755  -1.18924095  0.07196928  0.2429892\n",
      "  -1.27428473 -0.89316778  0.530657  ]\n",
      " [ 0.53674152 -0.34986332  0.84159048 -0.89193581  0.40559141 -0.44388381\n",
      "  -0.94189567  0.693963    0.65793935]\n",
      " [ 0.96416156 -1.25971622  0.37259443  1.06907022 -0.15400541 -0.20184107\n",
      "   1.16719926  0.3798304  -0.76782736]\n",
      " [-0.94823648  1.24964227 -0.35824962 -0.93618028  0.21073001  0.19786132\n",
      "  -1.1891502  -0.77306756  0.60206923]\n",
      " [ 0.03216734  0.32912133  0.64912463 -1.09325446  0.02799187  0.10473176\n",
      "  -1.22227325  0.36707888  0.31598839]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.59389108  1.65265615 -2.94201787 -0.06067398  1.75753409 -2.69998877\n",
      "  -0.8306537 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:77 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.27197471]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 77 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.87447152 -1.10658461  0.30980345  1.04175199 -0.13590883 -0.29310451\n",
      "   1.07123755  0.69343801 -0.47793903]\n",
      " [-0.74691609  1.29384452 -0.4913755  -1.18924095  0.07466237  0.24568229\n",
      "  -1.27159164 -0.89316778  0.530657  ]\n",
      " [ 0.54132238 -0.34528246  0.84159048 -0.89193581  0.41017226 -0.43930295\n",
      "  -0.93731482  0.693963    0.65793935]\n",
      " [ 0.96091052 -1.26296726  0.37259443  1.06907022 -0.15725645 -0.20509211\n",
      "   1.16394822  0.3798304  -0.76782736]\n",
      " [-0.94518932  1.25268943 -0.35824962 -0.93618028  0.21377717  0.20090848\n",
      "  -1.18610304 -0.77306756  0.60206923]\n",
      " [ 0.03647307  0.33342706  0.64912463 -1.09325446  0.0322976   0.10903749\n",
      "  -1.21796753  0.36707888  0.31598839]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.62081719  1.63638253 -2.9527088  -0.06906034  1.74067392 -2.71028672\n",
      "  -0.8394202 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:77 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.90725925]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 77 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.8750041  -1.10658461  0.30980345  1.04228457 -0.13590883 -0.29310451\n",
      "   1.07177013  0.69343801 -0.47793903]\n",
      " [-0.74738372  1.29384452 -0.4913755  -1.18970857  0.07466237  0.24568229\n",
      "  -1.27205926 -0.89316778  0.530657  ]\n",
      " [ 0.54047085 -0.34528246  0.84159048 -0.89278734  0.41017226 -0.43930295\n",
      "  -0.93816635  0.693963    0.65793935]\n",
      " [ 0.9613821  -1.26296726  0.37259443  1.0695418  -0.15725645 -0.20509211\n",
      "   1.16441979  0.3798304  -0.76782736]\n",
      " [-0.94569786  1.25268943 -0.35824962 -0.93668882  0.21377717  0.20090848\n",
      "  -1.18661158 -0.77306756  0.60206923]\n",
      " [ 0.03572273  0.33342706  0.64912463 -1.09400479  0.0322976   0.10903749\n",
      "  -1.21871786  0.36707888  0.31598839]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.61691559  1.64009687 -2.95255712 -0.0682168   1.74442181 -2.71011322\n",
      "  -0.83905644]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:77 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.75078338]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 77 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.88022247 -1.10658461  0.30980345  1.04228457 -0.13069046 -0.28788613\n",
      "   1.0769885   0.69343801 -0.47793903]\n",
      " [-0.75256086  1.29384452 -0.4913755  -1.18970857  0.06948523  0.24050515\n",
      "  -1.2772364  -0.89316778  0.530657  ]\n",
      " [ 0.53809292 -0.34528246  0.84159048 -0.89278734  0.40779434 -0.44168088\n",
      "  -0.94054427  0.693963    0.65793935]\n",
      " [ 0.96651894 -1.26296726  0.37259443  1.0695418  -0.15211962 -0.19995527\n",
      "   1.16955663  0.3798304  -0.76782736]\n",
      " [-0.95086466  1.25268943 -0.35824962 -0.93668882  0.20861036  0.19574167\n",
      "  -1.19177838 -0.77306756  0.60206923]\n",
      " [ 0.03104036  0.33342706  0.64912463 -1.09400479  0.02761522  0.10435511\n",
      "  -1.22340023  0.36707888  0.31598839]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.59360041  1.65922017 -2.94895299 -0.05901     1.76432462 -2.70656517\n",
      "  -0.83297514]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:77 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57154525]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 77 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.86548884 -1.10658461  0.30980345  1.02755094 -0.13069046 -0.30261977\n",
      "   1.0769885   0.69343801 -0.49267266]\n",
      " [-0.73775583  1.29384452 -0.4913755  -1.17490354  0.06948523  0.25531018\n",
      "  -1.2772364  -0.89316778  0.54546203]\n",
      " [ 0.5405033  -0.34528246  0.84159048 -0.89037696  0.40779434 -0.4392705\n",
      "  -0.94054427  0.693963    0.66034973]\n",
      " [ 0.95228997 -1.26296726  0.37259443  1.05531283 -0.15211962 -0.21418424\n",
      "   1.16955663  0.3798304  -0.78205633]\n",
      " [-0.93650246  1.25268943 -0.35824962 -0.92232662  0.20861036  0.21010388\n",
      "  -1.19177838 -0.77306756  0.61643144]\n",
      " [ 0.04119782  0.33342706  0.64912463 -1.08384733  0.02761522  0.11451257\n",
      "  -1.22340023  0.36707888  0.32614586]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.66358078  1.60598406 -2.96551419 -0.09158209  1.71224038 -2.72417691\n",
      "  -0.85709418]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:77 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.73842481]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 77 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.8711437  -1.10092975  0.30980345  1.0332058  -0.13069046 -0.29696491\n",
      "   1.08264336  0.69343801 -0.49267266]\n",
      " [-0.74339292  1.28820743 -0.4913755  -1.18054063  0.06948523  0.24967309\n",
      "  -1.2828735  -0.89316778  0.54546203]\n",
      " [ 0.53530818 -0.35047758  0.84159048 -0.89557208  0.40779434 -0.44446562\n",
      "  -0.94573939  0.693963    0.66034973]\n",
      " [ 0.9578989  -1.25735833  0.37259443  1.06092176 -0.15211962 -0.20857531\n",
      "   1.17516556  0.3798304  -0.78205633]\n",
      " [-0.94215403  1.24703787 -0.35824962 -0.92797819  0.20861036  0.20445231\n",
      "  -1.19742995 -0.77306756  0.61643144]\n",
      " [ 0.03567916  0.3279084   0.64912463 -1.08936599  0.02761522  0.10899391\n",
      "  -1.2289189   0.36707888  0.32614586]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.63831868  1.62686246 -2.96141253 -0.08876402  1.73360038 -2.71989014\n",
      "  -0.85356605]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:77 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.80394592]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 77 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.874476   -1.09759745  0.30980345  1.03653809 -0.13069046 -0.29696491\n",
      "   1.08597566  0.69343801 -0.49267266]\n",
      " [-0.74670203  1.28489832 -0.4913755  -1.18384974  0.06948523  0.24967309\n",
      "  -1.2861826  -0.89316778  0.54546203]\n",
      " [ 0.53186429 -0.35392148  0.84159048 -0.89901598  0.40779434 -0.44446562\n",
      "  -0.94918329  0.693963    0.66034973]\n",
      " [ 0.96119448 -1.25406275  0.37259443  1.06421734 -0.15211962 -0.20857531\n",
      "   1.17846115  0.3798304  -0.78205633]\n",
      " [-0.94552794  1.24366396 -0.35824962 -0.9313521   0.20861036  0.20445231\n",
      "  -1.20080386 -0.77306756  0.61643144]\n",
      " [ 0.0323976   0.32462684  0.64912463 -1.09264755  0.02761522  0.10899391\n",
      "  -1.23220046  0.36707888  0.32614586]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.62286797  1.64027841 -2.95943456 -0.08628827  1.74710402 -2.71773676\n",
      "  -0.85164951]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:77 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.81848148]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 77 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.87732686 -1.09759745  0.31265431  1.03653809 -0.13069046 -0.29411404\n",
      "   1.08882652  0.69343801 -0.49267266]\n",
      " [-0.74929536  1.28489832 -0.49396883 -1.18384974  0.06948523  0.24707976\n",
      "  -1.28877594 -0.89316778  0.54546203]\n",
      " [ 0.53179622 -0.35392148  0.84152241 -0.89901598  0.40779434 -0.44453368\n",
      "  -0.94925136  0.693963    0.66034973]\n",
      " [ 0.96375937 -1.25406275  0.37515932  1.06421734 -0.15211962 -0.20601042\n",
      "   1.18102604  0.3798304  -0.78205633]\n",
      " [-0.94809632  1.24366396 -0.36081801 -0.9313521   0.20861036  0.20188392\n",
      "  -1.20337225 -0.77306756  0.61643144]\n",
      " [ 0.03097899  0.32462684  0.64770602 -1.09264755  0.02761522  0.10757531\n",
      "  -1.23361906  0.36707888  0.32614586]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.6093839   1.65211694 -2.95817718 -0.07961431  1.75936347 -2.71650819\n",
      "  -0.84637267]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:77 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.13021919]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 78 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.87674562 -1.09817869  0.31265431  1.03595686 -0.13069046 -0.29411404\n",
      "   1.08882652  0.69343801 -0.4932539 ]\n",
      " [-0.74910636  1.28508732 -0.49396883 -1.18366074  0.06948523  0.24707976\n",
      "  -1.28877594 -0.89316778  0.54565103]\n",
      " [ 0.53190819 -0.35380951  0.84152241 -0.898904    0.40779434 -0.44453368\n",
      "  -0.94925136  0.693963    0.6604617 ]\n",
      " [ 0.96377439 -1.25404773  0.37515932  1.06423235 -0.15211962 -0.20601042\n",
      "   1.18102604  0.3798304  -0.78204132]\n",
      " [-0.94806065  1.24369963 -0.36081801 -0.93131642  0.20861036  0.20188392\n",
      "  -1.20337225 -0.77306756  0.61646711]\n",
      " [ 0.03170543  0.32535327  0.64770602 -1.09192111  0.02761522  0.10757531\n",
      "  -1.23361906  0.36707888  0.32687229]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.61675836  1.64783828 -2.96167507 -0.0831895   1.75569126 -2.72015974\n",
      "  -0.84931284]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:78 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.84819491]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 78 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.87862271 -1.09817869  0.3145314   1.03595686 -0.13069046 -0.29411404\n",
      "   1.09070361  0.69343801 -0.4932539 ]\n",
      " [-0.75079468  1.28508732 -0.49565714 -1.18366074  0.06948523  0.24707976\n",
      "  -1.29046425 -0.89316778  0.54565103]\n",
      " [ 0.53289934 -0.35380951  0.84251356 -0.898904    0.40779434 -0.44453368\n",
      "  -0.94826021  0.693963    0.6604617 ]\n",
      " [ 0.96547188 -1.25404773  0.37685681  1.06423235 -0.15211962 -0.20601042\n",
      "   1.18272353  0.3798304  -0.78204132]\n",
      " [-0.94976408  1.24369963 -0.36252144 -0.93131642  0.20861036  0.20188392\n",
      "  -1.20507568 -0.77306756  0.61646711]\n",
      " [ 0.03045021  0.32535327  0.6464508  -1.09192111  0.02761522  0.10757531\n",
      "  -1.23487429  0.36707888  0.32687229]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.60698512  1.65670317 -2.96095522 -0.07728175  1.76473667 -2.71942671\n",
      "  -0.8457467 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:78 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.59221214]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 78 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.86342379 -1.11337761  0.29933248  1.03595686 -0.13069046 -0.29411404\n",
      "   1.07550469  0.69343801 -0.4932539 ]\n",
      " [-0.73531463  1.30056736 -0.4801771  -1.18366074  0.06948523  0.24707976\n",
      "  -1.2749842  -0.89316778  0.54565103]\n",
      " [ 0.53158993 -0.35511891  0.84120415 -0.898904    0.40779434 -0.44453368\n",
      "  -0.94956962  0.693963    0.6604617 ]\n",
      " [ 0.94992215 -1.26959747  0.36130708  1.06423235 -0.15211962 -0.20601042\n",
      "   1.1671738   0.3798304  -0.78204132]\n",
      " [-0.9342051   1.25925861 -0.34696246 -0.93131642  0.20861036  0.20188392\n",
      "  -1.1895167  -0.77306756  0.61646711]\n",
      " [ 0.03455304  0.32945611  0.65055363 -1.09192111  0.02761522  0.10757531\n",
      "  -1.23077145  0.36707888  0.32687229]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.67849383  1.60192983 -2.9768576  -0.11434668  1.70889448 -2.73506074\n",
      "  -0.87736111]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:78 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.68244581]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 78 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.87088908 -1.11337761  0.30679777  1.03595686 -0.13069046 -0.29411404\n",
      "   1.07550469  0.7009033  -0.4932539 ]\n",
      " [-0.74231917  1.30056736 -0.48718164 -1.18366074  0.06948523  0.24707976\n",
      "  -1.2749842  -0.90017232  0.54565103]\n",
      " [ 0.53868319 -0.35511891  0.84829741 -0.898904    0.40779434 -0.44453368\n",
      "  -0.94956962  0.70105626  0.6604617 ]\n",
      " [ 0.95756884 -1.26959747  0.36895378  1.06423235 -0.15211962 -0.20601042\n",
      "   1.1671738   0.38747709 -0.78204132]\n",
      " [-0.94132405  1.25925861 -0.35408141 -0.93131642  0.20861036  0.20188392\n",
      "  -1.1895167  -0.78018651  0.61646711]\n",
      " [ 0.04149804  0.32945611  0.65749863 -1.09192111  0.02761522  0.10757531\n",
      "  -1.23077145  0.37402388  0.32687229]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.64408468  1.63168867 -2.97313253 -0.08380409  1.7379483  -2.731151\n",
      "  -0.85185725]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:78 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.61939866]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 78 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.88086976 -1.11337761  0.30679777  1.04593754 -0.13069046 -0.29411404\n",
      "   1.07550469  0.7009033  -0.48327321]\n",
      " [-0.75226258  1.30056736 -0.48718164 -1.19360416  0.06948523  0.24707976\n",
      "  -1.2749842  -0.90017232  0.53570762]\n",
      " [ 0.5419758  -0.35511891  0.84829741 -0.8956114   0.40779434 -0.44453368\n",
      "  -0.94956962  0.70105626  0.66375431]\n",
      " [ 0.96725131 -1.26959747  0.36895378  1.07391482 -0.15211962 -0.20601042\n",
      "   1.1671738   0.38747709 -0.77235885]\n",
      " [-0.95104592  1.25925861 -0.35408141 -0.94103829  0.20861036  0.20188392\n",
      "  -1.1895167  -0.78018651  0.60674524]\n",
      " [ 0.03435888  0.32945611  0.65749863 -1.09906027  0.02761522  0.10757531\n",
      "  -1.23077145  0.37402388  0.31973313]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.59922245  1.6677728  -2.96411719 -0.05803068  1.77273996 -2.72120803\n",
      "  -0.83720472]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:78 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.07673843]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 78 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.88088825 -1.11337761  0.30679777  1.04593754 -0.13067197 -0.29409556\n",
      "   1.07550469  0.7009033  -0.48325472]\n",
      " [-0.75233038  1.30056736 -0.48718164 -1.19360416  0.06941743  0.24701196\n",
      "  -1.2749842  -0.90017232  0.53563982]\n",
      " [ 0.54140108 -0.35511891  0.84829741 -0.8956114   0.40721963 -0.4451084\n",
      "  -0.94956962  0.70105626  0.6631796 ]\n",
      " [ 0.96736151 -1.26959747  0.36895378  1.07391482 -0.15200941 -0.20590021\n",
      "   1.1671738   0.38747709 -0.77224865]\n",
      " [-0.95109086  1.25925861 -0.35408141 -0.94103829  0.20856543  0.20183899\n",
      "  -1.1895167  -0.78018651  0.60670031]\n",
      " [ 0.03404549  0.32945611  0.65749863 -1.09906027  0.02730183  0.10726192\n",
      "  -1.23077145  0.37402388  0.31941975]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.60194089  1.66643207 -2.96554432 -0.06010475  1.77149143 -2.72261222\n",
      "  -0.83888999]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:78 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.00662334]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 78 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.88089281 -1.11337305  0.30679777  1.04593754 -0.13066741 -0.294091\n",
      "   1.07550469  0.7009033  -0.48325016]\n",
      " [-0.75233522  1.30056252 -0.48718164 -1.19360416  0.06941259  0.24700712\n",
      "  -1.2749842  -0.90017232  0.53563498]\n",
      " [ 0.54139732 -0.35512268  0.84829741 -0.8956114   0.40721586 -0.44511216\n",
      "  -0.94956962  0.70105626  0.66317583]\n",
      " [ 0.96736637 -1.26959261  0.36895378  1.07391482 -0.15200456 -0.20589536\n",
      "   1.1671738   0.38747709 -0.77224379]\n",
      " [-0.95109565  1.25925382 -0.35408141 -0.94103829  0.20856064  0.2018342\n",
      "  -1.1895167  -0.78018651  0.60669552]\n",
      " [ 0.0340417   0.32945232  0.65749863 -1.09906027  0.02729805  0.10725813\n",
      "  -1.23077145  0.37402388  0.31941596]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.60196268  1.66642679 -2.9655618  -0.06011984  1.77148723 -2.72262944\n",
      "  -0.8389051 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:78 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.26666315]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 78 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.87828225 -1.11598361  0.30679777  1.04593754 -0.13327797 -0.29670156\n",
      "   1.07289412  0.7009033  -0.48325016]\n",
      " [-0.74976999  1.30312775 -0.48718164 -1.19360416  0.07197782  0.24957235\n",
      "  -1.27241898 -0.90017232  0.53563498]\n",
      " [ 0.54586384 -0.35065616  0.84829741 -0.8956114   0.41168238 -0.44064564\n",
      "  -0.9451031   0.70105626  0.66317583]\n",
      " [ 0.96426489 -1.27269409  0.36895378  1.07391482 -0.15510603 -0.20899683\n",
      "   1.16407232  0.38747709 -0.77224379]\n",
      " [-0.94818986  1.26215961 -0.35408141 -0.94103829  0.21146643  0.20473998\n",
      "  -1.18661091 -0.78018651  0.60669552]\n",
      " [ 0.03823009  0.33364071  0.65749863 -1.09906027  0.03148643  0.11144652\n",
      "  -1.22658307  0.37402388  0.31941596]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.62803619  1.65070268 -2.97596073 -0.06819665  1.75521438 -2.73265179\n",
      "  -0.84736851]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:78 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.9090976]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 78 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.87879215 -1.11598361  0.30679777  1.04644744 -0.13327797 -0.29670156\n",
      "   1.07340402  0.7009033  -0.48325016]\n",
      " [-0.75021796  1.30312775 -0.48718164 -1.19405213  0.07197782  0.24957235\n",
      "  -1.27286695 -0.90017232  0.53563498]\n",
      " [ 0.54504291 -0.35065616  0.84829741 -0.89643233  0.41168238 -0.44064564\n",
      "  -0.94592403  0.70105626  0.66317583]\n",
      " [ 0.96471658 -1.27269409  0.36895378  1.07436651 -0.15510603 -0.20899683\n",
      "   1.16452401  0.38747709 -0.77224379]\n",
      " [-0.94867704  1.26215961 -0.35408141 -0.94152547  0.21146643  0.20473998\n",
      "  -1.18709809 -0.78018651  0.60669552]\n",
      " [ 0.03751118  0.33364071  0.65749863 -1.09977918  0.03148643  0.11144652\n",
      "  -1.22730198  0.37402388  0.31941596]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.62428014  1.65428011 -2.97581584 -0.06738897  1.75882364 -2.73248609\n",
      "  -0.84702231]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:78 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.75298386]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 78 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.88393419 -1.11598361  0.30679777  1.04644744 -0.12813593 -0.29155952\n",
      "   1.07854606  0.7009033  -0.48325016]\n",
      " [-0.75531771  1.30312775 -0.48718164 -1.19405213  0.06687807  0.2444726\n",
      "  -1.27796669 -0.90017232  0.53563498]\n",
      " [ 0.54268488 -0.35065616  0.84829741 -0.89643233  0.40932435 -0.44300367\n",
      "  -0.94828206  0.70105626  0.66317583]\n",
      " [ 0.9697767  -1.27269409  0.36895378  1.07436651 -0.15004592 -0.20393672\n",
      "   1.16958413  0.38747709 -0.77224379]\n",
      " [-0.95376675  1.26215961 -0.35408141 -0.94152547  0.20637671  0.19965027\n",
      "  -1.1921878  -0.78018651  0.60669552]\n",
      " [ 0.03288616  0.33364071  0.65749863 -1.09977918  0.02686141  0.1068215\n",
      "  -1.231927    0.37402388  0.31941596]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.60130775  1.67313734 -2.97227201 -0.05833409  1.77843867 -2.72899597\n",
      "  -0.84105342]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:78 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57092361]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 78 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.86921687 -1.11598361  0.30679777  1.03173012 -0.12813593 -0.30627683\n",
      "   1.07854606  0.7009033  -0.49796748]\n",
      " [-0.74053239  1.30312775 -0.48718164 -1.17926681  0.06687807  0.25925792\n",
      "  -1.27796669 -0.90017232  0.5504203 ]\n",
      " [ 0.54500971 -0.35065616  0.84829741 -0.8941075   0.40932435 -0.44067884\n",
      "  -0.94828206  0.70105626  0.66550066]\n",
      " [ 0.95556007 -1.27269409  0.36895378  1.06014988 -0.15004592 -0.21815334\n",
      "   1.16958413  0.38747709 -0.78646042]\n",
      " [-0.9394198   1.26215961 -0.35408141 -0.92717851  0.20637671  0.21399722\n",
      "  -1.1921878  -0.78018651  0.62104247]\n",
      " [ 0.04301137  0.33364071  0.65749863 -1.08965397  0.02686141  0.11694671\n",
      "  -1.231927    0.37402388  0.32954118]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.67123728  1.61995403 -2.98884449 -0.09096711  1.72639646 -2.74660539\n",
      "  -0.86518599]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:78 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.73882339]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 78 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.87485778 -1.11034271  0.30679777  1.03737103 -0.12813593 -0.30063593\n",
      "   1.08418697  0.7009033  -0.49796748]\n",
      " [-0.74615728  1.29750286 -0.48718164 -1.1848917   0.06687807  0.25363303\n",
      "  -1.28359158 -0.90017232  0.5504203 ]\n",
      " [ 0.53984878 -0.35581708  0.84829741 -0.89926842  0.40932435 -0.44583976\n",
      "  -0.95344298  0.70105626  0.66550066]\n",
      " [ 0.96115809 -1.26709607  0.36895378  1.0657479  -0.15004592 -0.21255532\n",
      "   1.17518215  0.38747709 -0.78646042]\n",
      " [-0.94505804  1.25652136 -0.35408141 -0.93281676  0.20637671  0.20835898\n",
      "  -1.19782605 -0.78018651  0.62104247]\n",
      " [ 0.03751596  0.32814529  0.65749863 -1.09514938  0.02686141  0.11145129\n",
      "  -1.23742241  0.37402388  0.32954118]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.64603852  1.64076709 -2.98473471 -0.08819024  1.74768458 -2.74231093\n",
      "  -0.86169649]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:78 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.80546641]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 78 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.87814479 -1.1070557   0.30679777  1.04065804 -0.12813593 -0.30063593\n",
      "   1.08747398  0.7009033  -0.49796748]\n",
      " [-0.74942248  1.29423766 -0.48718164 -1.1881569   0.06687807  0.25363303\n",
      "  -1.28685679 -0.90017232  0.5504203 ]\n",
      " [ 0.53645499 -0.35921088  0.84829741 -0.90266222  0.40932435 -0.44583976\n",
      "  -0.95683678  0.70105626  0.66550066]\n",
      " [ 0.96441012 -1.26384404  0.36895378  1.06899993 -0.15004592 -0.21255532\n",
      "   1.17843418  0.38747709 -0.78646042]\n",
      " [-0.94838687  1.25319253 -0.35408141 -0.93614559  0.20637671  0.20835898\n",
      "  -1.20115488 -0.78018651  0.62104247]\n",
      " [ 0.03428859  0.32491793  0.65749863 -1.09837675  0.02686141  0.11145129\n",
      "  -1.24064978  0.37402388  0.32954118]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.63079776  1.65400075 -2.98278113 -0.08577306  1.76100197 -2.74018435\n",
      "  -0.85982613]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:78 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.81928834]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 78 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.88097421 -1.1070557   0.30962719  1.04065804 -0.12813593 -0.2978065\n",
      "   1.0903034   0.7009033  -0.49796748]\n",
      " [-0.75199992  1.29423766 -0.48975907 -1.1881569   0.06687807  0.2510556\n",
      "  -1.28943422 -0.90017232  0.5504203 ]\n",
      " [ 0.53639505 -0.35921088  0.84823747 -0.90266222  0.40932435 -0.44589971\n",
      "  -0.95689672  0.70105626  0.66550066]\n",
      " [ 0.9669591  -1.26384404  0.37150275  1.06899993 -0.15004592 -0.21000635\n",
      "   1.18098316  0.38747709 -0.78646042]\n",
      " [-0.95093973  1.25319253 -0.35663426 -0.93614559  0.20637671  0.20580612\n",
      "  -1.20370774 -0.78018651  0.62104247]\n",
      " [ 0.03289351  0.32491793  0.65610355 -1.09837675  0.02686141  0.11005621\n",
      "  -1.24204486  0.37402388  0.32954118]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.61742013  1.66574367 -2.98152828 -0.07914419  1.77315975 -2.73896009\n",
      "  -0.85457731]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:78 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.1264229]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 79 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.88043526 -1.10759465  0.30962719  1.04011909 -0.12813593 -0.2978065\n",
      "   1.0903034   0.7009033  -0.49850643]\n",
      " [-0.75183363  1.29440395 -0.48975907 -1.18799061  0.06687807  0.2510556\n",
      "  -1.28943422 -0.90017232  0.55058659]\n",
      " [ 0.53649963 -0.35910629  0.84823747 -0.90255764  0.40932435 -0.44589971\n",
      "  -0.95689672  0.70105626  0.66560524]\n",
      " [ 0.96698413 -1.263819    0.37150275  1.06902497 -0.15004592 -0.21000635\n",
      "   1.18098316  0.38747709 -0.78643539]\n",
      " [-0.9509173   1.25321495 -0.35663426 -0.93612317  0.20637671  0.20580612\n",
      "  -1.20370774 -0.78018651  0.6210649 ]\n",
      " [ 0.03358139  0.32560581  0.65610355 -1.09768887  0.02686141  0.11005621\n",
      "  -1.24204486  0.37402388  0.33022906]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.62440121  1.66170513 -2.98485227 -0.08253008  1.76969425 -2.7424282\n",
      "  -0.85736043]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:79 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.84965999]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 79 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.88227799 -1.10759465  0.31146992  1.04011909 -0.12813593 -0.2978065\n",
      "   1.09214613  0.7009033  -0.49850643]\n",
      " [-0.75349299  1.29440395 -0.49141844 -1.18799061  0.06687807  0.2510556\n",
      "  -1.29109359 -0.90017232  0.55058659]\n",
      " [ 0.53748106 -0.35910629  0.8492189  -0.90255764  0.40932435 -0.44589971\n",
      "  -0.95591529  0.70105626  0.66560524]\n",
      " [ 0.96865226 -1.263819    0.37317088  1.06902497 -0.15004592 -0.21000635\n",
      "   1.18265129  0.38747709 -0.78643539]\n",
      " [-0.95259165  1.25321495 -0.35830861 -0.93612317  0.20637671  0.20580612\n",
      "  -1.20538208 -0.78018651  0.6210649 ]\n",
      " [ 0.03235166  0.32560581  0.65487381 -1.09768887  0.02686141  0.11005621\n",
      "  -1.2432746   0.37402388  0.33022906]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.61479915  1.67041648 -2.98414449 -0.0767174   1.77858091 -2.74170735\n",
      "  -0.85385264]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:79 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.58836995]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 79 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.8671695  -1.12270313  0.29636143  1.04011909 -0.12813593 -0.2978065\n",
      "   1.07703764  0.7009033  -0.49850643]\n",
      " [-0.73810837  1.30978857 -0.47603381 -1.18799061  0.06687807  0.2510556\n",
      "  -1.27570897 -0.90017232  0.55058659]\n",
      " [ 0.53620595 -0.36038141  0.84794379 -0.90255764  0.40932435 -0.44589971\n",
      "  -0.9571904   0.70105626  0.66560524]\n",
      " [ 0.95319586 -1.27927541  0.35771448  1.06902497 -0.15004592 -0.21000635\n",
      "   1.16719488  0.38747709 -0.78643539]\n",
      " [-0.93712652  1.26868008 -0.34284348 -0.93612317  0.20637671  0.20580612\n",
      "  -1.18991696 -0.78018651  0.6210649 ]\n",
      " [ 0.03640235  0.3296565   0.6589245  -1.09768887  0.02686141  0.11005621\n",
      "  -1.23922391  0.37402388  0.33022906]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.68604803  1.6159366  -3.00011546 -0.11361805  1.72306813 -2.7574139\n",
      "  -0.88539044]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:79 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.68450875]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 79 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.87454863 -1.12270313  0.30374056  1.04011909 -0.12813593 -0.2978065\n",
      "   1.07703764  0.70828244 -0.49850643]\n",
      " [-0.74503093  1.30978857 -0.48295638 -1.18799061  0.06687807  0.2510556\n",
      "  -1.27570897 -0.90709488  0.55058659]\n",
      " [ 0.54319035 -0.36038141  0.85492819 -0.90255764  0.40932435 -0.44589971\n",
      "  -0.9571904   0.70804066  0.66560524]\n",
      " [ 0.96076081 -1.27927541  0.36527944  1.06902497 -0.15004592 -0.21000635\n",
      "   1.16719488  0.39504205 -0.78643539]\n",
      " [-0.94416254  1.26868008 -0.3498795  -0.93612317  0.20637671  0.20580612\n",
      "  -1.18991696 -0.78722253  0.6210649 ]\n",
      " [ 0.04333228  0.3296565   0.66585443 -1.09768887  0.02686141  0.11005621\n",
      "  -1.23922391  0.38095381  0.33022906]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.65198184  1.64543155 -2.99644607 -0.08331763  1.75186511 -2.7535634\n",
      "  -0.86002906]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:79 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.62083857]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 79 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.88447946 -1.12270313  0.30374056  1.05004992 -0.12813593 -0.2978065\n",
      "   1.07703764  0.70828244 -0.4885756 ]\n",
      " [-0.75492481  1.30978857 -0.48295638 -1.19788448  0.06687807  0.2510556\n",
      "  -1.27570897 -0.90709488  0.54069272]\n",
      " [ 0.5465281  -0.36038141  0.85492819 -0.89921989  0.40932435 -0.44589971\n",
      "  -0.9571904   0.70804066  0.66894299]\n",
      " [ 0.97040128 -1.27927541  0.36527944  1.07866543 -0.15004592 -0.21000635\n",
      "   1.16719488  0.39504205 -0.77679492]\n",
      " [-0.95384043  1.26868008 -0.3498795  -0.94580106  0.20637671  0.20580612\n",
      "  -1.18991696 -0.78722253  0.61138701]\n",
      " [ 0.03622633  0.3296565   0.66585443 -1.10479482  0.02686141  0.11005621\n",
      "  -1.23922391  0.38095381  0.32312311]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.60735491  1.68134445 -2.9874931  -0.057614    1.78650215 -2.74369602\n",
      "  -0.84545905]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:79 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.07442703]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 79 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.88449871 -1.12270313  0.30374056  1.05004992 -0.12811668 -0.29778726\n",
      "   1.07703764  0.70828244 -0.48855635]\n",
      " [-0.75499109  1.30978857 -0.48295638 -1.19788448  0.06681179  0.25098931\n",
      "  -1.27570897 -0.90709488  0.54062643]\n",
      " [ 0.54598439 -0.36038141  0.85492819 -0.89921989  0.40878064 -0.44644342\n",
      "  -0.9571904   0.70804066  0.66839928]\n",
      " [ 0.97050722 -1.27927541  0.36527944  1.07866543 -0.14993998 -0.20990041\n",
      "   1.16719488  0.39504205 -0.77668898]\n",
      " [-0.95388506  1.26868008 -0.3498795  -0.94580106  0.20633208  0.20576149\n",
      "  -1.18991696 -0.78722253  0.61134237]\n",
      " [ 0.03592708  0.3296565   0.66585443 -1.10479482  0.02656217  0.10975697\n",
      "  -1.23922391  0.38095381  0.32282386]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.60991846  1.68008192 -2.98884128 -0.05957448  1.7853268  -2.74502247\n",
      "  -0.84705251]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:79 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.00621316]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 79 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.88450274 -1.1226991   0.30374056  1.05004992 -0.12811265 -0.29778322\n",
      "   1.07703764  0.70828244 -0.48855232]\n",
      " [-0.75499536  1.3097843  -0.48295638 -1.19788448  0.06680752  0.25098505\n",
      "  -1.27570897 -0.90709488  0.54062217]\n",
      " [ 0.54598106 -0.36038474  0.85492819 -0.89921989  0.40877731 -0.44644674\n",
      "  -0.9571904   0.70804066  0.66839595]\n",
      " [ 0.97051149 -1.27927113  0.36527944  1.07866543 -0.1499357  -0.20989613\n",
      "   1.16719488  0.39504205 -0.77668471]\n",
      " [-0.95388929  1.26867585 -0.3498795  -0.94580106  0.20632785  0.20575726\n",
      "  -1.18991696 -0.78722253  0.61133815]\n",
      " [ 0.03592373  0.32965315  0.66585443 -1.10479482  0.02655881  0.10975362\n",
      "  -1.23922391  0.38095381  0.32282051]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.60993764  1.68007732 -2.9888567  -0.05958778  1.78532314 -2.74503766\n",
      "  -0.84706584]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:79 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.26150593]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 79 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.88200396 -1.12519788  0.30374056  1.05004992 -0.13061143 -0.30028201\n",
      "   1.07453886  0.70828244 -0.48855232]\n",
      " [-0.75255116  1.3122285  -0.48295638 -1.19788448  0.06925172  0.25342924\n",
      "  -1.27326477 -0.90709488  0.54062217]\n",
      " [ 0.55033682 -0.35602898  0.85492819 -0.89921989  0.41313307 -0.44209098\n",
      "  -0.95283464  0.70804066  0.66839595]\n",
      " [ 0.96755172 -1.2822309   0.36527944  1.07866543 -0.15289547 -0.2128559\n",
      "   1.16423511  0.39504205 -0.77668471]\n",
      " [-0.95111744  1.2714477  -0.3498795  -0.94580106  0.2090997   0.20852911\n",
      "  -1.18714511 -0.78722253  0.61133815]\n",
      " [ 0.03999892  0.33372834  0.66585443 -1.10479482  0.030634    0.11382881\n",
      "  -1.23514872  0.38095381  0.32282051]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.63518873  1.66488136 -2.99897131 -0.06736631  1.76961365 -2.75479124\n",
      "  -0.85523677]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:79 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.91088559]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 79 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.8824923  -1.12519788  0.30374056  1.05053826 -0.13061143 -0.30028201\n",
      "   1.0750272   0.70828244 -0.48855232]\n",
      " [-0.75298044  1.3122285  -0.48295638 -1.19831376  0.06925172  0.25342924\n",
      "  -1.27369405 -0.90709488  0.54062217]\n",
      " [ 0.54954523 -0.35602898  0.85492819 -0.90001148  0.41313307 -0.44209098\n",
      "  -0.95362623  0.70804066  0.66839595]\n",
      " [ 0.9679845  -1.2822309   0.36527944  1.07909821 -0.15289547 -0.2128559\n",
      "   1.16466788  0.39504205 -0.77668471]\n",
      " [-0.9515843   1.2714477  -0.3498795  -0.94626792  0.2090997   0.20852911\n",
      "  -1.18761197 -0.78722253  0.61133815]\n",
      " [ 0.03930995  0.33372834  0.66585443 -1.10548379  0.030634    0.11382881\n",
      "  -1.23583768  0.38095381  0.32282051]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.63157189  1.66832775 -2.99883284 -0.06659276  1.77309026 -2.75463293\n",
      "  -0.85490718]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:79 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.75517519]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 79 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.88755852 -1.12519788  0.30374056  1.05053826 -0.12554521 -0.29521579\n",
      "   1.08009342  0.70828244 -0.48855232]\n",
      " [-0.7580034   1.3122285  -0.48295638 -1.19831376  0.06422876  0.24840629\n",
      "  -1.27871701 -0.90709488  0.54062217]\n",
      " [ 0.54720643 -0.35602898  0.85492819 -0.90001148  0.41079428 -0.44442978\n",
      "  -0.95596502  0.70804066  0.66839595]\n",
      " [ 0.97296846 -1.2822309   0.36527944  1.07909821 -0.14791152 -0.20787195\n",
      "   1.16965184  0.39504205 -0.77668471]\n",
      " [-0.95659751  1.2714477  -0.3498795  -0.94626792  0.20408649  0.2035159\n",
      "  -1.19262518 -0.78722253  0.61133815]\n",
      " [ 0.0347422   0.33372834  0.66585443 -1.10548379  0.02606626  0.10926106\n",
      "  -1.24040543  0.38095381  0.32282051]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.60893959  1.68692045 -2.99534873 -0.05768919  1.79241986 -2.75120031\n",
      "  -0.84904929]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:79 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.5702363]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 79 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.87285941 -1.12519788  0.30374056  1.03583915 -0.12554521 -0.3099149\n",
      "   1.08009342  0.70828244 -0.50325143]\n",
      " [-0.74323966  1.3122285  -0.48295638 -1.18355002  0.06422876  0.26317003\n",
      "  -1.27871701 -0.90709488  0.55538591]\n",
      " [ 0.54944771 -0.35602898  0.85492819 -0.8977702   0.41079428 -0.44218851\n",
      "  -0.95596502  0.70804066  0.67063723]\n",
      " [ 0.9587662  -1.2822309   0.36527944  1.06489595 -0.14791152 -0.2220742\n",
      "   1.16965184  0.39504205 -0.79088696]\n",
      " [-0.94226782  1.2714477  -0.3498795  -0.93193822  0.20408649  0.2178456\n",
      "  -1.19262518 -0.78722253  0.62566784]\n",
      " [ 0.04483395  0.33372834  0.66585443 -1.09539204  0.02606626  0.1193528\n",
      "  -1.24040543  0.38095381  0.33291225]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.6788126   1.63379596 -3.01193277 -0.09037822  1.74042574 -2.76880781\n",
      "  -0.87319387]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:79 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.7392128]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 79 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.87848664 -1.11957065  0.30374056  1.04146638 -0.12554521 -0.30428767\n",
      "   1.08572065  0.70828244 -0.50325143]\n",
      " [-0.74885253  1.30661562 -0.48295638 -1.1891629   0.06422876  0.25755715\n",
      "  -1.28432988 -0.90709488  0.55538591]\n",
      " [ 0.54432097 -0.36115571  0.85492819 -0.90289694  0.41079428 -0.44731524\n",
      "  -0.96109176  0.70804066  0.67063723]\n",
      " [ 0.96435345 -1.27664365  0.36527944  1.07048321 -0.14791152 -0.21648695\n",
      "   1.17523909  0.39504205 -0.79088696]\n",
      " [-0.94789296  1.26582256 -0.3498795  -0.93756336  0.20408649  0.21222046\n",
      "  -1.19825032 -0.78722253  0.62566784]\n",
      " [ 0.03936171  0.3282561   0.66585443 -1.10086428  0.02606626  0.11388056\n",
      "  -1.24587767  0.38095381  0.33291225]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.65367568  1.65454509 -3.00781495 -0.08764197  1.76164366 -2.76450577\n",
      "  -0.86974253]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:79 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.80697349]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 79 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.88172895 -1.11632834  0.30374056  1.04470869 -0.12554521 -0.30428767\n",
      "   1.08896296  0.70828244 -0.50325143]\n",
      " [-0.75207436  1.3033938  -0.48295638 -1.19238473  0.06422876  0.25755715\n",
      "  -1.28755171 -0.90709488  0.55538591]\n",
      " [ 0.54097685 -0.36449983  0.85492819 -0.90624106  0.41079428 -0.44731524\n",
      "  -0.96443588  0.70804066  0.67063723]\n",
      " [ 0.96756243 -1.27343467  0.36527944  1.07369219 -0.14791152 -0.21648695\n",
      "   1.17844807  0.39504205 -0.79088696]\n",
      " [-0.95117727  1.26253825 -0.3498795  -0.94084767  0.20408649  0.21222046\n",
      "  -1.20153463 -0.78722253  0.62566784]\n",
      " [ 0.03618787  0.32508227  0.66585443 -1.10403812  0.02606626  0.11388056\n",
      "  -1.24905151  0.38095381  0.33291225]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.63864208  1.66759893 -3.00588556 -0.0852819   1.77477745 -2.76240576\n",
      "  -0.86791726]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:79 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.8200645]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 79 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.88453792 -1.11632834  0.30654953  1.04470869 -0.12554521 -0.3014787\n",
      "   1.09177193  0.70828244 -0.50325143]\n",
      " [-0.75463669  1.3033938  -0.4855187  -1.19238473  0.06422876  0.25499483\n",
      "  -1.29011404 -0.90709488  0.55538591]\n",
      " [ 0.54092426 -0.36449983  0.8548756  -0.90624106  0.41079428 -0.44736783\n",
      "  -0.96448847  0.70804066  0.67063723]\n",
      " [ 0.97009625 -1.27343467  0.36781325  1.07369219 -0.14791152 -0.21395314\n",
      "   1.18098189  0.39504205 -0.79088696]\n",
      " [-0.95371536  1.26253825 -0.35241759 -0.94084767  0.20408649  0.20968237\n",
      "  -1.20407272 -0.78722253  0.62566784]\n",
      " [ 0.03481574  0.32508227  0.6644823  -1.10403812  0.02606626  0.11250842\n",
      "  -1.25042364  0.38095381  0.33291225]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.62536655  1.67924988 -3.00463685 -0.07869673  1.78683754 -2.76118542\n",
      "  -0.86269494]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:79 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.12272503]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 80 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.88403853 -1.11682773  0.30654953  1.04420931 -0.12554521 -0.3014787\n",
      "   1.09177193  0.70828244 -0.50375082]\n",
      " [-0.75449123  1.30353925 -0.4855187  -1.19223927  0.06422876  0.25499483\n",
      "  -1.29011404 -0.90709488  0.55553137]\n",
      " [ 0.54102192 -0.36440218  0.8548756  -0.9061434   0.41079428 -0.44736783\n",
      "  -0.96448847  0.70804066  0.67073488]\n",
      " [ 0.97013016 -1.27340076  0.36781325  1.0737261  -0.14791152 -0.21395314\n",
      "   1.18098189  0.39504205 -0.79085305]\n",
      " [-0.95370486  1.26254875 -0.35241759 -0.94083718  0.20408649  0.20968237\n",
      "  -1.20407272 -0.78722253  0.62567834]\n",
      " [ 0.03546701  0.32573354  0.6644823  -1.10338684  0.02606626  0.11250842\n",
      "  -1.25042364  0.38095381  0.33356352]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.63197306  1.67543922 -3.00779445 -0.08190227  1.7835682  -2.76447818\n",
      "  -0.86532842]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:80 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.8510885]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 80 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.88584809 -1.11682773  0.30835909  1.04420931 -0.12554521 -0.3014787\n",
      "   1.09358148  0.70828244 -0.50375082]\n",
      " [-0.75612262  1.30353925 -0.4871501  -1.19223927  0.06422876  0.25499483\n",
      "  -1.29174543 -0.90709488  0.55553137]\n",
      " [ 0.54199372 -0.36440218  0.85584741 -0.9061434   0.41079428 -0.44736783\n",
      "  -0.96351667  0.70804066  0.67073488]\n",
      " [ 0.97176989 -1.27340076  0.36945299  1.0737261  -0.14791152 -0.21395314\n",
      "   1.18262162  0.39504205 -0.79085305]\n",
      " [-0.95535109  1.26254875 -0.35406382 -0.94083718  0.20408649  0.20968237\n",
      "  -1.20571895 -0.78722253  0.62567834]\n",
      " [ 0.03426201  0.32573354  0.6632773  -1.10338684  0.02606626  0.11250842\n",
      "  -1.25162864  0.38095381  0.33356352]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.62253677  1.68400176 -3.00709831 -0.07618189  1.79230109 -2.76376907\n",
      "  -0.86187705]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:80 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.58447931]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 80 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.87083351 -1.13184231  0.2933445   1.04420931 -0.12554521 -0.3014787\n",
      "   1.0785669   0.70828244 -0.50375082]\n",
      " [-0.74083739  1.31882449 -0.47186486 -1.19223927  0.06422876  0.25499483\n",
      "  -1.2764602  -0.90709488  0.55553137]\n",
      " [ 0.54075457 -0.36564133  0.85460825 -0.9061434   0.41079428 -0.44736783\n",
      "  -0.96475582  0.70804066  0.67073488]\n",
      " [ 0.95641074 -1.28875992  0.35409383  1.0737261  -0.14791152 -0.21395314\n",
      "   1.16726247  0.39504205 -0.79085305]\n",
      " [-0.93998391  1.27791592 -0.33869664 -0.94083718  0.20408649  0.20968237\n",
      "  -1.19035177 -0.78722253  0.62567834]\n",
      " [ 0.03826149  0.32973302  0.66727678 -1.10338684  0.02606626  0.11250842\n",
      "  -1.24762916  0.38095381  0.33356352]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.69351104  1.62982578 -3.02313301 -0.1129092   1.73712735 -2.77954331\n",
      "  -0.89332986]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:80 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.68653728]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 80 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.87812808 -1.13184231  0.30063907  1.04420931 -0.12554521 -0.3014787\n",
      "   1.0785669   0.71557701 -0.50375082]\n",
      " [-0.74767973  1.31882449 -0.4787072  -1.19223927  0.06422876  0.25499483\n",
      "  -1.2764602  -0.91393722  0.55553137]\n",
      " [ 0.54763182 -0.36564133  0.8614855  -0.9061434   0.41079428 -0.44736783\n",
      "  -0.96475582  0.71491791  0.67073488]\n",
      " [ 0.96389529 -1.28875992  0.36157838  1.0737261  -0.14791152 -0.21395314\n",
      "   1.16726247  0.4025266  -0.79085305]\n",
      " [-0.94693873  1.27791592 -0.34565147 -0.94083718  0.20408649  0.20968237\n",
      "  -1.19035177 -0.79417735  0.62567834]\n",
      " [ 0.04517429  0.32973302  0.67418958 -1.10338684  0.02606626  0.11250842\n",
      "  -1.24762916  0.38786661  0.33356352]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.65978185  1.65906031 -3.01951766 -0.08284861  1.76567094 -2.77575029\n",
      "  -0.86810985]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:80 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.62223992]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 80 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.88801018 -1.13184231  0.30063907  1.05409141 -0.12554521 -0.3014787\n",
      "   1.0785669   0.71557701 -0.49386872]\n",
      " [-0.75752515  1.31882449 -0.4787072  -1.20208469  0.06422876  0.25499483\n",
      "  -1.2764602  -0.91393722  0.54568595]\n",
      " [ 0.55101421 -0.36564133  0.8614855  -0.90276101  0.41079428 -0.44736783\n",
      "  -0.96475582  0.71491791  0.67411727]\n",
      " [ 0.97349443 -1.28875992  0.36157838  1.08332524 -0.14791152 -0.21395314\n",
      "   1.16726247  0.4025266  -0.78125391]\n",
      " [-0.95657341  1.27791592 -0.34565147 -0.95047185  0.20408649  0.20968237\n",
      "  -1.19035177 -0.79417735  0.61604366]\n",
      " [ 0.03810103  0.32973302  0.67418958 -1.1104601   0.02606626  0.11250842\n",
      "  -1.24762916  0.38786661  0.32649027]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.6153842   1.6948056  -3.01062458 -0.05721217  1.80015637 -2.76595559\n",
      "  -0.85361978]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:80 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.07219322]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 80 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.88803005 -1.13184231  0.30063907  1.05409141 -0.12552534 -0.30145883\n",
      "   1.0785669   0.71557701 -0.49384885]\n",
      " [-0.75758987  1.31882449 -0.4787072  -1.20208469  0.06416404  0.2549301\n",
      "  -1.2764602  -0.91393722  0.54562122]\n",
      " [ 0.55049986 -0.36564133  0.8614855  -0.90276101  0.41027992 -0.44788218\n",
      "  -0.96475582  0.71491791  0.67360292]\n",
      " [ 0.97359622 -1.28875992  0.36157838  1.08332524 -0.14780972 -0.21385134\n",
      "   1.16726247  0.4025266  -0.78115211]\n",
      " [-0.95661762  1.27791592 -0.34565147 -0.95047185  0.20404228  0.20963816\n",
      "  -1.19035177 -0.79417735  0.61599945]\n",
      " [ 0.03781536  0.32973302  0.67418958 -1.1104601   0.02578058  0.11222275\n",
      "  -1.24762916  0.38786661  0.32620459]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.617802    1.69361657 -3.01189832 -0.05906537  1.79904975 -2.76720874\n",
      "  -0.85512656]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:80 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.00583185]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 80 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.88803362 -1.13183874  0.30063907  1.05409141 -0.12552177 -0.30145526\n",
      "   1.0785669   0.71557701 -0.49384528]\n",
      " [-0.75759364  1.31882072 -0.4787072  -1.20208469  0.06416028  0.25492634\n",
      "  -1.2764602  -0.91393722  0.54561746]\n",
      " [ 0.55049691 -0.36564428  0.8614855  -0.90276101  0.41027698 -0.44788513\n",
      "  -0.96475582  0.71491791  0.67359998]\n",
      " [ 0.9736     -1.28875614  0.36157838  1.08332524 -0.14780595 -0.21384757\n",
      "   1.16726247  0.4025266  -0.78114834]\n",
      " [-0.95662135  1.27791219 -0.34565147 -0.95047185  0.20403855  0.20963442\n",
      "  -1.19035177 -0.79417735  0.61599572]\n",
      " [ 0.03781239  0.32973005  0.67418958 -1.1104601   0.02577761  0.11221978\n",
      "  -1.24762916  0.38786661  0.32620162]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.61781891  1.69361255 -3.01191195 -0.05907711  1.79904656 -2.76722217\n",
      "  -0.85513834]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:80 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.25649887]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 80 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.88564106 -1.1342313   0.30063907  1.05409141 -0.12791433 -0.30384782\n",
      "   1.07617434  0.71557701 -0.49384528]\n",
      " [-0.75526399  1.32115037 -0.4787072  -1.20208469  0.06648992  0.25725599\n",
      "  -1.27413055 -0.91393722  0.54561746]\n",
      " [ 0.55474535 -0.36139584  0.8614855  -0.90276101  0.41452541 -0.44363669\n",
      "  -0.96050739  0.71491791  0.67359998]\n",
      " [ 0.97077446 -1.29158168  0.36157838  1.08332524 -0.15063148 -0.2166731\n",
      "   1.16443694  0.4025266  -0.78114834]\n",
      " [-0.95397639  1.28055716 -0.34565147 -0.95047185  0.20668352  0.21227939\n",
      "  -1.18770681 -0.79417735  0.61599572]\n",
      " [ 0.04177837  0.33369603  0.67418958 -1.1104601   0.0297436   0.11618577\n",
      "  -1.24366317  0.38786661  0.32620162]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.642277    1.67892408 -3.02174994 -0.06656843  1.78387733 -2.77671387\n",
      "  -0.86302724]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:80 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.91262538]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 80 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.88610891 -1.1342313   0.30063907  1.05455926 -0.12791433 -0.30384782\n",
      "   1.07664219  0.71557701 -0.49384528]\n",
      " [-0.75567549  1.32115037 -0.4787072  -1.20249619  0.06648992  0.25725599\n",
      "  -1.27454206 -0.91393722  0.54561746]\n",
      " [ 0.55398191 -0.36139584  0.8614855  -0.90352445  0.41452541 -0.44363669\n",
      "  -0.96127083  0.71491791  0.67359998]\n",
      " [ 0.97118925 -1.29158168  0.36157838  1.08374002 -0.15063148 -0.2166731\n",
      "   1.16485172  0.4025266  -0.78114834]\n",
      " [-0.95442392  1.28055716 -0.34565147 -0.95091938  0.20668352  0.21227939\n",
      "  -1.18815433 -0.79417735  0.61599572]\n",
      " [ 0.04111794  0.33369603  0.67418958 -1.11112053  0.0297436   0.11618577\n",
      "  -1.24432361  0.38786661  0.32620162]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.63879336  1.68224499 -3.02161757 -0.06582734  1.78722694 -2.77656257\n",
      "  -0.86271334]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:80 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.75735677]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 80 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89109984 -1.1342313   0.30063907  1.05455926 -0.1229234  -0.29885689\n",
      "   1.08163313  0.71557701 -0.49384528]\n",
      " [-0.76062228  1.32115037 -0.4787072  -1.20249619  0.06154313  0.25230919\n",
      "  -1.27948885 -0.91393722  0.54561746]\n",
      " [ 0.55166176 -0.36139584  0.8614855  -0.90352445  0.41220526 -0.44595685\n",
      "  -0.96359098  0.71491791  0.67359998]\n",
      " [ 0.97609763 -1.29158168  0.36157838  1.08374002 -0.14572309 -0.21176471\n",
      "   1.16976011  0.4025266  -0.78114834]\n",
      " [-0.95936123  1.28055716 -0.34565147 -0.95091938  0.20174621  0.20734208\n",
      "  -1.19309164 -0.79417735  0.61599572]\n",
      " [ 0.03660736  0.33369603  0.67418958 -1.11112053  0.02523302  0.11167519\n",
      "  -1.24883419  0.38786661  0.32620162]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.61649839  1.70057482 -3.01819257 -0.05707436  1.80627354 -2.77318698\n",
      "  -0.85696502]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:80 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.56948426]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 80 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.8764208  -1.1342313   0.30063907  1.03988021 -0.1229234  -0.31353593\n",
      "   1.08163313  0.71557701 -0.50852432]\n",
      " [-0.74588197  1.32115037 -0.4787072  -1.18775588  0.06154313  0.26704951\n",
      "  -1.27948885 -0.91393722  0.56035777]\n",
      " [ 0.55382138 -0.36139584  0.8614855  -0.90136482  0.41220526 -0.44379722\n",
      "  -0.96359098  0.71491791  0.6757596 ]\n",
      " [ 0.96191176 -1.29158168  0.36157838  1.06955414 -0.14572309 -0.22595059\n",
      "   1.16976011  0.4025266  -0.79533421]\n",
      " [-0.94505077  1.28055716 -0.34565147 -0.93660893  0.20174621  0.22165253\n",
      "  -1.19309164 -0.79417735  0.63030617]\n",
      " [ 0.04666444  0.33369603  0.67418958 -1.10106345  0.02523302  0.12173227\n",
      "  -1.24883419  0.38786661  0.33625871]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.68630917  1.6475151  -3.03478835 -0.08981457  1.75433351 -2.79079287\n",
      "  -0.88112006]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:80 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.73959477]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 80 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.88203458 -1.12861752  0.30063907  1.04549399 -0.1229234  -0.30792215\n",
      "   1.08724691  0.71557701 -0.50852432]\n",
      " [-0.75148297  1.31554937 -0.4787072  -1.19335688  0.06154313  0.26144851\n",
      "  -1.28508985 -0.91393722  0.56035777]\n",
      " [ 0.54872885 -0.36648837  0.8614855  -0.90645735  0.41220526 -0.44888975\n",
      "  -0.96868351  0.71491791  0.6757596 ]\n",
      " [ 0.96748833 -1.2860051   0.36157838  1.07513072 -0.14572309 -0.22037402\n",
      "   1.17533668  0.4025266  -0.79533421]\n",
      " [-0.95066297  1.27494496 -0.34565147 -0.94222112  0.20174621  0.21604034\n",
      "  -1.19870384 -0.79417735  0.63030617]\n",
      " [ 0.04121536  0.32824695  0.67418958 -1.10651254  0.02523302  0.11628318\n",
      "  -1.25428328  0.38786661  0.33625871]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.66123288  1.66820148 -3.03066264 -0.08711837  1.77548268 -2.7864834\n",
      "  -0.87770641]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:80 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.80846862]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 80 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.88523273 -1.12541937  0.30063907  1.04869215 -0.1229234  -0.30792215\n",
      "   1.09044506  0.71557701 -0.50852432]\n",
      " [-0.75466191  1.31237043 -0.4787072  -1.19653581  0.06154313  0.26144851\n",
      "  -1.28826878 -0.91393722  0.56035777]\n",
      " [ 0.54543401 -0.36978322  0.8614855  -0.9097522   0.41220526 -0.44888975\n",
      "  -0.97197835  0.71491791  0.6757596 ]\n",
      " [ 0.97065472 -1.28283871  0.36157838  1.07829711 -0.14572309 -0.22037402\n",
      "   1.17850307  0.4025266  -0.79533421]\n",
      " [-0.95390327  1.27170466 -0.34565147 -0.94546143  0.20174621  0.21604034\n",
      "  -1.20194414 -0.79417735  0.63030617]\n",
      " [ 0.03809443  0.32512602  0.67418958 -1.10963346  0.02523302  0.11628318\n",
      "  -1.2574042   0.38786661  0.33625871]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.64640384  1.68107778 -3.02875726 -0.08481401  1.78843533 -2.78440975\n",
      "  -0.8759252 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:80 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.82081181]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 80 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.88802216 -1.12541937  0.3034285   1.04869215 -0.1229234  -0.30513273\n",
      "   1.09323449  0.71557701 -0.50852432]\n",
      " [-0.75720986  1.31237043 -0.48125515 -1.19653581  0.06154313  0.25890055\n",
      "  -1.29081673 -0.91393722  0.56035777]\n",
      " [ 0.54538806 -0.36978322  0.86143955 -0.9097522   0.41220526 -0.4489357\n",
      "  -0.9720243   0.71491791  0.6757596 ]\n",
      " [ 0.97317407 -1.28283871  0.36409773  1.07829711 -0.14572309 -0.21785467\n",
      "   1.18102243  0.4025266  -0.79533421]\n",
      " [-0.95642731  1.27170466 -0.3481755  -0.94546143  0.20174621  0.2135163\n",
      "  -1.20446818 -0.79417735  0.63030617]\n",
      " [ 0.0367447   0.32512602  0.67283984 -1.10963346  0.02523302  0.11493345\n",
      "  -1.25875393  0.38786661  0.33625871]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.63322636  1.69264021 -3.02751233 -0.07827122  1.80040148 -2.78319301\n",
      "  -0.87072795]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:80 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.11912453]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 81 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.88755975 -1.12588177  0.3034285   1.04822974 -0.1229234  -0.30513273\n",
      "   1.09323449  0.71557701 -0.50898673]\n",
      " [-0.75708347  1.31249682 -0.48125515 -1.19640943  0.06154313  0.25890055\n",
      "  -1.29081673 -0.91393722  0.56048416]\n",
      " [ 0.54547921 -0.36969206  0.86143955 -0.90966104  0.41220526 -0.4489357\n",
      "  -0.9720243   0.71491791  0.67585075]\n",
      " [ 0.97321579 -1.282797    0.36409773  1.07833882 -0.14572309 -0.21785467\n",
      "   1.18102243  0.4025266  -0.7952925 ]\n",
      " [-0.9564275   1.27170447 -0.3481755  -0.94546162  0.20174621  0.2135163\n",
      "  -1.20446818 -0.79417735  0.63030598]\n",
      " [ 0.03736121  0.32574254  0.67283984 -1.10901694  0.02523302  0.11493345\n",
      "  -1.25875393  0.38786661  0.33687522]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.63947646  1.68904566 -3.03051086 -0.08130506  1.79731815 -2.78631825\n",
      "  -0.87321894]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:81 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.85248261]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 81 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.88933724 -1.12588177  0.30520599  1.04822974 -0.1229234  -0.30513273\n",
      "   1.09501198  0.71557701 -0.50898673]\n",
      " [-0.7586878   1.31249682 -0.48285948 -1.19640943  0.06154313  0.25890055\n",
      "  -1.29242106 -0.91393722  0.56048416]\n",
      " [ 0.54644147 -0.36969206  0.86240181 -0.90966104  0.41220526 -0.4489357\n",
      "  -0.97106205  0.71491791  0.67585075]\n",
      " [ 0.97482804 -1.282797    0.36570998  1.07833882 -0.14572309 -0.21785467\n",
      "   1.18263467  0.4025266  -0.7952925 ]\n",
      " [-0.95804652  1.27170447 -0.34979452 -0.94546162  0.20174621  0.2135163\n",
      "  -1.2060872  -0.79417735  0.63030598]\n",
      " [ 0.03618026  0.32574254  0.67165889 -1.10901694  0.02523302  0.11493345\n",
      "  -1.25993488  0.38786661  0.33687522]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.63020086  1.69746384 -3.02982595 -0.07567439  1.80590194 -2.7856205\n",
      "  -0.86982221]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:81 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.58054567]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 81 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.87441991 -1.1407991   0.29028866  1.04822974 -0.1229234  -0.30513273\n",
      "   1.08009465  0.71557701 -0.50898673]\n",
      " [-0.74350577  1.32767884 -0.46767745 -1.19640943  0.06154313  0.25890055\n",
      "  -1.27723903 -0.91393722  0.56048416]\n",
      " [ 0.54523964 -0.37089389  0.86119998 -0.90966104  0.41220526 -0.4489357\n",
      "  -0.97226387  0.71491791  0.67585075]\n",
      " [ 0.95956992 -1.29805511  0.35045186  1.07833882 -0.14572309 -0.21785467\n",
      "   1.16737656  0.4025266  -0.7952925 ]\n",
      " [-0.94278124  1.28696975 -0.33452924 -0.94546162  0.20174621  0.2135163\n",
      "  -1.19082192 -0.79417735  0.63030598]\n",
      " [ 0.04012941  0.32969168  0.67560804 -1.10901694  0.02523302  0.11493345\n",
      "  -1.25598574  0.38786661  0.33687522]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.7008859   1.64360187 -3.04591935 -0.11221966  1.75107655 -2.80145742\n",
      "  -0.90118177]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:81 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.68853423]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 81 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.88163141 -1.1407991   0.29750016  1.04822974 -0.1229234  -0.30513273\n",
      "   1.08009465  0.72278851 -0.50898673]\n",
      " [-0.75026951  1.32767884 -0.47444119 -1.19640943  0.06154313  0.25890055\n",
      "  -1.27723903 -0.92070096  0.56048416]\n",
      " [ 0.55201141 -0.37089389  0.86797175 -0.90966104  0.41220526 -0.4489357\n",
      "  -0.97226387  0.72168968  0.67585075]\n",
      " [ 0.96697528 -1.29805511  0.35785722  1.07833882 -0.14572309 -0.21785467\n",
      "   1.16737656  0.40993196 -0.7952925 ]\n",
      " [-0.94965647  1.28696975 -0.34140447 -0.94546162  0.20174621  0.2135163\n",
      "  -1.19082192 -0.80105259  0.63030598]\n",
      " [ 0.04702299  0.32969168  0.68250161 -1.10901694  0.02523302  0.11493345\n",
      "  -1.25598574  0.39476019  0.33687522]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.66748822  1.67257916 -3.04235652 -0.08239682  1.77936994 -2.79772025\n",
      "  -0.8761023 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:81 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.62360406]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 81 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89146589 -1.1407991   0.29750016  1.05806421 -0.1229234  -0.30513273\n",
      "   1.08009465  0.72278851 -0.49915225]\n",
      " [-0.76006754  1.32767884 -0.47444119 -1.20620746  0.06154313  0.25890055\n",
      "  -1.27723903 -0.92070096  0.55068612]\n",
      " [ 0.55543801 -0.37089389  0.86797175 -0.90623444  0.41220526 -0.4489357\n",
      "  -0.97226387  0.72168968  0.67927735]\n",
      " [ 0.97653379 -1.29805511  0.35785722  1.08789733 -0.14572309 -0.21785467\n",
      "   1.16737656  0.40993196 -0.78573399]\n",
      " [-0.9592487   1.28696975 -0.34140447 -0.95505386  0.20174621  0.2135163\n",
      "  -1.19082192 -0.80105259  0.62071375]\n",
      " [ 0.03998192  0.32969168  0.68250161 -1.11605802  0.02523302  0.11493345\n",
      "  -1.25598574  0.39476019  0.32983415]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.62331401  1.7081604  -3.03352097 -0.056825    1.81370672 -2.78799545\n",
      "  -0.86168972]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:81 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.07003324]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 81 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89148626 -1.1407991   0.29750016  1.05806421 -0.12290303 -0.30511235\n",
      "   1.08009465  0.72278851 -0.49913188]\n",
      " [-0.76013067  1.32767884 -0.47444119 -1.20620746  0.06148     0.25883743\n",
      "  -1.27723903 -0.92070096  0.55062299]\n",
      " [ 0.55495145 -0.37089389  0.86797175 -0.90623444  0.41171871 -0.44942225\n",
      "  -0.97226387  0.72168968  0.6787908 ]\n",
      " [ 0.97663157 -1.29805511  0.35785722  1.08789733 -0.14562532 -0.21775689\n",
      "   1.16737656  0.40993196 -0.78563621]\n",
      " [-0.95929239  1.28696975 -0.34140447 -0.95505386  0.20170252  0.21347262\n",
      "  -1.19082192 -0.80105259  0.62067007]\n",
      " [ 0.03970927  0.32969168  0.68250161 -1.11605802  0.02496037  0.1146608\n",
      "  -1.25598574  0.39476019  0.3295615 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.6255946   1.70704049 -3.03472452 -0.05857686  1.81266469 -2.78917946\n",
      "  -0.86311464]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:81 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.0054771]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 81 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89148942 -1.14079594  0.29750016  1.05806421 -0.12289986 -0.30510919\n",
      "   1.08009465  0.72278851 -0.49912872]\n",
      " [-0.76013399  1.32767552 -0.47444119 -1.20620746  0.06147668  0.2588341\n",
      "  -1.27723903 -0.92070096  0.55061967]\n",
      " [ 0.55494885 -0.3708965   0.86797175 -0.90623444  0.4117161  -0.44942486\n",
      "  -0.97226387  0.72168968  0.67878819]\n",
      " [ 0.9766349  -1.29805178  0.35785722  1.08789733 -0.14562198 -0.21775355\n",
      "   1.16737656  0.40993196 -0.78563288]\n",
      " [-0.95929569  1.28696645 -0.34140447 -0.95505386  0.20169922  0.21346932\n",
      "  -1.19082192 -0.80105259  0.62066677]\n",
      " [ 0.03970663  0.32968905  0.68250161 -1.11605802  0.02495773  0.11465816\n",
      "  -1.25598574  0.39476019  0.32955887]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.62560951  1.70703697 -3.03473657 -0.05858723  1.8126619  -2.78919134\n",
      "  -0.86312505]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:81 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.25163779]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 81 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.88919779 -1.14308757  0.29750016  1.05806421 -0.12519149 -0.30740082\n",
      "   1.07780302  0.72278851 -0.49912872]\n",
      " [-0.75791275  1.32989676 -0.47444119 -1.20620746  0.06369792  0.26105534\n",
      "  -1.27501779 -0.92070096  0.55061967]\n",
      " [ 0.55909326 -0.36675209  0.86797175 -0.90623444  0.41586051 -0.44528045\n",
      "  -0.96811946  0.72168968  0.67878819]\n",
      " [ 0.97393652 -1.30075016  0.35785722  1.08789733 -0.14832036 -0.22045193\n",
      "   1.16467818  0.40993196 -0.78563288]\n",
      " [-0.9567709   1.28949124 -0.34140447 -0.95505386  0.20422401  0.21599411\n",
      "  -1.18829713 -0.80105259  0.62066677]\n",
      " [ 0.04356725  0.33354966  0.68250161 -1.11605802  0.02881835  0.11851878\n",
      "  -1.25212512  0.39476019  0.32955887]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.64930325  1.69283601 -3.04430564 -0.06580218  1.79801069 -2.79842812\n",
      "  -0.87074214]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:81 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.91431894]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 81 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.88964615 -1.14308757  0.29750016  1.05851257 -0.12519149 -0.30740082\n",
      "   1.07825138  0.72278851 -0.49912872]\n",
      " [-0.75830733  1.32989676 -0.47444119 -1.20660204  0.06369792  0.26105534\n",
      "  -1.27541237 -0.92070096  0.55061967]\n",
      " [ 0.55835684 -0.36675209  0.86797175 -0.90697086  0.41586051 -0.44528045\n",
      "  -0.96885588  0.72168968  0.67878819]\n",
      " [ 0.97433417 -1.30075016  0.35785722  1.08829499 -0.14832036 -0.22045193\n",
      "   1.16507583  0.40993196 -0.78563288]\n",
      " [-0.95720002  1.28949124 -0.34140447 -0.95548298  0.20422401  0.21599411\n",
      "  -1.18872625 -0.80105259  0.62066677]\n",
      " [ 0.04293402  0.33354966  0.68250161 -1.11669124  0.02881835  0.11851878\n",
      "  -1.25275835  0.39476019  0.32955887]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.64594713  1.69603672 -3.04417905 -0.06509199  1.80123868 -2.79828346\n",
      "  -0.87044309]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:81 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.75952801]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 81 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89456237 -1.14308757  0.29750016  1.05851257 -0.12027527 -0.3024846\n",
      "   1.0831676   0.72278851 -0.49912872]\n",
      " [-0.76317861  1.32989676 -0.47444119 -1.20660204  0.05882664  0.25618406\n",
      "  -1.28028364 -0.92070096  0.55061967]\n",
      " [ 0.55605481 -0.36675209  0.86797175 -0.90697086  0.41355848 -0.44758248\n",
      "  -0.97115791  0.72168968  0.67878819]\n",
      " [ 0.97916761 -1.30075016  0.35785722  1.08829499 -0.14348692 -0.2156185\n",
      "   1.16990926  0.40993196 -0.78563288]\n",
      " [-0.96206207  1.28949124 -0.34140447 -0.95548298  0.19936196  0.21113206\n",
      "  -1.1935883  -0.80105259  0.62066677]\n",
      " [ 0.03848048  0.33354966  0.68250161 -1.11669124  0.02436481  0.11406524\n",
      "  -1.25721189  0.39476019  0.32955887]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.6239866   1.71410543 -3.04081255 -0.05648873  1.82000476 -2.79496441\n",
      "  -0.86480287]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:81 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.56866847]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 81 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.87990525 -1.14308757  0.29750016  1.04385544 -0.12027527 -0.31714172\n",
      "   1.0831676   0.72278851 -0.51378584]\n",
      " [-0.74846356  1.32989676 -0.47444119 -1.191887    0.05882664  0.27089911\n",
      "  -1.28028364 -0.92070096  0.56533471]\n",
      " [ 0.55813459 -0.36675209  0.86797175 -0.90489109  0.41355848 -0.44550271\n",
      "  -0.97115791  0.72168968  0.68086797]\n",
      " [ 0.96500009 -1.30075016  0.35785722  1.07412746 -0.14348692 -0.22978602\n",
      "   1.16990926  0.40993196 -0.7998004 ]\n",
      " [-0.94777282  1.28949124 -0.34140447 -0.94119373  0.19936196  0.22542131\n",
      "  -1.1935883  -0.80105259  0.63495602]\n",
      " [ 0.04850174  0.33354966  0.68250161 -1.10666998  0.02436481  0.1240865\n",
      "  -1.25721189  0.39476019  0.33958013]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.69372942  1.66111638 -3.05742019 -0.0892754   1.76812477 -2.8125689\n",
      "  -0.88896675]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:81 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.73997099]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 81 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.88550573 -1.13748708  0.29750016  1.04945593 -0.12027527 -0.31154123\n",
      "   1.08876809  0.72278851 -0.51378584]\n",
      " [-0.75405277  1.32430756 -0.47444119 -1.1974762   0.05882664  0.26530991\n",
      "  -1.28587285 -0.92070096  0.56533471]\n",
      " [ 0.5530763  -0.37181037  0.86797175 -0.90994937  0.41355848 -0.45056099\n",
      "  -0.97621619  0.72168968  0.68086797]\n",
      " [ 0.97056603 -1.29518422  0.35785722  1.0796934  -0.14348692 -0.22422008\n",
      "   1.1754752   0.40993196 -0.7998004 ]\n",
      " [-0.95337217  1.28389189 -0.34140447 -0.94679308  0.19936196  0.21982196\n",
      "  -1.19918765 -0.80105259  0.63495602]\n",
      " [ 0.04307583  0.32812376  0.68250161 -1.11209589  0.02436481  0.11866059\n",
      "  -1.26263779  0.39476019  0.33958013]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.66871282  1.68174098 -3.05328676 -0.08661866  1.7892064  -2.80825222\n",
      "  -0.88559036]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:81 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.80995319]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 81 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.88866023 -1.13433259  0.29750016  1.05261043 -0.12027527 -0.31154123\n",
      "   1.09192258  0.72278851 -0.51378584]\n",
      " [-0.75718925  1.32117108 -0.47444119 -1.20061268  0.05882664  0.26530991\n",
      "  -1.28900933 -0.92070096  0.56533471]\n",
      " [ 0.54983036 -0.37505631  0.86797175 -0.91319531  0.41355848 -0.45056099\n",
      "  -0.97946213  0.72168968  0.68086797]\n",
      " [ 0.97369025 -1.29206     0.35785722  1.08281762 -0.14348692 -0.22422008\n",
      "   1.17859942  0.40993196 -0.7998004 ]\n",
      " [-0.95656894  1.28069512 -0.34140447 -0.94998984  0.19936196  0.21982196\n",
      "  -1.20238442 -0.80105259  0.63495602]\n",
      " [ 0.04000723  0.32505515  0.68250161 -1.11516449  0.02436481  0.11866059\n",
      "  -1.2657064   0.39476019  0.33958013]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.65408596  1.69444187 -3.05140526 -0.08436866  1.80198019 -2.80620476\n",
      "  -0.88385221]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:81 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.82153202]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 81 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89143097 -1.13433259  0.3002709   1.05261043 -0.12027527 -0.30877049\n",
      "   1.09469332  0.72278851 -0.51378584]\n",
      " [-0.7597235   1.32117108 -0.47697544 -1.20061268  0.05882664  0.26277565\n",
      "  -1.29154358 -0.92070096  0.56533471]\n",
      " [ 0.54979039 -0.37505631  0.86793178 -0.91319531  0.41355848 -0.45060096\n",
      "  -0.9795021   0.72168968  0.68086797]\n",
      " [ 0.97619579 -1.29206     0.36036276  1.08281762 -0.14348692 -0.22171454\n",
      "   1.18110496  0.40993196 -0.7998004 ]\n",
      " [-0.95907957  1.28069512 -0.34391511 -0.94998984  0.19936196  0.21731133\n",
      "  -1.20489505 -0.80105259  0.63495602]\n",
      " [ 0.03867938  0.32505515  0.68117377 -1.11516449  0.02436481  0.11733274\n",
      "  -1.26703424  0.39476019  0.33958013]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.64100273  1.705919   -3.05016378 -0.07786702  1.81385591 -2.80499131\n",
      "  -0.87867869]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:81 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.11562029]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 82 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89100309 -1.13476047  0.3002709   1.05218255 -0.12027527 -0.30877049\n",
      "   1.09469332  0.72278851 -0.51421372]\n",
      " [-0.75961454  1.32128003 -0.47697544 -1.20050372  0.05882664  0.26277565\n",
      "  -1.29154358 -0.92070096  0.56544367]\n",
      " [ 0.54987543 -0.37497127  0.86793178 -0.91311027  0.41355848 -0.45060096\n",
      "  -0.9795021   0.72168968  0.68095301]\n",
      " [ 0.97624432 -1.29201147  0.36036276  1.08286615 -0.14348692 -0.22171454\n",
      "   1.18110496  0.40993196 -0.79975187]\n",
      " [-0.9590893   1.2806854  -0.34391511 -0.94999957  0.19936196  0.21731133\n",
      "  -1.20489505 -0.80105259  0.63494629]\n",
      " [ 0.03926292  0.32563869  0.68117377 -1.11458096  0.02436481  0.11733274\n",
      "  -1.26703424  0.39476019  0.34016366]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.64691394  1.70252924 -3.05301033 -0.08073754  1.81094884 -2.80795665\n",
      "  -0.88103412]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:82 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.85384438]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 82 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89274955 -1.13476047  0.30201737  1.05218255 -0.12027527 -0.30877049\n",
      "   1.09643979  0.72278851 -0.51421372]\n",
      " [-0.76119263  1.32128003 -0.47855353 -1.20050372  0.05882664  0.26277565\n",
      "  -1.29312167 -0.92070096  0.56544367]\n",
      " [ 0.55082824 -0.37497127  0.86888458 -0.91311027  0.41355848 -0.45060096\n",
      "  -0.9785493   0.72168968  0.68095301]\n",
      " [ 0.97782991 -1.29201147  0.36194836  1.08286615 -0.14348692 -0.22171454\n",
      "   1.18269056  0.40993196 -0.79975187]\n",
      " [-0.96068194  1.2806854  -0.34550775 -0.94999957  0.19936196  0.21731133\n",
      "  -1.2064877  -0.80105259  0.63494629]\n",
      " [ 0.03810536  0.32563869  0.68001621 -1.11458096  0.02436481  0.11733274\n",
      "  -1.26819181  0.39476019  0.34016366]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.63779426  1.71080725 -3.0523363  -0.07519409  1.81938796 -2.80726988\n",
      "  -0.87769035]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:82 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57657454]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 82 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.87793268 -1.14957734  0.2872005   1.05218255 -0.12027527 -0.30877049\n",
      "   1.08162292  0.72278851 -0.51421372]\n",
      " [-0.74611747  1.3363552  -0.46347837 -1.20050372  0.05882664  0.26277565\n",
      "  -1.27804651 -0.92070096  0.56544367]\n",
      " [ 0.54966483 -0.37613467  0.86772118 -0.91311027  0.41355848 -0.45060096\n",
      "  -0.9797127   0.72168968  0.68095301]\n",
      " [ 0.96267645 -1.30716493  0.3467949   1.08286615 -0.14348692 -0.22171454\n",
      "   1.1675371   0.40993196 -0.79975187]\n",
      " [-0.94552234  1.295845   -0.33034815 -0.94999957  0.19936196  0.21731133\n",
      "  -1.19132809 -0.80105259  0.63494629]\n",
      " [ 0.042005    0.32953833  0.68391585 -1.11458096  0.02436481  0.11733274\n",
      "  -1.26429217  0.39476019  0.34016366]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.70817566  1.65726908 -3.06848322 -0.11154904  1.76491989 -2.82316432\n",
      "  -0.90894859]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:82 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.69050217]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 82 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.8850625  -1.14957734  0.29433031  1.05218255 -0.12027527 -0.30877049\n",
      "   1.08162292  0.72991832 -0.51421372]\n",
      " [-0.75280411  1.3363552  -0.470165   -1.20050372  0.05882664  0.26277565\n",
      "  -1.27804651 -0.92738759  0.56544367]\n",
      " [ 0.55633271 -0.37613467  0.87438906 -0.91311027  0.41355848 -0.45060096\n",
      "  -0.9797127   0.72835756  0.68095301]\n",
      " [ 0.97000378 -1.30716493  0.35412223  1.08286615 -0.14348692 -0.22171454\n",
      "   1.1675371   0.41725928 -0.79975187]\n",
      " [-0.95231948  1.295845   -0.33714529 -0.94999957  0.19936196  0.21731133\n",
      "  -1.19132809 -0.80784973  0.63494629]\n",
      " [ 0.04887729  0.32953833  0.69078814 -1.11458096  0.02436481  0.11733274\n",
      "  -1.26429217  0.40163248  0.34016366]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.67510444  1.68599204 -3.06497149 -0.08196209  1.792966   -2.81948151\n",
      "  -0.88400909]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:82 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.6249323]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 82 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89485043 -1.14957734  0.29433031  1.06197048 -0.12027527 -0.30877049\n",
      "   1.08162292  0.72991832 -0.50442579]\n",
      " [-0.7625558   1.3363552  -0.470165   -1.21025541  0.05882664  0.26277565\n",
      "  -1.27804651 -0.92738759  0.55569198]\n",
      " [ 0.55980313 -0.37613467  0.87438906 -0.90963985  0.41355848 -0.45060096\n",
      "  -0.9797127   0.72835756  0.68442343]\n",
      " [ 0.97952235 -1.30716493  0.35412223  1.09238472 -0.14348692 -0.22171454\n",
      "   1.1675371   0.41725928 -0.79023331]\n",
      " [-0.96187004  1.295845   -0.33714529 -0.95955013  0.19936196  0.21731133\n",
      "  -1.19132809 -0.80784973  0.62539573]\n",
      " [ 0.04186789  0.32953833  0.69078814 -1.12159036  0.02436481  0.11733274\n",
      "  -1.26429217  0.40163248  0.33315426]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.63114802  1.7214127  -3.05619124 -0.05645235  1.82715703 -2.80982396\n",
      "  -0.86967164]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:82 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.06794358]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 82 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89487119 -1.14957734  0.29433031  1.06197048 -0.1202545  -0.30874973\n",
      "   1.08162292  0.72991832 -0.50440503]\n",
      " [-0.76261731  1.3363552  -0.470165   -1.21025541  0.05876513  0.26271415\n",
      "  -1.27804651 -0.92738759  0.55563047]\n",
      " [ 0.55934291 -0.37613467  0.87438906 -0.90963985  0.41309826 -0.45106118\n",
      "  -0.9797127   0.72835756  0.68396321]\n",
      " [ 0.97961623 -1.30716493  0.35412223  1.09238472 -0.14339304 -0.22162066\n",
      "   1.1675371   0.41725928 -0.79013942]\n",
      " [-0.9619131   1.295845   -0.33714529 -0.95955013  0.1993189   0.21726826\n",
      "  -1.19132809 -0.80784973  0.62535267]\n",
      " [ 0.04160774  0.32953833  0.69078814 -1.12159036  0.02410465  0.11707259\n",
      "  -1.26429217  0.40163248  0.33289411]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.63329936  1.7203578  -3.05732856 -0.05810845  1.82617573 -2.81094274\n",
      "  -0.87101919]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:82 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.00514679]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 82 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.894874   -1.14957453  0.29433031  1.06197048 -0.1202517  -0.30874693\n",
      "   1.08162292  0.72991832 -0.50440223]\n",
      " [-0.76262025  1.33635226 -0.470165   -1.21025541  0.05876219  0.26271121\n",
      "  -1.27804651 -0.92738759  0.55562753]\n",
      " [ 0.5593406  -0.37613698  0.87438906 -0.90963985  0.41309595 -0.45106349\n",
      "  -0.9797127   0.72835756  0.6839609 ]\n",
      " [ 0.97961918 -1.30716198  0.35412223  1.09238472 -0.14339009 -0.22161771\n",
      "   1.1675371   0.41725928 -0.79013648]\n",
      " [-0.96191602  1.29584208 -0.33714529 -0.95955013  0.19931598  0.21726534\n",
      "  -1.19132809 -0.80784973  0.62534975]\n",
      " [ 0.0416054   0.32953599  0.69078814 -1.12159036  0.02410231  0.11707025\n",
      "  -1.26429217  0.40163248  0.33289177]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.63331253  1.72035472 -3.05733923 -0.05811763  1.82617329 -2.81095326\n",
      "  -0.87102841]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:82 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.24691857]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 82 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89267826 -1.15177027  0.29433031  1.06197048 -0.12244744 -0.31094266\n",
      "   1.07942719  0.72991832 -0.50440223]\n",
      " [-0.76050159  1.33847092 -0.470165   -1.21025541  0.06088085  0.26482986\n",
      "  -1.27592785 -0.92738759  0.55562753]\n",
      " [ 0.56338417 -0.37209341  0.87438906 -0.90963985  0.41713952 -0.44701992\n",
      "  -0.97566913  0.72835756  0.6839609 ]\n",
      " [ 0.97704124 -1.30973992  0.35412223  1.09238472 -0.14596803 -0.22419565\n",
      "   1.16495917  0.41725928 -0.79013648]\n",
      " [-0.95950505  1.29825305 -0.33714529 -0.95955013  0.20172695  0.21967631\n",
      "  -1.18891712 -0.80784973  0.62534975]\n",
      " [ 0.04536432  0.33329491  0.69078814 -1.12159036  0.02786124  0.12082917\n",
      "  -1.26053324  0.40163248  0.33289177]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.65626976  1.70662197 -3.06664705 -0.06506677  1.8120187  -2.81994206\n",
      "  -0.87838369]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:82 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.91596813]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 82 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89310806 -1.15177027  0.29433031  1.06240027 -0.12244744 -0.31094266\n",
      "   1.07985698  0.72991832 -0.50440223]\n",
      " [-0.76088004  1.33847092 -0.470165   -1.21063387  0.06088085  0.26482986\n",
      "  -1.2763063  -0.92738759  0.55562753]\n",
      " [ 0.5626737  -0.37209341  0.87438906 -0.91035032  0.41713952 -0.44701992\n",
      "  -0.9763796   0.72835756  0.6839609 ]\n",
      " [ 0.97742257 -1.30973992  0.35412223  1.09276605 -0.14596803 -0.22419565\n",
      "   1.1653405   0.41725928 -0.79013648]\n",
      " [-0.95991664  1.29825305 -0.33714529 -0.95996172  0.20172695  0.21967631\n",
      "  -1.18932871 -0.80784973  0.62534975]\n",
      " [ 0.04475705  0.33329491  0.69078814 -1.12219763  0.02786124  0.12082917\n",
      "  -1.26114051  0.40163248  0.33289177]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.65303577  1.70970748 -3.06652595 -0.06438602  1.81513015 -2.81980371\n",
      "  -0.87809869]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:82 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.76168832]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 82 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89795017 -1.15177027  0.29433031  1.06240027 -0.11760533 -0.30610055\n",
      "   1.08469909  0.72991832 -0.50440223]\n",
      " [-0.76567648  1.33847092 -0.470165   -1.21063387  0.05608441  0.26003342\n",
      "  -1.28110274 -0.92738759  0.55562753]\n",
      " [ 0.56038934 -0.37209341  0.87438906 -0.91035032  0.41485516 -0.44930428\n",
      "  -0.97866396  0.72835756  0.6839609 ]\n",
      " [ 0.98218171 -1.30973992  0.35412223  1.09276605 -0.1412089  -0.21943652\n",
      "   1.17009963  0.41725928 -0.79013648]\n",
      " [-0.96470409  1.29825305 -0.33714529 -0.95996172  0.1969395   0.21488886\n",
      "  -1.19411617 -0.80784973  0.62534975]\n",
      " [ 0.04036041  0.33329491  0.69078814 -1.12219763  0.02346459  0.11643253\n",
      "  -1.26553716  0.40163248  0.33289177]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.6314067   1.72751693 -3.0632173  -0.0559315   1.83361826 -2.81654066\n",
      "  -0.87256507]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:82 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.56778999]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 82 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.8833168  -1.15177027  0.29433031  1.0477669  -0.11760533 -0.32073392\n",
      "   1.08469909  0.72991832 -0.5190356 ]\n",
      " [-0.75098853  1.33847092 -0.470165   -1.19594591  0.05608441  0.27472137\n",
      "  -1.28110274 -0.92738759  0.57031548]\n",
      " [ 0.56239097 -0.37209341  0.87438906 -0.90834869  0.41485516 -0.44730266\n",
      "  -0.97866396  0.72835756  0.68596253]\n",
      " [ 0.96803449 -1.30973992  0.35412223  1.07861884 -0.1412089  -0.23358373\n",
      "   1.17009963  0.41725928 -0.80428369]\n",
      " [-0.95043797  1.29825305 -0.33714529 -0.9456956   0.1969395   0.22915498\n",
      "  -1.19411617 -0.80784973  0.63961587]\n",
      " [ 0.05034471  0.33329491  0.69078814 -1.11221333  0.02346459  0.12641683\n",
      "  -1.26553716  0.40163248  0.34287607]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.70107581  1.67460441 -3.07983682 -0.08875999  1.78180421 -2.8341439\n",
      "  -0.89673614]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:82 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.74034307]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 82 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.8889041  -1.14618297  0.29433031  1.0533542  -0.11760533 -0.31514662\n",
      "   1.09028639  0.72991832 -0.5190356 ]\n",
      " [-0.75656596  1.33289349 -0.470165   -1.20152334  0.05608441  0.26914395\n",
      "  -1.28668017 -0.92738759  0.57031548]\n",
      " [ 0.55736699 -0.37711738  0.87438906 -0.91337267  0.41485516 -0.45232663\n",
      "  -0.98368794  0.72835756  0.68596253]\n",
      " [ 0.97358978 -1.30418463  0.35412223  1.08417413 -0.1412089  -0.22802844\n",
      "   1.17565492  0.41725928 -0.80428369]\n",
      " [-0.95602453  1.2926665  -0.33714529 -0.95128215  0.1969395   0.22356842\n",
      "  -1.19970272 -0.80784973  0.63961587]\n",
      " [ 0.04494206  0.32789226  0.69078814 -1.11761598  0.02346459  0.12101418\n",
      "  -1.27093981  0.40163248  0.34287607]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.67611821  1.695168   -3.07569593 -0.08614215  1.80281932 -2.82982026\n",
      "  -0.89339659]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:82 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.8114285]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 82 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89201539 -1.14307168  0.29433031  1.05646549 -0.11760533 -0.31514662\n",
      "   1.09339768  0.72991832 -0.5190356 ]\n",
      " [-0.75966039  1.32979906 -0.470165   -1.20461777  0.05608441  0.26914395\n",
      "  -1.2897746  -0.92738759  0.57031548]\n",
      " [ 0.5541696  -0.38031478  0.87438906 -0.91657006  0.41485516 -0.45232663\n",
      "  -0.98688533  0.72835756  0.68596253]\n",
      " [ 0.97667222 -1.30110219  0.35412223  1.08725656 -0.1412089  -0.22802844\n",
      "   1.17873735  0.41725928 -0.80428369]\n",
      " [-0.95917818  1.28951284 -0.33714529 -0.95443581  0.1969395   0.22356842\n",
      "  -1.20285638 -0.80784973  0.63961587]\n",
      " [ 0.0419252   0.3248754   0.69078814 -1.12063283  0.02346459  0.12101418\n",
      "  -1.27395667  0.40163248  0.34287607]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.66169133  1.70769544 -3.07383817 -0.08394519  1.81541636 -2.82779885\n",
      "  -0.89170054]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:82 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.82222678]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 82 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89476824 -1.14307168  0.29708316  1.05646549 -0.11760533 -0.31239377\n",
      "   1.09615053  0.72991832 -0.5190356 ]\n",
      " [-0.76218156  1.32979906 -0.47268618 -1.20461777  0.05608441  0.26662277\n",
      "  -1.29229578 -0.92738759  0.57031548]\n",
      " [ 0.554135   -0.38031478  0.87435445 -0.91657006  0.41485516 -0.45236123\n",
      "  -0.98691993  0.72835756  0.68596253]\n",
      " [ 0.97916454 -1.30110219  0.35661455  1.08725656 -0.1412089  -0.22553611\n",
      "   1.18122968  0.41725928 -0.80428369]\n",
      " [-0.96167603  1.28951284 -0.33964313 -0.95443581  0.1969395   0.22107058\n",
      "  -1.20535422 -0.80784973  0.63961587]\n",
      " [ 0.04061877  0.3248754   0.68948171 -1.12063283  0.02346459  0.11970775\n",
      "  -1.2752631   0.40163248  0.34287607]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.64869878  1.71909029 -3.07259985 -0.07748352  1.82720493 -2.82658843\n",
      "  -0.88654949]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:82 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.11221113]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 83 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89437257 -1.14346735  0.29708316  1.05606982 -0.11760533 -0.31239377\n",
      "   1.09615053  0.72991832 -0.51943127]\n",
      " [-0.76208849  1.32989214 -0.47268618 -1.20452469  0.05608441  0.26662277\n",
      "  -1.29229578 -0.92738759  0.57040856]\n",
      " [ 0.55421429 -0.38023549  0.87435445 -0.91649077  0.41485516 -0.45236123\n",
      "  -0.98691993  0.72835756  0.68604181]\n",
      " [ 0.97921897 -1.30104777  0.35661455  1.08731099 -0.1412089  -0.22553611\n",
      "   1.18122968  0.41725928 -0.80422927]\n",
      " [-0.96169422  1.28949465 -0.33964313 -0.954454    0.1969395   0.22107058\n",
      "  -1.20535422 -0.80784973  0.63959768]\n",
      " [ 0.04117103  0.32542766  0.68948171 -1.12008058  0.02346459  0.11970775\n",
      "  -1.2752631   0.40163248  0.34342833]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.65428801  1.71589447 -3.07530132 -0.0801988   1.82446476 -2.82940123\n",
      "  -0.88877606]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:83 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.85517573]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 83 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89608897 -1.14346735  0.29879956  1.05606982 -0.11760533 -0.31239377\n",
      "   1.09786693  0.72991832 -0.51943127]\n",
      " [-0.76364113  1.32989214 -0.47423882 -1.20452469  0.05608441  0.26662277\n",
      "  -1.29384842 -0.92738759  0.57040856]\n",
      " [ 0.55515773 -0.38023549  0.8752979  -0.91649077  0.41485516 -0.45236123\n",
      "  -0.98597648  0.72835756  0.68604181]\n",
      " [ 0.98077869 -1.30104777  0.35817428  1.08731099 -0.1412089  -0.22553611\n",
      "   1.18278941  0.41725928 -0.80422927]\n",
      " [-0.96326127  1.28949465 -0.34121019 -0.954454    0.1969395   0.22107058\n",
      "  -1.20692127 -0.80784973  0.63959768]\n",
      " [ 0.04003622  0.32542766  0.68834691 -1.12008058  0.02346459  0.11970775\n",
      "  -1.2763979   0.40163248  0.34342833]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.64531975  1.72403626 -3.07463783 -0.07474026  1.83276335 -2.82872511\n",
      "  -0.88548368]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:83 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57257143]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 83 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.88137562 -1.1581807   0.28408621  1.05606982 -0.11760533 -0.31239377\n",
      "   1.08315358  0.72991832 -0.51943127]\n",
      " [-0.74867629  1.34485698 -0.45927399 -1.20452469  0.05608441  0.26662277\n",
      "  -1.27888358 -0.92738759  0.57040856]\n",
      " [ 0.55403361 -0.38135962  0.87417377 -0.91649077  0.41485516 -0.45236123\n",
      "  -0.98710061  0.72835756  0.68604181]\n",
      " [ 0.96573334 -1.31609313  0.34312892  1.08731099 -0.1412089  -0.22553611\n",
      "   1.16774405  0.41725928 -0.80422927]\n",
      " [-0.94821094  1.30454498 -0.32615986 -0.954454    0.1969395   0.22107058\n",
      "  -1.19187095 -0.80784973  0.63959768]\n",
      " [ 0.04388714  0.32927858  0.69219783 -1.12008058  0.02346459  0.11970775\n",
      "  -1.27254698  0.40163248  0.34342833]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.71538343  1.67083125 -3.09083296 -0.11089699  1.77866118 -2.84467178\n",
      "  -0.91663271]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:83 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.69244348]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 83 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.88842505 -1.1581807   0.29113565  1.05606982 -0.11760533 -0.31239377\n",
      "   1.08315358  0.73696776 -0.51943127]\n",
      " [-0.75528723  1.34485698 -0.46588492 -1.20452469  0.05608441  0.26662277\n",
      "  -1.27888358 -0.93399853  0.57040856]\n",
      " [ 0.56059915 -0.38135962  0.88073932 -0.91649077  0.41485516 -0.45236123\n",
      "  -0.98710061  0.7349231   0.68604181]\n",
      " [ 0.9729837  -1.31609313  0.35037928  1.08731099 -0.1412089  -0.22553611\n",
      "   1.16774405  0.42450965 -0.80422927]\n",
      " [-0.95493137  1.30454498 -0.33288029 -0.954454    0.1969395   0.22107058\n",
      "  -1.19187095 -0.81457015  0.63959768]\n",
      " [ 0.0507361   0.32927858  0.69904679 -1.12008058  0.02346459  0.11970775\n",
      "  -1.27254698  0.40848144  0.34342833]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.68263396  1.69930258 -3.08737103 -0.08154428  1.80646271 -2.84104193\n",
      "  -0.89183284]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:83 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.62622596]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 83 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89816749 -1.1581807   0.29113565  1.06581226 -0.11760533 -0.31239377\n",
      "   1.08315358  0.73696776 -0.50968883]\n",
      " [-0.7649936   1.34485698 -0.46588492 -1.21423106  0.05608441  0.26662277\n",
      "  -1.27888358 -0.93399853  0.56070219]\n",
      " [ 0.56411305 -0.38135962  0.88073932 -0.91297687  0.41485516 -0.45236123\n",
      "  -0.98710061  0.7349231   0.68955572]\n",
      " [ 0.98246301 -1.31609313  0.35037928  1.0967903  -0.1412089  -0.22553611\n",
      "   1.16774405  0.42450965 -0.79474995]\n",
      " [-0.96444101  1.30454498 -0.33288029 -0.96396364  0.1969395   0.22107058\n",
      "  -1.19187095 -0.81457015  0.63008804]\n",
      " [ 0.04375786  0.32927858  0.69904679 -1.12705882  0.02346459  0.11970775\n",
      "  -1.27254698  0.40848144  0.33645008]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.63888987  1.73456604 -3.07864396 -0.05609415  1.84051084 -2.83144913\n",
      "  -0.87756827]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:83 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.06592099]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 83 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89818855 -1.1581807   0.29113565  1.06581226 -0.11758427 -0.31237271\n",
      "   1.08315358  0.73696776 -0.50966777]\n",
      " [-0.76505347  1.34485698 -0.46588492 -1.21423106  0.05602453  0.26656289\n",
      "  -1.27888358 -0.93399853  0.56064231]\n",
      " [ 0.56367777 -0.38135962  0.88073932 -0.91297687  0.41441988 -0.45279651\n",
      "  -0.98710061  0.7349231   0.68912044]\n",
      " [ 0.98255313 -1.31609313  0.35037928  1.0967903  -0.14111878 -0.225446\n",
      "   1.16774405  0.42450965 -0.79465983]\n",
      " [-0.96448338  1.30454498 -0.33288029 -0.96396364  0.19689713  0.22102821\n",
      "  -1.19187095 -0.81457015  0.63004566]\n",
      " [ 0.04350969  0.32927858  0.69904679 -1.12705882  0.02321642  0.11945958\n",
      "  -1.27254698  0.40848144  0.33620191]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.64091943  1.73357233 -3.07971875 -0.05765972  1.83958666 -2.83250633\n",
      "  -0.8788427 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:83 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.00483902]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 83 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89819103 -1.15817821  0.29113565  1.06581226 -0.11758178 -0.31237022\n",
      "   1.08315358  0.73696776 -0.50966529]\n",
      " [-0.76505607  1.34485437 -0.46588492 -1.21423106  0.05602193  0.26656029\n",
      "  -1.27888358 -0.93399853  0.56063971]\n",
      " [ 0.56367572 -0.38136167  0.88073932 -0.91297687  0.41441783 -0.45279856\n",
      "  -0.98710061  0.7349231   0.68911839]\n",
      " [ 0.98255573 -1.31609052  0.35037928  1.0967903  -0.14111618 -0.22544339\n",
      "   1.16774405  0.42450965 -0.79465723]\n",
      " [-0.96448597  1.30454239 -0.33288029 -0.96396364  0.19689454  0.22102562\n",
      "  -1.19187095 -0.81457015  0.63004308]\n",
      " [ 0.04350761  0.3292765   0.69904679 -1.12705882  0.02321434  0.1194575\n",
      "  -1.27254698  0.40848144  0.33619983]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.64093108  1.73356963 -3.07972821 -0.05766784  1.83958452 -2.83251566\n",
      "  -0.87885087]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:83 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.2423371]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 83 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89608641 -1.16028284  0.29113565  1.06581226 -0.11968641 -0.31447485\n",
      "   1.08104895  0.73696776 -0.50966529]\n",
      " [-0.76303448  1.34687597 -0.46588492 -1.21423106  0.05804352  0.26858188\n",
      "  -1.27686199 -0.93399853  0.56063971]\n",
      " [ 0.56762152 -0.37741587  0.88073932 -0.91297687  0.41836363 -0.44885276\n",
      "  -0.98315481  0.7349231   0.68911839]\n",
      " [ 0.98009188 -1.31855437  0.35037928  1.0967903  -0.14358003 -0.22790725\n",
      "   1.16528019  0.42450965 -0.79465723]\n",
      " [-0.96218279  1.30684557 -0.33288029 -0.96396364  0.19919772  0.2233288\n",
      "  -1.18956777 -0.81457015  0.63004308]\n",
      " [ 0.04716838  0.33293728  0.69904679 -1.12705882  0.02687511  0.12311827\n",
      "  -1.26888621  0.40848144  0.33619983]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.66317882  1.72028648 -3.08878238 -0.06436148  1.825906   -2.8412634\n",
      "  -0.88595408]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:83 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.9175747]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 83 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89649853 -1.16028284  0.29113565  1.06622437 -0.11968641 -0.31447485\n",
      "   1.08146107  0.73696776 -0.50966529]\n",
      " [-0.76339756  1.34687597 -0.46588492 -1.21459415  0.05804352  0.26858188\n",
      "  -1.27722507 -0.93399853  0.56063971]\n",
      " [ 0.56693597 -0.37741587  0.88073932 -0.91366242  0.41836363 -0.44885276\n",
      "  -0.98384036  0.7349231   0.68911839]\n",
      " [ 0.98045765 -1.31855437  0.35037928  1.09715608 -0.14358003 -0.22790725\n",
      "   1.16564597  0.42450965 -0.79465723]\n",
      " [-0.96257767  1.30684557 -0.33288029 -0.96435852  0.19919772  0.2233288\n",
      "  -1.18996265 -0.81457015  0.63004308]\n",
      " [ 0.04658588  0.33293728  0.69904679 -1.12764133  0.02687511  0.12311827\n",
      "  -1.26946871  0.40848144  0.33619983]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.66006185  1.72326155 -3.0886665  -0.06370877  1.82890575 -2.84113104\n",
      "  -0.8856824 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:83 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.76383715]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 83 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90126715 -1.16028284  0.29113565  1.06622437 -0.11491778 -0.30970623\n",
      "   1.08622969  0.73696776 -0.50966529]\n",
      " [-0.76811987  1.34687597 -0.46588492 -1.21459415  0.05332122  0.26385958\n",
      "  -1.28194738 -0.93399853  0.56063971]\n",
      " [ 0.56466887 -0.37741587  0.88073932 -0.91366242  0.41609653 -0.45111986\n",
      "  -0.98610746  0.7349231   0.68911839]\n",
      " [ 0.98514315 -1.31855437  0.35037928  1.09715608 -0.13889453 -0.22322175\n",
      "   1.17033147  0.42450965 -0.79465723]\n",
      " [-0.96729121  1.30684557 -0.33288029 -0.96435852  0.19448418  0.21861526\n",
      "  -1.19467618 -0.81457015  0.63004308]\n",
      " [ 0.04224596  0.33293728  0.69904679 -1.12764133  0.02253519  0.11877835\n",
      "  -1.27380863  0.40848144  0.33619983]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.63876115  1.74081368 -3.08541503 -0.05540191  1.8471185  -2.83792345\n",
      "  -0.88025383]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:83 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.56684988]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 83 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.88665935 -1.16028284  0.29113565  1.05161658 -0.11491778 -0.32431402\n",
      "   1.08622969  0.73696776 -0.52427308]\n",
      " [-0.75346082  1.34687597 -0.46588492 -1.19993509  0.05332122  0.27851863\n",
      "  -1.28194738 -0.93399853  0.57529877]\n",
      " [ 0.56659396 -0.37741587  0.88073932 -0.91173733  0.41609653 -0.44919477\n",
      "  -0.98610746  0.7349231   0.69104348]\n",
      " [ 0.97101817 -1.31855437  0.35037928  1.0830311  -0.13889453 -0.23734673\n",
      "   1.17033147  0.42450965 -0.80878221]\n",
      " [-0.95305013  1.30684557 -0.33288029 -0.95011744  0.19448418  0.23285634\n",
      "  -1.19467618 -0.81457015  0.64428416]\n",
      " [ 0.05219219  0.33293728  0.69904679 -1.11769509  0.02253519  0.12872459\n",
      "  -1.27380863  0.40848144  0.34614607]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.70835078  1.6879835  -3.10204638 -0.08826768  1.79537622 -2.85552548\n",
      "  -0.90443038]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:83 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.74071258]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 83 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89223352 -1.15470868  0.29113565  1.05719074 -0.11491778 -0.31873986\n",
      "   1.09180386  0.73696776 -0.52427308]\n",
      " [-0.75902645  1.34131034 -0.46588492 -1.20550072  0.05332122  0.272953\n",
      "  -1.28751301 -0.93399853  0.57529877]\n",
      " [ 0.56160439 -0.38240544  0.88073932 -0.91672691  0.41609653 -0.45418435\n",
      "  -0.99109704  0.7349231   0.69104348]\n",
      " [ 0.97656276 -1.31300979  0.35037928  1.08857568 -0.13889453 -0.23180214\n",
      "   1.17587606  0.42450965 -0.80878221]\n",
      " [-0.95862389  1.30127181 -0.33288029 -0.9556912   0.19448418  0.22728258\n",
      "  -1.20024995 -0.81457015  0.64428416]\n",
      " [ 0.04681291  0.327558    0.69904679 -1.12307437  0.02253519  0.12334531\n",
      "  -1.27918791  0.40848144  0.34614607]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.68345174  1.70848665 -3.09789831 -0.08568816  1.8163256  -2.8511952\n",
      "  -0.9011273 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:83 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.81289574]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 83 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89530202 -1.15164018  0.29113565  1.06025924 -0.11491778 -0.31873986\n",
      "   1.09487236  0.73696776 -0.52427308]\n",
      " [-0.76207918  1.3382576  -0.46588492 -1.20855346  0.05332122  0.272953\n",
      "  -1.29056575 -0.93399853  0.57529877]\n",
      " [ 0.5584552  -0.38555463  0.88073932 -0.91987609  0.41609653 -0.45418435\n",
      "  -0.99424622  0.7349231   0.69104348]\n",
      " [ 0.97960375 -1.3099688   0.35037928  1.09161668 -0.13889453 -0.23180214\n",
      "   1.17891705  0.42450965 -0.80878221]\n",
      " [-0.96173482  1.29816088 -0.33288029 -0.95880213  0.19448418  0.22728258\n",
      "  -1.20336088 -0.81457015  0.64428416]\n",
      " [ 0.04384727  0.32459235  0.69904679 -1.12604001  0.02253519  0.12334531\n",
      "  -1.28215356  0.40848144  0.34614607]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.66922281  1.72084246 -3.09606422 -0.08354296  1.82874786 -2.84919973\n",
      "  -0.89947239]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:83 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.82289767]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 83 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89803771 -1.15164018  0.29387135  1.06025924 -0.11491778 -0.31600416\n",
      "   1.09760806  0.73696776 -0.52427308]\n",
      " [-0.76458786  1.3382576  -0.4683936  -1.20855346  0.05332122  0.27044432\n",
      "  -1.29307443 -0.93399853  0.57529877]\n",
      " [ 0.55842541 -0.38555463  0.88070952 -0.91987609  0.41609653 -0.45421414\n",
      "  -0.99427602  0.7349231   0.69104348]\n",
      " [ 0.98208342 -1.3099688   0.35285895  1.09161668 -0.13889453 -0.22932247\n",
      "   1.18139672  0.42450965 -0.80878221]\n",
      " [-0.96422043  1.29816088 -0.3353659  -0.95880213  0.19448418  0.22479697\n",
      "  -1.20584649 -0.81457015  0.64428416]\n",
      " [ 0.0425618   0.32459235  0.69776132 -1.12604001  0.02253519  0.12205984\n",
      "  -1.28343902  0.40848144  0.34614607]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.65631762  1.73215787 -3.09482878 -0.07712016  1.84045238 -2.84799207\n",
      "  -0.89434265]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:83 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.10889583]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 84 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89767206 -1.15200583  0.29387135  1.05989359 -0.11491778 -0.31600416\n",
      "   1.09760806  0.73696776 -0.52463874]\n",
      " [-0.76450923  1.33833624 -0.4683936  -1.20847482  0.05332122  0.27044432\n",
      "  -1.29307443 -0.93399853  0.5753774 ]\n",
      " [ 0.55849927 -0.38548077  0.88070952 -0.91980223  0.41609653 -0.45421414\n",
      "  -0.99427602  0.7349231   0.69111734]\n",
      " [ 0.9821429  -1.30990932  0.35285895  1.09167615 -0.13889453 -0.22932247\n",
      "   1.18139672  0.42450965 -0.80872273]\n",
      " [-0.96424608  1.29813522 -0.3353659  -0.95882778  0.19448418  0.22479697\n",
      "  -1.20584649 -0.81457015  0.64425851]\n",
      " [ 0.04308441  0.32511496  0.69776132 -1.12551741  0.02253519  0.12205984\n",
      "  -1.28343902  0.40848144  0.34666867]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.66160111  1.72914559 -3.09739185 -0.07968801  1.83787013 -2.85065947\n",
      "  -0.89644683]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:84 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.85647848]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 84 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.8993593  -1.15200583  0.29555858  1.05989359 -0.11491778 -0.31600416\n",
      "   1.09929529  0.73696776 -0.52463874]\n",
      " [-0.76603715  1.33833624 -0.46992152 -1.20847482  0.05332122  0.27044432\n",
      "  -1.29460235 -0.93399853  0.5753774 ]\n",
      " [ 0.55943346 -0.38548077  0.88164371 -0.91980223  0.41609653 -0.45421414\n",
      "  -0.99334183  0.7349231   0.69111734]\n",
      " [ 0.98367748 -1.30990932  0.35439354  1.09167615 -0.13889453 -0.22932247\n",
      "   1.1829313   0.42450965 -0.80872273]\n",
      " [-0.96578827  1.29813522 -0.33690808 -0.95882778  0.19448418  0.22479697\n",
      "  -1.20738868 -0.81457015  0.64425851]\n",
      " [ 0.04197177  0.32511496  0.69664868 -1.12551741  0.02253519  0.12205984\n",
      "  -1.28455166  0.40848144  0.34666867]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.65278006  1.73715488 -3.09673859 -0.07431216  1.84603211 -2.84999369\n",
      "  -0.89320434]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:84 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.56854182]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 84 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.88475235 -1.16661278  0.28095163  1.05989359 -0.11491778 -0.31600416\n",
      "   1.08468834  0.73696776 -0.52463874]\n",
      " [-0.7511859   1.35318748 -0.45507027 -1.20847482  0.05332122  0.27044432\n",
      "  -1.2797511  -0.93399853  0.5753774 ]\n",
      " [ 0.55834922 -0.386565    0.88055947 -0.91980223  0.41609653 -0.45421414\n",
      "  -0.99442606  0.7349231   0.69111734]\n",
      " [ 0.96874347 -1.32484334  0.33945952  1.09167615 -0.13889453 -0.22932247\n",
      "   1.16799729  0.42450965 -0.80872273]\n",
      " [-0.95085061  1.31307288 -0.32197043 -0.95882778  0.19448418  0.22479697\n",
      "  -1.19245102 -0.81457015  0.64425851]\n",
      " [ 0.04577472  0.32891791  0.70045164 -1.12551741  0.02253519  0.12205984\n",
      "  -1.2807487   0.40848144  0.34666867]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.72251229  1.68429195 -3.11297652 -0.11026321  1.79230396 -2.86598721\n",
      "  -0.92423652]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:84 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.69436031]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 84 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89172263 -1.16661278  0.28792191  1.05989359 -0.11491778 -0.31600416\n",
      "   1.08468834  0.74393804 -0.52463874]\n",
      " [-0.75772243  1.35318748 -0.4616068  -1.20847482  0.05332122  0.27044432\n",
      "  -1.2797511  -0.94053505  0.5753774 ]\n",
      " [ 0.56481393 -0.386565    0.88702418 -0.91980223  0.41609653 -0.45421414\n",
      "  -0.99442606  0.74138781  0.69111734]\n",
      " [ 0.97591786 -1.32484334  0.34663391  1.09167615 -0.13889453 -0.22932247\n",
      "   1.16799729  0.43168404 -0.80872273]\n",
      " [-0.95749562  1.31307288 -0.32861543 -0.95882778  0.19448418  0.22479697\n",
      "  -1.19245102 -0.82121516  0.64425851]\n",
      " [ 0.05259831  0.32891791  0.70727523 -1.12551741  0.02253519  0.12205984\n",
      "  -1.2807487   0.41530503  0.34666867]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.69008024  1.71251411 -3.1095632  -0.08114329  1.81986339 -2.86240903\n",
      "  -0.89957617]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:84 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.62748629]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 84 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90142059 -1.16661278  0.28792191  1.06959156 -0.11491778 -0.31600416\n",
      "   1.08468834  0.74393804 -0.51494077]\n",
      " [-0.76738447  1.35318748 -0.4616068  -1.21813687  0.05332122  0.27044432\n",
      "  -1.2797511  -0.94053505  0.56571536]\n",
      " [ 0.56837102 -0.386565    0.88702418 -0.91624514  0.41609653 -0.45421414\n",
      "  -0.99442606  0.74138781  0.69467443]\n",
      " [ 0.98535861 -1.32484334  0.34663391  1.1011169  -0.13889453 -0.22932247\n",
      "   1.16799729  0.43168404 -0.79928198]\n",
      " [-0.9669651   1.31307288 -0.32861543 -0.96829726  0.19448418  0.22479697\n",
      "  -1.19245102 -0.82121516  0.63478903]\n",
      " [ 0.04565071  0.32891791  0.70727523 -1.13246501  0.02253519  0.12205984\n",
      "  -1.2807487   0.41530503  0.33972106]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.64654321  1.74762367 -3.10088727 -0.05575032  1.85377141 -2.8528786\n",
      "  -0.88538231]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:84 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.06396246]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 84 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90144186 -1.16661278  0.28792191  1.06959156 -0.11489652 -0.3159829\n",
      "   1.08468834  0.74393804 -0.51491951]\n",
      " [-0.7674427   1.35318748 -0.4616068  -1.21813687  0.05326299  0.27038609\n",
      "  -1.2797511  -0.94053505  0.56565713]\n",
      " [ 0.56795937 -0.386565    0.88702418 -0.91624514  0.41568489 -0.45462579\n",
      "  -0.99442606  0.74138781  0.69426279]\n",
      " [ 0.98544508 -1.32484334  0.34663391  1.1011169  -0.13880806 -0.229236\n",
      "   1.16799729  0.43168404 -0.79919551]\n",
      " [-0.96700672  1.31307288 -0.32861543 -0.96829726  0.19444256  0.22475536\n",
      "  -1.19245102 -0.82121516  0.63474741]\n",
      " [ 0.04541403  0.32891791  0.70727523 -1.13246501  0.02229851  0.12182317\n",
      "  -1.2807487   0.41530503  0.33948439]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.64845797  1.74668756 -3.10190303 -0.0572303   1.85290098 -2.85387765\n",
      "  -0.8865876 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:84 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.00455203]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 84 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90144407 -1.16661057  0.28792191  1.06959156 -0.11489431 -0.31598069\n",
      "   1.08468834  0.74393804 -0.5149173 ]\n",
      " [-0.767445    1.35318518 -0.4616068  -1.21813687  0.05326068  0.27038379\n",
      "  -1.2797511  -0.94053505  0.56565483]\n",
      " [ 0.56795756 -0.38656682  0.88702418 -0.91624514  0.41568307 -0.45462761\n",
      "  -0.99442606  0.74138781  0.69426097]\n",
      " [ 0.98544738 -1.32484103  0.34663391  1.1011169  -0.13880576 -0.2292337\n",
      "   1.16799729  0.43168404 -0.79919321]\n",
      " [-0.96700901  1.31307059 -0.32861543 -0.96829726  0.19444027  0.22475306\n",
      "  -1.19245102 -0.82121516  0.63474512]\n",
      " [ 0.04541218  0.32891606  0.70727523 -1.13246501  0.02229666  0.12182132\n",
      "  -1.2807487   0.41530503  0.33948254]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.64846828  1.74668519 -3.10191142 -0.05723749  1.85289911 -2.85388593\n",
      "  -0.88659485]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:84 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.23788932]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 84 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.899426   -1.16862864  0.28792191  1.06959156 -0.11691238 -0.31799876\n",
      "   1.08267028  0.74393804 -0.5149173 ]\n",
      " [-0.76551525  1.35511492 -0.4616068  -1.21813687  0.05519043  0.27231354\n",
      "  -1.27782135 -0.94053505  0.56565483]\n",
      " [ 0.57180853 -0.38271584  0.88702418 -0.91624514  0.41953405 -0.45077663\n",
      "  -0.99057508  0.74138781  0.69426097]\n",
      " [ 0.98309158 -1.32719683  0.34663391  1.1011169  -0.14116155 -0.2315895\n",
      "   1.16564149  0.43168404 -0.79919321]\n",
      " [-0.96480791  1.31527169 -0.32861543 -0.96829726  0.19664137  0.22695416\n",
      "  -1.19024992 -0.82121516  0.63474512]\n",
      " [ 0.0489782   0.33248208  0.70727523 -1.13246501  0.02586268  0.12538733\n",
      "  -1.27718269  0.41530503  0.33948254]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.67003271  1.73383372 -3.11071946 -0.06368564  1.83967692 -2.86239948\n",
      "  -0.8934555 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:84 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.91914028]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 84 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89982127 -1.16862864  0.28792191  1.06998683 -0.11691238 -0.31799876\n",
      "   1.08306555  0.74393804 -0.5149173 ]\n",
      " [-0.76586369  1.35511492 -0.4616068  -1.2184853   0.05519043  0.27231354\n",
      "  -1.27816979 -0.94053505  0.56565483]\n",
      " [ 0.57114693 -0.38271584  0.88702418 -0.91690674  0.41953405 -0.45077663\n",
      "  -0.99123669  0.74138781  0.69426097]\n",
      " [ 0.98344253 -1.32719683  0.34663391  1.10146785 -0.14116155 -0.2315895\n",
      "   1.16599243  0.43168404 -0.79919321]\n",
      " [-0.96518685  1.31527169 -0.32861543 -0.96867621  0.19664137  0.22695416\n",
      "  -1.19062886 -0.82121516  0.63474512]\n",
      " [ 0.04841933  0.33248208  0.70727523 -1.13302388  0.02586268  0.12538733\n",
      "  -1.27774155  0.41530503  0.33948254]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.6670279   1.73670288 -3.11060854 -0.06305964  1.84256955 -2.86227281\n",
      "  -0.89319642]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:84 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.76597392]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 84 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90451706 -1.16862864  0.28792191  1.06998683 -0.11221659 -0.31330297\n",
      "   1.08776134  0.74393804 -0.5149173 ]\n",
      " [-0.77051258  1.35511492 -0.4616068  -1.2184853   0.05054154  0.26766464\n",
      "  -1.28281868 -0.94053505  0.56565483]\n",
      " [ 0.56889675 -0.38271584  0.88702418 -0.91690674  0.41728387 -0.45302681\n",
      "  -0.99348687  0.74138781  0.69426097]\n",
      " [ 0.98805509 -1.32719683  0.34663391  1.10146785 -0.13654899 -0.22697693\n",
      "   1.170605    0.43168404 -0.79919321]\n",
      " [-0.96982718  1.31527169 -0.32861543 -0.96867621  0.19200104  0.22231383\n",
      "  -1.1952692  -0.82121516  0.63474512]\n",
      " [ 0.04413594  0.33248208  0.70727523 -1.13302388  0.0215793   0.12110395\n",
      "  -1.28202494  0.41530503  0.33948254]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.6460524   1.75399973 -3.10741357 -0.05489925  1.86050963 -2.85912011\n",
      "  -0.88787134]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:84 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.56584924]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 84 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.88993664 -1.16862864  0.28792191  1.05540641 -0.11221659 -0.3278834\n",
      "   1.08776134  0.74393804 -0.52949772]\n",
      " [-0.75588421  1.35511492 -0.4616068  -1.20385693  0.05054154  0.28229301\n",
      "  -1.28281868 -0.94053505  0.58028319]\n",
      " [ 0.57074681 -0.38271584  0.88702418 -0.91505667  0.41728387 -0.45117674\n",
      "  -0.99348687  0.74138781  0.69611103]\n",
      " [ 0.97395424 -1.32719683  0.34663391  1.087367   -0.13654899 -0.24107779\n",
      "   1.170605    0.43168404 -0.81329406]\n",
      " [-0.95561302  1.31527169 -0.32861543 -0.95446204  0.19200104  0.236528\n",
      "  -1.1952692  -0.82121516  0.64895929]\n",
      " [ 0.05404304  0.33248208  0.70727523 -1.12311679  0.0215793   0.13101104\n",
      "  -1.28202494  0.41530503  0.34938963]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.71555676  1.70125763 -3.12405663 -0.08779784  1.8088449  -2.87672089\n",
      "  -0.91205162]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:84 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.741081]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 84 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89549767 -1.16306761  0.28792191  1.06096744 -0.11221659 -0.32232236\n",
      "   1.09332237  0.74393804 -0.52949772]\n",
      " [-0.76143797  1.34956117 -0.4616068  -1.20941069  0.05054154  0.27673925\n",
      "  -1.28837244 -0.94053505  0.58028319]\n",
      " [ 0.56579174 -0.38767092  0.88702418 -0.92001175  0.41728387 -0.45613181\n",
      "  -0.99844194  0.74138781  0.69611103]\n",
      " [ 0.97948802 -1.32166305  0.34663391  1.09290078 -0.13654899 -0.23554401\n",
      "   1.17613877  0.43168404 -0.81329406]\n",
      " [-0.96117394  1.30971077 -0.32861543 -0.96002296  0.19200104  0.23096708\n",
      "  -1.20083012 -0.82121516  0.64895929]\n",
      " [ 0.04868729  0.32712633  0.70727523 -1.12847253  0.0215793   0.12565529\n",
      "  -1.28738069  0.41530503  0.34938963]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.69071607  1.72170074 -3.11990171 -0.0852561   1.82972915 -2.87238433\n",
      "  -0.90878463]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:84 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.81435602]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 84 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89852376 -1.16004152  0.28792191  1.06399353 -0.11221659 -0.32232236\n",
      "   1.09634846  0.74393804 -0.52949772]\n",
      " [-0.76444934  1.34654979 -0.4616068  -1.21242206  0.05054154  0.27673925\n",
      "  -1.29138381 -0.94053505  0.58028319]\n",
      " [ 0.56269044 -0.39077222  0.88702418 -0.92311305  0.41728387 -0.45613181\n",
      "  -1.00154325  0.74138781  0.69611103]\n",
      " [ 0.98248788 -1.31866318  0.34663391  1.09590064 -0.13654899 -0.23554401\n",
      "   1.17913864  0.43168404 -0.81329406]\n",
      " [-0.96424251  1.3066422  -0.32861543 -0.96309153  0.19200104  0.23096708\n",
      "  -1.20389869 -0.82121516  0.64895929]\n",
      " [ 0.04577234  0.32421138  0.70727523 -1.13138749  0.0215793   0.12565529\n",
      "  -1.29029564  0.41530503  0.34938963]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.67668322  1.73388661 -3.1180912  -0.0831614   1.84197846 -2.87041471\n",
      "  -0.90716997]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:84 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.82354617]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 84 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90124299 -1.16004152  0.29064114  1.06399353 -0.11221659 -0.31960313\n",
      "   1.09906769  0.74393804 -0.52949772]\n",
      " [-0.76694605  1.34654979 -0.46410351 -1.21242206  0.05054154  0.27424254\n",
      "  -1.29388052 -0.94053505  0.58028319]\n",
      " [ 0.56266493 -0.39077222  0.88699866 -0.92311305  0.41728387 -0.45615733\n",
      "  -1.00156876  0.74138781  0.69611103]\n",
      " [ 0.98495541 -1.31866318  0.34910143  1.09590064 -0.13654899 -0.23307648\n",
      "   1.18160616  0.43168404 -0.81329406]\n",
      " [-0.96671639  1.3066422  -0.33108932 -0.96309153  0.19200104  0.22849319\n",
      "  -1.20637258 -0.82121516  0.64895929]\n",
      " [ 0.04450742  0.32421138  0.70601031 -1.13138749  0.0215793   0.12439037\n",
      "  -1.29156056  0.41530503  0.34938963]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.66386227  1.74512524 -3.11685841 -0.07677644  1.85360183 -2.86920959\n",
      "  -0.90206043]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:84 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.1056731]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 85 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90090528 -1.16037922  0.29064114  1.06365582 -0.11221659 -0.31960313\n",
      "   1.09906769  0.74393804 -0.52983543]\n",
      " [-0.76688052  1.34661533 -0.46410351 -1.21235652  0.05054154  0.27424254\n",
      "  -1.29388052 -0.94053505  0.58034873]\n",
      " [ 0.56273367 -0.39070348  0.88699866 -0.92304431  0.41728387 -0.45615733\n",
      "  -1.00156876  0.74138781  0.69617978]\n",
      " [ 0.98501916 -1.31859943  0.34910143  1.09596439 -0.13654899 -0.23307648\n",
      "   1.18160616  0.43168404 -0.81323031]\n",
      " [-0.96674859  1.30661    -0.33108932 -0.96312372  0.19200104  0.22849319\n",
      "  -1.20637258 -0.82121516  0.64892709]\n",
      " [ 0.04500192  0.32470588  0.70601031 -1.13089299  0.0215793   0.12439037\n",
      "  -1.29156056  0.41530503  0.34988413]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.66885566  1.74228655 -3.11928953 -0.07920436  1.85116891 -2.87173848\n",
      "  -0.90404843]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:85 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.85775433]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 85 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90256421 -1.16037922  0.29230006  1.06365582 -0.11221659 -0.31960313\n",
      "   1.10072661  0.74393804 -0.52983543]\n",
      " [-0.76838439  1.34661533 -0.46560738 -1.21235652  0.05054154  0.27424254\n",
      "  -1.2953844  -0.94053505  0.58034873]\n",
      " [ 0.56365871 -0.39070348  0.88792371 -0.92304431  0.41728387 -0.45615733\n",
      "  -1.00064372  0.74138781  0.69617978]\n",
      " [ 0.98652929 -1.31859943  0.35061156  1.09596439 -0.13654899 -0.23307648\n",
      "   1.18311629  0.43168404 -0.81323031]\n",
      " [-0.96826659  1.30661    -0.33260732 -0.96312372  0.19200104  0.22849319\n",
      "  -1.20789057 -0.82121516  0.64892709]\n",
      " [ 0.04391088  0.32470588  0.70491927 -1.13089299  0.0215793   0.12439037\n",
      "  -1.2926516   0.41530503  0.34988413]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.66017783  1.75016686 -3.11864622 -0.07390913  1.85919796 -2.87108275\n",
      "  -0.90085447]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:85 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.56449116]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 85 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.88806636 -1.17487707  0.27780222  1.06365582 -0.11221659 -0.31960313\n",
      "   1.08622877  0.74393804 -0.52983543]\n",
      " [-0.75364979  1.36134993 -0.45087278 -1.21235652  0.05054154  0.27424254\n",
      "  -1.2806498  -0.94053505  0.58034873]\n",
      " [ 0.56261476 -0.39174742  0.88687976 -0.92304431  0.41728387 -0.45615733\n",
      "  -1.00168767  0.74138781  0.69617978]\n",
      " [ 0.97170965 -1.33341907  0.33579192  1.09596439 -0.13654899 -0.23307648\n",
      "   1.16829665  0.43168404 -0.81323031]\n",
      " [-0.95344478  1.32143181 -0.31778551 -0.96312372  0.19200104  0.22849319\n",
      "  -1.19306877 -0.82121516  0.64892709]\n",
      " [ 0.04766659  0.32846159  0.70867498 -1.13089299  0.0215793   0.12439037\n",
      "  -1.28889589  0.41530503  0.34988413]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.72956533  1.69765443 -3.13492148 -0.10964746  1.80585149 -2.88711766\n",
      "  -0.93176239]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:85 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.6962546]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 85 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89495863 -1.17487707  0.28469448  1.06365582 -0.11221659 -0.31960313\n",
      "   1.08622877  0.75083031 -0.52983543]\n",
      " [-0.76011311  1.36134993 -0.4573361  -1.21235652  0.05054154  0.27424254\n",
      "  -1.2806498  -0.94699837  0.58034873]\n",
      " [ 0.56898009 -0.39174742  0.89324509 -0.92304431  0.41728387 -0.45615733\n",
      "  -1.00168767  0.74775314  0.69617978]\n",
      " [ 0.978809   -1.33341907  0.34289128  1.09596439 -0.13654899 -0.23307648\n",
      "   1.16829665  0.43878339 -0.81323031]\n",
      " [-0.96001556  1.32143181 -0.32435629 -0.96312372  0.19200104  0.22849319\n",
      "  -1.19306877 -0.82778594  0.64892709]\n",
      " [ 0.0544628   0.32846159  0.7154712  -1.13089299  0.0215793   0.12439037\n",
      "  -1.28889589  0.42210125  0.34988413]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Theta two: \n",
      "[[-0.69744667  1.72562971 -3.13155564 -0.08075904  1.8331711  -2.88358996\n",
      "  -0.90724163]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:85 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.62871456]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 85 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90461312 -1.17487707  0.28469448  1.07331032 -0.11221659 -0.31960313\n",
      "   1.08622877  0.75083031 -0.52018093]\n",
      " [-0.7697318   1.36134993 -0.4573361  -1.22197521  0.05054154  0.27424254\n",
      "  -1.2806498  -0.94699837  0.57073005]\n",
      " [ 0.57258012 -0.39174742  0.89324509 -0.91944428  0.41728387 -0.45615733\n",
      "  -1.00168767  0.74775314  0.69977981]\n",
      " [ 0.98821187 -1.33341907  0.34289128  1.10536726 -0.13654899 -0.23307648\n",
      "   1.16829665  0.43878339 -0.80382744]\n",
      " [-0.96944562  1.32143181 -0.32435629 -0.97255379  0.19200104  0.22849319\n",
      "  -1.19306877 -0.82778594  0.63949703]\n",
      " [ 0.04754533  0.32846159  0.7154712  -1.13781047  0.0215793   0.12439037\n",
      "  -1.28889589  0.42210125  0.34296665]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.65411161  1.76058857 -3.12292896 -0.05542084  1.86694175 -2.87411964\n",
      "  -0.89311643]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:85 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.06206519]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 85 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90463451 -1.17487707  0.28469448  1.07331032 -0.1121952  -0.31958175\n",
      "   1.08622877  0.75083031 -0.52015955]\n",
      " [-0.76978838  1.36134993 -0.4573361  -1.22197521  0.05048495  0.27418596\n",
      "  -1.2806498  -0.94699837  0.57067346]\n",
      " [ 0.57219086 -0.39174742  0.89324509 -0.91944428  0.41689461 -0.45654659\n",
      "  -1.00168767  0.74775314  0.69939055]\n",
      " [ 0.98829481 -1.33341907  0.34289128  1.10536726 -0.13646605 -0.23299354\n",
      "   1.16829665  0.43878339 -0.8037445 ]\n",
      " [-0.96948643  1.32143181 -0.32435629 -0.97255379  0.19196023  0.22845238\n",
      "  -1.19306877 -0.82778594  0.63945622]\n",
      " [ 0.04731967  0.32846159  0.7154712  -1.13781047  0.02135364  0.12416472\n",
      "  -1.28889589  0.42210125  0.34274099]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.65591811  1.75970672 -3.12388894 -0.05681985  1.86612191 -2.87506376\n",
      "  -0.89425633]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:85 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.00428422]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 85 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90463647 -1.17487511  0.28469448  1.07331032 -0.11219324 -0.31957978\n",
      "   1.08622877  0.75083031 -0.52015758]\n",
      " [-0.76979042  1.36134789 -0.4573361  -1.22197521  0.05048291  0.27418392\n",
      "  -1.2806498  -0.94699837  0.57067142]\n",
      " [ 0.57218925 -0.39174904  0.89324509 -0.91944428  0.41689299 -0.45654821\n",
      "  -1.00168767  0.74775314  0.69938893]\n",
      " [ 0.98829686 -1.33341703  0.34289128  1.10536726 -0.136464   -0.23299149\n",
      "   1.16829665  0.43878339 -0.80374245]\n",
      " [-0.96948847  1.32142977 -0.32435629 -0.97255379  0.1919582   0.22845035\n",
      "  -1.19306877 -0.82778594  0.63945419]\n",
      " [ 0.04731802  0.32845994  0.7154712  -1.13781047  0.02135199  0.12416307\n",
      "  -1.28889589  0.42210125  0.34273935]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.65592725  1.75970464 -3.1238964  -0.05682623  1.86612027 -2.87507111\n",
      "  -0.89426275]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:85 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.2335712]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 85 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90270064 -1.17681094  0.28469448  1.07331032 -0.11412907 -0.32151561\n",
      "   1.08429294  0.75083031 -0.52015758]\n",
      " [-0.76794758  1.36319073 -0.4573361  -1.22197521  0.05232576  0.27602676\n",
      "  -1.27880695 -0.94699837  0.57067142]\n",
      " [ 0.57594826 -0.38799003  0.89324509 -0.91944428  0.420652   -0.4527892\n",
      "  -0.99792866  0.74775314  0.69938893]\n",
      " [ 0.98604341 -1.33567047  0.34289128  1.10536726 -0.13871745 -0.23524494\n",
      "   1.16604321  0.43878339 -0.80374245]\n",
      " [-0.96738404  1.3235342  -0.32435629 -0.97255379  0.19406263  0.23055478\n",
      "  -1.19096434 -0.82778594  0.63945419]\n",
      " [ 0.05079255  0.33193447  0.7154712  -1.13781047  0.02482651  0.12763759\n",
      "  -1.28542136  0.42210125  0.34273935]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.67683371  1.7472676  -3.13246571 -0.0630386   1.85333548 -2.88335727\n",
      "  -0.90089006]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:85 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.92066637]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 85 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90307986 -1.17681094  0.28469448  1.07368954 -0.11412907 -0.32151561\n",
      "   1.08467216  0.75083031 -0.52015758]\n",
      " [-0.76828203  1.36319073 -0.4573361  -1.22230966  0.05232576  0.27602676\n",
      "  -1.2791414  -0.94699837  0.57067142]\n",
      " [ 0.57530966 -0.38799003  0.89324509 -0.92008287  0.420652   -0.4527892\n",
      "  -0.99856725  0.74775314  0.69938893]\n",
      " [ 0.98638021 -1.33567047  0.34289128  1.10570405 -0.13871745 -0.23524494\n",
      "   1.16638001  0.43878339 -0.80374245]\n",
      " [-0.96774777  1.3235342  -0.32435629 -0.97291753  0.19406263  0.23055478\n",
      "  -1.19132808 -0.82778594  0.63945419]\n",
      " [ 0.05025625  0.33193447  0.7154712  -1.13834676  0.02482651  0.12763759\n",
      "  -1.28595766  0.42210125  0.34273935]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.67393645  1.75003514 -3.1323595  -0.06243807  1.85612537 -2.88323602\n",
      "  -0.90064292]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:85 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.76809809]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 85 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90770349 -1.17681094  0.28469448  1.07368954 -0.10950544 -0.31689198\n",
      "   1.08929578  0.75083031 -0.52015758]\n",
      " [-0.77285825  1.36319073 -0.4573361  -1.22230966  0.04774953  0.27145054\n",
      "  -1.28371762 -0.94699837  0.57067142]\n",
      " [ 0.5730761  -0.38799003  0.89324509 -0.92008287  0.41841843 -0.45502276\n",
      "  -1.00080082  0.74775314  0.69938893]\n",
      " [ 0.99092056 -1.33567047  0.34289128  1.10570405 -0.1341771  -0.23070459\n",
      "   1.17092035  0.43878339 -0.80374245]\n",
      " [-0.97231563  1.3235342  -0.32435629 -0.97291753  0.18949477  0.22598692\n",
      "  -1.19589594 -0.82778594  0.63945419]\n",
      " [ 0.04602918  0.33193447  0.7154712  -1.13834676  0.02059944  0.12341052\n",
      "  -1.29018473  0.42210125  0.34273935]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.65328287  1.76707884 -3.12922035 -0.05442285  1.8737955  -2.88013761\n",
      "  -0.89541976]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:85 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.56478922]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 85 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89315223 -1.17681094  0.28469448  1.05913827 -0.10950544 -0.33144325\n",
      "   1.08929578  0.75083031 -0.53470885]\n",
      " [-0.75826234  1.36319073 -0.4573361  -1.20771375  0.04774953  0.28604645\n",
      "  -1.28371762 -0.94699837  0.58526733]\n",
      " [ 0.57485257 -0.38799003  0.89324509 -0.9183064   0.41841843 -0.45324629\n",
      "  -1.00080082  0.74775314  0.7011654 ]\n",
      " [ 0.97684571 -1.33567047  0.34289128  1.0916292  -0.1341771  -0.24477944\n",
      "   1.17092035  0.43878339 -0.8178173 ]\n",
      " [-0.95813023  1.3235342  -0.32435629 -0.95873212  0.18949477  0.24017232\n",
      "  -1.19589594 -0.82778594  0.65363959]\n",
      " [ 0.05589608  0.33193447  0.7154712  -1.12847986  0.02059944  0.13327742\n",
      "  -1.29018473  0.42210125  0.35260625]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.72269613  1.71443053 -3.14587489 -0.08734989  1.82221406 -2.89773702\n",
      "  -0.91960195]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:85 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.74144976]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 85 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89870008 -1.17126308  0.28469448  1.06468613 -0.10950544 -0.3258954\n",
      "   1.09484364  0.75083031 -0.53470885]\n",
      " [-0.76380411  1.35764896 -0.4573361  -1.21325552  0.04774953  0.28050468\n",
      "  -1.28925939 -0.94699837  0.58526733]\n",
      " [ 0.56993212 -0.39291047  0.89324509 -0.92322684  0.41841843 -0.45816674\n",
      "  -1.00572126  0.74775314  0.7011654 ]\n",
      " [ 0.98236853 -1.33014765  0.34289128  1.09715203 -0.1341771  -0.23925662\n",
      "   1.17644317  0.43878339 -0.8178173 ]\n",
      " [-0.96367822  1.31798621 -0.32435629 -0.96428011  0.18949477  0.23462433\n",
      "  -1.20144393 -0.82778594  0.65363959]\n",
      " [ 0.05056406  0.32660245  0.7154712  -1.13381188  0.02059944  0.1279454\n",
      "  -1.29551675  0.42210125  0.35260625]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.69791383  1.73481381 -3.1417135  -0.08484537  1.84303358 -2.89339458\n",
      "  -0.91637071]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:85 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.81581033]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 85 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90168411 -1.16827905  0.28469448  1.06767016 -0.10950544 -0.3258954\n",
      "   1.09782767  0.75083031 -0.53470885]\n",
      " [-0.76677443  1.35467865 -0.4573361  -1.21622583  0.04774953  0.28050468\n",
      "  -1.29222971 -0.94699837  0.58526733]\n",
      " [ 0.56687838 -0.39596421  0.89324509 -0.92628059  0.41841843 -0.45816674\n",
      "  -1.008775    0.74775314  0.7011654 ]\n",
      " [ 0.98532755 -1.32718863  0.34289128  1.10011105 -0.1341771  -0.23925662\n",
      "   1.1794022   0.43878339 -0.8178173 ]\n",
      " [-0.96670475  1.31495968 -0.32435629 -0.96730664  0.18949477  0.23462433\n",
      "  -1.20447046 -0.82778594  0.65363959]\n",
      " [ 0.0476993   0.32373768  0.7154712  -1.13667664  0.02059944  0.1279454\n",
      "  -1.29838151  0.42210125  0.35260625]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.68407531  1.74683131 -3.13992653 -0.08279997  1.85511165 -2.89145073\n",
      "  -0.91479541]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:85 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.82417368]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 85 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90438751 -1.16827905  0.28739788  1.06767016 -0.10950544 -0.323192\n",
      "   1.10053107  0.75083031 -0.53470885]\n",
      " [-0.76925965  1.35467865 -0.45982133 -1.21622583  0.04774953  0.27801945\n",
      "  -1.29471494 -0.94699837  0.58526733]\n",
      " [ 0.56685667 -0.39596421  0.89322338 -0.92628059  0.41841843 -0.45818845\n",
      "  -1.00879672  0.74775314  0.7011654 ]\n",
      " [ 0.9877834  -1.32718863  0.34534712  1.10011105 -0.1341771  -0.23680077\n",
      "   1.18185804  0.43878339 -0.8178173 ]\n",
      " [-0.96916739  1.31495968 -0.32681893 -0.96730664  0.18949477  0.23216169\n",
      "  -1.2069331  -0.82778594  0.65363959]\n",
      " [ 0.04645453  0.32373768  0.71422643 -1.13667664  0.02059944  0.12670063\n",
      "  -1.29962628  0.42210125  0.35260625]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.67133569  1.75799568 -3.13869616 -0.07645188  1.86665658 -2.89024794\n",
      "  -0.90970507]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:85 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.10254159]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 86 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.9040758  -1.16859076  0.28739788  1.06735845 -0.10950544 -0.323192\n",
      "   1.10053107  0.75083031 -0.53502056]\n",
      " [-0.76920596  1.35473234 -0.45982133 -1.21617214  0.04774953  0.27801945\n",
      "  -1.29471494 -0.94699837  0.58532102]\n",
      " [ 0.56692058 -0.3959003   0.89322338 -0.92621667  0.41841843 -0.45818845\n",
      "  -1.00879672  0.74775314  0.70122931]\n",
      " [ 0.98785071 -1.32712131  0.34534712  1.10017836 -0.1341771  -0.23680077\n",
      "   1.18185804  0.43878339 -0.81774999]\n",
      " [-0.96920527  1.3149218  -0.32681893 -0.96734453  0.18949477  0.23216169\n",
      "  -1.2069331  -0.82778594  0.65360171]\n",
      " [ 0.0469224   0.32420556  0.71422643 -1.13620877  0.02059944  0.12670063\n",
      "  -1.29962628  0.42210125  0.35307412]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.67605397  1.75532105 -3.14100159 -0.07874708  1.86436479 -2.89264498\n",
      "  -0.91158286]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:86 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.85900484]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 86 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90570719 -1.16859076  0.28902928  1.06735845 -0.10950544 -0.323192\n",
      "   1.10216246  0.75083031 -0.53502056]\n",
      " [-0.77068642  1.35473234 -0.46130179 -1.21617214  0.04774953  0.27801945\n",
      "  -1.29619539 -0.94699837  0.58532102]\n",
      " [ 0.56783657 -0.3959003   0.89413937 -0.92621667  0.41841843 -0.45818845\n",
      "  -1.00788073  0.74775314  0.70122931]\n",
      " [ 0.98933702 -1.32712131  0.34683343  1.10017836 -0.1341771  -0.23680077\n",
      "   1.18334435  0.43878339 -0.81774999]\n",
      " [-0.97069971  1.3149218  -0.32831337 -0.96734453  0.18949477  0.23216169\n",
      "  -1.20842754 -0.82778594  0.65360171]\n",
      " [ 0.04585243  0.32420556  0.71315645 -1.13620877  0.02059944  0.12670063\n",
      "  -1.30069626  0.42210125  0.35307412]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.66751562  1.76307569 -3.14036796 -0.07353049  1.87226439 -2.89199904\n",
      "  -0.90843612]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:86 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.56042477]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 86 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89132096 -1.18297699  0.27464305  1.06735845 -0.10950544 -0.323192\n",
      "   1.08777623  0.75083031 -0.53502056]\n",
      " [-0.75607129  1.36934747 -0.44668666 -1.21617214  0.04774953  0.27801945\n",
      "  -1.28158026 -0.94699837  0.58532102]\n",
      " [ 0.56683312 -0.39690375  0.89313592 -0.92621667  0.41841843 -0.45818845\n",
      "  -1.00888417  0.74775314  0.70122931]\n",
      " [ 0.97463458 -1.34182376  0.33213099  1.10017836 -0.1341771  -0.23680077\n",
      "   1.16864191  0.43878339 -0.81774999]\n",
      " [-0.95599671  1.3296248  -0.31361037 -0.96734453  0.18949477  0.23216169\n",
      "  -1.19372453 -0.82778594  0.65360171]\n",
      " [ 0.04956159  0.32791472  0.71686561 -1.13620877  0.02059944  0.12670063\n",
      "  -1.2969871   0.42210125  0.35307412]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.73654562  1.71092169 -3.15667503 -0.10904951  1.81930672 -2.90806983\n",
      "  -0.93921265]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:86 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.6981281]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 86 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.8981363  -1.18297699  0.28145838  1.06735845 -0.10950544 -0.323192\n",
      "   1.08777623  0.75764564 -0.53502056]\n",
      " [-0.76246253  1.36934747 -0.4530779  -1.21617214  0.04774953  0.27801945\n",
      "  -1.28158026 -0.95338961  0.58532102]\n",
      " [ 0.5731005  -0.39690375  0.8994033  -0.92621667  0.41841843 -0.45818845\n",
      "  -1.00888417  0.75402051  0.70122931]\n",
      " [ 0.98165977 -1.34182376  0.33915618  1.10017836 -0.1341771  -0.23680077\n",
      "   1.16864191  0.44580857 -0.81774999]\n",
      " [-0.96249438  1.3296248  -0.32010804 -0.96734453  0.18949477  0.23216169\n",
      "  -1.19372453 -0.83428361  0.65360171]\n",
      " [ 0.05632846  0.32791472  0.72363248 -1.13620877  0.02059944  0.12670063\n",
      "  -1.2969871   0.42886812  0.35307412]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.70473659  1.73865219 -3.15335563 -0.08039145  1.84638863 -2.9045915\n",
      "  -0.91483173]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:86 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.62991198]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 86 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90774829 -1.18297699  0.28145838  1.07697044 -0.10950544 -0.323192\n",
      "   1.08777623  0.75764564 -0.52540857]\n",
      " [-0.7720388   1.36934747 -0.4530779  -1.22574841  0.04774953  0.27801945\n",
      "  -1.28158026 -0.95338961  0.57574475]\n",
      " [ 0.57674325 -0.39690375  0.8994033  -0.92257393  0.41841843 -0.45818845\n",
      "  -1.00888417  0.75402051  0.70487206]\n",
      " [ 0.99102542 -1.34182376  0.33915618  1.10954402 -0.1341771  -0.23680077\n",
      "   1.16864191  0.44580857 -0.80838433]\n",
      " [-0.97188576  1.3296248  -0.32010804 -0.97673591  0.18949477  0.23216169\n",
      "  -1.19372453 -0.83428361  0.64421033]\n",
      " [ 0.0494406   0.32791472  0.72363248 -1.14309663  0.02059944  0.12670063\n",
      "  -1.2969871   0.42886812  0.34618626]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.66159859  1.77346348 -3.14477638 -0.05510568  1.88002458 -2.89517916\n",
      "  -0.90077324]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:86 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.06022657]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 86 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90776973 -1.18297699  0.28145838  1.07697044 -0.109484   -0.32317056\n",
      "   1.08777623  0.75764564 -0.52538712]\n",
      " [-0.77209374  1.36934747 -0.4530779  -1.22574841  0.0476946   0.27796451\n",
      "  -1.28158026 -0.95338961  0.57568981]\n",
      " [ 0.5763752  -0.39690375  0.8994033  -0.92257393  0.41805038 -0.4585565\n",
      "  -1.00888417  0.75402051  0.70450401]\n",
      " [ 0.99110496 -1.34182376  0.33915618  1.10954402 -0.13409756 -0.23672123\n",
      "   1.16864191  0.44580857 -0.80830479]\n",
      " [-0.97192572  1.3296248  -0.32010804 -0.97673591  0.18945481  0.23212174\n",
      "  -1.19372453 -0.83428361  0.64417037]\n",
      " [ 0.0492255   0.32791472  0.72363248 -1.14309663  0.02038435  0.12648554\n",
      "  -1.2969871   0.42886812  0.34597116]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.66330299  1.77263274 -3.14568367 -0.0564281   1.8792524  -2.89607137\n",
      "  -0.90185125]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:86 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.00403415]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 86 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90777148 -1.18297525  0.28145838  1.07697044 -0.10948225 -0.32316881\n",
      "   1.08777623  0.75764564 -0.52538538]\n",
      " [-0.77209555  1.36934566 -0.4530779  -1.22574841  0.04769278  0.2779627\n",
      "  -1.28158026 -0.95338961  0.575688  ]\n",
      " [ 0.57637376 -0.39690519  0.8994033  -0.92257393  0.41804895 -0.45855794\n",
      "  -1.00888417  0.75402051  0.70450257]\n",
      " [ 0.99110678 -1.34182194  0.33915618  1.10954402 -0.13409574 -0.23671942\n",
      "   1.16864191  0.44580857 -0.80830298]\n",
      " [-0.97192752  1.329623   -0.32010804 -0.97673591  0.18945301  0.23211993\n",
      "  -1.19372453 -0.83428361  0.64416857]\n",
      " [ 0.04922403  0.32791325  0.72363248 -1.14309663  0.02038288  0.12648407\n",
      "  -1.2969871   0.42886812  0.3459697 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.66331109  1.77263091 -3.14569029 -0.05643376  1.87925095 -2.8960779\n",
      "  -0.90185696]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:86 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.22937877]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 86 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90591379 -1.18483294  0.28145838  1.07697044 -0.11133994 -0.3250265\n",
      "   1.08591854  0.75764564 -0.52538538]\n",
      " [-0.77033494  1.37110628 -0.4530779  -1.22574841  0.0494534   0.27972331\n",
      "  -1.27981965 -0.95338961  0.575688  ]\n",
      " [ 0.58004355 -0.3932354   0.8994033  -0.92257393  0.42171874 -0.45488815\n",
      "  -1.00521439  0.75402051  0.70450257]\n",
      " [ 0.98895029 -1.34397843  0.33915618  1.10954402 -0.13625223 -0.2388759\n",
      "   1.16648542  0.44580857 -0.80830298]\n",
      " [-0.96991464  1.33163588 -0.32010804 -0.97673591  0.19146588  0.23413281\n",
      "  -1.19171165 -0.83428361  0.64416857]\n",
      " [ 0.0526102   0.33129941  0.72363248 -1.14309663  0.02376904  0.12987023\n",
      "  -1.29360093  0.42886812  0.3459697 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.68358406  1.76059172 -3.15402817 -0.06241976  1.86688541 -2.90414338\n",
      "  -0.90825986]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:86 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.92215439]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 86 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90627769 -1.18483294  0.28145838  1.07733434 -0.11133994 -0.3250265\n",
      "   1.08628245  0.75764564 -0.52538538]\n",
      " [-0.77065605  1.37110628 -0.4530779  -1.22606952  0.0494534   0.27972331\n",
      "  -1.28014076 -0.95338961  0.575688  ]\n",
      " [ 0.57942708 -0.3932354   0.8994033  -0.92319039  0.42171874 -0.45488815\n",
      "  -1.00583085  0.75402051  0.70450257]\n",
      " [ 0.98927359 -1.34397843  0.33915618  1.10986731 -0.13625223 -0.2388759\n",
      "   1.16680872  0.44580857 -0.80830298]\n",
      " [-0.97026387  1.33163588 -0.32010804 -0.97708514  0.19146588  0.23413281\n",
      "  -1.19206088 -0.83428361  0.64416857]\n",
      " [ 0.05209546  0.33129941  0.72363248 -1.14361137  0.02376904  0.12987023\n",
      "  -1.29411567  0.42886812  0.3459697 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.68078996  1.76326173 -3.15392644 -0.06184351  1.86957671 -2.90402728\n",
      "  -0.90802406]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:86 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.77020912]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 86 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91082986 -1.18483294  0.28145838  1.07733434 -0.10678777 -0.32047433\n",
      "   1.09083461  0.75764564 -0.52538538]\n",
      " [-0.77516036  1.37110628 -0.4530779  -1.22606952  0.04494908  0.275219\n",
      "  -1.28464507 -0.95338961  0.575688  ]\n",
      " [ 0.57720988 -0.3932354   0.8994033  -0.92319039  0.41950153 -0.45710536\n",
      "  -1.00804806  0.75402051  0.70450257]\n",
      " [ 0.99374246 -1.34397843  0.33915618  1.10986731 -0.13178336 -0.23440703\n",
      "   1.17127759  0.44580857 -0.80830298]\n",
      " [-0.97476001  1.33163588 -0.32010804 -0.97708514  0.18696975  0.22963668\n",
      "  -1.19655702 -0.83428361  0.64416857]\n",
      " [ 0.04792447  0.33129941  0.72363248 -1.14361137  0.01959805  0.12569924\n",
      "  -1.29828666  0.42886812  0.3459697 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.66045496  1.7800545  -3.15084238 -0.05397209  1.8869797  -2.90098254\n",
      "  -0.90290119]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:86 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.56367101]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 86 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89630952 -1.18483294  0.28145838  1.06281401 -0.10678777 -0.33499467\n",
      "   1.09083461  0.75764564 -0.53990571]\n",
      " [-0.76059867  1.37110628 -0.4530779  -1.21150783  0.04494908  0.28978069\n",
      "  -1.28464507 -0.95338961  0.59024969]\n",
      " [ 0.57891409 -0.3932354   0.8994033  -0.92148618  0.41950153 -0.45540114\n",
      "  -1.00804806  0.75402051  0.70620679]\n",
      " [ 0.97969545 -1.34397843  0.33915618  1.09582031 -0.13178336 -0.24845404\n",
      "   1.17127759  0.44580857 -0.82234998]\n",
      " [-0.96060519  1.33163588 -0.32010804 -0.96293032  0.18696975  0.24379149\n",
      "  -1.19655702 -0.83428361  0.65832338]\n",
      " [ 0.05775015  0.33129941  0.72363248 -1.13378568  0.01959805  0.13552492\n",
      "  -1.29828666  0.42886812  0.35579538]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.72977127  1.72750563 -3.16750811 -0.08692327  1.83548721 -2.91858036\n",
      "  -0.92708345]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:86 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.74182017]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 86 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90184411 -1.17929835  0.28145838  1.06834859 -0.10678777 -0.32946008\n",
      "   1.0963692   0.75764564 -0.53990571]\n",
      " [-0.76612829  1.36557665 -0.4530779  -1.21703745  0.04494908  0.28425107\n",
      "  -1.29017469 -0.95338961  0.59024969]\n",
      " [ 0.5740284  -0.39812108  0.8994033  -0.92637187  0.41950153 -0.46028683\n",
      "  -1.01293375  0.75402051  0.70620679]\n",
      " [ 0.98520713 -1.33846675  0.33915618  1.10133199 -0.13178336 -0.24294236\n",
      "   1.17678927  0.44580857 -0.82234998]\n",
      " [-0.96614012  1.32610095 -0.32010804 -0.96846525  0.18696975  0.23825656\n",
      "  -1.20209195 -0.83428361  0.65832338]\n",
      " [ 0.05244208  0.32599135  0.72363248 -1.13909375  0.01959805  0.13021686\n",
      "  -1.30359473  0.42886812  0.35579538]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.70504759  1.74782913 -3.16334068 -0.08445542  1.85624224 -2.9142325\n",
      "  -0.92388761]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:86 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.81725958]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 86 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.9047864  -1.17635605  0.28145838  1.07129089 -0.10678777 -0.32946008\n",
      "   1.09931149  0.75764564 -0.53990571]\n",
      " [-0.76905782  1.36264713 -0.4530779  -1.21996698  0.04494908  0.28425107\n",
      "  -1.29310422 -0.95338961  0.59024969]\n",
      " [ 0.57102191 -0.40112757  0.8994033  -0.92937836  0.41950153 -0.46028683\n",
      "  -1.01594024  0.75402051  0.70620679]\n",
      " [ 0.98812558 -1.3355483   0.33915618  1.10425043 -0.13178336 -0.24294236\n",
      "   1.17970772  0.44580857 -0.82234998]\n",
      " [-0.96912491  1.32311616 -0.32010804 -0.97145004  0.18696975  0.23825656\n",
      "  -1.20507674 -0.83428361  0.65832338]\n",
      " [ 0.04962703  0.32317629  0.72363248 -1.14190881  0.01959805  0.13021686\n",
      "  -1.30640979  0.42886812  0.35579538]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.69140178  1.75967974 -3.1615772  -0.08245815  1.86815066 -2.91231436\n",
      "  -0.92235082]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:86 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.82478152]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 86 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90747456 -1.17635605  0.28414654  1.07129089 -0.10678777 -0.32677193\n",
      "   1.10199965  0.75764564 -0.53990571]\n",
      " [-0.77153201  1.36264713 -0.45555209 -1.21996698  0.04494908  0.28177688\n",
      "  -1.29557841 -0.95338961  0.59024969]\n",
      " [ 0.57100355 -0.40112757  0.89938493 -0.92937836  0.41950153 -0.46030519\n",
      "  -1.0159586   0.75402051  0.70620679]\n",
      " [ 0.99057018 -1.3355483   0.34160078  1.10425043 -0.13178336 -0.24049775\n",
      "   1.18215232  0.44580857 -0.82234998]\n",
      " [-0.97157673  1.32311616 -0.32255986 -0.97145004  0.18696975  0.23580474\n",
      "  -1.20752856 -0.83428361  0.65832338]\n",
      " [ 0.04840204  0.32317629  0.72240749 -1.14190881  0.01959805  0.12899187\n",
      "  -1.30763478  0.42886812  0.35579538]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.67874076  1.77077219 -3.16034906 -0.07614601  1.87961972 -2.91111374\n",
      "  -0.91727872]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:86 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.0994999]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 87 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90718701 -1.17664361  0.28414654  1.07100334 -0.10678777 -0.32677193\n",
      "   1.10199965  0.75764564 -0.54019326]\n",
      " [-0.77148901  1.36269013 -0.45555209 -1.21992398  0.04494908  0.28177688\n",
      "  -1.29557841 -0.95338961  0.59029269]\n",
      " [ 0.5710629  -0.40106822  0.89938493 -0.92931901  0.41950153 -0.46030519\n",
      "  -1.0159586   0.75402051  0.70626614]\n",
      " [ 0.99064041 -1.33547808  0.34160078  1.10432066 -0.13178336 -0.24049775\n",
      "   1.18215232  0.44580857 -0.82227976]\n",
      " [-0.97161952  1.32307337 -0.32255986 -0.97149283  0.18696975  0.23580474\n",
      "  -1.20752856 -0.83428361  0.65828059]\n",
      " [ 0.0488447   0.32361895  0.72240749 -1.14146614  0.01959805  0.12899187\n",
      "  -1.30763478  0.42886812  0.35623805]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.68319834  1.76825253 -3.16253483 -0.07831542  1.8774612  -2.91338533\n",
      "  -0.91905206]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:87 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.86023151]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 87 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90879162 -1.17664361  0.28575115  1.07100334 -0.10678777 -0.32677193\n",
      "   1.10360426  0.75764564 -0.54019326]\n",
      " [-0.77294664  1.36269013 -0.45700973 -1.21992398  0.04494908  0.28177688\n",
      "  -1.29703604 -0.95338961  0.59029269]\n",
      " [ 0.57196995 -0.40106822  0.90029198 -0.92931901  0.41950153 -0.46030519\n",
      "  -1.01505155  0.75402051  0.70626614]\n",
      " [ 0.99210349 -1.33547808  0.34306387  1.10432066 -0.13178336 -0.24049775\n",
      "   1.18361541  0.44580857 -0.82227976]\n",
      " [-0.97309099  1.32307337 -0.32403133 -0.97149283  0.18696975  0.23580474\n",
      "  -1.20900003 -0.83428361  0.65828059]\n",
      " [ 0.04779528  0.32361895  0.72135808 -1.14146614  0.01959805  0.12899187\n",
      "  -1.3086842   0.42886812  0.35623805]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.67479592  1.77588464 -3.16191064 -0.07317561  1.88523464 -2.91274894\n",
      "  -0.91595132]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:87 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.55634789]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 87 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89451932 -1.19091591  0.27147885  1.07100334 -0.10678777 -0.32677193\n",
      "   1.08933196  0.75764564 -0.54019326]\n",
      " [-0.75845357  1.3771832  -0.44251665 -1.21992398  0.04494908  0.28177688\n",
      "  -1.28254297 -0.95338961  0.59029269]\n",
      " [ 0.57100702 -0.40203115  0.89932906 -0.92931901  0.41950153 -0.46030519\n",
      "  -1.01601448  0.75402051  0.70626614]\n",
      " [ 0.97752083 -1.35006074  0.32848121  1.10432066 -0.13178336 -0.24049775\n",
      "   1.16903274  0.44580857 -0.82227976]\n",
      " [-0.9585095   1.33765486 -0.30944984 -0.97149283  0.18696975  0.23580474\n",
      "  -1.19441854 -0.83428361  0.65828059]\n",
      " [ 0.05145856  0.32728224  0.72502136 -1.14146614  0.01959805  0.12899187\n",
      "  -1.30502092  0.42886812  0.35623805]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.74345618  1.72409644 -3.17824398 -0.10846917  1.83267237 -2.92885008\n",
      "  -0.94658964]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:87 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.69998237]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 87 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90125873 -1.19091591  0.27821826  1.07100334 -0.10678777 -0.32677193\n",
      "   1.08933196  0.76438506 -0.54019326]\n",
      " [-0.76477378  1.3771832  -0.44883686 -1.21992398  0.04494908  0.28177688\n",
      "  -1.28254297 -0.95970982  0.59029269]\n",
      " [ 0.57717784 -0.40203115  0.90549987 -0.92931901  0.41950153 -0.46030519\n",
      "  -1.01601448  0.76019133  0.70626614]\n",
      " [ 0.98447267 -1.35006074  0.33543305  1.10432066 -0.13178336 -0.24049775\n",
      "   1.16903274  0.45276042 -0.82227976]\n",
      " [-0.96493511  1.33765486 -0.31587545 -0.97149283  0.18696975  0.23580474\n",
      "  -1.19441854 -0.84070922  0.65828059]\n",
      " [ 0.05819416  0.32728224  0.73175695 -1.14146614  0.01959805  0.12899187\n",
      "  -1.30502092  0.43560371  0.35623805]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.71195328  1.75158412 -3.17497006 -0.08004046  1.85951855 -2.92542007\n",
      "  -0.92234894]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:87 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.63107975]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 87 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91082916 -1.19091591  0.27821826  1.08057376 -0.10678777 -0.32677193\n",
      "   1.08933196  0.76438506 -0.53062284]\n",
      " [-0.77430854  1.3771832  -0.44883686 -1.22945875  0.04494908  0.28177688\n",
      "  -1.28254297 -0.95970982  0.58075792]\n",
      " [ 0.58086312 -0.40203115  0.90549987 -0.92563372  0.41950153 -0.46030519\n",
      "  -1.01601448  0.76019133  0.70995142]\n",
      " [ 0.99380178 -1.35006074  0.33543305  1.11364977 -0.13178336 -0.24049775\n",
      "   1.16903274  0.45276042 -0.81295065]\n",
      " [-0.97428853  1.33765486 -0.31587545 -0.98084625  0.18696975  0.23580474\n",
      "  -1.19441854 -0.84070922  0.64892717]\n",
      " [ 0.0513354   0.32728224  0.73175695 -1.1483249   0.01959805  0.12899187\n",
      "  -1.30502092  0.43560371  0.34937929]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.66900762  1.78625086 -3.16643652 -0.05480484  1.8930224  -2.91606368\n",
      "  -0.90835528]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:87 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.05844419]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 87 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91085059 -1.19091591  0.27821826  1.08057376 -0.10676634 -0.3267505\n",
      "   1.08933196  0.76438506 -0.53060141]\n",
      " [-0.77436185  1.3771832  -0.44883686 -1.22945875  0.04489578  0.28172357\n",
      "  -1.28254297 -0.95970982  0.58070462]\n",
      " [ 0.58051517 -0.40203115  0.90549987 -0.92563372  0.41915358 -0.46065314\n",
      "  -1.01601448  0.76019133  0.70960347]\n",
      " [ 0.99387804 -1.35006074  0.33543305  1.11364977 -0.1317071  -0.2404215\n",
      "   1.16903274  0.45276042 -0.81287439]\n",
      " [-0.9743276   1.33765486 -0.31587545 -0.98084625  0.18693068  0.23576568\n",
      "  -1.19441854 -0.84070922  0.6488881 ]\n",
      " [ 0.05113043  0.32728224  0.73175695 -1.1483249   0.01939308  0.1287869\n",
      "  -1.30502092  0.43560371  0.34917432]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.67061567  1.78546828 -3.167294   -0.05605478  1.8922951  -2.91690684\n",
      "  -0.90937474]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:87 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.00380048]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 87 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91085214 -1.19091435  0.27821826  1.08057376 -0.10676479 -0.32674894\n",
      "   1.08933196  0.76438506 -0.53059985]\n",
      " [-0.77436346  1.37718159 -0.44883686 -1.22945875  0.04489417  0.28172196\n",
      "  -1.28254297 -0.95970982  0.58070301]\n",
      " [ 0.58051389 -0.40203243  0.90549987 -0.92563372  0.4191523  -0.46065442\n",
      "  -1.01601448  0.76019133  0.70960219]\n",
      " [ 0.99387965 -1.35005913  0.33543305  1.11364977 -0.13170549 -0.24041989\n",
      "   1.16903274  0.45276042 -0.81287278]\n",
      " [-0.97432921  1.33765325 -0.31587545 -0.98084625  0.18692908  0.23576407\n",
      "  -1.19441854 -0.84070922  0.6488865 ]\n",
      " [ 0.05112912  0.32728093  0.73175695 -1.1483249   0.01939177  0.12878559\n",
      "  -1.30502092  0.43560371  0.34917301]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.67062286  1.78546667 -3.1672999  -0.05605981  1.89229383 -2.91691265\n",
      "  -0.90937982]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:87 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.22530808]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 87 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90906871 -1.19269779  0.27821826  1.08057376 -0.10854823 -0.32853238\n",
      "   1.08754852  0.76438506 -0.53059985]\n",
      " [-0.77268065  1.3788644  -0.44883686 -1.22945875  0.04657697  0.28340477\n",
      "  -1.28086017 -0.95970982  0.58070301]\n",
      " [ 0.58409711 -0.39844921  0.90549987 -0.92563372  0.42273552 -0.4570712\n",
      "  -1.01243126  0.76019133  0.70960219]\n",
      " [ 0.99181502 -1.35212376  0.33543305  1.11364977 -0.13377012 -0.24248452\n",
      "   1.16696811  0.45276042 -0.81287278]\n",
      " [-0.97240304  1.33957942 -0.31587545 -0.98084625  0.18885525  0.23769024\n",
      "  -1.19249237 -0.84070922  0.6488865 ]\n",
      " [ 0.05442993  0.33058173  0.73175695 -1.1483249   0.02269257  0.13208639\n",
      "  -1.30172011  0.43560371  0.34917301]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.69028599  1.7738094  -3.17541347 -0.06182853  1.88033015 -2.92476405\n",
      "  -0.91556697]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:87 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.92360569]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 87 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.909418   -1.19269779  0.27821826  1.08092305 -0.10854823 -0.32853238\n",
      "   1.08789781  0.76438506 -0.53059985]\n",
      " [-0.77298902  1.3788644  -0.44883686 -1.22976712  0.04657697  0.28340477\n",
      "  -1.28116854 -0.95970982  0.58070301]\n",
      " [ 0.58350191 -0.39844921  0.90549987 -0.92622892  0.42273552 -0.4570712\n",
      "  -1.01302645  0.76019133  0.70960219]\n",
      " [ 0.99212542 -1.35212376  0.33543305  1.11396017 -0.13377012 -0.24248452\n",
      "   1.16727852  0.45276042 -0.81287278]\n",
      " [-0.97273841  1.33957942 -0.31587545 -0.98118162  0.18885525  0.23769024\n",
      "  -1.19282774 -0.84070922  0.6488865 ]\n",
      " [ 0.05393579  0.33058173  0.73175695 -1.14881904  0.02269257  0.13208639\n",
      "  -1.30221425  0.43560371  0.34917301]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.68759086  1.77638579 -3.17531601 -0.06127544  1.88292682 -2.92465285\n",
      "  -0.9153419 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:87 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.77230644]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 87 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91389943 -1.19269779  0.27821826  1.08092305 -0.10406679 -0.32405095\n",
      "   1.09237924  0.76438506 -0.53059985]\n",
      " [-0.77742221  1.3788644  -0.44883686 -1.22976712  0.04214379  0.27897158\n",
      "  -1.28560172 -0.95970982  0.58070301]\n",
      " [ 0.58130085 -0.39844921  0.90549987 -0.92622892  0.42053445 -0.45927227\n",
      "  -1.01522752  0.76019133  0.70960219]\n",
      " [ 0.99652358 -1.35212376  0.33543305  1.11396017 -0.12937196 -0.23808636\n",
      "   1.17167668  0.45276042 -0.81287278]\n",
      " [-0.97716359  1.33957942 -0.31587545 -0.98118162  0.18443006  0.23326506\n",
      "  -1.19725292 -0.84070922  0.6488865 ]\n",
      " [ 0.0498206   0.33058173  0.73175695 -1.14881904  0.01857739  0.12797121\n",
      "  -1.30632943  0.43560371  0.34917301]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.667571    1.79292994 -3.17228633 -0.05354636  1.90006553 -2.92166113\n",
      "  -0.91031772]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:87 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.56249583]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 87 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89941177 -1.19269779  0.27821826  1.06643539 -0.10406679 -0.3385386\n",
      "   1.09237924  0.76438506 -0.54508751]\n",
      " [-0.76289647  1.3788644  -0.44883686 -1.21524138  0.04214379  0.29349732\n",
      "  -1.28560172 -0.95970982  0.59522875]\n",
      " [ 0.58293406 -0.39844921  0.90549987 -0.9245957   0.42053445 -0.45763906\n",
      "  -1.01522752  0.76019133  0.7112354 ]\n",
      " [ 0.98250624 -1.35212376  0.33543305  1.09994283 -0.12937196 -0.2521037\n",
      "   1.17167668  0.45276042 -0.82689012]\n",
      " [-0.96304116  1.33957942 -0.31587545 -0.96705919  0.18443006  0.24738749\n",
      "  -1.19725292 -0.84070922  0.66300893]\n",
      " [ 0.05960408  0.33058173  0.73175695 -1.13903556  0.01857739  0.13775469\n",
      "  -1.30632943  0.43560371  0.35895649]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.7367845   1.7404861  -3.18896286 -0.08651746  1.84866761 -2.93925707\n",
      "  -0.93449814]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:87 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.7421935]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 87 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90493295 -1.18717661  0.27821826  1.07195658 -0.10406679 -0.33301742\n",
      "   1.09790043  0.76438506 -0.54508751]\n",
      " [-0.76841375  1.37334712 -0.44883686 -1.22075866  0.04214379  0.28798004\n",
      "  -1.291119   -0.95970982  0.59522875]\n",
      " [ 0.57808328 -0.40329999  0.90549987 -0.92944649  0.42053445 -0.46248984\n",
      "  -1.0200783   0.76019133  0.7112354 ]\n",
      " [ 0.98800655 -1.34662345  0.33543305  1.10544314 -0.12937196 -0.24660339\n",
      "   1.17717699  0.45276042 -0.82689012]\n",
      " [-0.96856285  1.33405773 -0.31587545 -0.97258088  0.18443006  0.2418658\n",
      "  -1.20277462 -0.84070922  0.66300893]\n",
      " [ 0.05432023  0.32529788  0.73175695 -1.14431942  0.01857739  0.13247083\n",
      "  -1.31161329  0.43560371  0.35895649]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.71211985  1.76074972 -3.18478984 -0.08408574  1.86935822 -2.93490428\n",
      "  -0.93133739]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:87 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.81870455]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 87 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90783381 -1.18427575  0.27821826  1.07485744 -0.10406679 -0.33301742\n",
      "   1.10080129  0.76438506 -0.54508751]\n",
      " [-0.77130274  1.37045813 -0.44883686 -1.22364765  0.04214379  0.28798004\n",
      "  -1.29400799 -0.95970982  0.59522875]\n",
      " [ 0.57512373 -0.40625954  0.90549987 -0.93240603  0.42053445 -0.46248984\n",
      "  -1.02303784  0.76019133  0.7112354 ]\n",
      " [ 0.99088466 -1.34374534  0.33543305  1.10832125 -0.12937196 -0.24660339\n",
      "   1.1800551   0.45276042 -0.82689012]\n",
      " [-0.97150618  1.3311144  -0.31587545 -0.97552421  0.18443006  0.2418658\n",
      "  -1.20571795 -0.84070922  0.66300893]\n",
      " [ 0.0515544   0.32253205  0.73175695 -1.14708524  0.01857739  0.13247083\n",
      "  -1.31437911  0.43560371  0.35895649]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.69866525  1.77243482 -3.18304983 -0.08213545  1.8810985  -2.93301182\n",
      "  -0.92983827]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:87 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.82537091]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 87 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91050728 -1.18427575  0.28089172  1.07485744 -0.10406679 -0.33034396\n",
      "   1.10347475  0.76438506 -0.54508751]\n",
      " [-0.77376631  1.37045813 -0.45130043 -1.22364765  0.04214379  0.28551647\n",
      "  -1.29647156 -0.95970982  0.59522875]\n",
      " [ 0.5751083  -0.40625954  0.90548444 -0.93240603  0.42053445 -0.46250527\n",
      "  -1.02305327  0.76019133  0.7112354 ]\n",
      " [ 0.99331843 -1.34374534  0.33786681  1.10832125 -0.12937196 -0.24416962\n",
      "   1.18248886  0.45276042 -0.82689012]\n",
      " [-0.97394758  1.3311144  -0.31831685 -0.97552421  0.18443006  0.2394244\n",
      "  -1.20815934 -0.84070922  0.66300893]\n",
      " [ 0.05034884  0.32253205  0.73055139 -1.14708524  0.01857739  0.13126527\n",
      "  -1.31558467  0.43560371  0.35895649]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.68608027  1.78345755 -3.18182374 -0.07585839  1.89249407 -2.93181319\n",
      "  -0.92478352]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:87 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.09654658]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 88 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91024216 -1.18454087  0.28089172  1.07459232 -0.10406679 -0.33034396\n",
      "   1.10347475  0.76438506 -0.54535263]\n",
      " [-0.77373292  1.37049152 -0.45130043 -1.22361426  0.04214379  0.28551647\n",
      "  -1.29647156 -0.95970982  0.59526214]\n",
      " [ 0.57516334 -0.4062045   0.90548444 -0.93235099  0.42053445 -0.46250527\n",
      "  -1.02305327  0.76019133  0.71129044]\n",
      " [ 0.99339097 -1.3436728   0.33786681  1.10839379 -0.12937196 -0.24416962\n",
      "   1.18248886  0.45276042 -0.82681758]\n",
      " [-0.97399456  1.33106742 -0.31831685 -0.97557119  0.18443006  0.2394244\n",
      "  -1.20815934 -0.84070922  0.66296195]\n",
      " [ 0.05076764  0.32295085  0.73055139 -1.14666644  0.01857739  0.13126527\n",
      "  -1.31558467  0.43560371  0.35937529]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.69029093  1.78108421 -3.18389567 -0.07790865  1.89046134 -2.93396551\n",
      "  -0.92645791]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:88 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.86143569]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 88 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91182068 -1.18454087  0.28247025  1.07459232 -0.10406679 -0.33034396\n",
      "   1.10505327  0.76438506 -0.54535263]\n",
      " [-0.77516828  1.37049152 -0.45273579 -1.22361426  0.04214379  0.28551647\n",
      "  -1.29790692 -0.95970982  0.59526214]\n",
      " [ 0.57606155 -0.4062045   0.90638265 -0.93235099  0.42053445 -0.46250527\n",
      "  -1.02215506  0.76019133  0.71129044]\n",
      " [ 0.99483139 -1.3436728   0.33930723  1.10839379 -0.12937196 -0.24416962\n",
      "   1.18392928  0.45276042 -0.82681758]\n",
      " [-0.97544361  1.33106742 -0.3197659  -0.97557119  0.18443006  0.2394244\n",
      "  -1.2096084  -0.84070922  0.66296195]\n",
      " [ 0.04973829  0.32295085  0.72952204 -1.14666644  0.01857739  0.13126527\n",
      "  -1.31661403  0.43560371  0.35937529]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.68202111  1.78859675 -3.18328069 -0.07284386  1.89811174 -2.93333845\n",
      "  -0.92340203]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:88 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.55226558]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 88 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89766441 -1.19869714  0.26831398  1.07459232 -0.10406679 -0.33034396\n",
      "   1.090897    0.76438506 -0.54535263]\n",
      " [-0.76079961  1.38486019 -0.43836711 -1.22361426  0.04214379  0.28551647\n",
      "  -1.28353825 -0.95970982  0.59526214]\n",
      " [ 0.57513902 -0.40712703  0.90546012 -0.93235099  0.42053445 -0.46250527\n",
      "  -1.0230776   0.76019133  0.71129044]\n",
      " [ 0.98037086 -1.35813332  0.3248467   1.10839379 -0.12937196 -0.24416962\n",
      "   1.16946875  0.45276042 -0.82681758]\n",
      " [-0.9609861   1.34552493 -0.30530839 -0.97557119  0.18443006  0.2394244\n",
      "  -1.19515089 -0.84070922  0.66296195]\n",
      " [ 0.05335635  0.32656891  0.7331401  -1.14666644  0.01857739  0.13126527\n",
      "  -1.31299597  0.43560371  0.35937529]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.7503      1.73718114 -3.19963477 -0.10790629  1.84595087 -2.9494644\n",
      "  -0.95389563]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:88 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.70181877]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 88 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90432888 -1.19869714  0.27497844  1.07459232 -0.10406679 -0.33034396\n",
      "   1.090897    0.77104952 -0.54535263]\n",
      " [-0.76704977  1.38486019 -0.44461728 -1.22361426  0.04214379  0.28551647\n",
      "  -1.28353825 -0.96595999  0.59526214]\n",
      " [ 0.58121462 -0.40712703  0.91153572 -0.93235099  0.42053445 -0.46250527\n",
      "  -1.0230776   0.76626693  0.71129044]\n",
      " [ 0.98725013 -1.35813332  0.33172597  1.10839379 -0.12937196 -0.24416962\n",
      "   1.16946875  0.45963969 -0.82681758]\n",
      " [-0.96734063  1.34552493 -0.31166292 -0.97557119  0.18443006  0.2394244\n",
      "  -1.19515089 -0.84706375  0.66296195]\n",
      " [ 0.06005878  0.32656891  0.73984253 -1.14666644  0.01857739  0.13126527\n",
      "  -1.31299597  0.44230615  0.35937529]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.71909993  1.76442784 -3.19640542 -0.07970601  1.87256316 -2.94608176\n",
      "  -0.92979567]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:88 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.63221906]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 88 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91385864 -1.19869714  0.27497844  1.08412208 -0.10406679 -0.33034396\n",
      "   1.090897    0.77104952 -0.53582286]\n",
      " [-0.77654392  1.38486019 -0.44461728 -1.23310841  0.04214379  0.28551647\n",
      "  -1.28353825 -0.96595999  0.58576798]\n",
      " [ 0.58494229 -0.40712703  0.91153572 -0.92862333  0.42053445 -0.46250527\n",
      "  -1.0230776   0.76626693  0.7150181 ]\n",
      " [ 0.99654335 -1.35813332  0.33172597  1.11768702 -0.12937196 -0.24416962\n",
      "   1.16946875  0.45963969 -0.81752436]\n",
      " [-0.9766568   1.34552493 -0.31166292 -0.98488736  0.18443006  0.2394244\n",
      "  -1.19515089 -0.84706375  0.65364579]\n",
      " [ 0.05322863  0.32656891  0.73984253 -1.1534966   0.01857739  0.13126527\n",
      "  -1.31299597  0.44230615  0.35254513]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.67634206  1.79895297 -3.18791598 -0.0545183   1.90593746 -2.93677941\n",
      "  -0.91586505]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:88 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.05671579]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 88 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91388    -1.19869714  0.27497844  1.08412208 -0.10404543 -0.3303226\n",
      "   1.090897    0.77104952 -0.5358015 ]\n",
      " [-0.7765956   1.38486019 -0.44461728 -1.23310841  0.04209211  0.2854648\n",
      "  -1.28353825 -0.96595999  0.58571631]\n",
      " [ 0.58461338 -0.40712703  0.91153572 -0.92862333  0.42020554 -0.46283418\n",
      "  -1.0230776   0.76626693  0.71468919]\n",
      " [ 0.99661643 -1.35813332  0.33172597  1.11768702 -0.12929888 -0.24409654\n",
      "   1.16946875  0.45963969 -0.81745127]\n",
      " [-0.97669495  1.34552493 -0.31166292 -0.98488736  0.18439191  0.23938625\n",
      "  -1.19515089 -0.84706375  0.65360763]\n",
      " [ 0.05303335  0.32656891  0.73984253 -1.1534966   0.01838212  0.13107\n",
      "  -1.31299597  0.44230615  0.35234986]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.67785919  1.79821578 -3.18872638 -0.05569965  1.90525245 -2.93757618\n",
      "  -0.91682909]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:88 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.003582]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 88 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91388139 -1.19869575  0.27497844  1.08412208 -0.10404404 -0.33032121\n",
      "   1.090897    0.77104952 -0.53580011]\n",
      " [-0.77659703  1.38485876 -0.44461728 -1.23310841  0.04209068  0.28546336\n",
      "  -1.28353825 -0.96595999  0.58571488]\n",
      " [ 0.58461224 -0.40712817  0.91153572 -0.92862333  0.4202044  -0.46283532\n",
      "  -1.0230776   0.76626693  0.71468805]\n",
      " [ 0.99661786 -1.35813189  0.33172597  1.11768702 -0.12929745 -0.24409511\n",
      "   1.16946875  0.45963969 -0.81744984]\n",
      " [-0.97669638  1.3455235  -0.31166292 -0.98488736  0.18439049  0.23938482\n",
      "  -1.19515089 -0.84706375  0.65360621]\n",
      " [ 0.05303219  0.32656775  0.73984253 -1.1534966   0.01838095  0.13106883\n",
      "  -1.31299597  0.44230615  0.35234869]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.67786558  1.79821436 -3.18873163 -0.05570413  1.90525133 -2.93758136\n",
      "  -0.91683361]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:88 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.22135525]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 88 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91216851 -1.20040862  0.27497844  1.08412208 -0.10575692 -0.33203408\n",
      "   1.08918413  0.77104952 -0.53580011]\n",
      " [-0.77498787  1.38646792 -0.44461728 -1.23310841  0.04369984  0.28707253\n",
      "  -1.28192908 -0.96595999  0.58571488]\n",
      " [ 0.58811144 -0.40362896  0.91153572 -0.92862333  0.4237036  -0.45933612\n",
      "  -1.01957839  0.76626693  0.71468805]\n",
      " [ 0.99464027 -1.36010949  0.33172597  1.11768702 -0.13127504 -0.2460727\n",
      "   1.16749116  0.45963969 -0.81744984]\n",
      " [-0.97485234  1.34736755 -0.31166292 -0.98488736  0.18623453  0.24122887\n",
      "  -1.19330685 -0.84706375  0.65360621]\n",
      " [ 0.05625052  0.32978608  0.73984253 -1.1534966   0.02159928  0.13428717\n",
      "  -1.30977764  0.44230615  0.35234869]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.69694165  1.78692373 -3.19662788 -0.06126436  1.89367287 -2.94522514\n",
      "  -0.92281339]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:88 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.9250215]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 88 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91250386 -1.20040862  0.27497844  1.08445743 -0.10575692 -0.33203408\n",
      "   1.08951947  0.77104952 -0.53580011]\n",
      " [-0.77528407  1.38646792 -0.44461728 -1.23340461  0.04369984  0.28707253\n",
      "  -1.28222528 -0.96595999  0.58571488]\n",
      " [ 0.58753671 -0.40362896  0.91153572 -0.92919806  0.4237036  -0.45933612\n",
      "  -1.02015312  0.76626693  0.71468805]\n",
      " [ 0.99493836 -1.36010949  0.33172597  1.11798511 -0.13127504 -0.2460727\n",
      "   1.16778925  0.45963969 -0.81744984]\n",
      " [-0.97517447  1.34736755 -0.31166292 -0.98520949  0.18623453  0.24122887\n",
      "  -1.19362898 -0.84706375  0.65360621]\n",
      " [ 0.05577606  0.32978608  0.73984253 -1.15397106  0.02159928  0.13428717\n",
      "  -1.3102521   0.44230615  0.35234869]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.69434152  1.78941021 -3.19653449 -0.06073334  1.89617869 -2.94511861\n",
      "  -0.92259852]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:88 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.77438954]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 88 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.9169153  -1.20040862  0.27497844  1.08445743 -0.10134548 -0.32762264\n",
      "   1.09393091  0.77104952 -0.53580011]\n",
      " [-0.77964693  1.38646792 -0.44461728 -1.23340461  0.03933698  0.28270966\n",
      "  -1.28658815 -0.96595999  0.58571488]\n",
      " [ 0.5853516  -0.40362896  0.91153572 -0.92919806  0.42151849 -0.46152123\n",
      "  -1.02233823  0.76626693  0.71468805]\n",
      " [ 0.99926659 -1.36010949  0.33172597  1.11798511 -0.12694681 -0.24174447\n",
      "   1.17211748  0.45963969 -0.81744984]\n",
      " [-0.97952949  1.34736755 -0.31166292 -0.98520949  0.18187951  0.23687384\n",
      "  -1.19798401 -0.84706375  0.65360621]\n",
      " [ 0.0517164   0.32978608  0.73984253 -1.15397106  0.01753962  0.13022751\n",
      "  -1.31431175  0.44230615  0.35234869]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.67463328  1.80570814 -3.19355844 -0.05314506  1.91305604 -2.94217927\n",
      "  -0.91767137]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:88 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.56126495]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 88 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90246206 -1.20040862  0.27497844  1.07000418 -0.10134548 -0.34207589\n",
      "   1.09393091  0.77104952 -0.55025336]\n",
      " [-0.76515887  1.38646792 -0.44461728 -1.21891655  0.03933698  0.29719773\n",
      "  -1.28658815 -0.96595999  0.60020294]\n",
      " [ 0.58691499 -0.40362896  0.91153572 -0.92763467  0.42151849 -0.45995784\n",
      "  -1.02233823  0.76626693  0.71625144]\n",
      " [ 0.9852807  -1.36010949  0.33172597  1.10399921 -0.12694681 -0.25573036\n",
      "   1.17211748  0.45963969 -0.83143574]\n",
      " [-0.96544121  1.34736755 -0.31166292 -0.97112121  0.18187951  0.25096213\n",
      "  -1.19798401 -0.84706375  0.6676945 ]\n",
      " [ 0.06145671  0.32978608  0.73984253 -1.14423075  0.01753962  0.13996781\n",
      "  -1.31431175  0.44230615  0.362089  ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.74373807  1.75337487 -3.2102453  -0.08613193  1.86175823 -2.95977294\n",
      "  -0.941848  ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:88 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.7425709]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 88 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90796967 -1.19490101  0.27497844  1.0755118  -0.10134548 -0.33656828\n",
      "   1.09943852  0.77104952 -0.55025336]\n",
      " [-0.77066357  1.38096322 -0.44461728 -1.22442125  0.03933698  0.29169303\n",
      "  -1.29209285 -0.96595999  0.60020294]\n",
      " [ 0.58209926 -0.40844469  0.91153572 -0.9324504   0.42151849 -0.46477357\n",
      "  -1.02715396  0.76626693  0.71625144]\n",
      " [ 0.99076938 -1.3546208   0.33172597  1.10948789 -0.12694681 -0.25024168\n",
      "   1.17760616  0.45963969 -0.83143574]\n",
      " [-0.97094946  1.34185929 -0.31166292 -0.97662946  0.18187951  0.24545387\n",
      "  -1.20349226 -0.84706375  0.6676945 ]\n",
      " [ 0.05619735  0.32452672  0.73984253 -1.14949011  0.01753962  0.13470846\n",
      "  -1.31957111  0.44230615  0.362089  ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.71913308  1.77357837 -3.20606721 -0.08373582  1.88238435 -2.95541574\n",
      "  -0.93872203]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:88 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.82014596]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 88 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91082937 -1.19204131  0.27497844  1.0783715  -0.10134548 -0.33656828\n",
      "   1.10229823  0.77104952 -0.55025336]\n",
      " [-0.77351226  1.37811453 -0.44461728 -1.22726994  0.03933698  0.29169303\n",
      "  -1.29494154 -0.96595999  0.60020294]\n",
      " [ 0.57918636 -0.41135759  0.91153572 -0.9353633   0.42151849 -0.46477357\n",
      "  -1.03006686  0.76626693  0.71625144]\n",
      " [ 0.99360738 -1.3517828   0.33172597  1.11232589 -0.12694681 -0.25024168\n",
      "   1.18044416  0.45963969 -0.83143574]\n",
      " [-0.97385159  1.33895716 -0.31166292 -0.97953159  0.18187951  0.24545387\n",
      "  -1.20639439 -0.84706375  0.6676945 ]\n",
      " [ 0.05348029  0.32180966  0.73984253 -1.15220717  0.01753962  0.13470846\n",
      "  -1.32228817  0.44230615  0.362089  ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.70586825  1.78509926 -3.20435065 -0.08183139  1.8939579  -2.95354894\n",
      "  -0.93725977]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:88 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.825943]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 88 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91348865 -1.19204131  0.27763771  1.0783715  -0.10134548 -0.333909\n",
      "   1.1049575   0.77104952 -0.55025336]\n",
      " [-0.77596558  1.37811453 -0.44707059 -1.22726994  0.03933698  0.28923971\n",
      "  -1.29739486 -0.96595999  0.60020294]\n",
      " [ 0.57917348 -0.41135759  0.91152284 -0.9353633   0.42151849 -0.46478645\n",
      "  -1.03007974  0.76626693  0.71625144]\n",
      " [ 0.99603067 -1.3517828   0.33414926  1.11232589 -0.12694681 -0.24781839\n",
      "   1.18286745  0.45963969 -0.83143574]\n",
      " [-0.97628293  1.33895716 -0.31409426 -0.97953159  0.18187951  0.24302253\n",
      "  -1.20882573 -0.84706375  0.6676945 ]\n",
      " [ 0.05229383  0.32180966  0.73865607 -1.15220717  0.01753962  0.13352199\n",
      "  -1.32347463  0.44230615  0.362089  ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.69335694  1.79605435 -3.20312646 -0.07558861  1.90528223 -2.95235216\n",
      "  -0.93222155]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:88 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.09368009]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 89 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91324434 -1.19228561  0.27763771  1.0781272  -0.10134548 -0.333909\n",
      "   1.1049575   0.77104952 -0.55049766]\n",
      " [-0.77594081  1.3781393  -0.44707059 -1.22724517  0.03933698  0.28923971\n",
      "  -1.29739486 -0.96595999  0.60022771]\n",
      " [ 0.57922444 -0.41130663  0.91152284 -0.93531234  0.42151849 -0.46478645\n",
      "  -1.03007974  0.76626693  0.7163024 ]\n",
      " [ 0.996105   -1.35170848  0.33414926  1.11240022 -0.12694681 -0.24781839\n",
      "   1.18286745  0.45963969 -0.83136141]\n",
      " [-0.97633344  1.33890665 -0.31409426 -0.9795821   0.18187951  0.24302253\n",
      "  -1.20882573 -0.84706375  0.66764398]\n",
      " [ 0.05269005  0.32220588  0.73865607 -1.15181094  0.01753962  0.13352199\n",
      "  -1.32347463  0.44230615  0.36248522]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.69733385  1.79381904 -3.20509014 -0.07752608  1.90336817 -2.95439115\n",
      "  -0.93380226]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:89 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.86261866]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 89 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91479743 -1.19228561  0.2791908   1.0781272  -0.10134548 -0.333909\n",
      "   1.10651059  0.77104952 -0.55049766]\n",
      " [-0.7773544   1.3781393  -0.44848419 -1.22724517  0.03933698  0.28923971\n",
      "  -1.29880845 -0.96595999  0.60022771]\n",
      " [ 0.58011391 -0.41130663  0.91241231 -0.93531234  0.42151849 -0.46478645\n",
      "  -1.02919027  0.76626693  0.7163024 ]\n",
      " [ 0.99752327 -1.35170848  0.33556753  1.11240022 -0.12694681 -0.24781839\n",
      "   1.18428572  0.45963969 -0.83136141]\n",
      " [-0.97776059  1.33890665 -0.31552142 -0.9795821   0.18187951  0.24302253\n",
      "  -1.21025288 -0.84706375  0.66764398]\n",
      " [ 0.0516803   0.32220588  0.73764632 -1.15181094  0.01753962  0.13352199\n",
      "  -1.32448438  0.44230615  0.36248522]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.68919348  1.80121483 -3.20448417 -0.07253462  1.91089848 -2.95377321\n",
      "  -0.93079016]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:89 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.54818276]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 89 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90075909 -1.20632396  0.26515246  1.0781272  -0.10134548 -0.333909\n",
      "   1.09247225  0.77104952 -0.55049766]\n",
      " [-0.76311223  1.39238147 -0.43424202 -1.22724517  0.03933698  0.28923971\n",
      "  -1.28456628 -0.96595999  0.60022771]\n",
      " [ 0.57923149 -0.41218905  0.91152989 -0.93531234  0.42151849 -0.46478645\n",
      "  -1.0300727   0.76626693  0.7163024 ]\n",
      " [ 0.98318698 -1.36604477  0.32123124  1.11240022 -0.12694681 -0.24781839\n",
      "   1.16994944  0.45963969 -0.83136141]\n",
      " [-0.96342928  1.35323797 -0.3011901  -0.9795821   0.18187951  0.24302253\n",
      "  -1.19592157 -0.84706375  0.66764398]\n",
      " [ 0.05525378  0.32577936  0.7412198  -1.15181094  0.01753962  0.13352199\n",
      "  -1.32091091  0.44230615  0.36248522]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.75708     1.75017803 -3.22085348 -0.10736071  1.85914443 -2.96991848\n",
      "  -0.96113287]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:89 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.70363852]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 89 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90734951 -1.20632396  0.27174288  1.0781272  -0.10134548 -0.333909\n",
      "   1.09247225  0.77763994 -0.55049766]\n",
      " [-0.76929327  1.39238147 -0.44042306 -1.22724517  0.03933698  0.28923971\n",
      "  -1.28456628 -0.97214103  0.60022771]\n",
      " [ 0.58521322 -0.41218905  0.91751162 -0.93531234  0.42151849 -0.46478645\n",
      "  -1.0300727   0.77224867  0.7163024 ]\n",
      " [ 0.98999441 -1.36604477  0.32803867  1.11240022 -0.12694681 -0.24781839\n",
      "   1.16994944  0.46644712 -0.83136141]\n",
      " [-0.96971365  1.35323797 -0.30747447 -0.9795821   0.18187951  0.24302253\n",
      "  -1.19592157 -0.85334812  0.66764398]\n",
      " [ 0.06192122  0.32577936  0.74788724 -1.15181094  0.01753962  0.13352199\n",
      "  -1.32091091  0.44897359  0.36248522]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.72617967  1.77718547 -3.21766786 -0.07938803  1.88552458 -2.96658228\n",
      "  -0.93717425]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:89 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.63333107]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 89 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91683949 -1.20632396  0.27174288  1.08761718 -0.10134548 -0.333909\n",
      "   1.09247225  0.77763994 -0.54100768]\n",
      " [-0.77874768  1.39238147 -0.44042306 -1.23669957  0.03933698  0.28923971\n",
      "  -1.28456628 -0.97214103  0.59077331]\n",
      " [ 0.58898314 -0.41218905  0.91751162 -0.93154242  0.42151849 -0.46478645\n",
      "  -1.0300727   0.77224867  0.72007232]\n",
      " [ 0.99925238 -1.36604477  0.32803867  1.12165819 -0.12694681 -0.24781839\n",
      "   1.16994944  0.46644712 -0.82210344]\n",
      " [-0.97899326  1.35323797 -0.30747447 -0.98886171  0.18187951  0.24302253\n",
      "  -1.19592157 -0.85334812  0.65836438]\n",
      " [ 0.05511916  0.32577936  0.74788724 -1.158613    0.01753962  0.13352199\n",
      "  -1.32091091  0.44897359  0.35568316]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.68360522  1.81157183 -3.20922099 -0.05424606  1.9187718  -2.95733216\n",
      "  -0.92330501]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:89 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.05503928]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 89 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91686074 -1.20632396  0.27174288  1.08761718 -0.10132423 -0.33388776\n",
      "   1.09247225  0.77763994 -0.54098644]\n",
      " [-0.77879774  1.39238147 -0.44042306 -1.23669957  0.03928692  0.28918965\n",
      "  -1.28456628 -0.97214103  0.59072325]\n",
      " [ 0.58867227 -0.41218905  0.91751162 -0.93154242  0.42120762 -0.46509732\n",
      "  -1.0300727   0.77224867  0.71976145]\n",
      " [ 0.99932241 -1.36604477  0.32803867  1.12165819 -0.12687678 -0.24774837\n",
      "   1.16994944  0.46644712 -0.82203341]\n",
      " [-0.97903047  1.35323797 -0.30747447 -0.98886171  0.18184229  0.24298532\n",
      "  -1.19592157 -0.85334812  0.65832717]\n",
      " [ 0.05493318  0.32577936  0.74788724 -1.158613    0.01735364  0.13333601\n",
      "  -1.32091091  0.44897359  0.35549718]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.68503651  1.81087744 -3.20998687 -0.0553625   1.91812663 -2.95808509\n",
      "  -0.92421658]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:89 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.0033776]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 89 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91686197 -1.20632272  0.27174288  1.08761718 -0.101323   -0.33388652\n",
      "   1.09247225  0.77763994 -0.5409852 ]\n",
      " [-0.77879901  1.3923802  -0.44042306 -1.23669957  0.03928564  0.28918838\n",
      "  -1.28456628 -0.97214103  0.59072197]\n",
      " [ 0.58867125 -0.41219007  0.91751162 -0.93154242  0.42120661 -0.46509833\n",
      "  -1.0300727   0.77224867  0.71976043]\n",
      " [ 0.99932368 -1.36604349  0.32803867  1.12165819 -0.12687551 -0.24774709\n",
      "   1.16994944  0.46644712 -0.82203214]\n",
      " [-0.97903174  1.3532367  -0.30747447 -0.98886171  0.18184103  0.24298406\n",
      "  -1.19592157 -0.85334812  0.6583259 ]\n",
      " [ 0.05493214  0.32577832  0.74788724 -1.158613    0.0173526   0.13333497\n",
      "  -1.32091091  0.44897359  0.35549614]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.6850422   1.81087619 -3.20999154 -0.05536649  1.91812565 -2.9580897\n",
      "  -0.9242206 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:89 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.21751646]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 89 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91521617 -1.20796852  0.27174288  1.08761718 -0.1029688  -0.33553232\n",
      "   1.09082645  0.77763994 -0.5409852 ]\n",
      " [-0.77725955  1.39391966 -0.44042306 -1.23669957  0.0408251   0.29072784\n",
      "  -1.28302682 -0.97214103  0.59072197]\n",
      " [ 0.59208891 -0.40877241  0.91751162 -0.93154242  0.42462427 -0.46168067\n",
      "  -1.02665504  0.77224867  0.71976043]\n",
      " [ 0.99742857 -1.3679386   0.32803867  1.12165819 -0.12877062 -0.2496422\n",
      "   1.16805433  0.46644712 -0.82203214]\n",
      " [-0.9772655   1.35500294 -0.30747447 -0.98886171  0.18360727  0.24475029\n",
      "  -1.19415533 -0.85334812  0.6583259 ]\n",
      " [ 0.05807076  0.32891694  0.74788724 -1.158613    0.02049123  0.1364736\n",
      "  -1.31777228  0.44897359  0.35549614]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.70355318  1.79993753 -3.2176773  -0.06072667  1.90691649 -2.96553221\n",
      "  -0.93000109]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:89 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.92640299]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 89 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91553819 -1.20796852  0.27174288  1.0879392  -0.1029688  -0.33553232\n",
      "   1.09114847  0.77763994 -0.5409852 ]\n",
      " [-0.77754412  1.39391966 -0.44042306 -1.23698414  0.0408251   0.29072784\n",
      "  -1.28331139 -0.97214103  0.59072197]\n",
      " [ 0.59153387 -0.40877241  0.91751162 -0.93209746  0.42462427 -0.46168067\n",
      "  -1.02721008  0.77224867  0.71976043]\n",
      " [ 0.9977149  -1.3679386   0.32803867  1.12194452 -0.12877062 -0.2496422\n",
      "   1.16834065  0.46644712 -0.82203214]\n",
      " [-0.97757499  1.35500294 -0.30747447 -0.98917119  0.18360727  0.24475029\n",
      "  -1.19446482 -0.85334812  0.6583259 ]\n",
      " [ 0.05761512  0.32891694  0.74788724 -1.15906865  0.02049123  0.1364736\n",
      "  -1.31822793  0.44897359  0.35549614]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.70104424  1.80233766 -3.21758779 -0.06021673  1.90933507 -2.96543013\n",
      "  -0.92979589]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:89 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.77645788]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 89 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91988041 -1.20796852  0.27174288  1.0879392  -0.09862658 -0.33119011\n",
      "   1.09549069  0.77763994 -0.5409852 ]\n",
      " [-0.78183748  1.39391966 -0.44042306 -1.23698414  0.03653175  0.28643448\n",
      "  -1.28760475 -0.97214103  0.59072197]\n",
      " [ 0.58936456 -0.40877241  0.91751162 -0.93209746  0.42245496 -0.46384998\n",
      "  -1.02937939  0.77224867  0.71976043]\n",
      " [ 1.00197401 -1.3679386   0.32803867  1.12194452 -0.12451151 -0.2453831\n",
      "   1.17259976  0.46644712 -0.82203214]\n",
      " [-0.98186066  1.35500294 -0.30747447 -0.98917119  0.17932159  0.24046462\n",
      "  -1.19875049 -0.85334812  0.6583259 ]\n",
      " [ 0.05361067  0.32891694  0.74788724 -1.15906865  0.01648678  0.13246915\n",
      "  -1.32223238  0.44897359  0.35549614]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.68164402  1.81839182 -3.21466463 -0.05276764  1.92595403 -2.96254249\n",
      "  -0.92496413]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:89 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.55997967]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 89 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90546329 -1.20796852  0.27174288  1.07352209 -0.09862658 -0.34560722\n",
      "   1.09549069  0.77763994 -0.55540232]\n",
      " [-0.76738878  1.39391966 -0.44042306 -1.22253545  0.03653175  0.30088318\n",
      "  -1.28760475 -0.97214103  0.60517067]\n",
      " [ 0.59085922 -0.40877241  0.91751162 -0.93060281  0.42245496 -0.46235532\n",
      "  -1.02937939  0.77224867  0.72125509]\n",
      " [ 0.98802132 -1.3679386   0.32803867  1.10799183 -0.12451151 -0.25933578\n",
      "   1.17259976  0.46644712 -0.83598483]\n",
      " [-0.96780826  1.35500294 -0.30747447 -0.97511879  0.17932159  0.25451703\n",
      "  -1.19875049 -0.85334812  0.6723783 ]\n",
      " [ 0.06330687  0.32891694  0.74788724 -1.14937245  0.01648678  0.14216535\n",
      "  -1.32223238  0.44897359  0.36519234]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.7506342   1.7661746  -3.23136127 -0.08576619  1.87476182 -2.9801334\n",
      "  -0.94913497]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:89 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.74295344]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 89 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91095713 -1.20247469  0.27174288  1.07901592 -0.09862658 -0.34011339\n",
      "   1.10098453  0.77763994 -0.55540232]\n",
      " [-0.77288064  1.3884278  -0.44042306 -1.2280273   0.03653175  0.29539132\n",
      "  -1.29309661 -0.97214103  0.60517067]\n",
      " [ 0.58607871 -0.41355291  0.91751162 -0.93538331  0.42245496 -0.46713583\n",
      "  -1.0341599   0.77224867  0.72125509]\n",
      " [ 0.99349808 -1.36246184  0.32803867  1.11346859 -0.12451151 -0.25385902\n",
      "   1.17807652  0.46644712 -0.83598483]\n",
      " [-0.97330284  1.34950835 -0.30747447 -0.98061337  0.17932159  0.24902244\n",
      "  -1.20424507 -0.85334812  0.6723783 ]\n",
      " [ 0.05807232  0.32368239  0.74788724 -1.154607    0.01648678  0.1369308\n",
      "  -1.32746693  0.44897359  0.36519234]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.72608964  1.78631761 -3.22717864 -0.08340516  1.89532321 -2.97577237\n",
      "  -0.94604348]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:89 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.8215844]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 89 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91377593 -1.19965588  0.27174288  1.08183473 -0.09862658 -0.34011339\n",
      "   1.10380333  0.77763994 -0.55540232]\n",
      " [-0.77568925  1.38561919 -0.44042306 -1.23083592  0.03653175  0.29539132\n",
      "  -1.29590522 -0.97214103  0.60517067]\n",
      " [ 0.58321214 -0.41641949  0.91751162 -0.93824989  0.42245496 -0.46713583\n",
      "  -1.03702647  0.77224867  0.72125509]\n",
      " [ 0.99629618 -1.35966374  0.32803867  1.11626669 -0.12451151 -0.25385902\n",
      "   1.18087462  0.46644712 -0.83598483]\n",
      " [-0.976164    1.34664719 -0.30747447 -0.98347454  0.17932159  0.24902244\n",
      "  -1.20710623 -0.85334812  0.6723783 ]\n",
      " [ 0.05540357  0.32101364  0.74788724 -1.15727575  0.01648678  0.1369308\n",
      "  -1.33013568  0.44897359  0.36519234]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.71301325  1.79767553 -3.22548552 -0.08154551  1.9067314  -2.9739312\n",
      "  -0.9446173 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:89 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.82649886]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 89 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.9164215  -1.19965588  0.27438845  1.08183473 -0.09862658 -0.33746783\n",
      "   1.1064489   0.77763994 -0.55540232]\n",
      " [-0.77813266  1.38561919 -0.44286647 -1.23083592  0.03653175  0.29294791\n",
      "  -1.29834863 -0.97214103  0.60517067]\n",
      " [ 0.58320145 -0.41641949  0.91750093 -0.93824989  0.42245496 -0.46714653\n",
      "  -1.03703716  0.77224867  0.72125509]\n",
      " [ 0.99870933 -1.35966374  0.33045182  1.11626669 -0.12451151 -0.25144587\n",
      "   1.18328777  0.46644712 -0.83598483]\n",
      " [-0.97858561  1.34664719 -0.30989609 -0.98347454  0.17932159  0.24660083\n",
      "  -1.20952784 -0.85334812  0.6723783 ]\n",
      " [ 0.05423589  0.32101364  0.74671956 -1.15727575  0.01648678  0.13576312\n",
      "  -1.33130336  0.44897359  0.36519234]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.70057335  1.80856493 -3.22426309 -0.07533625  1.91798661 -2.97273615\n",
      "  -0.93959484]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:89 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.09089888]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 90 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91619649 -1.19988089  0.27438845  1.08160972 -0.09862658 -0.33746783\n",
      "   1.1064489   0.77763994 -0.55562733]\n",
      " [-0.77811559  1.38563626 -0.44286647 -1.23081885  0.03653175  0.29294791\n",
      "  -1.29834863 -0.97214103  0.60518774]\n",
      " [ 0.58324856 -0.41637237  0.91750093 -0.93820277  0.42245496 -0.46714653\n",
      "  -1.03703716  0.77224867  0.72130221]\n",
      " [ 0.99878496 -1.35958812  0.33045182  1.11634232 -0.12451151 -0.25144587\n",
      "   1.18328777  0.46644712 -0.8359092 ]\n",
      " [-0.97863906  1.34659375 -0.30989609 -0.98352798  0.17932159  0.24660083\n",
      "  -1.20952784 -0.85334812  0.67232486]\n",
      " [ 0.05461076  0.3213885   0.74671956 -1.15690088  0.01648678  0.13576312\n",
      "  -1.33130336  0.44897359  0.36556721]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.70432912  1.80645981 -3.22612391 -0.077167    1.91618443 -2.97466751\n",
      "  -0.94108691]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:90 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.86378159]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 90 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91772476 -1.19988089  0.27591671  1.08160972 -0.09862658 -0.33746783\n",
      "   1.10797716  0.77763994 -0.55562733]\n",
      " [-0.77950791  1.38563626 -0.44425879 -1.23081885  0.03653175  0.29294791\n",
      "  -1.29974095 -0.97214103  0.60518774]\n",
      " [ 0.5841294  -0.41637237  0.91838176 -0.93820277  0.42245496 -0.46714653\n",
      "  -1.03615633  0.77224867  0.72130221]\n",
      " [ 1.00018158 -1.35958812  0.33184844  1.11634232 -0.12451151 -0.25144587\n",
      "   1.18468439  0.46644712 -0.8359092 ]\n",
      " [-0.98004479  1.34659375 -0.31130182 -0.98352798  0.17932159  0.24660083\n",
      "  -1.21093358 -0.85334812  0.67232486]\n",
      " [ 0.05362016  0.3213885   0.74572897 -1.15690088  0.01648678  0.13576312\n",
      "  -1.33229395  0.44897359  0.36556721]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.69631519  1.81374152 -3.22552676 -0.07224729  1.92359743 -2.9740585\n",
      "  -0.93811756]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:90 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.54410412]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 90 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90380603 -1.21379962  0.26199798  1.08160972 -0.09862658 -0.33746783\n",
      "   1.09405843  0.77763994 -0.55562733]\n",
      " [-0.7653941   1.39975007 -0.43014497 -1.23081885  0.03653175  0.29294791\n",
      "  -1.28562713 -0.97214103  0.60518774]\n",
      " [ 0.58328667 -0.41721509  0.91753904 -0.93820277  0.42245496 -0.46714653\n",
      "  -1.03699905  0.77224867  0.72130221]\n",
      " [ 0.9859714  -1.3737983   0.31763827  1.11634232 -0.12451151 -0.25144587\n",
      "   1.17047422  0.46644712 -0.8359092 ]\n",
      " [-0.96584164  1.3607969  -0.29709867 -0.98352798  0.17932159  0.24660083\n",
      "  -1.19673043 -0.85334812  0.67232486]\n",
      " [ 0.05714968  0.32491802  0.74925849 -1.15690088  0.01648678  0.13576312\n",
      "  -1.32876443  0.44897359  0.36556721]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.76379902  1.76308913 -3.24190585 -0.10683228  1.87225505 -2.99021763\n",
      "  -0.96830357]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:90 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.70544265]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 90 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91032328 -1.21379962  0.26851523  1.08160972 -0.09862658 -0.33746783\n",
      "   1.09405843  0.78415719 -0.55562733]\n",
      " [-0.77150689  1.39975007 -0.43625777 -1.23081885  0.03653175  0.29294791\n",
      "  -1.28562713 -0.97825382  0.60518774]\n",
      " [ 0.58917584 -0.41721509  0.92342821 -0.93820277  0.42245496 -0.46714653\n",
      "  -1.03699905  0.77813784  0.72130221]\n",
      " [ 0.99270769 -1.3737983   0.32437456  1.11634232 -0.12451151 -0.25144587\n",
      "   1.17047422  0.47318341 -0.8359092 ]\n",
      " [-0.97205673  1.3607969  -0.30331376 -0.98352798  0.17932159  0.24660083\n",
      "  -1.19673043 -0.85956321  0.67232486]\n",
      " [ 0.06378036  0.32491802  0.75588917 -1.15690088  0.01648678  0.13576312\n",
      "  -1.32876443  0.45560427  0.36556721]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.7331955   1.78985895 -3.23876315 -0.07908644  1.89840469 -2.98692703\n",
      "  -0.94448699]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:90 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.63441694]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 90 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91977433 -1.21379962  0.26851523  1.09106077 -0.09862658 -0.33746783\n",
      "   1.09405843  0.78415719 -0.54617628]\n",
      " [-0.78092237  1.39975007 -0.43625777 -1.24023433  0.03653175  0.29294791\n",
      "  -1.28562713 -0.97825382  0.59577226]\n",
      " [ 0.59298791 -0.41721509  0.92342821 -0.93439071  0.42245496 -0.46714653\n",
      "  -1.03699905  0.77813784  0.72511427]\n",
      " [ 1.00193104 -1.3737983   0.32437456  1.12556566 -0.12451151 -0.25144587\n",
      "   1.17047422  0.47318341 -0.82668585]\n",
      " [-0.98130045  1.3607969  -0.30331376 -0.9927717   0.17932159  0.24660083\n",
      "  -1.19673043 -0.85956321  0.66308113]\n",
      " [ 0.0570059   0.32491802  0.75588917 -1.16367534  0.01648678  0.13576312\n",
      "  -1.32876443  0.45560427  0.35879275]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.69080027  1.82410929 -3.23035743 -0.0539881   1.93152722 -2.97772744\n",
      "  -0.93067751]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:90 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.05341271]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 90 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.9197954  -1.21379962  0.26851523  1.09106077 -0.09860551 -0.33744675\n",
      "   1.09405843  0.78415719 -0.5461552 ]\n",
      " [-0.78097084  1.39975007 -0.43625777 -1.24023433  0.03648328  0.29289944\n",
      "  -1.28562713 -0.97825382  0.59572379]\n",
      " [ 0.59269413 -0.41721509  0.92342821 -0.93439071  0.42216118 -0.4674403\n",
      "  -1.03699905  0.77813784  0.72482049]\n",
      " [ 1.00199812 -1.3737983   0.32437456  1.12556566 -0.12444443 -0.25137879\n",
      "   1.17047422  0.47318341 -0.82661878]\n",
      " [-0.9813367   1.3607969  -0.30331376 -0.9927717   0.17928534  0.24656458\n",
      "  -1.19673043 -0.85956321  0.66304488]\n",
      " [ 0.05682882  0.32491802  0.75588917 -1.16367534  0.0163097   0.13558604\n",
      "  -1.32876443  0.45560427  0.35861567]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.69215054  1.82345524 -3.2310812  -0.0550431   1.93091961 -2.97843889\n",
      "  -0.93153941]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:90 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.00318624]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 90 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.9197965  -1.21379852  0.26851523  1.09106077 -0.0986044  -0.33744565\n",
      "   1.09405843  0.78415719 -0.5461541 ]\n",
      " [-0.78097197  1.39974894 -0.43625777 -1.24023433  0.03648215  0.29289831\n",
      "  -1.28562713 -0.97825382  0.59572266]\n",
      " [ 0.59269322 -0.417216    0.92342821 -0.93439071  0.42216028 -0.46744121\n",
      "  -1.03699905  0.77813784  0.72481959]\n",
      " [ 1.00199925 -1.37379716  0.32437456  1.12556566 -0.1244433  -0.25137766\n",
      "   1.17047422  0.47318341 -0.82661765]\n",
      " [-0.98133783  1.36079577 -0.30331376 -0.9927717   0.17928421  0.24656345\n",
      "  -1.19673043 -0.85956321  0.66304375]\n",
      " [ 0.05682789  0.32491709  0.75588917 -1.16367534  0.01630876  0.13558511\n",
      "  -1.32876443  0.45560427  0.35861474]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.6921556   1.82345414 -3.23108537 -0.05504665  1.93091875 -2.97844301\n",
      "  -0.93154299]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:90 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.21378792]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 90 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91821446 -1.21538056  0.26851523  1.09106077 -0.10018644 -0.33902769\n",
      "   1.09247639  0.78415719 -0.5461541 ]\n",
      " [-0.77949849  1.40122242 -0.43625777 -1.24023433  0.03795563  0.29437179\n",
      "  -1.28415365 -0.97825382  0.59572266]\n",
      " [ 0.59603172 -0.4138775   0.92342821 -0.93439071  0.42549877 -0.46410271\n",
      "  -1.03366056  0.77813784  0.72481959]\n",
      " [ 1.00018233 -1.37561409  0.32437456  1.12556566 -0.12626023 -0.25319459\n",
      "   1.16865729  0.47318341 -0.82661765]\n",
      " [-0.97964531  1.36248829 -0.30331376 -0.9927717   0.18097673  0.24825597\n",
      "  -1.1950379  -0.85956321  0.66304375]\n",
      " [ 0.05988946  0.32797866  0.75588917 -1.16367534  0.01937034  0.13864668\n",
      "  -1.32570286  0.45560427  0.35861474]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.71012262  1.81285342 -3.23856727 -0.06021495  1.92006368 -2.98569044\n",
      "  -0.93713198]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:90 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.92775128]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 90 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91852377 -1.21538056  0.26851523  1.09137007 -0.10018644 -0.33902769\n",
      "   1.09278569  0.78415719 -0.5461541 ]\n",
      " [-0.77977194  1.40122242 -0.43625777 -1.24050778  0.03795563  0.29437179\n",
      "  -1.28442711 -0.97825382  0.59572266]\n",
      " [ 0.59549562 -0.4138775   0.92342821 -0.93492681  0.42549877 -0.46410271\n",
      "  -1.03419666  0.77813784  0.72481959]\n",
      " [ 1.00045741 -1.37561409  0.32437456  1.12584075 -0.12626023 -0.25319459\n",
      "   1.16893237  0.47318341 -0.82661765]\n",
      " [-0.97994271  1.36248829 -0.30331376 -0.9930691   0.18097673  0.24825597\n",
      "  -1.1953353  -0.85956321  0.66304375]\n",
      " [ 0.0594518   0.32797866  0.75588917 -1.16411301  0.01937034  0.13864668\n",
      "  -1.32614052  0.45560427  0.35861474]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.70770125  1.81517059 -3.23848146 -0.05972512  1.92239846 -2.9855926\n",
      "  -0.93693597]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:90 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.77851094]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 90 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92279755 -1.21538056  0.26851523  1.09137007 -0.09591266 -0.3347539\n",
      "   1.09705948  0.78415719 -0.5461541 ]\n",
      " [-0.78399663  1.40122242 -0.43625777 -1.24050778  0.03373094  0.2901471\n",
      "  -1.2886518  -0.97825382  0.59572266]\n",
      " [ 0.59334199 -0.4138775   0.92342821 -0.93492681  0.42334514 -0.46625634\n",
      "  -1.03635029  0.77813784  0.72481959]\n",
      " [ 1.00464821 -1.37561409  0.32437456  1.12584075 -0.12206942 -0.24900378\n",
      "   1.17312318  0.47318341 -0.82661765]\n",
      " [-0.98415985  1.36248829 -0.30331376 -0.9930691   0.17675959  0.24403883\n",
      "  -1.19955245 -0.85956321  0.66304375]\n",
      " [ 0.05550222  0.32797866  0.75588917 -1.16411301  0.01542076  0.1346971\n",
      "  -1.3300901   0.45560427  0.35861474]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.68860539  1.83098353 -3.23561043 -0.05241353  1.93876207 -2.98275597\n",
      "  -0.93219793]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:90 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.55864134]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 90 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90841825 -1.21538056  0.26851523  1.07699077 -0.09591266 -0.3491332\n",
      "   1.09705948  0.78415719 -0.5605334 ]\n",
      " [-0.76958898  1.40122242 -0.43625777 -1.22610013  0.03373094  0.30455475\n",
      "  -1.2886518  -0.97825382  0.61013031]\n",
      " [ 0.59476894 -0.4138775   0.92342821 -0.93349985  0.42334514 -0.46482939\n",
      "  -1.03635029  0.77813784  0.72624654]\n",
      " [ 0.99073046 -1.37561409  0.32437456  1.111923   -0.12206942 -0.26292153\n",
      "   1.17312318  0.47318341 -0.8405354 ]\n",
      " [-0.97014504  1.36248829 -0.30331376 -0.97905429  0.17675959  0.25805364\n",
      "  -1.19955245 -0.85956321  0.67705856]\n",
      " [ 0.06515342  0.32797866  0.75588917 -1.15446181  0.01542076  0.1443483\n",
      "  -1.3300901   0.45560427  0.36826594]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.75747502  1.77888779 -3.25231621 -0.08541975  1.88768087 -3.00034357\n",
      "  -0.95636094]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:90 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.74334209]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 90 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91389807 -1.20990074  0.26851523  1.08247059 -0.09591266 -0.34365338\n",
      "   1.1025393   0.78415719 -0.5605334 ]\n",
      " [-0.7750677   1.3957437  -0.43625777 -1.23157885  0.03373094  0.29907604\n",
      "  -1.29413051 -0.97825382  0.61013031]\n",
      " [ 0.59002381 -0.41862264  0.92342821 -0.93824498  0.42334514 -0.46957452\n",
      "  -1.04109542  0.77813784  0.72624654]\n",
      " [ 0.99619498 -1.37014957  0.32437456  1.11738752 -0.12206942 -0.25745702\n",
      "   1.1785877   0.47318341 -0.8405354 ]\n",
      " [-0.97562568  1.35700765 -0.30331376 -0.98453493  0.17675959  0.252573\n",
      "  -1.20503308 -0.85956321  0.67705856]\n",
      " [ 0.05994399  0.32276924  0.75588917 -1.15967123  0.01542076  0.13913888\n",
      "  -1.33529953  0.45560427  0.36826594]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.73299183  1.79896981 -3.24812961 -0.08309328  1.9081772  -2.9959793\n",
      "  -0.95330365]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:90 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.82302039]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 90 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91667623 -1.20712258  0.26851523  1.08524875 -0.09591266 -0.34365338\n",
      "   1.10531746  0.78415719 -0.5605334 ]\n",
      " [-0.77783644  1.39297497 -0.43625777 -1.23434759  0.03373094  0.29907604\n",
      "  -1.29689925 -0.97825382  0.61013031]\n",
      " [ 0.58720327 -0.42144318  0.92342821 -0.94106553  0.42334514 -0.46957452\n",
      "  -1.04391597  0.77813784  0.72624654]\n",
      " [ 0.99895338 -1.36739117  0.32437456  1.12014592 -0.12206942 -0.25745702\n",
      "   1.1813461   0.47318341 -0.8405354 ]\n",
      " [-0.97844611  1.35418722 -0.30331376 -0.98735536  0.17675959  0.252573\n",
      "  -1.20785352 -0.85956321  0.67705856]\n",
      " [ 0.05732311  0.32014835  0.75588917 -1.16229212  0.01542076  0.13913888\n",
      "  -1.33792041  0.45560427  0.36826594]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.7201026   1.81016594 -3.24645994 -0.08127735  1.9194213  -2.99416376\n",
      "  -0.95191275]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:90 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.82703949]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 90 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91930852 -1.20712258  0.27114752  1.08524875 -0.09591266 -0.34102109\n",
      "   1.10794975  0.78415719 -0.5605334 ]\n",
      " [-0.78027025  1.39297497 -0.43869158 -1.23434759  0.03373094  0.29664222\n",
      "  -1.29933307 -0.97825382  0.61013031]\n",
      " [ 0.58719442 -0.42144318  0.92341937 -0.94106553  0.42334514 -0.46958336\n",
      "  -1.04392481  0.77813784  0.72624654]\n",
      " [ 1.0013567  -1.36739117  0.32677788  1.12014592 -0.12206942 -0.25505369\n",
      "   1.18374942  0.47318341 -0.8405354 ]\n",
      " [-0.9808583   1.35418722 -0.30572595 -0.98735536  0.17675959  0.25016081\n",
      "  -1.21026571 -0.85956321  0.67705856]\n",
      " [ 0.05617392  0.32014835  0.75473998 -1.16229212  0.01542076  0.13798969\n",
      "  -1.3390696   0.45560427  0.36826594]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.70773202  1.82099148 -3.24523915 -0.0751009   1.93060938 -2.99297033\n",
      "  -0.94690534]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:90 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.08820134]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 91 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91910139 -1.20732971  0.27114752  1.08504162 -0.09591266 -0.34102109\n",
      "   1.10794975  0.78415719 -0.56074053]\n",
      " [-0.78026005  1.39298517 -0.43869158 -1.23433738  0.03373094  0.29664222\n",
      "  -1.29933307 -0.97825382  0.61014051]\n",
      " [ 0.5872379  -0.4213997   0.92341937 -0.94102205  0.42334514 -0.46958336\n",
      "  -1.04392481  0.77813784  0.72629002]\n",
      " [ 1.00143319 -1.36731468  0.32677788  1.1202224  -0.12206942 -0.25505369\n",
      "   1.18374942  0.47318341 -0.84045891]\n",
      " [-0.98091414  1.35413139 -0.30572595 -0.98741119  0.17675959  0.25016081\n",
      "  -1.21026571 -0.85956321  0.67700273]\n",
      " [ 0.05652859  0.32050302  0.75473998 -1.16193745  0.01542076  0.13798969\n",
      "  -1.3390696   0.45560427  0.3686206 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.71127868  1.81900907 -3.24700227 -0.07683073  1.92891263 -2.99479953\n",
      "  -0.94831359]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:91 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.86492556]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 91 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92060542 -1.20732971  0.27265155  1.08504162 -0.09591266 -0.34102109\n",
      "   1.10945378  0.78415719 -0.56074053]\n",
      " [-0.78163154  1.39298517 -0.44006308 -1.23433738  0.03373094  0.29664222\n",
      "  -1.30070456 -0.97825382  0.61014051]\n",
      " [ 0.5881102  -0.4213997   0.92429166 -0.94102205  0.42334514 -0.46958336\n",
      "  -1.04305251  0.77813784  0.72629002]\n",
      " [ 1.00280862 -1.36731468  0.32815331  1.1202224  -0.12206942 -0.25505369\n",
      "   1.18512485  0.47318341 -0.84045891]\n",
      " [-0.98229891  1.35413139 -0.30711072 -0.98741119  0.17675959  0.25016081\n",
      "  -1.21165048 -0.85956321  0.67700273]\n",
      " [ 0.05555673  0.32050302  0.75376812 -1.16193745  0.01542076  0.13798969\n",
      "  -1.34004146  0.45560427  0.3686206 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.70338835  1.82617923 -3.24641376 -0.07198126  1.936211   -2.99419927\n",
      "  -0.94538605]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:91 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.54003416]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 91 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90680776 -1.22112737  0.25885389  1.08504162 -0.09591266 -0.34102109\n",
      "   1.09565612  0.78415719 -0.56074053]\n",
      " [-0.76764768  1.40696904 -0.42607922 -1.23433738  0.03373094  0.29664222\n",
      "  -1.2867207  -0.97825382  0.61014051]\n",
      " [ 0.58730665 -0.42220325  0.92348811 -0.94102205  0.42334514 -0.46958336\n",
      "  -1.04385607  0.77813784  0.72629002]\n",
      " [ 0.98872618 -1.38139712  0.31407087  1.1202224  -0.12206942 -0.25505369\n",
      "   1.17104241  0.47318341 -0.84045891]\n",
      " [-0.96822562  1.36820467 -0.29303744 -0.98741119  0.17675959  0.25016081\n",
      "  -1.19757719 -0.85956321  0.67700273]\n",
      " [ 0.05904291  0.3239892   0.7572543  -1.16193745  0.01542076  0.13798969\n",
      "  -1.33655528  0.45560427  0.3686206 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.77045985  1.77591628 -3.26279727 -0.10632087  1.8852845  -3.01036689\n",
      "  -0.97540989]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:91 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.70723207]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 91 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91325268 -1.22112737  0.26529881  1.08504162 -0.09591266 -0.34102109\n",
      "   1.09565612  0.79060211 -0.56074053]\n",
      " [-0.77369305  1.40696904 -0.43212459 -1.23433738  0.03373094  0.29664222\n",
      "  -1.2867207  -0.98429919  0.61014051]\n",
      " [ 0.59310455 -0.42220325  0.92928601 -0.94102205  0.42334514 -0.46958336\n",
      "  -1.04385607  0.78393575  0.72629002]\n",
      " [ 0.995392   -1.38139712  0.32073669  1.1202224  -0.12206942 -0.25505369\n",
      "   1.17104241  0.47984924 -0.84045891]\n",
      " [-0.97437226  1.36820467 -0.29918407 -0.98741119  0.17675959  0.25016081\n",
      "  -1.19757719 -0.86570984  0.67700273]\n",
      " [ 0.06563511  0.3239892   0.7638465  -1.16193745  0.01542076  0.13798969\n",
      "  -1.33655528  0.46219647  0.3686206 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.74015034  1.80245004 -3.25969675 -0.07880116  1.91120519 -3.00712107\n",
      "  -0.95173609]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:91 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.63547781]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 91 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92266561 -1.22112737  0.26529881  1.09445455 -0.09591266 -0.34102109\n",
      "   1.09565612  0.79060211 -0.55132761]\n",
      " [-0.78307041  1.40696904 -0.43212459 -1.24371474  0.03373094  0.29664222\n",
      "  -1.2867207  -0.98429919  0.60076316]\n",
      " [ 0.59695868 -0.42220325  0.92928601 -0.93716792  0.42334514 -0.46958336\n",
      "  -1.04385607  0.78393575  0.73014415]\n",
      " [ 1.00458134 -1.38139712  0.32073669  1.12941174 -0.12206942 -0.25505369\n",
      "   1.17104241  0.47984924 -0.83126957]\n",
      " [-0.98358075  1.36820467 -0.29918407 -0.99661969  0.17675959  0.25016081\n",
      "  -1.19757719 -0.86570984  0.66779423]\n",
      " [ 0.05888776  0.3239892   0.7638465  -1.1686848   0.01542076  0.13798969\n",
      "  -1.33655528  0.46219647  0.36187325]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.69793033  1.836567   -3.25133081 -0.0537444   1.94420537 -2.99797041\n",
      "  -0.93798488]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:91 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.05183428]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 91 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92268647 -1.22112737  0.26529881  1.09445455 -0.09589179 -0.34100023\n",
      "   1.09565612  0.79060211 -0.55130674]\n",
      " [-0.7831173   1.40696904 -0.43212459 -1.24371474  0.03368405  0.29659532\n",
      "  -1.2867207  -0.98429919  0.60071626]\n",
      " [ 0.59668109 -0.42220325  0.92928601 -0.93716792  0.42306756 -0.46986095\n",
      "  -1.04385607  0.78393575  0.72986656]\n",
      " [ 1.00464557 -1.38139712  0.32073669  1.12941174 -0.12200519 -0.25498946\n",
      "   1.17104241  0.47984924 -0.83120534]\n",
      " [-0.98361603  1.36820467 -0.29918407 -0.99661969  0.17672431  0.25012553\n",
      "  -1.19757719 -0.86570984  0.66775895]\n",
      " [ 0.05871919  0.3239892   0.7638465  -1.1686848   0.01525219  0.13782113\n",
      "  -1.33655528  0.46219647  0.36170469]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.69920409  1.835951   -3.25201476 -0.05474124  1.94363316 -2.99864264\n",
      "  -0.93879974]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:91 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.00300699]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 91 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Theta One: \n",
      "[[ 0.92268746 -1.22112639  0.26529881  1.09445455 -0.09589081 -0.34099924\n",
      "   1.09565612  0.79060211 -0.55130576]\n",
      " [-0.78311831  1.40696803 -0.43212459 -1.24371474  0.03368304  0.29659432\n",
      "  -1.2867207  -0.98429919  0.60071526]\n",
      " [ 0.59668028 -0.42220406  0.92928601 -0.93716792  0.42306675 -0.46986175\n",
      "  -1.04385607  0.78393575  0.72986576]\n",
      " [ 1.00464658 -1.38139612  0.32073669  1.12941174 -0.12200418 -0.25498845\n",
      "   1.17104241  0.47984924 -0.83120433]\n",
      " [-0.98361704  1.36820367 -0.29918407 -0.99661969  0.1767233   0.25012452\n",
      "  -1.19757719 -0.86570984  0.66775794]\n",
      " [ 0.05871836  0.32398837  0.7638465  -1.1686848   0.01525136  0.1378203\n",
      "  -1.33655528  0.46219647  0.36170386]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.6992086   1.83595002 -3.25201848 -0.05474441  1.9436324  -2.99864631\n",
      "  -0.93880294]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:91 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.21016594]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 91 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92116605 -1.2226478   0.26529881  1.09445455 -0.09741222 -0.34252065\n",
      "   1.09413471  0.79060211 -0.55130576]\n",
      " [-0.7817073   1.40837903 -0.43212459 -1.24371474  0.03509404  0.29800532\n",
      "  -1.2853097  -0.98429919  0.60071526]\n",
      " [ 0.59994191 -0.41894243  0.92928601 -0.93716792  0.42632838 -0.46660013\n",
      "  -1.04059444  0.78393575  0.72986576]\n",
      " [ 1.00290378 -1.38313892  0.32073669  1.12941174 -0.12374698 -0.25673125\n",
      "   1.16929961  0.47984924 -0.83120433]\n",
      " [-0.98199438  1.36982633 -0.29918407 -0.99661969  0.17834597  0.25174718\n",
      "  -1.19595453 -0.86570984  0.66775794]\n",
      " [ 0.06170542  0.32697543  0.7638465  -1.1686848   0.01823842  0.14080736\n",
      "  -1.33356822  0.46219647  0.36170386]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.71665197  1.82567379 -3.259303   -0.05972864  1.93311689 -3.00570471\n",
      "  -0.94420793]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:91 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.92906739]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 91 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.9214632  -1.2226478   0.26529881  1.0947517  -0.09741222 -0.34252065\n",
      "   1.09443186  0.79060211 -0.55130576]\n",
      " [-0.78197013  1.40837903 -0.43212459 -1.24397757  0.03509404  0.29800532\n",
      "  -1.28557252 -0.98429919  0.60071526]\n",
      " [ 0.59942404 -0.41894243  0.92928601 -0.9376858   0.42632838 -0.46660013\n",
      "  -1.04111231  0.78393575  0.72986576]\n",
      " [ 1.00316811 -1.38313892  0.32073669  1.12967607 -0.12374698 -0.25673125\n",
      "   1.16956394  0.47984924 -0.83120433]\n",
      " [-0.98228021  1.36982633 -0.29918407 -0.99690553  0.17834597  0.25174718\n",
      "  -1.19624037 -0.86570984  0.66775794]\n",
      " [ 0.06128496  0.32697543  0.7638465  -1.16910527  0.01823842  0.14080736\n",
      "  -1.33398868  0.46219647  0.36170386]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.7143147   1.82791124 -3.25922072 -0.05925801  1.93537115 -3.0056109\n",
      "  -0.94402064]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:91 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.78054822]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 91 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92566935 -1.2226478   0.26529881  1.0947517  -0.09320607 -0.3383145\n",
      "   1.09863801  0.79060211 -0.55130576]\n",
      " [-0.786127    1.40837903 -0.43212459 -1.24397757  0.03093718  0.29384845\n",
      "  -1.28972939 -0.98429919  0.60071526]\n",
      " [ 0.59728598 -0.41894243  0.92928601 -0.9376858   0.42419032 -0.46873818\n",
      "  -1.04325037  0.78393575  0.72986576]\n",
      " [ 1.00729146 -1.38313892  0.32073669  1.12967607 -0.11962364 -0.25260791\n",
      "   1.17368728  0.47984924 -0.83120433]\n",
      " [-0.98642968  1.36982633 -0.29918407 -0.99690553  0.1741965   0.24759772\n",
      "  -1.20038983 -0.86570984  0.66775794]\n",
      " [ 0.05738989  0.32697543  0.7638465  -1.16910527  0.01434335  0.13691229\n",
      "  -1.33788376  0.46219647  0.36170386]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.69551946  1.84348558 -3.25640105 -0.05208219  1.95148249 -3.00282459\n",
      "  -0.93937465]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:91 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.55725137]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 91 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91132954 -1.2226478   0.26529881  1.08041189 -0.09320607 -0.35265431\n",
      "   1.09863801  0.79060211 -0.56564556]\n",
      " [-0.77176205  1.40837903 -0.43212459 -1.22961261  0.03093718  0.30821341\n",
      "  -1.28972939 -0.98429919  0.61508021]\n",
      " [ 0.5986462  -0.41894243  0.92928601 -0.93632558  0.42419032 -0.46737797\n",
      "  -1.04325037  0.78393575  0.73122597]\n",
      " [ 0.99341033 -1.38313892  0.32073669  1.11579495 -0.11962364 -0.26648903\n",
      "   1.17368728  0.47984924 -0.84508545]\n",
      " [-0.97245413  1.36982633 -0.29918407 -0.98292998  0.1741965   0.26157327\n",
      "  -1.20038983 -0.86570984  0.68173349]\n",
      " [ 0.06699521  0.32697543  0.7638465  -1.15949994  0.01434335  0.14651761\n",
      "  -1.33788376  0.46219647  0.37130918]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.76426262  1.79151666 -3.27311527 -0.08509213  1.90051766 -3.02040823\n",
      "  -0.96352774]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:91 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.74373776]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 91 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91679508 -1.21718226  0.26529881  1.08587743 -0.09320607 -0.34718876\n",
      "   1.10410356  0.79060211 -0.56564556]\n",
      " [-0.7772273   1.40291378 -0.43212459 -1.23507787  0.03093718  0.30274815\n",
      "  -1.29519465 -0.98429919  0.61508021]\n",
      " [ 0.5939366  -0.42365203  0.92928601 -0.94103517  0.42419032 -0.47208756\n",
      "  -1.04795997  0.78393575  0.73122597]\n",
      " [ 0.99886227 -1.37768698  0.32073669  1.12124689 -0.11962364 -0.2610371\n",
      "   1.17913922  0.47984924 -0.84508545]\n",
      " [-0.97792054  1.36435992 -0.29918407 -0.98839639  0.1741965   0.25610686\n",
      "  -1.20585623 -0.86570984  0.68173349]\n",
      " [ 0.06181126  0.32179148  0.7638465  -1.16468389  0.01434335  0.14133366\n",
      "  -1.34306771  0.46219647  0.37130918]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.73984187  1.8115371  -3.26892529 -0.08279971  1.92094847 -3.01604136\n",
      "  -0.96050437]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:91 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.82445434]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 91 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91953284 -1.21444451  0.26529881  1.08861518 -0.09320607 -0.34718876\n",
      "   1.10684131  0.79060211 -0.56564556]\n",
      " [-0.77995636  1.40018471 -0.43212459 -1.23780693  0.03093718  0.30274815\n",
      "  -1.29792371 -0.98429919  0.61508021]\n",
      " [ 0.59116177 -0.42642686  0.92928601 -0.94381001  0.42419032 -0.47208756\n",
      "  -1.0507348   0.78393575  0.73122597]\n",
      " [ 1.00158116 -1.37496809  0.32073669  1.12396578 -0.11962364 -0.2610371\n",
      "   1.18185811  0.47984924 -0.84508545]\n",
      " [-0.98070045  1.36158001 -0.29918407 -0.99117631  0.1741965   0.25610686\n",
      "  -1.20863615 -0.86570984  0.68173349]\n",
      " [ 0.05923778  0.319218    0.7638465  -1.16725737  0.01434335  0.14133366\n",
      "  -1.34564119  0.46219647  0.37130918]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.72713856  1.82257258 -3.26727907 -0.08102647  1.93202972 -3.01425143\n",
      "  -0.95914801]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:91 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.82756581]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 91 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92215226 -1.21444451  0.26791823  1.08861518 -0.09320607 -0.34456934\n",
      "   1.10946073  0.79060211 -0.56564556]\n",
      " [-0.78238088  1.40018471 -0.43454911 -1.23780693  0.03093718  0.30032364\n",
      "  -1.30034823 -0.98429919  0.61508021]\n",
      " [ 0.59115447 -0.42642686  0.92927871 -0.94381001  0.42419032 -0.47209486\n",
      "  -1.0507421   0.78393575  0.73122597]\n",
      " [ 1.00397494 -1.37496809  0.32313047  1.12396578 -0.11962364 -0.25864332\n",
      "   1.18425189  0.47984924 -0.84508545]\n",
      " [-0.9831035   1.36158001 -0.30158712 -0.99117631  0.1741965   0.25370381\n",
      "  -1.2110392  -0.86570984  0.68173349]\n",
      " [ 0.0581068   0.319218    0.76271552 -1.16725737  0.01434335  0.14020268\n",
      "  -1.34677216  0.46219647  0.37130918]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.71483533  1.83333599 -3.26605981 -0.07488215  1.94315254 -3.01305952\n",
      "  -0.95415497]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:91 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.08558582]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 92 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92196167 -1.21463509  0.26791823  1.08842459 -0.09320607 -0.34456934\n",
      "   1.10946073  0.79060211 -0.56583615]\n",
      " [-0.78237676  1.40018884 -0.43454911 -1.23780281  0.03093718  0.30032364\n",
      "  -1.30034823 -0.98429919  0.61508433]\n",
      " [ 0.59119451 -0.42638682  0.92927871 -0.94376996  0.42419032 -0.47209486\n",
      "  -1.0507421   0.78393575  0.73126602]\n",
      " [ 1.0040519  -1.37489114  0.32313047  1.12404274 -0.11962364 -0.25864332\n",
      "   1.18425189  0.47984924 -0.84500849]\n",
      " [-0.98316123  1.36152228 -0.30158712 -0.99123404  0.1741965   0.25370381\n",
      "  -1.2110392  -0.86570984  0.68167576]\n",
      " [ 0.05844238  0.31955357  0.76271552 -1.16692179  0.01434335  0.14020268\n",
      "  -1.34677216  0.46219647  0.37164476]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.71818434  1.8314692  -3.26773019 -0.0765166   1.94155511 -3.01479181\n",
      "  -0.95548401]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:92 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.86605157]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 92 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92344201 -1.21463509  0.26939857  1.08842459 -0.09320607 -0.34456934\n",
      "   1.11094107  0.79060211 -0.56583615]\n",
      " [-0.78372786  1.40018884 -0.43590021 -1.23780281  0.03093718  0.30032364\n",
      "  -1.30169933 -0.98429919  0.61508433]\n",
      " [ 0.59205836 -0.42638682  0.93014256 -0.94376996  0.42419032 -0.47209486\n",
      "  -1.04987825  0.78393575  0.73126602]\n",
      " [ 1.00540657 -1.37489114  0.32448515  1.12404274 -0.11962364 -0.25864332\n",
      "   1.18560656  0.47984924 -0.84500849]\n",
      " [-0.98452547  1.36152228 -0.30295135 -0.99123404  0.1741965   0.25370381\n",
      "  -1.21240344 -0.86570984  0.68167576]\n",
      " [ 0.05748884  0.31955357  0.76176198 -1.16692179  0.01434335  0.14020268\n",
      "  -1.34772571  0.46219647  0.37164476]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.71041491  1.83853023 -3.26715016 -0.07173591  1.94874136 -3.01420013\n",
      "  -0.95259735]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:92 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.53597713]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 92 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90976668 -1.22831042  0.25572324  1.08842459 -0.09320607 -0.34456934\n",
      "   1.09726574  0.79060211 -0.56583615]\n",
      " [-0.76987531  1.41404139 -0.42204766 -1.23780281  0.03093718  0.30032364\n",
      "  -1.28784678 -0.98429919  0.61508433]\n",
      " [ 0.59129336 -0.42715182  0.92937756 -0.94376996  0.42419032 -0.47209486\n",
      "  -1.05064326  0.78393575  0.73126602]\n",
      " [ 0.99145325 -1.38884446  0.31053183  1.12404274 -0.11962364 -0.25864332\n",
      "   1.17165324  0.47984924 -0.84500849]\n",
      " [-0.97058351  1.37546423 -0.2890094  -0.99123404  0.1741965   0.25370381\n",
      "  -1.19846148 -0.86570984  0.68167576]\n",
      " [ 0.06093229  0.32299703  0.76520543 -1.16692179  0.01434335  0.14020268\n",
      "  -1.34428225  0.46219647  0.37164476]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.77706518  1.7886611  -3.2835328  -0.10582632  1.89823438 -3.03037095\n",
      "  -0.98245393]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:92 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.70900755]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 92 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91614007 -1.22831042  0.26209663  1.08842459 -0.09320607 -0.34456934\n",
      "   1.09726574  0.7969755  -0.56583615]\n",
      " [-0.77585405  1.41404139 -0.4280264  -1.23780281  0.03093718  0.30032364\n",
      "  -1.28784678 -0.99027793  0.61508433]\n",
      " [ 0.59700127 -0.42715182  0.93508547 -0.94376996  0.42419032 -0.47209486\n",
      "  -1.05064326  0.78964366  0.73126602]\n",
      " [ 0.99804925 -1.38884446  0.31712783  1.12404274 -0.11962364 -0.25864332\n",
      "   1.17165324  0.48644523 -0.84500849]\n",
      " [-0.97666248  1.37546423 -0.29508837 -0.99123404  0.1741965   0.25370381\n",
      "  -1.19846148 -0.87178881  0.68167576]\n",
      " [ 0.06748437  0.32299703  0.77175751 -1.16692179  0.01434335  0.14020268\n",
      "  -1.34428225  0.46874855  0.37164476]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.747047    1.81496031 -3.28047377 -0.07853206  1.92392763 -3.02716916\n",
      "  -0.95892373]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:92 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.63651481]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 92 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92551566 -1.22831042  0.26209663  1.09780018 -0.09320607 -0.34456934\n",
      "   1.09726574  0.7969755  -0.55646056]\n",
      " [-0.78519405  1.41404139 -0.4280264  -1.24714281  0.03093718  0.30032364\n",
      "  -1.28784678 -0.99027793  0.60574433]\n",
      " [ 0.60089738 -0.42715182  0.93508547 -0.93987385  0.42419032 -0.47209486\n",
      "  -1.05064326  0.78964366  0.73516214]\n",
      " [ 1.00720517 -1.38884446  0.31712783  1.13319866 -0.11962364 -0.25864332\n",
      "   1.17165324  0.48644523 -0.83585257]\n",
      " [-0.98583639  1.37546423 -0.29508837 -1.00040795  0.1741965   0.25370381\n",
      "  -1.19846148 -0.87178881  0.67250185]\n",
      " [ 0.06076364  0.32299703  0.77175751 -1.17364252  0.01434335  0.14020268\n",
      "  -1.34428225  0.46874855  0.36492403]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.70499836  1.84894645 -3.27214634 -0.05351491  1.95680769 -3.01806592\n",
      "  -0.94522936]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:92 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.05030229]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 92 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92553628 -1.22831042  0.26209663  1.09780018 -0.09318545 -0.34454872\n",
      "   1.09726574  0.7969755  -0.55643994]\n",
      " [-0.78523939  1.41404139 -0.4280264  -1.24714281  0.03089184  0.3002783\n",
      "  -1.28784678 -0.99027793  0.60569899]\n",
      " [ 0.60063514 -0.42715182  0.93508547 -0.93987385  0.42392807 -0.47235711\n",
      "  -1.05064326  0.78964366  0.73489989]\n",
      " [ 1.00726666 -1.38884446  0.31712783  1.13319866 -0.11956215 -0.25858183\n",
      "   1.17165324  0.48644523 -0.83579108]\n",
      " [-0.98587069  1.37546423 -0.29508837 -1.00040795  0.1741622   0.25366951\n",
      "  -1.19846148 -0.87178881  0.67246755]\n",
      " [ 0.06060324  0.32299703  0.77175751 -1.17364252  0.01418295  0.14004228\n",
      "  -1.34428225  0.46874855  0.36476363]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.70619988  1.84836632 -3.27279261 -0.05445671  1.95626886 -3.01870105\n",
      "  -0.94599968]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:92 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.00283899]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 92 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92553716 -1.22830954  0.26209663  1.09780018 -0.09318457 -0.34454784\n",
      "   1.09726574  0.7969755  -0.55643907]\n",
      " [-0.78524029  1.41404049 -0.4280264  -1.24714281  0.03089094  0.3002774\n",
      "  -1.28784678 -0.99027793  0.60569809]\n",
      " [ 0.60063442 -0.42715254  0.93508547 -0.93987385  0.42392735 -0.47235783\n",
      "  -1.05064326  0.78964366  0.73489917]\n",
      " [ 1.00726756 -1.38884356  0.31712783  1.13319866 -0.11956125 -0.25858093\n",
      "   1.17165324  0.48644523 -0.83579018]\n",
      " [-0.98587159  1.37546333 -0.29508837 -1.00040795  0.1741613   0.25366861\n",
      "  -1.19846148 -0.87178881  0.67246665]\n",
      " [ 0.06060249  0.32299628  0.77175751 -1.17364252  0.0141822   0.14004153\n",
      "  -1.34428225  0.46874855  0.36476288]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.7062039   1.84836546 -3.27279594 -0.05445953  1.95626818 -3.01870433\n",
      "  -0.94600254]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:92 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.20664685]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 92 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92407341 -1.22977328  0.26209663  1.09780018 -0.09464831 -0.34601159\n",
      "   1.095802    0.7969755  -0.55643907]\n",
      " [-0.78388845  1.41539233 -0.4280264  -1.24714281  0.03224278  0.30162924\n",
      "  -1.28649493 -0.99027793  0.60569809]\n",
      " [ 0.60382139 -0.42396557  0.93508547 -0.93987385  0.42711433 -0.46917086\n",
      "  -1.04745628  0.78964366  0.73489917]\n",
      " [ 1.00559506 -1.39051606  0.31712783  1.13319866 -0.12123375 -0.26025343\n",
      "   1.16998073  0.48644523 -0.83579018]\n",
      " [-0.98431514  1.37701978 -0.29508837 -1.00040795  0.17571775  0.25522506\n",
      "  -1.19690503 -0.87178881  0.67246665]\n",
      " [ 0.06351748  0.32591128  0.77175751 -1.17364252  0.01709719  0.14295653\n",
      "  -1.34136726  0.46874855  0.36476288]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.72314315  1.83840084 -3.27988937 -0.05926723  1.94607835 -3.02557957\n",
      "  -0.95123076]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:92 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.9303523]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 92 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92435894 -1.22977328  0.26209663  1.09808572 -0.09464831 -0.34601159\n",
      "   1.09608753  0.7969755  -0.55643907]\n",
      " [-0.78414111  1.41539233 -0.4280264  -1.24739547  0.03224278  0.30162924\n",
      "  -1.28674759 -0.99027793  0.60569809]\n",
      " [ 0.60332106 -0.42396557  0.93508547 -0.94037418  0.42711433 -0.46917086\n",
      "  -1.04795661  0.78964366  0.73489917]\n",
      " [ 1.00584911 -1.39051606  0.31712783  1.13345271 -0.12123375 -0.26025343\n",
      "   1.17023479  0.48644523 -0.83579018]\n",
      " [-0.98458992  1.37701978 -0.29508837 -1.00068272  0.17571775  0.25522506\n",
      "  -1.19717981 -0.87178881  0.67246665]\n",
      " [ 0.06311347  0.32591128  0.77175751 -1.17404654  0.01709719  0.14295653\n",
      "  -1.34177128  0.46874855  0.36476288]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.72088667  1.84056168 -3.27981045 -0.05881492  1.94825523 -3.02548962\n",
      "  -0.95105175]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:92 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.78256922]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 92 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.9284983  -1.22977328  0.26209663  1.09808572 -0.09050896 -0.34187223\n",
      "   1.10022688  0.7969755  -0.55643907]\n",
      " [-0.78823102  1.41539233 -0.4280264  -1.24739547  0.02815287  0.29753933\n",
      "  -1.29083751 -0.99027793  0.60569809]\n",
      " [ 0.60119849 -0.42396557  0.93508547 -0.94037418  0.42499176 -0.47129342\n",
      "  -1.05007918  0.78964366  0.73489917]\n",
      " [ 1.00990584 -1.39051606  0.31712783  1.13345271 -0.11717702 -0.2561967\n",
      "   1.17429152  0.48644523 -0.83579018]\n",
      " [-0.98867256  1.37701978 -0.29508837 -1.00068272  0.17163511  0.25114242\n",
      "  -1.20126245 -0.87178881  0.67246665]\n",
      " [ 0.05927252  0.32591128  0.77175751 -1.17404654  0.01325624  0.13911557\n",
      "  -1.34561223  0.46874855  0.36476288]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.70238824  1.8559001  -3.27704137 -0.05177307  1.96411745 -3.02275291\n",
      "  -0.94649614]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:92 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.5558112]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 92 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91419963 -1.22977328  0.26209663  1.08378705 -0.09050896 -0.3561709\n",
      "   1.10022688  0.7969755  -0.57073773]\n",
      " [-0.77391039  1.41539233 -0.4280264  -1.23307484  0.02815287  0.31185996\n",
      "  -1.29083751 -0.99027793  0.62001872]\n",
      " [ 0.60249287 -0.42396557  0.93508547 -0.9390798   0.42499176 -0.46999904\n",
      "  -1.05007918  0.78964366  0.73619355]\n",
      " [ 0.99606302 -1.39051606  0.31712783  1.11960989 -0.11717702 -0.27003953\n",
      "   1.17429152  0.48644523 -0.84963301]\n",
      " [-0.97473792  1.37701978 -0.29508837 -0.98674809  0.17163511  0.26507706\n",
      "  -1.20126245 -0.87178881  0.68640128]\n",
      " [ 0.06883113  0.32591128  0.77175751 -1.16448792  0.01325624  0.14867419\n",
      "  -1.34561223  0.46874855  0.3743215 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.770999    1.8040633  -3.29376322 -0.08478284  1.91327426 -3.04033187\n",
      "  -0.97063717]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:92 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.74414122]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 92 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.9196506  -1.22432231  0.26209663  1.08923802 -0.09050896 -0.35071993\n",
      "   1.10567785  0.7969755  -0.57073773]\n",
      " [-0.77936184  1.40994088 -0.4280264  -1.23852629  0.02815287  0.30640851\n",
      "  -1.29628896 -0.99027793  0.62001872]\n",
      " [ 0.59781897 -0.42863947  0.93508547 -0.9437537   0.42499176 -0.47467294\n",
      "  -1.05475308  0.78964366  0.73619355]\n",
      " [ 1.001502   -1.38507708  0.31712783  1.12504887 -0.11717702 -0.26460055\n",
      "   1.1797305   0.48644523 -0.84963301]\n",
      " [-0.98018978  1.37156792 -0.29508837 -0.99219995  0.17163511  0.2596252\n",
      "  -1.20671431 -0.87178881  0.68640128]\n",
      " [ 0.06367301  0.32075315  0.77175751 -1.16964605  0.01325624  0.14351606\n",
      "  -1.35077035  0.46874855  0.3743215 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.74664187  1.82402147 -3.2895705  -0.08252396  1.93363897 -3.03596304\n",
      "  -0.96764744]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:92 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.82588658]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 92 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92234817 -1.22162474  0.26209663  1.09193559 -0.09050896 -0.35071993\n",
      "   1.10837543  0.7969755  -0.57073773]\n",
      " [-0.78205142  1.4072513  -0.4280264  -1.24121587  0.02815287  0.30640851\n",
      "  -1.29897854 -0.99027793  0.62001872]\n",
      " [ 0.59508954 -0.4313689   0.93508547 -0.94648314  0.42499176 -0.47467294\n",
      "  -1.05748251  0.78964366  0.73619355]\n",
      " [ 1.00418156 -1.38239752  0.31712783  1.12772843 -0.11717702 -0.26460055\n",
      "   1.18241007  0.48644523 -0.84963301]\n",
      " [-0.9829294   1.36882831 -0.29508837 -0.99493956  0.17163511  0.2596252\n",
      "  -1.20945393 -0.87178881  0.68640128]\n",
      " [ 0.06114649  0.31822663  0.77175751 -1.17217257  0.01325624  0.14351606\n",
      "  -1.35329687  0.46874855  0.3743215 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.7341233   1.8348974  -3.28794772 -0.08079241  1.94455858 -3.03419872\n",
      "  -0.96632489]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:92 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.82807867]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 92 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92495511 -1.22162474  0.26470357  1.09193559 -0.09050896 -0.34811299\n",
      "   1.11098237  0.7969755  -0.57073773]\n",
      " [-0.7844669   1.4072513  -0.43044188 -1.24121587  0.02815287  0.30399303\n",
      "  -1.30139402 -0.99027793  0.62001872]\n",
      " [ 0.59508348 -0.4313689   0.93507941 -0.94648314  0.42499176 -0.474679\n",
      "  -1.05748857  0.78964366  0.73619355]\n",
      " [ 1.00656606 -1.38239752  0.31951233  1.12772843 -0.11717702 -0.26221605\n",
      "   1.18479457  0.48644523 -0.84963301]\n",
      " [-0.98532356  1.36882831 -0.29748253 -0.99493956  0.17163511  0.25723104\n",
      "  -1.21184809 -0.87178881  0.68640128]\n",
      " [ 0.06003345  0.31822663  0.77064447 -1.17217257  0.01325624  0.14240303\n",
      "  -1.35440991  0.46874855  0.3743215 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.72188556  1.84560031 -3.28672991 -0.0746796   1.95561792 -3.03300826\n",
      "  -0.96134559]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:92 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.08305062]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 93 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92477983 -1.22180003  0.26470357  1.09176031 -0.09050896 -0.34811299\n",
      "   1.11098237  0.7969755  -0.57091302]\n",
      " [-0.78446816  1.40725004 -0.43044188 -1.24121713  0.02815287  0.30399303\n",
      "  -1.30139402 -0.99027793  0.62001747]\n",
      " [ 0.59512028 -0.4313321   0.93507941 -0.94644633  0.42499176 -0.474679\n",
      "  -1.05748857  0.78964366  0.73623035]\n",
      " [ 1.00664315 -1.38232043  0.31951233  1.12780552 -0.11717702 -0.26221605\n",
      "   1.18479457  0.48644523 -0.84955592]\n",
      " [-0.98538274  1.36876912 -0.29748253 -0.99499875  0.17163511  0.25723104\n",
      "  -1.21184809 -0.87178881  0.6863421 ]\n",
      " [ 0.06035098  0.31854417  0.77064447 -1.17185503  0.01325624  0.14240303\n",
      "  -1.35440991  0.46874855  0.37463903]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.72504785  1.8438424  -3.28831231 -0.07622393  1.95411399 -3.03464864\n",
      "  -0.9625998 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:93 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.86716054]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 93 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92623699 -1.22180003  0.26616073  1.09176031 -0.09050896 -0.34811299\n",
      "   1.11243953  0.7969755  -0.57091302]\n",
      " [-0.78579927  1.40725004 -0.43177298 -1.24121713  0.02815287  0.30399303\n",
      "  -1.30272513 -0.99027793  0.62001747]\n",
      " [ 0.59597578 -0.4313321   0.93593491 -0.94644633  0.42499176 -0.474679\n",
      "  -1.05663307  0.78964366  0.73623035]\n",
      " [ 1.00797748 -1.38232043  0.32084666  1.12780552 -0.11717702 -0.26221605\n",
      "   1.1861289   0.48644523 -0.84955592]\n",
      " [-0.98672684  1.36876912 -0.29882663 -0.99499875  0.17163511  0.25723104\n",
      "  -1.21319219 -0.87178881  0.6863421 ]\n",
      " [ 0.05941537  0.31854417  0.76970886 -1.17185503  0.01325624  0.14240303\n",
      "  -1.35534553  0.46874855  0.37463903]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.71739675  1.8507966  -3.28774059 -0.07151066  1.96119053 -3.0340654\n",
      "  -0.95975318]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:93 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.53193703]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 93 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91268504 -1.23535197  0.25260878  1.09176031 -0.09050896 -0.34811299\n",
      "   1.09888758  0.7969755  -0.57091302]\n",
      " [-0.77207914  1.42097017 -0.41805285 -1.24121713  0.02815287  0.30399303\n",
      "  -1.289005   -0.99027793  0.62001747]\n",
      " [ 0.5952486  -0.43205928  0.93520773 -0.94644633  0.42499176 -0.474679\n",
      "  -1.05736025  0.78964366  0.73623035]\n",
      " [ 0.99415443 -1.39614349  0.30702361  1.12780552 -0.11717702 -0.26221605\n",
      "   1.17230585  0.48644523 -0.84955592]\n",
      " [-0.97291744  1.38257853 -0.28501722 -0.99499875  0.17163511  0.25723104\n",
      "  -1.19938278 -0.87178881  0.6863421 ]\n",
      " [ 0.0628167   0.3219455   0.77311019 -1.17185503  0.01325624  0.14240303\n",
      "  -1.35194419  0.46874855  0.37463903]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.7836176   1.80132511 -3.30411722 -0.10534849  1.91110609 -3.05023424\n",
      "  -0.98943777]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:93 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.71076973]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 93 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91898767 -1.23535197  0.25891141  1.09176031 -0.09050896 -0.34811299\n",
      "   1.09888758  0.80327813 -0.57091302]\n",
      " [-0.777992    1.42097017 -0.42396572 -1.24121713  0.02815287  0.30399303\n",
      "  -1.289005   -0.99619079  0.62001747]\n",
      " [ 0.60086779 -0.43205928  0.94082692 -0.94644633  0.42499176 -0.474679\n",
      "  -1.05736025  0.79526284  0.73623035]\n",
      " [ 1.00068122 -1.39614349  0.3135504   1.12780552 -0.11717702 -0.26221605\n",
      "   1.17230585  0.49297203 -0.84955592]\n",
      " [-0.97892949  1.38257853 -0.29102928 -0.99499875  0.17163511  0.25723104\n",
      "  -1.19938278 -0.87780086  0.6863421 ]\n",
      " [ 0.06932708  0.3219455   0.77962057 -1.17185503  0.01325624  0.14240303\n",
      "  -1.35194419  0.47525893  0.37463903]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.75388818  1.82739122 -3.30109899 -0.07827902  1.93657337 -3.04707575\n",
      "  -0.96605199]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:93 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.63752909]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 93 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92832667 -1.23535197  0.25891141  1.10109931 -0.09050896 -0.34811299\n",
      "   1.09888758  0.80327813 -0.56157402]\n",
      " [-0.78729539  1.42097017 -0.42396572 -1.25052052  0.02815287  0.30399303\n",
      "  -1.289005   -0.99619079  0.61071408]\n",
      " [ 0.60480584 -0.43205928  0.94082692 -0.94250828  0.42499176 -0.474679\n",
      "  -1.05736025  0.79526284  0.7401684 ]\n",
      " [ 1.0098043  -1.39614349  0.3135504   1.1369286  -0.11717702 -0.26221605\n",
      "   1.17230585  0.49297203 -0.84043284]\n",
      " [-0.98806943  1.38257853 -0.29102928 -1.00413869  0.17163511  0.25723104\n",
      "  -1.19938278 -0.87780086  0.67720216]\n",
      " [ 0.0626325   0.3219455   0.77962057 -1.17854962  0.01325624  0.14240303\n",
      "  -1.35194419  0.47525893  0.36794445]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.71200725  1.86124898 -3.29280889 -0.05329958  1.96933547 -3.03801851\n",
      "  -0.95241312]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:93 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.04881516]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 93 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92834701 -1.23535197  0.25891141  1.10109931 -0.09048863 -0.34809265\n",
      "   1.09888758  0.80327813 -0.56155368]\n",
      " [-0.7873392   1.42097017 -0.42396572 -1.25052052  0.02810906  0.30394922\n",
      "  -1.289005   -0.99619079  0.61067027]\n",
      " [ 0.60455812 -0.43205928  0.94082692 -0.94250828  0.42474404 -0.47492672\n",
      "  -1.05736025  0.79526284  0.73992068]\n",
      " [ 1.00986316 -1.39614349  0.3135504   1.1369286  -0.11711817 -0.26215719\n",
      "   1.17230585  0.49297203 -0.84037399]\n",
      " [-0.98810275  1.38257853 -0.29102928 -1.00413869  0.1716018   0.25719772\n",
      "  -1.19938278 -0.87780086  0.67716885]\n",
      " [ 0.0624799   0.3219455   0.77962057 -1.17854962  0.01310364  0.14225042\n",
      "  -1.35194419  0.47525893  0.36779185]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.71314055  1.86070268 -3.29341953 -0.05418926  1.9688281  -3.03861855\n",
      "  -0.95314127]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:93 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.00268146]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 93 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.9283478  -1.23535119  0.25891141  1.10109931 -0.09048784 -0.34809187\n",
      "   1.09888758  0.80327813 -0.5615529 ]\n",
      " [-0.78734001  1.42096937 -0.42396572 -1.25052052  0.02810825  0.30394842\n",
      "  -1.289005   -0.99619079  0.61066947]\n",
      " [ 0.60455747 -0.43205992  0.94082692 -0.94250828  0.4247434  -0.47492737\n",
      "  -1.05736025  0.79526284  0.73992003]\n",
      " [ 1.00986396 -1.39614269  0.3135504   1.1369286  -0.11711737 -0.26215639\n",
      "   1.17230585  0.49297203 -0.84037319]\n",
      " [-0.98810355  1.38257773 -0.29102928 -1.00413869  0.171601    0.25719692\n",
      "  -1.19938278 -0.87780086  0.67716804]\n",
      " [ 0.06247923  0.32194483  0.77962057 -1.17854962  0.01310297  0.14224976\n",
      "  -1.35194419  0.47525893  0.36779118]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.71314413  1.86070192 -3.2934225  -0.05419179  1.9688275  -3.03862149\n",
      "  -0.95314382]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:93 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.20322709]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 93 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92693891 -1.23676007  0.25891141  1.10109931 -0.09189672 -0.34950075\n",
      "   1.0974787   0.80327813 -0.5615529 ]\n",
      " [-0.78604421  1.42226517 -0.42396572 -1.25052052  0.02940406  0.30524422\n",
      "  -1.2877092  -0.99619079  0.61066947]\n",
      " [ 0.60767193 -0.42894546  0.94082692 -0.94250828  0.42785786 -0.47181291\n",
      "  -1.05424579  0.79526284  0.73992003]\n",
      " [ 1.00825813 -1.39774851  0.3135504   1.1369286  -0.11872319 -0.26376222\n",
      "   1.17070003  0.49297203 -0.84037319]\n",
      " [-0.98660988  1.3840714  -0.29102928 -1.00413869  0.17309467  0.25869059\n",
      "  -1.19788911 -0.87780086  0.67716804]\n",
      " [ 0.06532449  0.32479009  0.77962057 -1.17854962  0.01594823  0.14509502\n",
      "  -1.34909893  0.47525893  0.36779118]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.72959799  1.85103662 -3.30033093 -0.05883016  1.9589501  -3.04531928\n",
      "  -0.95820221]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:93 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.93160695]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 93 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92721334 -1.23676007  0.25891141  1.10137374 -0.09189672 -0.34950075\n",
      "   1.09775312  0.80327813 -0.5615529 ]\n",
      " [-0.78628714  1.42226517 -0.42396572 -1.25076346  0.02940406  0.30524422\n",
      "  -1.28795214 -0.99619079  0.61066947]\n",
      " [ 0.60718849 -0.42894546  0.94082692 -0.94299173  0.42785786 -0.47181291\n",
      "  -1.05472923  0.79526284  0.73992003]\n",
      " [ 1.00850235 -1.39774851  0.3135504   1.13717282 -0.11872319 -0.26376222\n",
      "   1.17094425  0.49297203 -0.84037319]\n",
      " [-0.98687408  1.3840714  -0.29102928 -1.00440289  0.17309467  0.25869059\n",
      "  -1.19815331 -0.87780086  0.67716804]\n",
      " [ 0.06493621  0.32479009  0.77962057 -1.1789379   0.01594823  0.14509502\n",
      "  -1.34948722  0.47525893  0.36779118]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.72741915  1.8531238  -3.30025522 -0.05839536  1.96105262 -3.04523301\n",
      "  -0.95803108]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:93 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.78457348]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 93 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93128673 -1.23676007  0.25891141  1.10137374 -0.08782333 -0.34542736\n",
      "   1.10182651  0.80327813 -0.5615529 ]\n",
      " [-0.79031098  1.42226517 -0.42396572 -1.25076346  0.02538021  0.30122037\n",
      "  -1.29197598 -0.99619079  0.61066947]\n",
      " [ 0.60508135 -0.42894546  0.94082692 -0.94299173  0.42575071 -0.47392005\n",
      "  -1.05683637  0.79526284  0.73992003]\n",
      " [ 1.01249335 -1.39774851  0.3135504   1.13717282 -0.1147322  -0.25977123\n",
      "   1.17493524  0.49297203 -0.84037319]\n",
      " [-0.99089076  1.3840714  -0.29102928 -1.00440289  0.16907798  0.2546739\n",
      "  -1.20217    -0.87780086  0.67716804]\n",
      " [ 0.06114896  0.32479009  0.77962057 -1.1789379   0.01216099  0.14130778\n",
      "  -1.35327446  0.47525893  0.36779118]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.70921367  1.86822906 -3.29753596 -0.05148563  1.97666889 -3.04254517\n",
      "  -0.95356416]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:93 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.55432231]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 93 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91703082 -1.23676007  0.25891141  1.08711783 -0.08782333 -0.35968327\n",
      "   1.10182651  0.80327813 -0.5758088 ]\n",
      " [-0.77603627  1.42226517 -0.42396572 -1.23648874  0.02538021  0.31549509\n",
      "  -1.29197598 -0.99619079  0.62494418]\n",
      " [ 0.60631073 -0.42894546  0.94082692 -0.94176235  0.42575071 -0.47269067\n",
      "  -1.05683637  0.79526284  0.74114941]\n",
      " [ 0.99869044 -1.39774851  0.3135504   1.12336992 -0.1147322  -0.27357413\n",
      "   1.17493524  0.49297203 -0.85417609]\n",
      " [-0.97699865  1.3840714  -0.29102928 -0.99051077  0.16907798  0.26856602\n",
      "  -1.20217    -0.87780086  0.69106016]\n",
      " [ 0.07066007  0.32479009  0.77962057 -1.1694268   0.01216099  0.15081888\n",
      "  -1.35327446  0.47525893  0.37730228]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.77768608  1.81652959 -3.31426457 -0.0844914   1.92595254 -3.06011864\n",
      "  -0.97769097]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:93 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.7445532]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 93 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92246691 -1.23132398  0.25891141  1.09255392 -0.08782333 -0.35424718\n",
      "   1.1072626   0.80327813 -0.5758088 ]\n",
      " [-0.78147356  1.41682788 -0.42396572 -1.24192603  0.02538021  0.3100578\n",
      "  -1.29741327 -0.99619079  0.62494418]\n",
      " [ 0.60167268 -0.43358351  0.94082692 -0.9464004   0.42575071 -0.47732871\n",
      "  -1.06147442  0.79526284  0.74114941]\n",
      " [ 1.00411609 -1.39232287  0.3135504   1.12879556 -0.1147322  -0.26814849\n",
      "   1.18036088  0.49297203 -0.85417609]\n",
      " [-0.98243563  1.37863442 -0.29102928 -0.99594775  0.16907798  0.26312904\n",
      "  -1.20760698 -0.87780086  0.69106016]\n",
      " [ 0.06552814  0.31965816  0.77962057 -1.17455873  0.01216099  0.14568695\n",
      "  -1.35840639  0.47525893  0.37730228]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.75339389  1.8364247  -3.31006976 -0.08226557  1.94625051 -3.05574854\n",
      "  -0.97473462]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:93 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.82731736]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 93 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92512453 -1.22866636  0.25891141  1.09521154 -0.08782333 -0.35424718\n",
      "   1.10992022  0.80327813 -0.5758088 ]\n",
      " [-0.78412384  1.4141776  -0.42396572 -1.24457632  0.02538021  0.3100578\n",
      "  -1.30006355 -0.99619079  0.62494418]\n",
      " [ 0.59898831 -0.43626788  0.94082692 -0.94908477  0.42575071 -0.47732871\n",
      "  -1.06415879  0.79526284  0.74114941]\n",
      " [ 1.00675651 -1.38968244  0.3135504   1.13143598 -0.1147322  -0.26814849\n",
      "   1.1830013   0.49297203 -0.85417609]\n",
      " [-0.98513515  1.3759349  -0.29102928 -0.99864727  0.16907798  0.26312904\n",
      "  -1.2103065  -0.87780086  0.69106016]\n",
      " [ 0.06304812  0.31717815  0.77962057 -1.17703874  0.01216099  0.14568695\n",
      "  -1.36088641  0.47525893  0.37730228]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.74105888  1.84714215 -3.30847042 -0.08057473  1.95700965 -3.05400982\n",
      "  -0.97344513]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:93 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.82857886]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 93 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92771934 -1.22866636  0.26150622  1.09521154 -0.08782333 -0.35165237\n",
      "   1.11251503  0.80327813 -0.5758088 ]\n",
      " [-0.78653053  1.4141776  -0.4263724  -1.24457632  0.02538021  0.30765112\n",
      "  -1.30247024 -0.99619079  0.62494418]\n",
      " [ 0.59898322 -0.43626788  0.94082183 -0.94908477  0.42575071 -0.4773338\n",
      "  -1.06416388  0.79526284  0.74114941]\n",
      " [ 1.00913197 -1.38968244  0.31592586  1.13143598 -0.1147322  -0.26577303\n",
      "   1.18537677  0.49297203 -0.85417609]\n",
      " [-0.98752066  1.3759349  -0.29341478 -0.99864727  0.16907798  0.26074353\n",
      "  -1.21269201 -0.87780086  0.69106016]\n",
      " [ 0.06195277  0.31717815  0.77852522 -1.17703874  0.01216099  0.1445916\n",
      "  -1.36198176  0.47525893  0.37730228]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.7288849   1.85778611 -3.30725398 -0.07449283  1.96800717 -3.05282074\n",
      "  -0.968479  ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:93 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.08059402]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 94 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.9275582  -1.2288275   0.26150622  1.0950504  -0.08782333 -0.35165237\n",
      "   1.11251503  0.80327813 -0.57596995]\n",
      " [-0.78653651  1.41417161 -0.4263724  -1.2445823   0.02538021  0.30765112\n",
      "  -1.30247024 -0.99619079  0.6249382 ]\n",
      " [ 0.59901696 -0.43623414  0.94082183 -0.94905103  0.42575071 -0.4773338\n",
      "  -1.06416388  0.79526284  0.74118315]\n",
      " [ 1.00920887 -1.38960554  0.31592586  1.13151288 -0.1147322  -0.26577303\n",
      "   1.18537677  0.49297203 -0.85409919]\n",
      " [-0.9875809   1.37587466 -0.29341478 -0.99870751  0.16907798  0.26074353\n",
      "  -1.21269201 -0.87780086  0.69099992]\n",
      " [ 0.06225326  0.31747864  0.77852522 -1.17673825  0.01216099  0.1445916\n",
      "  -1.36198176  0.47525893  0.37760277]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.73187086  1.8561307  -3.30875294 -0.07595205  1.96659123 -3.05437402\n",
      "  -0.96966255]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:94 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.86825329]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 94 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92899267 -1.2288275   0.2629407   1.0950504  -0.08782333 -0.35165237\n",
      "   1.11394951  0.80327813 -0.57596995]\n",
      " [-0.78784801  1.41417161 -0.4276839  -1.2445823   0.02538021  0.30765112\n",
      "  -1.30378173 -0.99619079  0.6249382 ]\n",
      " [ 0.5998642  -0.43623414  0.94166907 -0.94905103  0.42575071 -0.4773338\n",
      "  -1.06331664  0.79526284  0.74118315]\n",
      " [ 1.01052326 -1.38960554  0.31724025  1.13151288 -0.1147322  -0.26577303\n",
      "   1.18669115  0.49297203 -0.85409919]\n",
      " [-0.98890525  1.37587466 -0.29473914 -0.99870751  0.16907798  0.26074353\n",
      "  -1.21401636 -0.87780086  0.69099992]\n",
      " [ 0.06133519  0.31747864  0.77760715 -1.17673825  0.01216099  0.1445916\n",
      "  -1.36289983  0.47525893  0.37760277]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.72433563  1.86298028 -3.3081894  -0.07130489  1.97356037 -3.05379907\n",
      "  -0.96685515]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:94 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.52791762]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 94 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91556495 -1.24225523  0.24951297  1.0950504  -0.08782333 -0.35165237\n",
      "   1.10052178  0.80327813 -0.57596995]\n",
      " [-0.77426118  1.42775844 -0.41409707 -1.2445823   0.02538021  0.30765112\n",
      "  -1.29019491 -0.99619079  0.6249382 ]\n",
      " [ 0.59917406 -0.43692428  0.94097893 -0.94905103  0.42575071 -0.4773338\n",
      "  -1.06400678  0.79526284  0.74118315]\n",
      " [ 0.99683139 -1.40329741  0.30354839  1.13151288 -0.1147322  -0.26577303\n",
      "   1.17299929  0.49297203 -0.85409919]\n",
      " [-0.97522935  1.38955055 -0.28106324 -0.99870751  0.16907798  0.26074353\n",
      "  -1.20034046 -0.87780086  0.69099992]\n",
      " [ 0.064695    0.32083845  0.78096696 -1.17673825  0.01216099  0.1445916\n",
      "  -1.35954002  0.47525893  0.37760277]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.79011961  1.81390962 -3.32455498 -0.10488722  1.9239009  -3.06996089\n",
      "  -0.9963634 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:94 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.71251916]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 94 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92179758 -1.24225523  0.2557456   1.0950504  -0.08782333 -0.35165237\n",
      "   1.10052178  0.80951076 -0.57596995]\n",
      " [-0.7801089   1.42775844 -0.41994479 -1.2445823   0.02538021  0.30765112\n",
      "  -1.29019491 -1.00203851  0.6249382 ]\n",
      " [ 0.60470577 -0.43692428  0.94651064 -0.94905103  0.42575071 -0.4773338\n",
      "  -1.06400678  0.80079455  0.74118315]\n",
      " [ 1.00328959 -1.40329741  0.31000658  1.13151288 -0.1147322  -0.26577303\n",
      "   1.17299929  0.49943022 -0.85409919]\n",
      " [-0.98117523  1.38955055 -0.28700912 -0.99870751  0.16907798  0.26074353\n",
      "  -1.20034046 -0.88374674  0.69099992]\n",
      " [ 0.07116218  0.32083845  0.78743414 -1.17673825  0.01216099  0.1445916\n",
      "  -1.35954002  0.48172611  0.37760277]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.76067645  1.83974406 -3.32157691 -0.07804189  1.94914363 -3.06684501\n",
      "  -0.97312289]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:94 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.63852176]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 94 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93110071 -1.24225523  0.2557456   1.10435353 -0.08782333 -0.35165237\n",
      "   1.10052178  0.80951076 -0.56666681]\n",
      " [-0.78937638  1.42775844 -0.41994479 -1.25384978  0.02538021  0.30765112\n",
      "  -1.29019491 -1.00203851  0.61567071]\n",
      " [ 0.6086857  -0.43692428  0.94651064 -0.94507109  0.42575071 -0.4773338\n",
      "  -1.06400678  0.80079455  0.74516309]\n",
      " [ 1.01238038 -1.40329741  0.31000658  1.14060368 -0.1147322  -0.26577303\n",
      "   1.17299929  0.49943022 -0.84500839]\n",
      " [-0.99028179  1.38955055 -0.28700912 -1.00781407  0.16907798  0.26074353\n",
      "  -1.20034046 -0.88374674  0.68189336]\n",
      " [ 0.06449328  0.32083845  0.78743414 -1.18340716  0.01216099  0.1445916\n",
      "  -1.35954002  0.48172611  0.37093387]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.71895975  1.8734758  -3.31332304 -0.05309831  1.98178985 -3.05783245\n",
      "  -0.95953827]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:94 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.04737141]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 94 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93112073 -1.24225523  0.2557456   1.10435353 -0.08780331 -0.35163234\n",
      "   1.10052178  0.80951076 -0.56664679]\n",
      " [-0.78941869  1.42775844 -0.41994479 -1.25384978  0.02533791  0.30760881\n",
      "  -1.29019491 -1.00203851  0.61562841]\n",
      " [ 0.60845174 -0.43692428  0.94651064 -0.94507109  0.42551675 -0.47756776\n",
      "  -1.06400678  0.80079455  0.74492912]\n",
      " [ 1.0124367  -1.40329741  0.31000658  1.14060368 -0.11467589 -0.26571671\n",
      "   1.17299929  0.49943022 -0.84495208]\n",
      " [-0.99031412  1.38955055 -0.28700912 -1.00781407  0.16904565  0.2607112\n",
      "  -1.20034046 -0.88374674  0.68186103]\n",
      " [ 0.06434814  0.32083845  0.78743414 -1.18340716  0.01201585  0.14444646\n",
      "  -1.35954002  0.48172611  0.37078873]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.72002862  1.8729614  -3.31389996 -0.05393868  1.98131215 -3.05839929\n",
      "  -0.96022648]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:94 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.00253365]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 94 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93112144 -1.24225452  0.2557456   1.10435353 -0.08780261 -0.35163164\n",
      "   1.10052178  0.80951076 -0.56664609]\n",
      " [-0.7894194   1.42775773 -0.41994479 -1.25384978  0.02533719  0.30760809\n",
      "  -1.29019491 -1.00203851  0.61562769]\n",
      " [ 0.60845116 -0.43692486  0.94651064 -0.94507109  0.42551618 -0.47756834\n",
      "  -1.06400678  0.80079455  0.74492855]\n",
      " [ 1.01243741 -1.40329669  0.31000658  1.14060368 -0.11467517 -0.265716\n",
      "   1.17299929  0.49943022 -0.84495136]\n",
      " [-0.99031484  1.38954984 -0.28700912 -1.00781407  0.16904493  0.26071049\n",
      "  -1.20034046 -0.88374674  0.68186032]\n",
      " [ 0.06434754  0.32083785  0.78743414 -1.18340716  0.01201525  0.14444586\n",
      "  -1.35954002  0.48172611  0.37078813]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.72003182  1.87296073 -3.31390262 -0.05394093  1.98131162 -3.05840191\n",
      "  -0.96022877]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:94 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.19990315]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 94 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92976477 -1.24361119  0.2557456   1.10435353 -0.08915927 -0.35298831\n",
      "   1.09916511  0.80951076 -0.56664609]\n",
      " [-0.7881767   1.42900042 -0.41994479 -1.25384978  0.02657989  0.30885079\n",
      "  -1.28895221 -1.00203851  0.61562769]\n",
      " [ 0.61149517 -0.43388085  0.94651064 -0.94507109  0.42856018 -0.47452433\n",
      "  -1.06096277  0.80079455  0.74492855]\n",
      " [ 1.01089486 -1.40483924  0.31000658  1.14060368 -0.11621772 -0.26725855\n",
      "   1.17145674  0.49943022 -0.84495136]\n",
      " [-0.98888071  1.39098396 -0.28700912 -1.00781407  0.17047906  0.26214462\n",
      "  -1.19890634 -0.88374674  0.68186032]\n",
      " [ 0.06712531  0.32361563  0.78743414 -1.18340716  0.01479303  0.14722363\n",
      "  -1.35676224  0.48172611  0.37078813]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.73601826  1.86358298 -3.32063197 -0.05841691  1.97173402 -3.06492782\n",
      "  -0.96512399]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:94 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.93283222]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 94 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Theta One: \n",
      "[[ 0.93002857 -1.24361119  0.2557456   1.10461733 -0.08915927 -0.35298831\n",
      "   1.09942892  0.80951076 -0.56664609]\n",
      " [-0.78841033  1.42900042 -0.41994479 -1.25408341  0.02657989  0.30885079\n",
      "  -1.28918584 -1.00203851  0.61562769]\n",
      " [ 0.61102798 -0.43388085  0.94651064 -0.94553828  0.42856018 -0.47452433\n",
      "  -1.06142996  0.80079455  0.74492855]\n",
      " [ 1.01112967 -1.40483924  0.31000658  1.14083849 -0.11621772 -0.26725855\n",
      "   1.17169155  0.49943022 -0.84495136]\n",
      " [-0.98913478  1.39098396 -0.28700912 -1.00806815  0.17047906  0.26214462\n",
      "  -1.19916041 -0.88374674  0.68186032]\n",
      " [ 0.06675208  0.32361563  0.78743414 -1.18378039  0.01479303  0.14722363\n",
      "  -1.35713547  0.48172611  0.37078813]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.73391402  1.86559935 -3.32055932 -0.05799883  1.97376505 -3.06484505\n",
      "  -0.96496035]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:94 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.78656053]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 94 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93403685 -1.24361119  0.2557456   1.10461733 -0.085151   -0.34898003\n",
      "   1.1034372   0.80951076 -0.56664609]\n",
      " [-0.79236899  1.42900042 -0.41994479 -1.25408341  0.02262123  0.30489213\n",
      "  -1.2931445  -1.00203851  0.61562769]\n",
      " [ 0.60893621 -0.43388085  0.94651064 -0.94553828  0.42646842 -0.4766161\n",
      "  -1.06352173  0.80079455  0.74492855]\n",
      " [ 1.01505581 -1.40483924  0.31000658  1.14083849 -0.11229159 -0.26333241\n",
      "   1.17561768  0.49943022 -0.84495136]\n",
      " [-0.9930864   1.39098396 -0.28700912 -1.00806815  0.16652744  0.258193\n",
      "  -1.20311203 -0.88374674  0.68186032]\n",
      " [ 0.06301811  0.32361563  0.78743414 -1.18378039  0.01105906  0.14348967\n",
      "  -1.36086944  0.48172611  0.37078813]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.71599758  1.88047425 -3.31788909 -0.05121934  1.98913859 -3.06220537\n",
      "  -0.96058045]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:94 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.55278625]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 94 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.9198253  -1.24361119  0.2557456   1.09040579 -0.085151   -0.36319158\n",
      "   1.1034372   0.80951076 -0.58085763]\n",
      " [-0.77814177  1.42900042 -0.41994479 -1.23985618  0.02262123  0.31911936\n",
      "  -1.2931445  -1.00203851  0.62985492]\n",
      " [ 0.61010138 -0.43388085  0.94651064 -0.94437312  0.42646842 -0.47545094\n",
      "  -1.06352173  0.80079455  0.74609371]\n",
      " [ 1.00129443 -1.40483924  0.31000658  1.12707711 -0.11229159 -0.2770938\n",
      "   1.17561768  0.49943022 -0.85871274]\n",
      " [-0.97923838  1.39098396 -0.28700912 -0.99422012  0.16652744  0.27204102\n",
      "  -1.20311203 -0.88374674  0.69570834]\n",
      " [ 0.07248094  0.32361563  0.78743414 -1.17431757  0.01105906  0.15295249\n",
      "  -1.36086944  0.48172611  0.38025095]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.78432573  1.82891725 -3.33462351 -0.08421734  1.93855419 -3.07977247\n",
      "  -0.98469082]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:94 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.74497431]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 94 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92524618 -1.23819031  0.2557456   1.09582667 -0.085151   -0.3577707\n",
      "   1.10885808  0.80951076 -0.58085763]\n",
      " [-0.78356451  1.42357768 -0.41994479 -1.24527893  0.02262123  0.31369661\n",
      "  -1.29856724 -1.00203851  0.62985492]\n",
      " [ 0.60549933 -0.4384829   0.94651064 -0.94897516  0.42646842 -0.48005298\n",
      "  -1.06812377  0.80079455  0.74609371]\n",
      " [ 1.00670632 -1.39942734  0.31000658  1.13248901 -0.11229159 -0.2716819\n",
      "   1.18102958  0.49943022 -0.85871274]\n",
      " [-0.98466013  1.38556222 -0.28700912 -0.99964187  0.16652744  0.26661928\n",
      "  -1.20853378 -0.88374674  0.69570834]\n",
      " [ 0.06737557  0.31851025  0.78743414 -1.17942294  0.01105906  0.14784712\n",
      "  -1.36597481  0.48172611  0.38025095]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.76009987  1.84874845 -3.33042726 -0.08202406  1.95878468 -3.07540179\n",
      "  -0.98176758]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:94 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.82874686]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 94 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92786407 -1.23557242  0.2557456   1.09844456 -0.085151   -0.3577707\n",
      "   1.11147597  0.80951076 -0.58085763]\n",
      " [-0.78617569  1.4209665  -0.41994479 -1.24789011  0.02262123  0.31369661\n",
      "  -1.30117842 -1.00203851  0.62985492]\n",
      " [ 0.6028597  -0.44112254  0.94651064 -0.9516148   0.42646842 -0.48005298\n",
      "  -1.07076341  0.80079455  0.74609371]\n",
      " [ 1.00930779 -1.39682588  0.31000658  1.13509048 -0.11229159 -0.2716819\n",
      "   1.18363105  0.49943022 -0.85871274]\n",
      " [-0.98731976  1.38290258 -0.28700912 -1.00230151  0.16652744  0.26661928\n",
      "  -1.21119342 -0.88374674  0.69570834]\n",
      " [ 0.06494161  0.31607629  0.78743414 -1.1818569   0.01105906  0.14784712\n",
      "  -1.36840877  0.48172611  0.38025095]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.74794728  1.85930849 -3.32885137 -0.08037297  1.96938451 -3.07368866\n",
      "  -0.98051045]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:94 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.82906711]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 94 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93044708 -1.23557242  0.25832861  1.09844456 -0.085151   -0.35518769\n",
      "   1.11405897  0.80951076 -0.58085763]\n",
      " [-0.7885738   1.4209665  -0.4223429  -1.24789011  0.02262123  0.3112985\n",
      "  -1.30357653 -1.00203851  0.62985492]\n",
      " [ 0.60285532 -0.44112254  0.94650626 -0.9516148   0.42646842 -0.48005736\n",
      "  -1.07076779  0.80079455  0.74609371]\n",
      " [ 1.01167444 -1.39682588  0.31237322  1.13509048 -0.11229159 -0.26931525\n",
      "   1.18599769  0.49943022 -0.85871274]\n",
      " [-0.98969683  1.38290258 -0.28938619 -1.00230151  0.16652744  0.26424221\n",
      "  -1.21357049 -0.88374674  0.69570834]\n",
      " [ 0.0638637   0.31607629  0.78635623 -1.1818569   0.01105906  0.14676921\n",
      "  -1.36948668  0.48172611  0.38025095]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.73583541  1.86989495 -3.32763623 -0.07432141  1.98032179 -3.0725009\n",
      "  -0.97555695]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:94 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.07821426]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 95 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.930299   -1.2357205   0.25832861  1.09829648 -0.085151   -0.35518769\n",
      "   1.11405897  0.80951076 -0.58100571]\n",
      " [-0.78858392  1.42095638 -0.4223429  -1.24790023  0.02262123  0.3112985\n",
      "  -1.30357653 -1.00203851  0.6298448 ]\n",
      " [ 0.60288617 -0.44109168  0.94650626 -0.95158395  0.42646842 -0.48005736\n",
      "  -1.07076779  0.80079455  0.74612457]\n",
      " [ 1.01175088 -1.39674943  0.31237322  1.13516692 -0.11229159 -0.26931525\n",
      "   1.18599769  0.49943022 -0.8586363 ]\n",
      " [-0.98975777  1.38284164 -0.28938619 -1.00236244  0.16652744  0.26424221\n",
      "  -1.21357049 -0.88374674  0.6956474 ]\n",
      " [ 0.06414809  0.31636068  0.78635623 -1.18157251  0.01105906  0.14676921\n",
      "  -1.36948668  0.48172611  0.38053535]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.73865491  1.868336   -3.3290561  -0.0757003   1.97898863 -3.07397166\n",
      "  -0.9766738 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:95 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.8693306]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 95 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93171126 -1.2357205   0.25974087  1.09829648 -0.085151   -0.35518769\n",
      "   1.11547124  0.80951076 -0.58100571]\n",
      " [-0.78987617  1.42095638 -0.42363515 -1.24790023  0.02262123  0.3112985\n",
      "  -1.30486878 -1.00203851  0.6298448 ]\n",
      " [ 0.60372524 -0.44109168  0.94734533 -0.95158395  0.42646842 -0.48005736\n",
      "  -1.06992872  0.80079455  0.74612457]\n",
      " [ 1.01304569 -1.39674943  0.31366803  1.13516692 -0.11229159 -0.26931525\n",
      "   1.18729251  0.49943022 -0.8586363 ]\n",
      " [-0.99106273  1.38284164 -0.29069115 -1.00236244  0.16652744  0.26424221\n",
      "  -1.21487545 -0.88374674  0.6956474 ]\n",
      " [ 0.06324719  0.31636068  0.78545534 -1.18157251  0.01105906  0.14676921\n",
      "  -1.37038758  0.48172611  0.38053535]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.73123322  1.87508305 -3.32850061 -0.07111798  1.98585257 -3.07340486\n",
      "  -0.97390484]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:95 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.52392239]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 95 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.9184084  -1.24902337  0.24643801  1.09829648 -0.085151   -0.35518769\n",
      "   1.10216837  0.80951076 -0.58100571]\n",
      " [-0.77642329  1.43440926 -0.41018227 -1.24790023  0.02262123  0.3112985\n",
      "  -1.2914159  -1.00203851  0.6298448 ]\n",
      " [ 0.60307128 -0.44174565  0.94669137 -0.95158395  0.42646842 -0.48005736\n",
      "  -1.07058268  0.80079455  0.74612457]\n",
      " [ 0.99948571 -1.41030942  0.30010805  1.13516692 -0.11229159 -0.26931525\n",
      "   1.17373252  0.49943022 -0.8586363 ]\n",
      " [-0.97752109  1.39638328 -0.27714951 -1.00236244  0.16652744  0.26424221\n",
      "  -1.20133381 -0.88374674  0.6956474 ]\n",
      " [ 0.06656608  0.31967957  0.78877423 -1.18157251  0.01105906  0.14676921\n",
      "  -1.36706869  0.48172611  0.38053535]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.7965736   1.82641586 -3.34485027 -0.10444231  1.93661993 -3.08955475\n",
      "  -1.00323277]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:95 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.71425625]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 95 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92457176 -1.24902337  0.25260137  1.09829648 -0.085151   -0.35518769\n",
      "   1.10216837  0.81567411 -0.58100571]\n",
      " [-0.78220656  1.43440926 -0.41596554 -1.24790023  0.02262123  0.3112985\n",
      "  -1.2914159  -1.00782178  0.6298448 ]\n",
      " [ 0.60851675 -0.44174565  0.95213684 -0.95158395  0.42646842 -0.48005736\n",
      "  -1.07058268  0.80624002  0.74612457]\n",
      " [ 1.00587589 -1.41030942  0.30649823  1.13516692 -0.11229159 -0.26931525\n",
      "   1.17373252  0.5058204  -0.8586363 ]\n",
      " [-0.98340149  1.39638328 -0.28302991 -1.00236244  0.16652744  0.26424221\n",
      "  -1.20133381 -0.88962714  0.6956474 ]\n",
      " [ 0.07298864  0.31967957  0.79519679 -1.18157251  0.01105906  0.14676921\n",
      "  -1.36706869  0.48814867  0.38053535]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.76741428  1.85202004 -3.34191174 -0.07782047  1.96163951 -3.08648081\n",
      "  -0.98013838]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:95 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.63949396]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 95 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.9338397  -1.24902337  0.25260137  1.10756442 -0.085151   -0.35518769\n",
      "   1.10216837  0.81567411 -0.57173777]\n",
      " [-0.79143881  1.43440926 -0.41596554 -1.25713248  0.02262123  0.3112985\n",
      "  -1.2914159  -1.00782178  0.62061254]\n",
      " [ 0.61253853 -0.44174565  0.95213684 -0.94756217  0.42646842 -0.48005736\n",
      "  -1.07058268  0.80624002  0.75014634]\n",
      " [ 1.01493494 -1.41030942  0.30649823  1.14422597 -0.11229159 -0.26931525\n",
      "   1.17373252  0.5058204  -0.84957725]\n",
      " [-0.99247525  1.39638328 -0.28302991 -1.0114362   0.16652744  0.26424221\n",
      "  -1.20133381 -0.88962714  0.68657365]\n",
      " [ 0.06634495  0.31967957  0.79519679 -1.1882162   0.01105906  0.14676921\n",
      "  -1.36706869  0.48814867  0.37389165]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.72585849  1.88562799 -3.33369306 -0.05291101  1.99417184 -3.0775117\n",
      "  -0.96660685]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:95 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.04596965]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 95 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93385939 -1.24902337  0.25260137  1.10756442 -0.08513131 -0.35516801\n",
      "   1.10216837  0.81567411 -0.57171808]\n",
      " [-0.79147964  1.43440926 -0.41596554 -1.25713248  0.0225804   0.31125767\n",
      "  -1.2914159  -1.00782178  0.62057172]\n",
      " [ 0.61231759 -0.44174565  0.95213684 -0.94756217  0.42624748 -0.48027829\n",
      "  -1.07058268  0.80624002  0.74992541]\n",
      " [ 1.0149888  -1.41030942  0.30649823  1.14422597 -0.11223773 -0.26926139\n",
      "   1.17373252  0.5058204  -0.84952339]\n",
      " [-0.99250659  1.39638328 -0.28302991 -1.0114362   0.1664961   0.26421086\n",
      "  -1.20133381 -0.88962714  0.6865423 ]\n",
      " [ 0.06620694  0.31967957  0.79519679 -1.1882162   0.01092106  0.14663121\n",
      "  -1.36706869  0.48814867  0.37375365]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.72686652  1.88514368 -3.33423809 -0.0537047   1.99372211 -3.07804714\n",
      "  -0.96725725]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:95 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.0023949]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 95 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93386002 -1.24902274  0.25260137  1.10756442 -0.08513068 -0.35516738\n",
      "   1.10216837  0.81567411 -0.57171745]\n",
      " [-0.79148028  1.43440862 -0.41596554 -1.25713248  0.02257976  0.31125703\n",
      "  -1.2914159  -1.00782178  0.62057108]\n",
      " [ 0.61231708 -0.44174616  0.95213684 -0.94756217  0.42624697 -0.48027881\n",
      "  -1.07058268  0.80624002  0.74992489]\n",
      " [ 1.01498944 -1.41030878  0.30649823  1.14422597 -0.11223709 -0.26926075\n",
      "   1.17373252  0.5058204  -0.84952275]\n",
      " [-0.99250723  1.39638264 -0.28302991 -1.0114362   0.16649546  0.26421022\n",
      "  -1.20133381 -0.88962714  0.68654166]\n",
      " [ 0.06620641  0.31967904  0.79519679 -1.1882162   0.01092052  0.14663067\n",
      "  -1.36706869  0.48814867  0.37375311]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.72686938  1.88514308 -3.33424047 -0.05370671  1.99372164 -3.07804949\n",
      "  -0.96725929]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:95 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.19667161]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 95 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93255305 -1.2503297   0.25260137  1.10756442 -0.08643764 -0.35647434\n",
      "   1.10086141  0.81567411 -0.57171745]\n",
      " [-0.79028791  1.43560099 -0.41596554 -1.25713248  0.02377213  0.3124494\n",
      "  -1.29022353 -1.00782178  0.62057108]\n",
      " [ 0.61529262 -0.43877062  0.95213684 -0.94756217  0.42922251 -0.47730327\n",
      "  -1.06760714  0.80624002  0.74992489]\n",
      " [ 1.01350696 -1.41179126  0.30649823  1.14422597 -0.11371957 -0.27074324\n",
      "   1.17225004  0.5058204  -0.84952275]\n",
      " [-0.99112959  1.39776029 -0.28302991 -1.0114362   0.1678731   0.26558787\n",
      "  -1.19995616 -0.88962714  0.68654166]\n",
      " [ 0.06891884  0.32239147  0.79519679 -1.1882162   0.01363295  0.1493431\n",
      "  -1.36435626  0.48814867  0.37375311]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.74240564  1.87604164 -3.34079646 -0.05802692  1.98443179 -3.08440888\n",
      "  -0.97199775]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:95 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.93402893]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 95 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.9328067  -1.2503297   0.25260137  1.10781806 -0.08643764 -0.35647434\n",
      "   1.10111505  0.81567411 -0.57171745]\n",
      " [-0.79051263  1.43560099 -0.41596554 -1.2573572   0.02377213  0.3124494\n",
      "  -1.29044825 -1.00782178  0.62057108]\n",
      " [ 0.61484108 -0.43877062  0.95213684 -0.94801371  0.42922251 -0.47730327\n",
      "  -1.06805868  0.80624002  0.74992489]\n",
      " [ 1.01373276 -1.41179126  0.30649823  1.14445178 -0.11371957 -0.27074324\n",
      "   1.17247585  0.5058204  -0.84952275]\n",
      " [-0.99137397  1.39776029 -0.28302991 -1.01168058  0.1678731   0.26558787\n",
      "  -1.20020055 -0.88962714  0.68654166]\n",
      " [ 0.06856001  0.32239147  0.79519679 -1.18857503  0.01363295  0.1493431\n",
      "  -1.36471508  0.48814867  0.37375311]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.74037311  1.87798993 -3.34072674 -0.05762481  1.98639409 -3.08432947\n",
      "  -0.97184122]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:95 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.78852994]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 95 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93675074 -1.2503297   0.25260137  1.10781806 -0.0824936  -0.3525303\n",
      "   1.10505909  0.81567411 -0.57171745]\n",
      " [-0.79440702  1.43560099 -0.41596554 -1.2573572   0.01987774  0.30855501\n",
      "  -1.29434264 -1.00782178  0.62057108]\n",
      " [ 0.61276464 -0.43877062  0.95213684 -0.94801371  0.42714607 -0.4793797\n",
      "  -1.07013511  0.80624002  0.74992489]\n",
      " [ 1.01759494 -1.41179126  0.30649823  1.14445178 -0.1098574  -0.26688106\n",
      "   1.17633802  0.5058204  -0.84952275]\n",
      " [-0.99526142  1.39776029 -0.28302991 -1.01168058  0.16398565  0.26170042\n",
      "  -1.204088   -0.88962714  0.68654166]\n",
      " [ 0.06487887  0.32239147  0.79519679 -1.18857503  0.00995181  0.14566196\n",
      "  -1.36839623  0.48814867  0.37375311]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.72274174  1.89263733 -3.33810474 -0.05097363  2.00152817 -3.08173719\n",
      "  -0.96754666]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:95 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.55120461]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 95 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92258512 -1.2503297   0.25260137  1.09365244 -0.0824936  -0.36669592\n",
      "   1.10505909  0.81567411 -0.58588307]\n",
      " [-0.78022882  1.43560099 -0.41596554 -1.243179    0.01987774  0.32273321\n",
      "  -1.29434264 -1.00782178  0.63474928]\n",
      " [ 0.61386634 -0.43877062  0.95213684 -0.94691202  0.42714607 -0.47827801\n",
      "  -1.07013511  0.80624002  0.75102658]\n",
      " [ 1.00387663 -1.41179126  0.30649823  1.13073347 -0.1098574  -0.28059937\n",
      "   1.17633802  0.5058204  -0.86324105]\n",
      " [-0.98145903  1.39776029 -0.28302991 -0.99787819  0.16398565  0.27550281\n",
      "  -1.204088   -0.88962714  0.70034406]\n",
      " [ 0.07429269  0.32239147  0.79519679 -1.17916121  0.00995181  0.15507577\n",
      "  -1.36839623  0.48814867  0.38316693]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.79091971  1.84122785 -3.35484395 -0.08396016  1.95108074 -3.09929697\n",
      "  -0.99163836]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:95 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.74540509]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 95 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92799044 -1.24492438  0.25260137  1.09905776 -0.0824936  -0.3612906\n",
      "   1.11046442  0.81567411 -0.58588307]\n",
      " [-0.78563663  1.43019318 -0.41596554 -1.24858681  0.01987774  0.3173254\n",
      "  -1.29975045 -1.00782178  0.63474928]\n",
      " [ 0.60930043 -0.44333652  0.95213684 -0.95147792  0.42714607 -0.48284392\n",
      "  -1.07470102  0.80624002  0.75102658]\n",
      " [ 1.00927437 -1.40639352  0.30649823  1.13613121 -0.1098574  -0.27520163\n",
      "   1.18173576  0.5058204  -0.86324105]\n",
      " [-0.98686518  1.39235413 -0.28302991 -1.00328434  0.16398565  0.27009666\n",
      "  -1.20949415 -0.88962714  0.70034406]\n",
      " [ 0.06921425  0.31731303  0.79519679 -1.18423965  0.00995181  0.14999733\n",
      "  -1.37347467  0.48814867  0.38316693]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.76676167  1.86099422 -3.35064697 -0.08179894  1.97124295 -3.09492642\n",
      "  -0.98874797]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:95 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.83017517]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 95 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93056882 -1.242346    0.25260137  1.10163615 -0.0824936  -0.3612906\n",
      "   1.1130428   0.81567411 -0.58588307]\n",
      " [-0.78820888  1.42762092 -0.41596554 -1.25115906  0.01987774  0.3173254\n",
      "  -1.30232271 -1.00782178  0.63474928]\n",
      " [ 0.60670518 -0.44593178  0.95213684 -0.95407317  0.42714607 -0.48284392\n",
      "  -1.07729627  0.80624002  0.75102658]\n",
      " [ 1.01183706 -1.40383083  0.30649823  1.1386939  -0.1098574  -0.27520163\n",
      "   1.18429845  0.5058204  -0.86324105]\n",
      " [-0.98948514  1.38973417 -0.28302991 -1.0059043   0.16398565  0.27009666\n",
      "  -1.21211411 -0.88962714  0.70034406]\n",
      " [ 0.06682588  0.31492466  0.79519679 -1.18662802  0.00995181  0.14999733\n",
      "  -1.37586303  0.48814867  0.38316693]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.75479034  1.8713979  -3.34909451 -0.08018667  1.9816846  -3.09323887\n",
      "  -0.98752249]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:95 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.82954407]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 95 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93314033 -1.242346    0.25517288  1.10163615 -0.0824936  -0.35871909\n",
      "   1.11561431  0.81567411 -0.58588307]\n",
      " [-0.79059863  1.42762092 -0.41835528 -1.25115906  0.01987774  0.31493566\n",
      "  -1.30471245 -1.00782178  0.63474928]\n",
      " [ 0.60670127 -0.44593178  0.95213293 -0.95407317  0.42714607 -0.48284783\n",
      "  -1.07730018  0.80624002  0.75102658]\n",
      " [ 1.0141951  -1.40383083  0.30885627  1.1386939  -0.1098574  -0.27284359\n",
      "   1.18665648  0.5058204  -0.86324105]\n",
      " [-0.99185397  1.38973417 -0.28539873 -1.0059043   0.16398565  0.26772783\n",
      "  -1.21448294 -0.88962714  0.70034406]\n",
      " [ 0.06576518  0.31492466  0.79413609 -1.18662802  0.00995181  0.14893663\n",
      "  -1.37692374  0.48814867  0.38316693]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.74273905  1.88192824 -3.34788062 -0.07416493  1.99256314 -3.09205238\n",
      "  -0.9825811 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:95 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.07590958]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 96 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93300431 -1.24248202  0.25517288  1.10150012 -0.0824936  -0.35871909\n",
      "   1.11561431  0.81567411 -0.5860191 ]\n",
      " [-0.79061235  1.4276072  -0.41835528 -1.25117278  0.01987774  0.31493566\n",
      "  -1.30471245 -1.00782178  0.63473556]\n",
      " [ 0.6067294  -0.44590365  0.95213293 -0.95404505  0.42714607 -0.48284783\n",
      "  -1.07730018  0.80624002  0.75105471]\n",
      " [ 1.01427085 -1.40375508  0.30885627  1.13876966 -0.1098574  -0.27284359\n",
      "   1.18665648  0.5058204  -0.8631653 ]\n",
      " [-0.99191528  1.38967285 -0.28539873 -1.00596562  0.16398565  0.26772783\n",
      "  -1.21448294 -0.88962714  0.70028274]\n",
      " [ 0.06603437  0.31519385  0.79413609 -1.18635883  0.00995181  0.14893663\n",
      "  -1.37692374  0.48814867  0.38343612]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.74540148  1.88046004 -3.34922555 -0.07546801  1.99130784 -3.093445\n",
      "  -0.98363504]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:96 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.87039315]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 96 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93439481 -1.24248202  0.25656338  1.10150012 -0.0824936  -0.35871909\n",
      "   1.11700481  0.81567411 -0.5860191 ]\n",
      " [-0.79188569  1.4276072  -0.41962863 -1.25117278  0.01987774  0.31493566\n",
      "  -1.3059858  -1.00782178  0.63473556]\n",
      " [ 0.60756039 -0.44590365  0.95296392 -0.95404505  0.42714607 -0.48284783\n",
      "  -1.07646919  0.80624002  0.75105471]\n",
      " [ 1.01554644 -1.40375508  0.31013186  1.13876966 -0.1098574  -0.27284359\n",
      "   1.18793207  0.5058204  -0.8631653 ]\n",
      " [-0.99320121  1.38967285 -0.28668466 -1.00596562  0.16398565  0.26772783\n",
      "  -1.21576886 -0.88962714  0.70028274]\n",
      " [ 0.06515029  0.31519385  0.79325201 -1.18635883  0.00995181  0.14893663\n",
      "  -1.37780781  0.48814867  0.38343612]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.73809107  1.88710659 -3.34867797 -0.07094933  1.99806868 -3.09288623\n",
      "  -0.98090378]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:96 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.51995453]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 96 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92121726 -1.25565957  0.24338583  1.10150012 -0.0824936  -0.35871909\n",
      "   1.10382726  0.81567411 -0.5860191 ]\n",
      " [-0.77856719  1.44092571 -0.40631013 -1.25117278  0.01987774  0.31493566\n",
      "  -1.2926673  -1.00782178  0.63473556]\n",
      " [ 0.60694169 -0.44652234  0.95234522 -0.95404505  0.42714607 -0.48284783\n",
      "  -1.07708789  0.80624002  0.75105471]\n",
      " [ 1.00211882 -1.4171827   0.29670424  1.13876966 -0.1098574  -0.27284359\n",
      "   1.17450445  0.5058204  -0.8631653 ]\n",
      " [-0.97979433  1.40307973 -0.27327778 -1.00596562  0.16398565  0.26772783\n",
      "  -1.20236198 -0.88962714  0.70028274]\n",
      " [ 0.06842886  0.31847242  0.79653058 -1.18635883  0.00995181  0.14893663\n",
      "  -1.37452925  0.48814867  0.38343612]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.80298187  1.83884493 -3.36500699 -0.10401357  1.94926418 -3.10901943\n",
      "  -1.01004778]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:96 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.71598138]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 96 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92731206 -1.25565957  0.24948064  1.10150012 -0.0824936  -0.35871909\n",
      "   1.10382726  0.82176892 -0.5860191 ]\n",
      " [-0.7842867   1.44092571 -0.41202963 -1.25117278  0.01987774  0.31493566\n",
      "  -1.2926673  -1.01354128  0.63473556]\n",
      " [ 0.61230216 -0.44652234  0.95770569 -0.95404505  0.42714607 -0.48284783\n",
      "  -1.07708789  0.81160049  0.75105471]\n",
      " [ 1.00844158 -1.4171827   0.30302699  1.13876966 -0.1098574  -0.27284359\n",
      "   1.17450445  0.51214316 -0.8631653 ]\n",
      " [-0.98560994  1.40307973 -0.27909339 -1.00596562  0.16398565  0.26772783\n",
      "  -1.20236198 -0.89544275  0.70028274]\n",
      " [ 0.07480544  0.31847242  0.80290717 -1.18635883  0.00995181  0.14893663\n",
      "  -1.37452925  0.49452526  0.38343612]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.77410399  1.86422023 -3.36210742 -0.07761455  1.974062   -3.10598681\n",
      "  -0.98710034]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:96 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.64044681]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 96 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93654547 -1.25565957  0.24948064  1.11073353 -0.0824936  -0.35871909\n",
      "   1.10382726  0.82176892 -0.57678569]\n",
      " [-0.79348436  1.44092571 -0.41202963 -1.26037044  0.01987774  0.31493566\n",
      "  -1.2926673  -1.01354128  0.6255379 ]\n",
      " [ 0.61636574 -0.44652234  0.95770569 -0.94998146  0.42714607 -0.48284783\n",
      "  -1.07708789  0.81160049  0.7551183 ]\n",
      " [ 1.01746938 -1.4171827   0.30302699  1.14779746 -0.1098574  -0.27284359\n",
      "   1.17450445  0.51214316 -0.8541375 ]\n",
      " [-0.99465143  1.40307973 -0.27909339 -1.01500711  0.16398565  0.26772783\n",
      "  -1.20236198 -0.89544275  0.69124125]\n",
      " [ 0.06818651  0.31847242  0.80290717 -1.19297777  0.00995181  0.14893663\n",
      "  -1.37452925  0.49452526  0.37681718]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.73270598  1.89770653 -3.35392298 -0.05273755  2.00648233 -3.09705999\n",
      "  -0.97362082]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:96 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.04460857]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 96 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93656479 -1.25565957  0.24948064  1.11073353 -0.08247428 -0.35869976\n",
      "   1.10382726  0.82176892 -0.57676637]\n",
      " [-0.79352374  1.44092571 -0.41202963 -1.26037044  0.01983836  0.31489628\n",
      "  -1.2926673  -1.01354128  0.62549852]\n",
      " [ 0.61615714 -0.44652234  0.95770569 -0.94998146  0.42693747 -0.48305643\n",
      "  -1.07708789  0.81160049  0.7549097 ]\n",
      " [ 1.01752089 -1.4171827   0.30302699  1.14779746 -0.10980589 -0.27279208\n",
      "   1.17450445  0.51214316 -0.85408599]\n",
      " [-0.9946818   1.40307973 -0.27909339 -1.01500711  0.16395529  0.26769747\n",
      "  -1.20236198 -0.89544275  0.69121088]\n",
      " [ 0.06805532  0.31847242  0.80290717 -1.19297777  0.00982062  0.14880545\n",
      "  -1.37452925  0.49452526  0.376686  ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.73365656  1.89725059 -3.35443783 -0.05348706  2.00605897 -3.09756573\n",
      "  -0.97423541]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:96 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.00226459]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 96 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93656536 -1.255659    0.24948064  1.11073353 -0.08247372 -0.3586992\n",
      "   1.10382726  0.82176892 -0.57676581]\n",
      " [-0.79352431  1.44092513 -0.41202963 -1.26037044  0.01983779  0.31489571\n",
      "  -1.2926673  -1.01354128  0.62549795]\n",
      " [ 0.61615668 -0.4465228   0.95770569 -0.94998146  0.42693701 -0.48305689\n",
      "  -1.07708789  0.81160049  0.75490923]\n",
      " [ 1.01752146 -1.41718213  0.30302699  1.14779746 -0.10980532 -0.27279151\n",
      "   1.17450445  0.51214316 -0.85408542]\n",
      " [-0.99468237  1.40307916 -0.27909339 -1.01500711  0.16395471  0.26769689\n",
      "  -1.20236198 -0.89544275  0.69121031]\n",
      " [ 0.06805484  0.31847194  0.80290717 -1.19297777  0.00982014  0.14880497\n",
      "  -1.37452925  0.49452526  0.37668552]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.73365912  1.89725005 -3.35443996 -0.05348886  2.00605855 -3.09756783\n",
      "  -0.97423724]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:96 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.19352912]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 96 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93530573 -1.25691863  0.24948064  1.11073353 -0.08373334 -0.35995883\n",
      "   1.10256763  0.82176892 -0.57676581]\n",
      " [-0.79237966  1.44206978 -0.41202963 -1.26037044  0.02098243  0.31604035\n",
      "  -1.29152265 -1.01354128  0.62549795]\n",
      " [ 0.61906568 -0.44361381  0.95770569 -0.94998146  0.429846   -0.4801479\n",
      "  -1.07417889  0.81160049  0.75490923]\n",
      " [ 1.01609601 -1.41860758  0.30302699  1.14779746 -0.11123077 -0.27421696\n",
      "   1.173079    0.51214316 -0.85408542]\n",
      " [-0.99335833  1.4044032  -0.27909339 -1.01500711  0.16527875  0.26902093\n",
      "  -1.20103794 -0.89544275  0.69121031]\n",
      " [ 0.07070399  0.32112109  0.80290717 -1.19297777  0.01246929  0.15145412\n",
      "  -1.3718801   0.49452526  0.37668552]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.74876171  1.88841421 -3.36082812 -0.05765964  1.99704497 -3.10376592\n",
      "  -0.97882506]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:96 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.9351979]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 96 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93554965 -1.25691863  0.24948064  1.11097745 -0.08373334 -0.35995883\n",
      "   1.10281156  0.82176892 -0.57676581]\n",
      " [-0.79259585  1.44206978 -0.41202963 -1.26058663  0.02098243  0.31604035\n",
      "  -1.29173884 -1.01354128  0.62549795]\n",
      " [ 0.61862921 -0.44361381  0.95770569 -0.95041793  0.429846   -0.4801479\n",
      "  -1.07461536  0.81160049  0.75490923]\n",
      " [ 1.0163132  -1.41860758  0.30302699  1.14801465 -0.11123077 -0.27421696\n",
      "   1.17329619  0.51214316 -0.85408542]\n",
      " [-0.99359344  1.4044032  -0.27909339 -1.01524222  0.16527875  0.26902093\n",
      "  -1.20127305 -0.89544275  0.69121031]\n",
      " [ 0.07035895  0.32112109  0.80290717 -1.19332281  0.01246929  0.15145412\n",
      "  -1.37222514  0.49452526  0.37668552]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.74679811  1.89029701 -3.3607612  -0.05727279  1.99894117 -3.10368971\n",
      "  -0.97867529]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:96 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.7904813]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 96 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93943034 -1.25691863  0.24948064  1.11097745 -0.07985266 -0.35607814\n",
      "   1.10669224  0.82176892 -0.57676581]\n",
      " [-0.79642688  1.44206978 -0.41202963 -1.26058663  0.01715141  0.31220933\n",
      "  -1.29556986 -1.01354128  0.62549795]\n",
      " [ 0.61656807 -0.44361381  0.95770569 -0.95041793  0.42778487 -0.48220903\n",
      "  -1.0766765   0.81160049  0.75490923]\n",
      " [ 1.02011232 -1.41860758  0.30302699  1.14801465 -0.10743166 -0.27041785\n",
      "   1.17709531  0.51214316 -0.85408542]\n",
      " [-0.99741762  1.4044032  -0.27909339 -1.01524222  0.16145457  0.26519675\n",
      "  -1.20509724 -0.89544275  0.69121031]\n",
      " [ 0.06673015  0.32112109  0.80290717 -1.19332281  0.00884049  0.14782532\n",
      "  -1.37585394  0.49452526  0.37668552]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.72944781  1.90471983 -3.35818665 -0.05074797  2.01383909 -3.10114408\n",
      "  -0.9744644 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:96 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.54957901]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 96 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92531218 -1.25691863  0.24948064  1.09685929 -0.07985266 -0.3701963\n",
      "   1.10669224  0.82176892 -0.59088396]\n",
      " [-0.7822992   1.44206978 -0.41202963 -1.24645896  0.01715141  0.32633701\n",
      "  -1.29556986 -1.01354128  0.63962562]\n",
      " [ 0.61760698 -0.44361381  0.95770569 -0.94937903  0.42778487 -0.48117012\n",
      "  -1.0766765   0.81160049  0.75594814]\n",
      " [ 1.00643861 -1.41860758  0.30302699  1.13434094 -0.10743166 -0.28409155\n",
      "   1.17709531  0.51214316 -0.86775912]\n",
      " [-0.98366235  1.4044032  -0.27909339 -1.00148695  0.16145457  0.27895202\n",
      "  -1.20509724 -0.89544275  0.70496558]\n",
      " [ 0.07609427  0.32112109  0.80290717 -1.18395869  0.00884049  0.15718943\n",
      "  -1.37585394  0.49452526  0.38604963]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.79746973  1.85346283 -3.37492956 -0.08371937  1.96353357 -3.11869552\n",
      "  -0.99853516]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:96 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.74584601]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 96 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93070158 -1.25152923  0.24948064  1.1022487  -0.07985266 -0.3648069\n",
      "   1.11208164  0.82176892 -0.59088396]\n",
      " [-0.78769167  1.43667731 -0.41202963 -1.25185143  0.01715141  0.32094453\n",
      "  -1.30096234 -1.01354128  0.63962562]\n",
      " [ 0.61307735 -0.44814344  0.95770569 -0.95390865  0.42778487 -0.48569975\n",
      "  -1.08120612  0.81160049  0.75594814]\n",
      " [ 1.01182177 -1.41322442  0.30302699  1.1397241  -0.10743166 -0.2787084\n",
      "   1.18247846  0.51214316 -0.86775912]\n",
      " [-0.98905254  1.39901301 -0.27909339 -1.00687714  0.16145457  0.27356183\n",
      "  -1.21048743 -0.89544275  0.70496558]\n",
      " [ 0.07104314  0.31606996  0.80290717 -1.18900982  0.00884049  0.1521383\n",
      "  -1.38090507  0.49452526  0.38604963]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.77338104  1.8731634  -3.37073253 -0.08158974  1.98362664 -3.11432584\n",
      "  -0.99567736]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:96 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.83160232]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 96 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93324068 -1.24899013  0.24948064  1.1047878  -0.07985266 -0.3648069\n",
      "   1.11462074  0.82176892 -0.59088396]\n",
      " [-0.7902252   1.43414378 -0.41202963 -1.25438496  0.01715141  0.32094453\n",
      "  -1.30349586 -1.01354128  0.63962562]\n",
      " [ 0.61052613 -0.45069466  0.95770569 -0.95645987  0.42778487 -0.48569975\n",
      "  -1.08375735  0.81160049  0.75594814]\n",
      " [ 1.01434587 -1.41070032  0.30302699  1.1422482  -0.10743166 -0.2787084\n",
      "   1.18500257  0.51214316 -0.86775912]\n",
      " [-0.99163303  1.39643252 -0.27909339 -1.00945763  0.16145457  0.27356183\n",
      "  -1.21306792 -0.89544275  0.70496558]\n",
      " [ 0.06869989  0.31372671  0.80290717 -1.19135307  0.00884049  0.1521383\n",
      "  -1.38324831  0.49452526  0.38604963]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.76158985  1.88341177 -3.36920349 -0.08001537  1.99391124 -3.11266384\n",
      "  -0.99448285]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:96 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.83001036]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 96 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93580099 -1.24899013  0.25204094  1.1047878  -0.07985266 -0.36224659\n",
      "   1.11718105  0.82176892 -0.59088396]\n",
      " [-0.79260677  1.43414378 -0.4144112  -1.25438496  0.01715141  0.31856297\n",
      "  -1.30587743 -1.01354128  0.63962562]\n",
      " [ 0.61052246 -0.45069466  0.95770202 -0.95645987  0.42778487 -0.48570342\n",
      "  -1.08376102  0.81160049  0.75594814]\n",
      " [ 1.01669548 -1.41070032  0.30537661  1.1422482  -0.10743166 -0.27635878\n",
      "   1.18735218  0.51214316 -0.86775912]\n",
      " [-0.9939938   1.39643252 -0.28145415 -1.00945763  0.16145457  0.27120106\n",
      "  -1.21542868 -0.89544275  0.70496558]\n",
      " [ 0.06765617  0.31372671  0.80186345 -1.19135307  0.00884049  0.15109458\n",
      "  -1.38429203  0.49452526  0.38604963]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.74959766  1.8938873  -3.3679908  -0.07402295  2.00473245 -3.11147859\n",
      "  -0.9895531 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:96 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.07367818]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 97 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93567609 -1.24911503  0.25204094  1.1046629  -0.07985266 -0.36224659\n",
      "   1.11718105  0.82176892 -0.59100886]\n",
      " [-0.7926236   1.43412695 -0.4144112  -1.25440179  0.01715141  0.31856297\n",
      "  -1.30587743 -1.01354128  0.63960879]\n",
      " [ 0.61054802 -0.4506691   0.95770202 -0.95643431  0.42778487 -0.48570342\n",
      "  -1.08376102  0.81160049  0.7559737 ]\n",
      " [ 1.01677034 -1.41062546  0.30537661  1.14232306 -0.10743166 -0.27635878\n",
      "   1.18735218  0.51214316 -0.86768427]\n",
      " [-0.99405522  1.3963711  -0.28145415 -1.00951905  0.16145457  0.27120106\n",
      "  -1.21542868 -0.89544275  0.70490416]\n",
      " [ 0.06791101  0.31398155  0.80186345 -1.19109823  0.00884049  0.15109458\n",
      "  -1.38429203  0.49452526  0.38630447]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.75211192  1.89250443 -3.36926476 -0.07525451  2.00355036 -3.11279724\n",
      "  -0.99054769]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:97 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.8714416]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 97 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93704524 -1.24911503  0.2534101   1.1046629  -0.07985266 -0.36224659\n",
      "   1.11855021  0.82176892 -0.59100886]\n",
      " [-0.79387837  1.43412695 -0.41566597 -1.25440179  0.01715141  0.31856297\n",
      "  -1.3071322  -1.01354128  0.63960879]\n",
      " [ 0.611371   -0.4506691   0.95852501 -0.95643431  0.42778487 -0.48570342\n",
      "  -1.08293803  0.81160049  0.7559737 ]\n",
      " [ 1.01802705 -1.41062546  0.30663332  1.14232306 -0.10743166 -0.27635878\n",
      "   1.18860889  0.51214316 -0.86768427]\n",
      " [-0.99532243  1.3963711  -0.28272137 -1.00951905  0.16145457  0.27120106\n",
      "  -1.21669589 -0.89544275  0.70490416]\n",
      " [ 0.06704342  0.31398155  0.80099585 -1.19109823  0.00884049  0.15109458\n",
      "  -1.38515963  0.49452526  0.38630447]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.74491064  1.89905242 -3.36872497 -0.07079831  2.01021013 -3.11224637\n",
      "  -0.98785344]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:97 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.516017]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 97 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92399328 -1.26216699  0.24035814  1.1046629  -0.07985266 -0.36224659\n",
      "   1.10549824  0.82176892 -0.59100886]\n",
      " [-0.78069446  1.44731086 -0.40248207 -1.25440179  0.01715141  0.31856297\n",
      "  -1.2939483  -1.01354128  0.63960879]\n",
      " [ 0.61078662 -0.45125348  0.95794062 -0.95643431  0.42778487 -0.48570342\n",
      "  -1.08352242  0.81160049  0.7559737 ]\n",
      " [ 1.00473206 -1.42392045  0.29333833  1.14232306 -0.10743166 -0.27635878\n",
      "   1.1753139   0.51214316 -0.86768427]\n",
      " [-0.98205061  1.40964292 -0.26944955 -1.00951905  0.16145457  0.27120106\n",
      "  -1.20342407 -0.89544275  0.70490416]\n",
      " [ 0.07028225  0.31722038  0.80423468 -1.19109823  0.00884049  0.15109458\n",
      "  -1.3819208   0.49452526  0.38630447]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.80934658  1.85119781 -3.3850288  -0.10360079  1.96183452 -3.1283583\n",
      "  -1.01681026]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:97 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.71769481]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 97 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93002023 -1.26216699  0.24638509  1.1046629  -0.07985266 -0.36224659\n",
      "   1.10549824  0.82779587 -0.59100886]\n",
      " [-0.78635087  1.44731086 -0.40813847 -1.25440179  0.01715141  0.31856297\n",
      "  -1.2939483  -1.01919769  0.63960879]\n",
      " [ 0.61606332 -0.45125348  0.96321732 -0.95643431  0.42778487 -0.48570342\n",
      "  -1.08352242  0.81687718  0.7559737 ]\n",
      " [ 1.01098796 -1.42392045  0.29959423  1.14232306 -0.10743166 -0.27635878\n",
      "   1.1753139   0.51839906 -0.86768427]\n",
      " [-0.9878021   1.40964292 -0.27520103 -1.00951905  0.16145457  0.27120106\n",
      "  -1.20342407 -0.90119423  0.70490416]\n",
      " [ 0.0766116   0.31722038  0.81056403 -1.19109823  0.00884049  0.15109458\n",
      "  -1.3819208   0.50085461  0.38630447]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.7807478   1.87634564 -3.38216761 -0.0774239   1.98641196 -3.12536637\n",
      "  -0.99401058]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:97 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.64138143]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 97 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93921971 -1.26216699  0.24638509  1.11386237 -0.07985266 -0.36224659\n",
      "   1.10549824  0.82779587 -0.58180939]\n",
      " [-0.79551455  1.44731086 -0.40813847 -1.26356547  0.01715141  0.31856297\n",
      "  -1.2939483  -1.01919769  0.63044511]\n",
      " [ 0.62016867 -0.45125348  0.96321732 -0.95232896  0.42778487 -0.48570342\n",
      "  -1.08352242  0.81687718  0.76007905]\n",
      " [ 1.01998501 -1.42392045  0.29959423  1.15132011 -0.10743166 -0.27635878\n",
      "   1.1753139   0.51839906 -0.85868722]\n",
      " [-0.99681185  1.40964292 -0.27520103 -1.0185288   0.16145457  0.27120106\n",
      "  -1.20342407 -0.90119423  0.69589441]\n",
      " [ 0.07001698  0.31722038  0.81056403 -1.19769284  0.00884049  0.15109458\n",
      "  -1.3819208   0.50085461  0.37970985]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.73950464  1.9097123  -3.37401654 -0.05257776  2.01872211 -3.11648078\n",
      "  -0.98058206]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:97 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.04328696]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 97 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93923865 -1.26216699  0.24638509  1.11386237 -0.07983372 -0.36222765\n",
      "   1.10549824  0.82779587 -0.58179044]\n",
      " [-0.79555251  1.44731086 -0.40813847 -1.26356547  0.01711345  0.31852501\n",
      "  -1.2939483  -1.01919769  0.63040715]\n",
      " [ 0.61997174 -0.45125348  0.96321732 -0.95232896  0.42758794 -0.48590035\n",
      "  -1.08352242  0.81687718  0.75988213]\n",
      " [ 1.02003426 -1.42392045  0.29959423  1.15132011 -0.10738241 -0.27630954\n",
      "   1.1753139   0.51839906 -0.85863797]\n",
      " [-0.99684124  1.40964292 -0.27520103 -1.0185288   0.16142517  0.27117167\n",
      "  -1.20342407 -0.90119423  0.69586502]\n",
      " [ 0.06989231  0.31722038  0.81056403 -1.19769284  0.00871583  0.15096992\n",
      "  -1.3819208   0.50085461  0.37958519]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.74040096  1.9092831  -3.37450284 -0.05328547  2.01832359 -3.11695842\n",
      "  -0.98116276]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:97 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.00214215]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 97 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93923916 -1.26216648  0.24638509  1.11386237 -0.07983321 -0.36222715\n",
      "   1.10549824  0.82779587 -0.58178994]\n",
      " [-0.79555302  1.44731034 -0.40813847 -1.26356547  0.01711294  0.31852449\n",
      "  -1.2939483  -1.01919769  0.63040664]\n",
      " [ 0.61997133 -0.4512539   0.96321732 -0.95232896  0.42758752 -0.48590076\n",
      "  -1.08352242  0.81687718  0.75988171]\n",
      " [ 1.02003477 -1.42391994  0.29959423  1.15132011 -0.1073819  -0.27630903\n",
      "   1.1753139   0.51839906 -0.85863746]\n",
      " [-0.99684175  1.40964241 -0.27520103 -1.0185288   0.16142466  0.27117116\n",
      "  -1.20342407 -0.90119423  0.6958645 ]\n",
      " [ 0.06989188  0.31721995  0.81056403 -1.19769284  0.0087154   0.15096949\n",
      "  -1.3819208   0.50085461  0.37958476]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.74040325  1.90928263 -3.37450476 -0.05328709  2.01832322 -3.1169603\n",
      "  -0.9811644 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:97 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.19047242]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 97 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93802463 -1.26338101  0.24638509  1.11386237 -0.08104774 -0.36344168\n",
      "   1.10428371  0.82779587 -0.58178994]\n",
      " [-0.79445364  1.44840973 -0.40813847 -1.26356547  0.01821232  0.31962388\n",
      "  -1.29284891 -1.01919769  0.63040664]\n",
      " [ 0.62281562 -0.4484096   0.96321732 -0.95232896  0.43043182 -0.48305647\n",
      "  -1.08067812  0.81687718  0.75988171]\n",
      " [ 1.0186635  -1.42529121  0.29959423  1.15132011 -0.10875317 -0.27768029\n",
      "   1.17394264  0.51839906 -0.85863746]\n",
      " [-0.99556861  1.41091556 -0.27520103 -1.0185288   0.16269781  0.27244431\n",
      "  -1.20215093 -0.90119423  0.6958645 ]\n",
      " [ 0.07247972  0.31980779  0.81056403 -1.19769284  0.01130324  0.15355733\n",
      "  -1.37933296  0.50085461  0.37958476]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.75508798  1.90070213 -3.38073044 -0.05731451  2.00957497 -3.12300215\n",
      "  -0.98560745]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:97 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.93633987]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 97 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93825925 -1.26338101  0.24638509  1.11409699 -0.08104774 -0.36344168\n",
      "   1.10451833  0.82779587 -0.58178994]\n",
      " [-0.79466166  1.44840973 -0.40813847 -1.26377349  0.01821232  0.31962388\n",
      "  -1.29305693 -1.01919769  0.63040664]\n",
      " [ 0.62239366 -0.4484096   0.96321732 -0.95275092  0.43043182 -0.48305647\n",
      "  -1.08110008  0.81687718  0.75988171]\n",
      " [ 1.01887244 -1.42529121  0.29959423  1.15152905 -0.10875317 -0.27768029\n",
      "   1.17415157  0.51839906 -0.85863746]\n",
      " [-0.99579483  1.41091556 -0.27520103 -1.01875502  0.16269781  0.27244431\n",
      "  -1.20237715 -0.90119423  0.6958645 ]\n",
      " [ 0.07214788  0.31980779  0.81056403 -1.19802469  0.01130324  0.15355733\n",
      "  -1.3796648   0.50085461  0.37958476]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.75319067  1.90252194 -3.38066618 -0.05694225  2.0114076  -3.12292899\n",
      "  -0.98546413]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:97 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.79241422]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 97 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94207747 -1.26338101  0.24638509  1.11409699 -0.07722952 -0.35962346\n",
      "   1.10833655  0.82779587 -0.58178994]\n",
      " [-0.79843024  1.44840973 -0.40813847 -1.26377349  0.01444374  0.31585529\n",
      "  -1.29682552 -1.01919769  0.63040664]\n",
      " [ 0.6203478  -0.4484096   0.96321732 -0.95275092  0.42838596 -0.48510233\n",
      "  -1.08314594  0.81687718  0.75988171]\n",
      " [ 1.02260941 -1.42529121  0.29959423  1.15152905 -0.1050162  -0.27394332\n",
      "   1.17788855  0.51839906 -0.85863746]\n",
      " [-0.99955667  1.41091556 -0.27520103 -1.01875502  0.15893597  0.26868247\n",
      "  -1.20613899 -0.90119423  0.6958645 ]\n",
      " [ 0.06857092  0.31980779  0.81056403 -1.19802469  0.00772628  0.14998037\n",
      "  -1.38324175  0.50085461  0.37958476]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.73611737  1.91672315 -3.37813827 -0.05054179  2.0260727  -3.12042926\n",
      "  -0.98133524]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:97 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: [0.] Net Result: [[0.54791113]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 97 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92800827 -1.26338101  0.24638509  1.1000278  -0.07722952 -0.37369265\n",
      "   1.10833655  0.82779587 -0.59585914]\n",
      " [-0.78435456  1.44840973 -0.40813847 -1.24969781  0.01444374  0.32993098\n",
      "  -1.29682552 -1.01919769  0.64448232]\n",
      " [ 0.62132458 -0.4484096   0.96321732 -0.95177414  0.42838596 -0.48412556\n",
      "  -1.08314594  0.81687718  0.76085849]\n",
      " [ 1.00898178 -1.42529121  0.29959423  1.13790142 -0.1050162  -0.28757095\n",
      "   1.17788855  0.51839906 -0.87226509]\n",
      " [-0.98584998  1.41091556 -0.27520103 -1.00504833  0.15893597  0.28238915\n",
      "  -1.20613899 -0.90119423  0.70957119]\n",
      " [ 0.07788468  0.31980779  0.81056403 -1.18871093  0.00772628  0.15929413\n",
      "  -1.38324175  0.50085461  0.38889851]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.8039774   1.86562349 -3.39488374 -0.08349449  1.97591392 -3.13797127\n",
      "  -1.00538275]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:97 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.74629743]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 97 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93338139 -1.2580079   0.24638509  1.10540091 -0.07722952 -0.36831954\n",
      "   1.11370967  0.82779587 -0.59585914]\n",
      " [-0.78973129  1.443033   -0.40813847 -1.25507453  0.01444374  0.32455425\n",
      "  -1.30220224 -1.01919769  0.64448232]\n",
      " [ 0.61683135 -0.45290283  0.96321732 -0.95626737  0.42838596 -0.48861878\n",
      "  -1.08763917  0.81687718  0.76085849]\n",
      " [ 1.01434992 -1.41992306  0.29959423  1.14326956 -0.1050162  -0.28220281\n",
      "   1.18325669  0.51839906 -0.87226509]\n",
      " [-0.99122382  1.40554172 -0.27520103 -1.01042217  0.15893597  0.27701531\n",
      "  -1.21151283 -0.90119423  0.70957119]\n",
      " [ 0.07286123  0.31478435  0.81056403 -1.19373438  0.00772628  0.15427068\n",
      "  -1.3882652   0.50085461  0.38889851]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.77995969  1.88525724 -3.39068736 -0.08139597  1.99593692 -3.13360319\n",
      "  -1.00255729]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:97 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.83302829]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 97 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93588143 -1.25550785  0.24638509  1.10790096 -0.07722952 -0.36831954\n",
      "   1.11620971  0.82779587 -0.59585914]\n",
      " [-0.79222628  1.44053801 -0.40813847 -1.25756953  0.01444374  0.32455425\n",
      "  -1.30469724 -1.01919769  0.64448232]\n",
      " [ 0.61432379 -0.45541039  0.96321732 -0.95877493  0.42838596 -0.48861878\n",
      "  -1.09014673  0.81687718  0.76085849]\n",
      " [ 1.01683562 -1.41743736  0.29959423  1.14575526 -0.1050162  -0.28220281\n",
      "   1.18574239  0.51839906 -0.87226509]\n",
      " [-0.99376506  1.40300048 -0.27520103 -1.01296341  0.15893597  0.27701531\n",
      "  -1.21405406 -0.90119423  0.70957119]\n",
      " [ 0.07056264  0.31248575  0.81056403 -1.19603297  0.00772628  0.15427068\n",
      "  -1.39056379  0.50085461  0.38889851]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.76834746  1.89535136 -3.38918173 -0.07985861  2.00606563 -3.13196672\n",
      "  -1.00139306]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:97 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.83046653]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 97 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.9384308  -1.25550785  0.24893446  1.10790096 -0.07722952 -0.36577017\n",
      "   1.11875908  0.82779587 -0.59585914]\n",
      " [-0.79459984  1.44053801 -0.41051203 -1.25756953  0.01444374  0.32218069\n",
      "  -1.3070708  -1.01919769  0.64448232]\n",
      " [ 0.61432014 -0.45541039  0.96321366 -0.95877493  0.42838596 -0.48862243\n",
      "  -1.09015038  0.81687718  0.76085849]\n",
      " [ 1.01917699 -1.41743736  0.3019356   1.14575526 -0.1050162  -0.27986144\n",
      "   1.18808376  0.51839906 -0.87226509]\n",
      " [-0.99611793  1.40300048 -0.27755391 -1.01296341  0.15893597  0.27466244\n",
      "  -1.21640694 -0.90119423  0.70957119]\n",
      " [ 0.06953569  0.31248575  0.80953708 -1.19603297  0.00772628  0.15324373\n",
      "  -1.39159075  0.50085461  0.38889851]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.75641299  1.90577333 -3.38797021 -0.07389503  2.01683086 -3.13078267\n",
      "  -0.99647451]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:97 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.07151826]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 98 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93831616 -1.2556225   0.24893446  1.10778631 -0.07722952 -0.36577017\n",
      "   1.11875908  0.82779587 -0.59597378]\n",
      " [-0.79461934  1.44051851 -0.41051203 -1.25758902  0.01444374  0.32218069\n",
      "  -1.3070708  -1.01919769  0.64446282]\n",
      " [ 0.61434328 -0.45538724  0.96321366 -0.95875179  0.42838596 -0.48862243\n",
      "  -1.09015038  0.81687718  0.76088164]\n",
      " [ 1.01925077 -1.41736358  0.3019356   1.14582904 -0.1050162  -0.27986144\n",
      "   1.18808376  0.51839906 -0.87219131]\n",
      " [-0.9961792   1.40293921 -0.27755391 -1.01302468  0.15893597  0.27466244\n",
      "  -1.21640694 -0.90119423  0.70950992]\n",
      " [ 0.06977698  0.31272705  0.80953708 -1.19579168  0.00772628  0.15324373\n",
      "  -1.39159075  0.50085461  0.38913981]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.75878752  1.9044707  -3.38917697 -0.07505914  2.01571756 -3.13203131\n",
      "  -0.99741315]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:98 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.8724765]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 98 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93966438 -1.2556225   0.25028269  1.10778631 -0.07722952 -0.36577017\n",
      "   1.12010731  0.82779587 -0.59597378]\n",
      " [-0.79585585  1.44051851 -0.41174855 -1.25758902  0.01444374  0.32218069\n",
      "  -1.30830731 -1.01919769  0.64446282]\n",
      " [ 0.61515835 -0.45538724  0.96402873 -0.95875179  0.42838596 -0.48862243\n",
      "  -1.08933532  0.81687718  0.76088164]\n",
      " [ 1.02048893 -1.41736358  0.30317376  1.14582904 -0.1050162  -0.27986144\n",
      "   1.18932192  0.51839906 -0.87219131]\n",
      " [-0.99742801  1.40293921 -0.27880272 -1.01302468  0.15893597  0.27466244\n",
      "  -1.21765575 -0.90119423  0.70950992]\n",
      " [ 0.06892553  0.31272705  0.80868562 -1.19579168  0.00772628  0.15324373\n",
      "  -1.3924422   0.50085461  0.38913981]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.75169331  1.91092198 -3.38864485 -0.07066431  2.02227821 -3.13148823\n",
      "  -0.99475523]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:98 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.51211245]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 98 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.9267381  -1.26854878  0.2373564   1.10778631 -0.07722952 -0.36577017\n",
      "   1.10718102  0.82779587 -0.59597378]\n",
      " [-0.78280656  1.4535678  -0.39869926 -1.25758902  0.01444374  0.32218069\n",
      "  -1.29525802 -1.01919769  0.64446282]\n",
      " [ 0.61460728 -0.45593831  0.96347767 -0.95875179  0.42838596 -0.48862243\n",
      "  -1.08988638  0.81687718  0.76088164]\n",
      " [ 1.00732666 -1.43052585  0.29001149  1.14582904 -0.1050162  -0.27986144\n",
      "   1.17615964  0.51839906 -0.87219131]\n",
      " [-0.98429135  1.41607588 -0.26566606 -1.01302468  0.15893597  0.27466244\n",
      "  -1.20451909 -0.90119423  0.70950992]\n",
      " [ 0.07212522  0.31592674  0.81188531 -1.19579168  0.00772628  0.15324373\n",
      "  -1.38924251  0.50085461  0.38913981]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.8156698   1.86347542 -3.40491912 -0.10320373  1.97433175 -3.14757448\n",
      "  -1.02352196]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:98 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.71939674]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 98 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93269789 -1.26854878  0.2433162   1.10778631 -0.07722952 -0.36577017\n",
      "   1.10718102  0.83375567 -0.59597378]\n",
      " [-0.78840052  1.4535678  -0.40429321 -1.25758902  0.01444374  0.32218069\n",
      "  -1.29525802 -1.02479164  0.64446282]\n",
      " [ 0.61980143 -0.45593831  0.96867181 -0.95875179  0.42838596 -0.48862243\n",
      "  -1.08988638  0.82207133  0.76088164]\n",
      " [ 1.01351627 -1.43052585  0.2962011   1.14582904 -0.1050162  -0.27986144\n",
      "   1.17615964  0.52458867 -0.87219131]\n",
      " [-0.98997937  1.41607588 -0.27135407 -1.01302468  0.15893597  0.27466244\n",
      "  -1.20451909 -0.90688225  0.70950992]\n",
      " [ 0.07840614  0.31592674  0.81816624 -1.19579168  0.00772628  0.15324373\n",
      "  -1.38924251  0.50713553  0.38913981]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.7873478   1.88839716 -3.40209577 -0.07724825  1.9986902  -3.14462266\n",
      "  -1.00087084]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:98 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.64229895]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 98 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94186402 -1.26854878  0.2433162   1.11695244 -0.07722952 -0.36577017\n",
      "   1.10718102  0.83375567 -0.58680765]\n",
      " [-0.79753079  1.4535678  -0.40429321 -1.2667193   0.01444374  0.32218069\n",
      "  -1.29525802 -1.02479164  0.63533255]\n",
      " [ 0.6239485  -0.45593831  0.96867181 -0.95460471  0.42838596 -0.48862243\n",
      "  -1.08988638  0.82207133  0.76502871]\n",
      " [ 1.02248303 -1.43052585  0.2962011   1.1547958  -0.1050162  -0.27986144\n",
      "   1.17615964  0.52458867 -0.86322456]\n",
      " [-0.99895787  1.41607588 -0.27135407 -1.02200319  0.15893597  0.27466244\n",
      "  -1.20451909 -0.90688225  0.70053141]\n",
      " [ 0.07183541  0.31592674  0.81816624 -1.20236241  0.00772628  0.15324373\n",
      "  -1.38924251  0.50713553  0.38256908]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.74625671  1.9216461  -3.39397724 -0.05243148  2.03089185 -3.13577731\n",
      "  -0.98749241]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:98 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.04200365]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 98 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94188256 -1.26854878  0.2433162   1.11695244 -0.07721098 -0.36575162\n",
      "   1.10718102  0.83375567 -0.58678911]\n",
      " [-0.79756736  1.4535678  -0.40429321 -1.2667193   0.01440717  0.32214412\n",
      "  -1.29525802 -1.02479164  0.63529598]\n",
      " [ 0.62376262 -0.45593831  0.96867181 -0.95460471  0.42820008 -0.48880831\n",
      "  -1.08988638  0.82207133  0.76484283]\n",
      " [ 1.02253009 -1.43052585  0.2962011   1.1547958  -0.10496913 -0.27981438\n",
      "   1.17615964  0.52458867 -0.86317749]\n",
      " [-0.9989863   1.41607588 -0.27135407 -1.02200319  0.15890754  0.27463401\n",
      "  -1.20451909 -0.90688225  0.70050298]\n",
      " [ 0.07171697  0.31592674  0.81816624 -1.20236241  0.00760784  0.15312529\n",
      "  -1.38924251  0.50713553  0.38245063]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.74710181  1.92124212 -3.39443655 -0.05309964  2.03051677 -3.13622837\n",
      "  -0.98804102]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:98 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.00202704]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 98 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94188302 -1.26854833  0.2433162   1.11695244 -0.07721053 -0.36575117\n",
      "   1.10718102  0.83375567 -0.58678866]\n",
      " [-0.79756782  1.45356734 -0.40429321 -1.2667193   0.01440671  0.32214366\n",
      "  -1.29525802 -1.02479164  0.63529552]\n",
      " [ 0.62376225 -0.45593868  0.96867181 -0.95460471  0.42819971 -0.48880868\n",
      "  -1.08988638  0.82207133  0.76484246]\n",
      " [ 1.02253055 -1.43052539  0.2962011   1.1547958  -0.10496867 -0.27981392\n",
      "   1.17615964  0.52458867 -0.86317703]\n",
      " [-0.99898676  1.41607542 -0.27135407 -1.02200319  0.15890708  0.27463355\n",
      "  -1.20451909 -0.90688225  0.70050253]\n",
      " [ 0.07171658  0.31592635  0.81816624 -1.20236241  0.00760745  0.1531249\n",
      "  -1.38924251  0.50713553  0.38245025]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.74710386  1.9212417  -3.39443827 -0.05310109  2.03051644 -3.13623006\n",
      "  -0.98804249]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:98 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.18749836]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 98 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94071147 -1.26971988  0.2433162   1.11695244 -0.07838207 -0.36692272\n",
      "   1.10600948  0.83375567 -0.58678866]\n",
      " [-0.79651139  1.45462378 -0.40429321 -1.2667193   0.01546314  0.32320009\n",
      "  -1.29420159 -1.02479164  0.63529552]\n",
      " [ 0.62654363 -0.45315731  0.96867181 -0.95460471  0.43098108 -0.48602731\n",
      "  -1.08710501  0.82207133  0.76484246]\n",
      " [ 1.02121077 -1.43184517  0.2962011   1.1547958  -0.10628845 -0.28113369\n",
      "   1.17483987  0.52458867 -0.86317703]\n",
      " [-0.99776195  1.41730023 -0.27135407 -1.02200319  0.1601319   0.27585837\n",
      "  -1.20329427 -0.90688225  0.70050253]\n",
      " [ 0.074245    0.31845477  0.81816624 -1.20236241  0.01013587  0.15565332\n",
      "  -1.38671409  0.50713553  0.38245025]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.76138586  1.91290678 -3.40050663 -0.05699097  2.02202309 -3.14212053\n",
      "  -0.9923464 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:98 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.93745559]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 98 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94093719 -1.26971988  0.2433162   1.11717815 -0.07838207 -0.36692272\n",
      "   1.10623519  0.83375567 -0.58678866]\n",
      " [-0.79671158  1.45462378 -0.40429321 -1.26691949  0.01546314  0.32320009\n",
      "  -1.29440179 -1.02479164  0.63529552]\n",
      " [ 0.62613564 -0.45315731  0.96867181 -0.95501269  0.43098108 -0.48602731\n",
      "  -1.08751299  0.82207133  0.76484246]\n",
      " [ 1.02141181 -1.43184517  0.2962011   1.15499683 -0.10628845 -0.28113369\n",
      "   1.1750409   0.52458867 -0.86317703]\n",
      " [-0.99797966  1.41730023 -0.27135407 -1.0222209   0.1601319   0.27585837\n",
      "  -1.20351199 -0.90688225  0.70050253]\n",
      " [ 0.07392579  0.31845477  0.81816624 -1.20268162  0.01013587  0.15565332\n",
      "  -1.3870333   0.50713553  0.38245025]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.75955229  1.91466599 -3.40044492 -0.05663265  2.02379456 -3.14205029\n",
      "  -0.9922092 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:98 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.79432836]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 98 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94469384 -1.26971988  0.2433162   1.11717815 -0.07462542 -0.36316606\n",
      "   1.10999185  0.83375567 -0.58678866]\n",
      " [-0.80041865  1.45462378 -0.40429321 -1.26691949  0.01175607  0.31949302\n",
      "  -1.29810886 -1.02479164  0.63529552]\n",
      " [ 0.62410504 -0.45315731  0.96867181 -0.95501269  0.42895048 -0.48805791\n",
      "  -1.0895436   0.82207133  0.76484246]\n",
      " [ 1.02508756 -1.43184517  0.2962011   1.15499683 -0.10261269 -0.27745794\n",
      "   1.17871666  0.52458867 -0.86317703]\n",
      " [-1.00168008  1.41730023 -0.27135407 -1.0222209   0.15643148  0.27215795\n",
      "  -1.2072124  -0.90688225  0.70050253]\n",
      " [ 0.07040017  0.31845477  0.81816624 -1.20268162  0.00661025  0.15212769\n",
      "  -1.39055893  0.50713553  0.38245025]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.74275192  1.92864859 -3.39796286 -0.05035454  2.03823019 -3.13959571\n",
      "  -0.98816066]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:98 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.54620267]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 98 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93067508 -1.26971988  0.2433162   1.10315939 -0.07462542 -0.37718483\n",
      "   1.10999185  0.83375567 -0.60080742]\n",
      " [-0.78639639  1.45462378 -0.40429321 -1.25289723  0.01175607  0.33351528\n",
      "  -1.29810886 -1.02479164  0.64931778]\n",
      " [ 0.6250203  -0.45315731  0.96867181 -0.95409743  0.42895048 -0.48714265\n",
      "  -1.0895436   0.82207133  0.76575772]\n",
      " [ 1.01150745 -1.43184517  0.2962011   1.14141672 -0.10261269 -0.29103805\n",
      "   1.17871666  0.52458867 -0.87675715]\n",
      " [-0.98802339  1.41730023 -0.27135407 -1.00856421  0.15643148  0.28581464\n",
      "  -1.2072124  -0.90688225  0.71415921]\n",
      " [ 0.07966295  0.31845477  0.81816624 -1.19341883  0.00661025  0.16139048\n",
      "  -1.39055893  0.50713553  0.39171303]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.81044427  1.87771102 -3.41470967 -0.083285    1.98822289 -3.15712714\n",
      "  -1.0121826 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:98 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.74675967]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 98 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93603153 -1.26436343  0.2433162   1.10851584 -0.07462542 -0.37182838\n",
      "   1.1153483   0.83375567 -0.60080742]\n",
      " [-0.79175696  1.44926321 -0.40429321 -1.25825779  0.01175607  0.32815472\n",
      "  -1.30346942 -1.02479164  0.64931778]\n",
      " [ 0.62056359 -0.45761402  0.96867181 -0.95855414  0.42895048 -0.49159937\n",
      "  -1.09400031  0.82207133  0.76575772]\n",
      " [ 1.01686013 -1.42649248  0.2962011   1.1467694  -0.10261269 -0.28568537\n",
      "   1.18406934  0.52458867 -0.87675715]\n",
      " [-0.99338049  1.41194313 -0.27135407 -1.01392131  0.15643148  0.28045753\n",
      "  -1.21256951 -0.90688225  0.71415921]\n",
      " [ 0.07466756  0.31345938  0.81816624 -1.19841423  0.00661025  0.15639508\n",
      "  -1.39555432  0.50713553  0.39171303]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.78649917  1.89727691 -3.41051466 -0.08121712  2.00817488 -3.15276139\n",
      "  -1.00938922]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:98 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.83445298]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 98 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93849275 -1.2619022   0.2433162   1.11097706 -0.07462542 -0.37182838\n",
      "   1.11780952  0.83375567 -0.60080742]\n",
      " [-0.79421362  1.44680655 -0.40429321 -1.26071446  0.01175607  0.32815472\n",
      "  -1.30592608 -1.02479164  0.64931778]\n",
      " [ 0.6180993  -0.4600783   0.96867181 -0.96101843  0.42895048 -0.49159937\n",
      "  -1.09646459  0.82207133  0.76575772]\n",
      " [ 1.01930764 -1.42404498  0.2962011   1.14921691 -0.10261269 -0.28568537\n",
      "   1.18651685  0.52458867 -0.87675715]\n",
      " [-0.99588269  1.40944093 -0.27135407 -1.01642351  0.15643148  0.28045753\n",
      "  -1.2150717  -0.90688225  0.71415921]\n",
      " [ 0.07241314  0.31120496  0.81816624 -1.20066865  0.00661025  0.15639508\n",
      "  -1.39780874  0.50713553  0.39171303]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.77506474  1.90721784 -3.4090324  -0.0797159   2.01814882 -3.15115041\n",
      "  -1.00825461]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:98 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.83091311]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 98 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94103144 -1.2619022   0.24585489  1.11097706 -0.07462542 -0.36928969\n",
      "   1.12034821  0.83375567 -0.60080742]\n",
      " [-0.79657934  1.44680655 -0.40665893 -1.26071446  0.01175607  0.325789\n",
      "  -1.3082918  -1.02479164  0.64931778]\n",
      " [ 0.61809546 -0.4600783   0.96866797 -0.96101843  0.42895048 -0.49160321\n",
      "  -1.09646843  0.82207133  0.76575772]\n",
      " [ 1.02164093 -1.42404498  0.29853439  1.14921691 -0.10261269 -0.28335208\n",
      "   1.18885014  0.52458867 -0.87675715]\n",
      " [-0.99822782  1.40944093 -0.27369921 -1.01642351  0.15643148  0.2781124\n",
      "  -1.21741684 -0.90688225  0.71415921]\n",
      " [ 0.07140274  0.31120496  0.81715584 -1.20066865  0.00661025  0.15538468\n",
      "  -1.39881914  0.50713553  0.39171303]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.76318668  1.91758744 -3.40782202 -0.07378071  2.02885936 -3.14996754\n",
      "  -1.00334684]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:98 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.06942799]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 99 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94092624 -1.2620074   0.24585489  1.11087187 -0.07462542 -0.36928969\n",
      "   1.12034821  0.83375567 -0.60091262]\n",
      " [-0.7966011   1.44678479 -0.40665893 -1.26073622  0.01175607  0.325789\n",
      "  -1.3082918  -1.02479164  0.64929602]\n",
      " [ 0.61811634 -0.46005743  0.96866797 -0.96099755  0.42895048 -0.49160321\n",
      "  -1.09646843  0.82207133  0.7657786 ]\n",
      " [ 1.02171348 -1.42397242  0.29853439  1.14928946 -0.10261269 -0.28335208\n",
      "   1.18885014  0.52458867 -0.8766846 ]\n",
      " [-0.99828873  1.40938002 -0.27369921 -1.01648442  0.15643148  0.2781124\n",
      "  -1.21741684 -0.90688225  0.71409831]\n",
      " [ 0.07163125  0.31143347  0.81715584 -1.20044014  0.00661025  0.15538468\n",
      "  -1.39881914  0.50713553  0.39194154]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.76542948  1.91636022 -3.40896518 -0.07488123  2.02781072 -3.15114997\n",
      "  -1.00423274]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:99 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.8734984]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 99 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94225393 -1.2620074   0.24718257  1.11087187 -0.07462542 -0.36928969\n",
      "   1.1216759   0.83375567 -0.60091262]\n",
      " [-0.79781966  1.44678479 -0.40787749 -1.26073622  0.01175607  0.325789\n",
      "  -1.30951036 -1.02479164  0.64929602]\n",
      " [ 0.61892357 -0.46005743  0.9694752  -0.96099755  0.42895048 -0.49160321\n",
      "  -1.09566121  0.82207133  0.7657786 ]\n",
      " [ 1.0229334  -1.42397242  0.2997543   1.14928946 -0.10261269 -0.28335208\n",
      "   1.19007005  0.52458867 -0.8766846 ]\n",
      " [-0.99951945  1.40938002 -0.27492993 -1.01648442  0.15643148  0.2781124\n",
      "  -1.21864756 -0.90688225  0.71409831]\n",
      " [ 0.07079561  0.31143347  0.8163202  -1.20044014  0.00661025  0.15538468\n",
      "  -1.39965478  0.50713553  0.39194154]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.75844033  1.92271661 -3.40844063 -0.07054669  2.03427412 -3.15061457\n",
      "  -1.00161051]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:99 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.5082433]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 99 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92945325 -1.27480808  0.23438189  1.11087187 -0.07462542 -0.36928969\n",
      "   1.10887522  0.83375567 -0.60091262]\n",
      " [-0.78490482  1.45969963 -0.39496265 -1.26073622  0.01175607  0.325789\n",
      "  -1.29659552 -1.02479164  0.64929602]\n",
      " [ 0.6184048  -0.46057619  0.96895643 -0.96099755  0.42895048 -0.49160321\n",
      "  -1.09617997  0.82207133  0.7657786 ]\n",
      " [ 1.00990373 -1.43700209  0.28672464  1.14928946 -0.10261269 -0.28335208\n",
      "   1.17704039  0.52458867 -0.8766846 ]\n",
      " [-0.98651784  1.42238163 -0.26192832 -1.01648442  0.15643148  0.2781124\n",
      "  -1.20564595 -0.90688225  0.71409831]\n",
      " [ 0.07395675  0.31459461  0.81948134 -1.20044014  0.00661025  0.15538468\n",
      "  -1.39649364  0.50713553  0.39194154]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.82195347  1.87567858 -3.42468114 -0.10282212  1.98675658 -3.1666709\n",
      "  -1.03018459]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:99 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.72108734]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 99 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93534657 -1.27480808  0.24027522  1.11087187 -0.07462542 -0.36928969\n",
      "   1.10887522  0.83964899 -0.60091262]\n",
      " [-0.79043696  1.45969963 -0.40049479 -1.26073622  0.01175607  0.325789\n",
      "  -1.29659552 -1.03032378  0.64929602]\n",
      " [ 0.62351761 -0.46057619  0.97406924 -0.96099755  0.42895048 -0.49160321\n",
      "  -1.09617997  0.82718414  0.7657786 ]\n",
      " [ 1.01602762 -1.43700209  0.29284853  1.14928946 -0.10261269 -0.28335208\n",
      "   1.17704039  0.53071256 -0.8766846 ]\n",
      " [-0.99214303  1.42238163 -0.26755351 -1.01648442  0.15643148  0.2781124\n",
      "  -1.20564595 -0.91250744  0.71409831]\n",
      " [ 0.08018815  0.31459461  0.82571274 -1.20044014  0.00661025  0.15538468\n",
      "  -1.39649364  0.51336693  0.39194154]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.79390596  1.90037565 -3.4218951  -0.0770873   2.01089741 -3.1637586\n",
      "  -1.00768277]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:99 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.64320049]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 99 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94447989 -1.27480808  0.24027522  1.12000518 -0.07462542 -0.36928969\n",
      "   1.10887522  0.83964899 -0.5917793 ]\n",
      " [-0.79953437  1.45969963 -0.40049479 -1.26983363  0.01175607  0.325789\n",
      "  -1.29659552 -1.03032378  0.64019861]\n",
      " [ 0.62770636 -0.46057619  0.97406924 -0.9568088   0.42895048 -0.49160321\n",
      "  -1.09617997  0.82718414  0.76996735]\n",
      " [ 1.02496452 -1.43700209  0.29284853  1.15822636 -0.10261269 -0.28335208\n",
      "   1.17704039  0.53071256 -0.8677477 ]\n",
      " [-1.00109076  1.42238163 -0.26755351 -1.02543214  0.15643148  0.2781124\n",
      "  -1.20564595 -0.91250744  0.70515058]\n",
      " [ 0.07364089  0.31459461  0.82571274 -1.2069874   0.00661025  0.15538468\n",
      "  -1.39649364  0.51336693  0.38539428]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.75296436  1.93350866 -3.41380838 -0.0522985   2.04299217 -3.15495259\n",
      "  -0.99435359]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:99 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.04075754]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 99 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94449802 -1.27480808  0.24027522  1.12000518 -0.07460729 -0.36927156\n",
      "   1.10887522  0.83964899 -0.59176117]\n",
      " [-0.79956958  1.45969963 -0.40049479 -1.26983363  0.01172085  0.32575379\n",
      "  -1.29659552 -1.03032378  0.6401634 ]\n",
      " [ 0.62753094 -0.46057619  0.97406924 -0.9568088   0.42877505 -0.49177863\n",
      "  -1.09617997  0.82718414  0.76979193]\n",
      " [ 1.02500949 -1.43700209  0.29284853  1.15822636 -0.10256772 -0.28330711\n",
      "   1.17704039  0.53071256 -0.86770273]\n",
      " [-1.00111823  1.42238163 -0.26755351 -1.02543214  0.156404    0.27808492\n",
      "  -1.20564595 -0.91250744  0.70512311]\n",
      " [ 0.07352839  0.31459461  0.82571274 -1.2069874   0.00649775  0.15527218\n",
      "  -1.39649364  0.51336693  0.38528178]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.75376109  1.93312845 -3.41424215 -0.05292925  2.04263917 -3.15537852\n",
      "  -0.99487183]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:99 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.00191879]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 99 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94449843 -1.27480767  0.24027522  1.12000518 -0.07460688 -0.36927115\n",
      "   1.10887522  0.83964899 -0.59176077]\n",
      " [-0.79956999  1.45969922 -0.40049479 -1.26983363  0.01172044  0.32575338\n",
      "  -1.29659552 -1.03032378  0.64016299]\n",
      " [ 0.6275306  -0.46057653  0.97406924 -0.9568088   0.42877472 -0.49177897\n",
      "  -1.09617997  0.82718414  0.76979159]\n",
      " [ 1.0250099  -1.43700168  0.29284853  1.15822636 -0.10256731 -0.2833067\n",
      "   1.17704039  0.53071256 -0.86770232]\n",
      " [-1.00111865  1.42238122 -0.26755351 -1.02543214  0.15640359  0.27808451\n",
      "  -1.20564595 -0.91250744  0.7051227 ]\n",
      " [ 0.07352804  0.31459426  0.82571274 -1.2069874   0.0064974   0.15527184\n",
      "  -1.39649364  0.51336693  0.38528143]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.75376293  1.93312808 -3.41424369 -0.05293055  2.04263888 -3.15538004\n",
      "  -0.99487315]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:99 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.18460383]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 99 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94336787 -1.27593823  0.24027522  1.12000518 -0.07573744 -0.37040171\n",
      "   1.10774466  0.83964899 -0.59176077]\n",
      " [-0.79855433  1.46071488 -0.40049479 -1.26983363  0.01273611  0.32676904\n",
      "  -1.29557986 -1.03032378  0.64016299]\n",
      " [ 0.63025078 -0.45785635  0.97406924 -0.9568088   0.43149489 -0.48905879\n",
      "  -1.0934598   0.82718414  0.76979159]\n",
      " [ 1.02373908 -1.43827249  0.29284853  1.15822636 -0.10383813 -0.28457751\n",
      "   1.17576958  0.53071256 -0.86770232]\n",
      " [-0.99993976  1.42356011 -0.26755351 -1.02543214  0.15758248  0.2792634\n",
      "  -1.20446706 -0.91250744  0.7051227 ]\n",
      " [ 0.07599885  0.31706507  0.82571274 -1.2069874   0.00896821  0.15774265\n",
      "  -1.39402283  0.51336693  0.38528143]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.7676567   1.9250294  -3.42015971 -0.05668842  2.03439051 -3.16112383\n",
      "  -0.99904331]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:99 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.93854575]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 99 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94358506 -1.27593823  0.24027522  1.12022237 -0.07573744 -0.37040171\n",
      "   1.10796185  0.83964899 -0.59176077]\n",
      " [-0.79874703  1.46071488 -0.40049479 -1.27002633  0.01273611  0.32676904\n",
      "  -1.29577256 -1.03032378  0.64016299]\n",
      " [ 0.62985625 -0.45785635  0.97406924 -0.95720332  0.43149489 -0.48905879\n",
      "  -1.09385433  0.82718414  0.76979159]\n",
      " [ 1.02393255 -1.43827249  0.29284853  1.15841982 -0.10383813 -0.28457751\n",
      "   1.17596304  0.53071256 -0.86770232]\n",
      " [-1.00014932  1.42356011 -0.26755351 -1.0256417   0.15758248  0.2792634\n",
      "  -1.20467662 -0.91250744  0.7051227 ]\n",
      " [ 0.07569174  0.31706507  0.82571274 -1.20729451  0.00896821  0.15774265\n",
      "  -1.39432994  0.51336693  0.38528143]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.76588443  1.92673031 -3.42010044 -0.05634343  2.03610315 -3.16105638\n",
      "  -0.99891195]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:99 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.79622338]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 99 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94728106 -1.27593823  0.24027522  1.12022237 -0.07204144 -0.36670571\n",
      "   1.11165785  0.83964899 -0.59176077]\n",
      " [-0.80239352  1.46071488 -0.40049479 -1.27002633  0.00908961  0.32312255\n",
      "  -1.29941906 -1.03032378  0.64016299]\n",
      " [ 0.62784089 -0.45785635  0.97406924 -0.95720332  0.42947953 -0.49107416\n",
      "  -1.09586969  0.82718414  0.76979159]\n",
      " [ 1.02754802 -1.43827249  0.29284853  1.15841982 -0.10022266 -0.28096204\n",
      "   1.17957851  0.53071256 -0.86770232]\n",
      " [-1.00378924  1.42356011 -0.26755351 -1.0256417   0.15394255  0.27562348\n",
      "  -1.20831655 -0.91250744  0.7051227 ]\n",
      " [ 0.07221691  0.31706507  0.82571274 -1.20729451  0.00549338  0.15426782\n",
      "  -1.39780477  0.51336693  0.38528143]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.74935288  1.94049734 -3.41766343 -0.05018564  2.05031269 -3.15864618\n",
      "  -0.99494211]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:99 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.5444554]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 99 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93331415 -1.27593823  0.24027522  1.10625547 -0.07204144 -0.38067262\n",
      "   1.11165785  0.83964899 -0.60572767]\n",
      " [-0.78842608  1.46071488 -0.40049479 -1.25605888  0.00908961  0.33708999\n",
      "  -1.29941906 -1.03032378  0.65413044]\n",
      " [ 0.62869521 -0.45785635  0.97406924 -0.956349    0.42947953 -0.49021983\n",
      "  -1.09586969  0.82718414  0.77064592]\n",
      " [ 1.01401681 -1.43827249  0.29284853  1.14488862 -0.10022266 -0.29449325\n",
      "   1.17957851  0.53071256 -0.88123353]\n",
      " [-0.99018392  1.42356011 -0.26755351 -1.01203638  0.15394255  0.2892288\n",
      "  -1.20831655 -0.91250744  0.71872801]\n",
      " [ 0.08142814  0.31706507  0.82571274 -1.19808328  0.00549338  0.16347905\n",
      "  -1.39780477  0.51336693  0.39449267]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.81687181  1.88972651 -3.43441031 -0.08309041  2.00046149 -3.17616584\n",
      "  -1.01893613]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:99 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.74723297]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 99 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93865355 -1.27059883  0.24027522  1.11159487 -0.07204144 -0.37533321\n",
      "   1.11699726  0.83964899 -0.60572767]\n",
      " [-0.79377005  1.4553709  -0.40049479 -1.26140286  0.00908961  0.33174602\n",
      "  -1.30476304 -1.03032378  0.65413044]\n",
      " [ 0.62427511 -0.46227645  0.97406924 -0.9607691   0.42947953 -0.49463993\n",
      "  -1.10028979  0.82718414  0.77064592]\n",
      " [ 1.0193536  -1.4329357   0.29284853  1.1502254  -0.10022266 -0.28915646\n",
      "   1.1849153   0.53071256 -0.88123353]\n",
      " [-0.9955239   1.41822013 -0.26755351 -1.01737636  0.15394255  0.28388882\n",
      "  -1.21365653 -0.91250744  0.71872801]\n",
      " [ 0.07646116  0.31209809  0.82571274 -1.20305026  0.00549338  0.15851207\n",
      "  -1.40277175  0.51336693  0.39449267]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.79300101  1.90922346 -3.43021739 -0.0810527   2.02034147 -3.17180318\n",
      "  -1.01617457]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:99 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.83587626]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 99 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94107619 -1.26817619  0.24027522  1.11401751 -0.07204144 -0.37533321\n",
      "   1.1194199   0.83964899 -0.60572767]\n",
      " [-0.7961886   1.45295236 -0.40049479 -1.2638214   0.00908961  0.33174602\n",
      "  -1.30718158 -1.03032378  0.65413044]\n",
      " [ 0.62185371 -0.46469785  0.97406924 -0.96319049  0.42947953 -0.49463993\n",
      "  -1.10271119  0.82718414  0.77064592]\n",
      " [ 1.02176312 -1.43052619  0.29284853  1.15263492 -0.10022266 -0.28915646\n",
      "   1.18732481  0.53071256 -0.88123353]\n",
      " [-0.99798728  1.41575675 -0.26755351 -1.01983975  0.15394255  0.28388882\n",
      "  -1.21611991 -0.91250744  0.71872801]\n",
      " [ 0.07425042  0.30988735  0.82571274 -1.205261    0.00549338  0.15851207\n",
      "  -1.40498249  0.51336693  0.39449267]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.78174318  1.91901228 -3.42875846 -0.07958677  2.03016181 -3.17021763\n",
      "  -1.01506893]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:99 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.83135054]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 99 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94360444 -1.26817619  0.24280347  1.11401751 -0.07204144 -0.37280497\n",
      "   1.12194814  0.83964899 -0.60572767]\n",
      " [-0.79854662  1.45295236 -0.40285281 -1.2638214   0.00908961  0.32938799\n",
      "  -1.3095396  -1.03032378  0.65413044]\n",
      " [ 0.62184949 -0.46469785  0.97406502 -0.96319049  0.42947953 -0.49464415\n",
      "  -1.10271541  0.82718414  0.77064592]\n",
      " [ 1.02408848 -1.43052619  0.29517389  1.15263492 -0.10022266 -0.2868311\n",
      "   1.18965018  0.53071256 -0.88123353]\n",
      " [-1.00032482  1.41575675 -0.26989105 -1.01983975  0.15394255  0.28155128\n",
      "  -1.21845745 -0.91250744  0.71872801]\n",
      " [ 0.07325638  0.30988735  0.8247187  -1.205261    0.00549338  0.15751803\n",
      "  -1.40597653  0.51336693  0.39449267]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.76992028  1.92933064 -3.42754919 -0.07367954  2.04081887 -3.16903592\n",
      "  -1.01017153]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:99 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.06740556]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Main:\n",
    "\n",
    "# load in fishing normalized set \n",
    "data = getArrayFromFile(\"normalizeFish.csv\")\n",
    "inputs = data[:,0:8] # get the input values\n",
    "targets = data[:, 8:9] # get the class values\n",
    "\n",
    "inputsWB = getInputs(inputs) # adds bias to the input matrix\n",
    "\n",
    "numInputNodes = inputsWB.shape[1]\n",
    "numOutputNodes = 1 if np.unique(targets).shape[0] == 2 else np.unique(targets).shape[0]\n",
    "numHiddenNodes = int((2/3)*(numInputNodes+numOutputNodes)) # + 1 for the bias node\n",
    "\n",
    "print(\"Input nodes with bias node: \" + str(numInputNodes))\n",
    "print(\"Output layer: \" + str(numOutputNodes))\n",
    "print(\"Hidden nodes with bias node: \" + str(numHiddenNodes))\n",
    "\n",
    "# inititlize weights\n",
    "# (-1/sqrt(n)) < w < (1/sqrt(n))\n",
    "lowRange = (-1/math.sqrt(numInputNodes))\n",
    "highRange = math.fabs(lowRange)\n",
    "theta1 = np.random.uniform(low=lowRange, high=highRange, size=(numHiddenNodes, numInputNodes))\n",
    "theta2 = np.random.uniform(low=lowRange, high=highRange, size=(numOutputNodes,numHiddenNodes+1))\n",
    "\n",
    "print(\"Theta1 dims: \" + str(theta1.shape))\n",
    "print(\"Theta2 dims: \" + str(theta2.shape))\n",
    "\n",
    "\n",
    "learningRate = .5\n",
    "numberProcess = 0\n",
    "batchError = []\n",
    "for r in range(0, 100): # batch\n",
    "    trackedNetError= []\n",
    "    \n",
    "    for index in range(0, inputsWB.shape[0]): # online\n",
    "        print(color.CYAN+color.BOLD+\"----------- Data for batch: \"+str(r)+\" Online Round: \"+str(index)+\" ------------\"+color.END)\n",
    "        print(\"Inputs: \\n\" + str(inputs[index])+\"\\n\")\n",
    "        print(\"Learning Rate: \\n\" + str(learningRate)+\"\\n\")\n",
    "        print(\"Theta One: \\n\" + str(theta1)+\"\\n\")\n",
    "        print(\"Theta two: \\n\" + str(theta2)+\"\\n\")\n",
    "        print(\"Target: \\n\"+ str(targets[index]) + \"\\n\")\n",
    "        print(color.YELLOW+color.BOLD+\"----------Processing on batch:\"+str(r)+\" with online instance: \"+str(index)+\"-------------\"+color.END)\n",
    "        theta1, theta2, netError = nn(learningRate, theta1, theta2, np.array([inputsWB[index]]), targets[index])\n",
    "        trackedNetError.append(netError[0][0])\n",
    "    numberProcess = numberProcess + 1 \n",
    "    batchError.append(statistics.mean(trackedNetError))\n",
    "saveNet(theta1, theta2, \"./FishWeights.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "netPlot(numberProcess, batchError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 1. 0. 1. 0. 0.]\n",
      "(8,)\n",
      "[1.]\n",
      "\t\tLayer One activations: \n",
      "[-0.4560759  -1.8905096  -1.49066094  1.05197795 -1.30777151  0.86557503\n",
      " -1.93706421]\n",
      "\n",
      "\t\tSigmoid Result: [0.38791714 0.13118638 0.18382255 0.74115454 0.21285999 0.70382412\n",
      " 0.12597074]\n",
      "\n",
      "[1.]\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "[1.         0.38791714 0.13118638 0.18382255 0.74115454 0.21285999\n",
      " 0.70382412 0.12597074]\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "[0.61300909]\n",
      "\n",
      "\t\tFinal output: \n",
      "[0.64862691]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Yes'"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remember for this set you target yes as 1 and no as 0, confusing I know\n",
    "weightMap = load_objects(\"./FishWeights.pkl\")\n",
    "theta1 = weightMap[\"theta1\"]\n",
    "theta2 = weightMap[\"theta2\"]\n",
    "\n",
    "test = getArrayFromFile(\"normalizeFishTest.csv\")\n",
    "print(test)\n",
    "print(test.shape)\n",
    "test = getInputs(test)\n",
    "\n",
    "testInstance = test\n",
    "\n",
    "classifyFish(testInstance, theta1, theta2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
