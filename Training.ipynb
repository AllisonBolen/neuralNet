{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports:\n",
    "import numpy as np\n",
    "import math, os, pickle\n",
    "from numpy import genfromtxt\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class color:\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   END = '\\033[0m'\n",
    "\n",
    "def softmax(A):  \n",
    "    expA = np.exp(A)\n",
    "    return expA / expA.sum()\n",
    "\n",
    "def load_objects(file):\n",
    "    with open(file, 'rb') as input:\n",
    "        return pickle.load(input)\n",
    "\n",
    "# functions to use:\n",
    "def sigmoid(matrix):\n",
    "    #print(\"SIGMOID: \\n\" +str(matrix)+\"\\n\")\n",
    "    return 1/(1+np.exp(-matrix))\n",
    "\n",
    "def getInputs(inputs):\n",
    "    #add bias to layerOne and inputs\n",
    "    row = inputs.shape[0] if np.ndim(inputs) != 1 else 1\n",
    "    inputBias = np.ones((row,1)) if np.ndim(inputs) != 1 else np.ones((1))\n",
    "    inputsWithBias = np.append(inputBias, inputs, 1) if np.ndim(inputs) != 1 else np.append(inputBias, inputs) \n",
    "    return inputsWithBias\n",
    "\n",
    "def networkError(target, netResult):\n",
    "    print(\"Target: \" + str(target) + \" Net Result: \" + str(netResult))\n",
    "    return .5*np.square(target - netResult)\n",
    "\n",
    "def learning(weights, lr, error, activationsForLayer):\n",
    "    print(\"Weights:  \" + str(weights.shape))\n",
    "    print(\"Learning Rate:  \" + str(lr))\n",
    "    print(\"Error:  \" + str(error.shape))\n",
    "    print(\"Activations: \"+str(activationsForLayer.shape))\n",
    "    return weights+lr*error*activationsForLayer\n",
    "\n",
    "def hiddenUnitError(temp, activations, error):\n",
    "    return temp*(1-temp)*(activations*error)\n",
    "\n",
    "def outputError(target, output):\n",
    "    # Eouput = output(1-output)(target - output)\n",
    "    return output*(1-output)*(target - output)\n",
    "\n",
    "def save_it_all(obj, filename):\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    with open(filename, 'wb') as output:  # Overwrites any existing file.\n",
    "        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "def saveNet(theta1, theta2, fileName):\n",
    "    weights = {\"theta1\":theta1, \"theta2\":theta2}\n",
    "    save_it_all(weights, fileName)\n",
    "    \n",
    "def sigmoidDerivative(target, output):\n",
    "    #E = (t − y) * y *  (1− y) // note: derivative of sigmoid func\n",
    "    return (target - output) * output * (1 - output)\n",
    "\n",
    "def netPlot(instance, error):\n",
    "    instance = list(range(0, instance))\n",
    "\n",
    "    with plt.rc_context({'axes.edgecolor':'orange', 'xtick.color':'red', 'ytick.color':'green', 'figure.facecolor':'white'}):\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(instance, error)\n",
    "\n",
    "        ax.set(xlabel='instance numebr (s)', ylabel='error (net)',\n",
    "               title='Average Net Error for each batch run')\n",
    "        ax.grid()\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def load_objects(file):\n",
    "    with open(file, 'rb') as input:\n",
    "        return pickle.load(input)\n",
    "    \n",
    "def classifyFish(inputInstance, theta1, theta2):\n",
    "    # just need to feed forward \n",
    "    layerOneActivations = theta1.dot(np.transpose(inputInstance))\n",
    "    print(\"\\t\\tLayer One activations: \\n\"+str(layerOneActivations)+\"\\n\")\n",
    "    layerOneSig = sigmoid(layerOneActivations)\n",
    "    print(\"\\t\\tSigmoid Result: \"+ str(layerOneSig) + \"\\n\")\n",
    "    inputsforhiddenlayer = getInputs(np.transpose(layerOneSig)) \n",
    "    print(\"\\t\\tInputs for the hiden layer is: (b,h1,h2)\\n\"+str(inputsforhiddenlayer)+\"\\n\")\n",
    "\n",
    "    outputActivation = theta2.dot(np.transpose(inputsforhiddenlayer)) \n",
    "    print(\"\\t\\tActivation for output layer: (h1,h2)\\n\" + str(outputActivation)+\"\\n\")\n",
    "        # inplace of sigmoid use softmax?? http://dataaspirant.com/2017/03/07/difference-between-softmax-function-and-sigmoid-function/\n",
    "    outputFinal = sigmoid(outputActivation)\n",
    "    print(\"\\t\\tFinal output: \\n\"+ str(outputFinal)+\"\\n\")\n",
    "    \n",
    "    return \"Yes\" if outputFinal >= .5 else \"No\"\n",
    "\n",
    "\n",
    "def getArrayFromFile(name):\n",
    "    array = genfromtxt(name, delimiter=',')\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural Net function:\n",
    "def nn(learningRate, theta1, theta2, inputInstance, targetInstance):\n",
    "    \n",
    "    # feed forward:\n",
    "    layerOneActivations = theta1.dot(np.transpose(inputInstance))\n",
    "    print(\"\\t\\tLayer One activations: \\n\"+str(layerOneActivations.shape)+\"\\n\")\n",
    "    layerOneSig = sigmoid(layerOneActivations)\n",
    "    print(\"\\t\\tSigmoid Result: \"+ str(layerOneSig.shape) + \"\\n\")\n",
    "    inputsforhiddenlayer = getInputs(np.transpose(layerOneSig)) \n",
    "    print(\"\\t\\tInputs for the hiden layer is: (b,h1,h2)\\n\"+str(inputsforhiddenlayer.shape)+\"\\n\")\n",
    "\n",
    "    outputActivation = theta2.dot(np.transpose(inputsforhiddenlayer)) \n",
    "    print(\"\\t\\tActivation for output layer: (h1,h2)\\n\" + str(outputActivation.shape)+\"\\n\")\n",
    "        # inplace of sigmoid use softmax?? http://dataaspirant.com/2017/03/07/difference-between-softmax-function-and-sigmoid-function/\n",
    "    outputFinal = sigmoid(outputActivation)\n",
    "    print(\"\\t\\tFinal output: \\n\"+ str(outputFinal.shape)+\"\\n\")\n",
    "\n",
    "    # network error:\n",
    "    netError = networkError(targetInstance, outputFinal)\n",
    "    print(\"\\t\\tNetwork Error: \\n\" + str(netError.shape)+\"\\n\")\n",
    "\n",
    "    # BACKPROPAGATE\n",
    "    outputErr = outputError(targetInstance, outputFinal[0])\n",
    "    print(\"\\t\\tOutput Error: \\n\"+str(outputErr.shape)+\"\\n\")\n",
    "\n",
    "    hidUnitErr = hiddenUnitError(layerOneSig, layerOneActivations, outputErr)\n",
    "    print(\"\\t\\tHidden unit errors: \\n\"+str(hidUnitErr.shape)+\"\\n\")\n",
    "\n",
    "    # learning:\n",
    "    theta1 = learning(theta1, learningRate, hidUnitErr, inputInstance)\n",
    "    print(\"\\t\\tNext round of weights for layerOne: (b,x1,x2) \\n\"+ str(theta1.shape)+\"\\n\")\n",
    "\n",
    "    theta2 = learning(theta2, learningRate, outputErr, inputsforhiddenlayer)\n",
    "    print(\"\\t\\tNext round of weights for layer 2: (b,h1,h2) \\n\"+ str(theta2.shape)+\"\\n\")\n",
    "    \n",
    "    return theta1, theta2, netError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input nodes with bias node: 9\n",
      "Output layer: 1\n",
      "Hidden nodes with bias node: 6\n",
      "Theta1 dims: (6, 9)\n",
      "Theta2 dims: (1, 7)\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 0 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.01306623 -0.17440084  0.23758615  0.00518256 -0.17724083 -0.01192858\n",
      "   0.32362298 -0.23150995 -0.10746475]\n",
      " [ 0.03610596  0.26389185 -0.29820874 -0.27908089  0.16165717 -0.24549854\n",
      "   0.24131713  0.10477724  0.23597296]\n",
      " [ 0.14644086  0.15028363  0.07565064 -0.12741461 -0.32996057  0.17292466\n",
      "  -0.04066798 -0.23382776 -0.26329298]\n",
      " [-0.10890531 -0.17328075  0.09878108 -0.01090153 -0.22945802  0.2377974\n",
      "   0.086447    0.33283112  0.11644475]\n",
      " [-0.13349162  0.04485081 -0.31915481 -0.17887167 -0.14277587  0.00692084\n",
      "  -0.02249693 -0.18527829  0.10948275]\n",
      " [ 0.21237596 -0.04542637  0.31645868 -0.13221656 -0.08894038 -0.23289226\n",
      "  -0.00735144  0.29885146 -0.12948024]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.30042929  0.30308467 -0.00480782  0.00517327 -0.31067472  0.04245241\n",
      "  -0.08821852]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:0 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.42353848]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 0 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.00411226 -0.17440084  0.24654012  0.00518256 -0.17724083 -0.01192858\n",
      "   0.33257695 -0.23150995 -0.10746475]\n",
      " [ 0.03574031  0.26389185 -0.29857439 -0.27908089  0.16165717 -0.24549854\n",
      "   0.24095148  0.10477724  0.23597296]\n",
      " [ 0.14960655  0.15028363  0.07881633 -0.12741461 -0.32996057  0.17292466\n",
      "  -0.03750229 -0.23382776 -0.26329298]\n",
      " [-0.1075645  -0.17328075  0.10012188 -0.01090153 -0.22945802  0.2377974\n",
      "   0.08778781  0.33283112  0.11644475]\n",
      " [-0.14139629  0.04485081 -0.32705948 -0.17887167 -0.14277587  0.00692084\n",
      "  -0.0304016  -0.18527829  0.10948275]\n",
      " [ 0.22095397 -0.04542637  0.32503669 -0.13221656 -0.08894038 -0.23289226\n",
      "   0.00122657  0.29885146 -0.12948024]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.2300567   0.3476801   0.03001281  0.04354265 -0.27414632  0.06943323\n",
      "  -0.04406011]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:0 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.47233452]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 0 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.00977692 -0.18006551  0.24087545  0.00518256 -0.17724083 -0.01192858\n",
      "   0.32691228 -0.23150995 -0.10746475]\n",
      " [ 0.03223072  0.26038226 -0.30208398 -0.27908089  0.16165717 -0.24549854\n",
      "   0.23744189  0.10477724  0.23597296]\n",
      " [ 0.14472898  0.14540606  0.07393876 -0.12741461 -0.32996057  0.17292466\n",
      "  -0.04237985 -0.23382776 -0.26329298]\n",
      " [-0.10619988 -0.17191613  0.1014865  -0.01090153 -0.22945802  0.2377974\n",
      "   0.08915243  0.33283112  0.11644475]\n",
      " [-0.13504824  0.05119886 -0.32071143 -0.17887167 -0.14277587  0.00692084\n",
      "  -0.02405355 -0.18527829  0.10948275]\n",
      " [ 0.21401596 -0.05236438  0.31809868 -0.13221656 -0.08894038 -0.23289226\n",
      "  -0.00571144  0.29885146 -0.12948024]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.28891776  0.31243217 -0.00296167  0.00913936 -0.30221026  0.04657109\n",
      "  -0.08072349]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:0 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.41449981]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 0 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.00978423 -0.18006551  0.24086814  0.00518256 -0.17724083 -0.01192858\n",
      "   0.32691228 -0.23151725 -0.10746475]\n",
      " [ 0.02931855  0.26038226 -0.30499615 -0.27908089  0.16165717 -0.24549854\n",
      "   0.23744189  0.10186507  0.23597296]\n",
      " [ 0.14445973  0.14540606  0.07366951 -0.12741461 -0.32996057  0.17292466\n",
      "  -0.04237985 -0.23409701 -0.26329298]\n",
      " [-0.10052599 -0.17191613  0.10716039 -0.01090153 -0.22945802  0.2377974\n",
      "   0.08915243  0.33850501  0.11644475]\n",
      " [-0.14534023  0.05119886 -0.33100342 -0.17887167 -0.14277587  0.00692084\n",
      "  -0.02405355 -0.19557029  0.10948275]\n",
      " [ 0.2264945  -0.05236438  0.33057722 -0.13221656 -0.08894038 -0.23289226\n",
      "  -0.00571144  0.31133    -0.12948024]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.21787032  0.34794859  0.02963663  0.04439382 -0.26091029  0.0710833\n",
      "  -0.03123475]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:0 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.4665936]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 0 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.01163809 -0.18006551  0.24086814  0.0033287  -0.17724083 -0.01192858\n",
      "   0.32691228 -0.23151725 -0.10931862]\n",
      " [ 0.02908973  0.26038226 -0.30499615 -0.27930971  0.16165717 -0.24549854\n",
      "   0.23744189  0.10186507  0.23574414]\n",
      " [ 0.14043469  0.14540606  0.07366951 -0.13143966 -0.32996057  0.17292466\n",
      "  -0.04237985 -0.23409701 -0.26731803]\n",
      " [-0.10044273 -0.17191613  0.10716039 -0.01081827 -0.22945802  0.2377974\n",
      "   0.08915243  0.33850501  0.11652801]\n",
      " [-0.1488628   0.05119886 -0.33100342 -0.18239424 -0.14277587  0.00692084\n",
      "  -0.02405355 -0.19557029  0.10596018]\n",
      " [ 0.22591051 -0.05236438  0.33057722 -0.13280055 -0.08894038 -0.23289226\n",
      "  -0.00571144  0.31133    -0.13006423]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.15149216  0.37927992  0.06259688  0.07351705 -0.22763795  0.10072268\n",
      "   0.00137023]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:0 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.50148611]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 0 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.00689302 -0.18006551  0.24086814  0.0033287  -0.17249576 -0.00718351\n",
      "   0.32691228 -0.23151725 -0.10457355]\n",
      " [ 0.02627644  0.26038226 -0.30499615 -0.27930971  0.15884388 -0.24831183\n",
      "   0.23744189  0.10186507  0.23293085]\n",
      " [ 0.1447956   0.14540606  0.07366951 -0.13143966 -0.32559966  0.17728557\n",
      "  -0.04237985 -0.23409701 -0.26295712]\n",
      " [-0.10082544 -0.17191613  0.10716039 -0.01081827 -0.22984073  0.23741469\n",
      "   0.08915243  0.33850501  0.1161453 ]\n",
      " [-0.1460837   0.05119886 -0.33100342 -0.18239424 -0.13999677  0.00969994\n",
      "  -0.02405355 -0.19557029  0.10873929]\n",
      " [ 0.22940718 -0.05236438  0.33057722 -0.13280055 -0.08544371 -0.22939559\n",
      "  -0.00571144  0.31133    -0.12656756]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.21417737  0.35275881  0.02842561  0.04659418 -0.2593633   0.07217401\n",
      "  -0.02644587]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:0 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.46579758]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 0 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.00043145 -0.17360394  0.24086814  0.0033287  -0.16603419 -0.00072194\n",
      "   0.32691228 -0.23151725 -0.09811198]\n",
      " [ 0.02032437  0.25443019 -0.30499615 -0.27930971  0.15289181 -0.25426389\n",
      "   0.23744189  0.10186507  0.22697878]\n",
      " [ 0.14654325  0.14715372  0.07366951 -0.13143966 -0.32385201  0.17903322\n",
      "  -0.04237985 -0.23409701 -0.26120947]\n",
      " [-0.09867834 -0.16976903  0.10716039 -0.01081827 -0.22769362  0.23956179\n",
      "   0.08915243  0.33850501  0.1182924 ]\n",
      " [-0.14440238  0.05288018 -0.33100342 -0.18239424 -0.13831545  0.01138126\n",
      "  -0.02405355 -0.19557029  0.1104206 ]\n",
      " [ 0.23317115 -0.0486004   0.33057722 -0.13280055 -0.08167974 -0.22563162\n",
      "  -0.00571144  0.31133    -0.12280359]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.27212962  0.33048605 -0.00668782  0.01936998 -0.28618437  0.044883\n",
      "  -0.05161403]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:0 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.44092375]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 0 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-2.42890547e-04 -1.73415378e-01  2.40868145e-01  3.32870007e-03\n",
      "  -1.65845624e-01 -5.33379152e-04  3.27100841e-01 -2.31517254e-01\n",
      "  -9.81119773e-02]\n",
      " [ 1.49717478e-02  2.49077563e-01 -3.04996153e-01 -2.79309705e-01\n",
      "   1.47539185e-01 -2.59616521e-01  2.32089265e-01  1.01865067e-01\n",
      "   2.26978784e-01]\n",
      " [ 1.45100401e-01  1.45710870e-01  7.36695081e-02 -1.31439657e-01\n",
      "  -3.25294857e-01  1.77590375e-01 -4.38226993e-02 -2.34097012e-01\n",
      "  -2.61209467e-01]\n",
      " [-9.64194589e-02 -1.67510150e-01  1.07160394e-01 -1.08182711e-02\n",
      "  -2.25434745e-01  2.41820670e-01  9.14113093e-02  3.38505009e-01\n",
      "   1.18292405e-01]\n",
      " [-1.41155488e-01  5.61270689e-02 -3.31003424e-01 -1.82394236e-01\n",
      "  -1.35068555e-01  1.46281538e-02 -2.08066565e-02 -1.95570291e-01\n",
      "   1.10420605e-01]\n",
      " [ 2.34909191e-01 -4.68623679e-02  3.30577222e-01 -1.32800546e-01\n",
      "  -7.99416992e-02 -2.23893582e-01 -3.97340493e-03  3.11330000e-01\n",
      "  -1.22803588e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.32647568  0.30350159 -0.03936531 -0.00924862 -0.31108795  0.02098878\n",
      "  -0.07704423]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:0 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.41099934]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 0 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.00548452 -0.17341538  0.24086814  0.00905611 -0.16584562 -0.00053338\n",
      "   0.33282825 -0.23151725 -0.09811198]\n",
      " [ 0.01439713  0.24907756 -0.30499615 -0.27988433  0.14753918 -0.25961652\n",
      "   0.23151464  0.10186507  0.22697878]\n",
      " [ 0.14456294  0.14571087  0.07366951 -0.13197711 -0.32529486  0.17759038\n",
      "  -0.04436016 -0.23409701 -0.26120947]\n",
      " [-0.09670152 -0.16751015  0.10716039 -0.01110033 -0.22543475  0.24182067\n",
      "   0.09112925  0.33850501  0.1182924 ]\n",
      " [-0.14711457  0.05612707 -0.33100342 -0.18835331 -0.13506856  0.01462815\n",
      "  -0.02676574 -0.19557029  0.1104206 ]\n",
      " [ 0.23665406 -0.04686237  0.33057722 -0.13105568 -0.0799417  -0.22389358\n",
      "  -0.00222854  0.31133    -0.12280359]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.25518337  0.3449798  -0.00429388  0.02586    -0.27572386  0.05055739\n",
      "  -0.03965041]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:0 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.45171881]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 0 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.00838147 -0.17341538  0.24086814  0.00905611 -0.16294868  0.00236357\n",
      "   0.3357252  -0.23151725 -0.09811198]\n",
      " [ 0.01665869  0.24907756 -0.30499615 -0.27988433  0.14980075 -0.25735495\n",
      "   0.23377621  0.10186507  0.22697878]\n",
      " [ 0.1437571   0.14571087  0.07366951 -0.13197711 -0.3261007   0.17678453\n",
      "  -0.045166   -0.23409701 -0.26120947]\n",
      " [-0.09651797 -0.16751015  0.10716039 -0.01110033 -0.2252512   0.24200422\n",
      "   0.0913128   0.33850501  0.1182924 ]\n",
      " [-0.15200373  0.05612707 -0.33100342 -0.18835331 -0.13995772  0.00973899\n",
      "  -0.0316549  -0.19557029  0.1104206 ]\n",
      " [ 0.23547731 -0.04686237  0.33057722 -0.13105568 -0.08111844 -0.22507033\n",
      "  -0.00340528  0.31133    -0.12280359]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.18728726  0.38183909  0.0319225   0.05900191 -0.24159226  0.07954539\n",
      "  -0.00688005]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:0 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.48366496]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 0 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.00956203 -0.17341538  0.24086814  0.01023667 -0.16294868  0.00354412\n",
      "   0.3357252  -0.23151725 -0.09693142]\n",
      " [ 0.02099743  0.24907756 -0.30499615 -0.27554559  0.14980075 -0.25301621\n",
      "   0.23377621  0.10186507  0.23131753]\n",
      " [ 0.14485248  0.14571087  0.07366951 -0.13088174 -0.3261007   0.17787991\n",
      "  -0.045166   -0.23409701 -0.26011409]\n",
      " [-0.10027276 -0.16751015  0.10716039 -0.01485511 -0.2252512   0.23824943\n",
      "   0.0913128   0.33850501  0.11453762]\n",
      " [-0.14871908  0.05612707 -0.33100342 -0.18506866 -0.13995772  0.01302365\n",
      "  -0.0316549  -0.19557029  0.11370526]\n",
      " [ 0.23909912 -0.04686237  0.33057722 -0.12743387 -0.08111844 -0.22144852\n",
      "  -0.00340528  0.31133    -0.11918178]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.24768085  0.35282406  0.00612705  0.02990145 -0.27558392  0.05265985\n",
      "  -0.03341915]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:0 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.45791599]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 0 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.01265797 -0.17031943  0.24086814  0.01333262 -0.16294868  0.00664007\n",
      "   0.33882115 -0.23151725 -0.09693142]\n",
      " [ 0.02058186  0.24866199 -0.30499615 -0.27596116  0.14980075 -0.25343178\n",
      "   0.23336064  0.10186507  0.23131753]\n",
      " [ 0.14966697  0.15052536  0.07366951 -0.12606725 -0.3261007   0.1826944\n",
      "  -0.04035151 -0.23409701 -0.26011409]\n",
      " [-0.09948392 -0.16672131  0.10716039 -0.01406628 -0.2252512   0.23903827\n",
      "   0.09210163  0.33850501  0.11453762]\n",
      " [-0.15359494  0.0512512  -0.33100342 -0.18994453 -0.13995772  0.00814778\n",
      "  -0.03653076 -0.19557029  0.11370526]\n",
      " [ 0.23642421 -0.04953728  0.33057722 -0.13010878 -0.08111844 -0.22412343\n",
      "  -0.00608019  0.31133    -0.11918178]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.18040038  0.38957805  0.03935167  0.06842507 -0.24115456  0.08135256\n",
      "  -0.00246526]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:0 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.5037958]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 0 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.01564533 -0.16733207  0.24086814  0.01631997 -0.16294868  0.00664007\n",
      "   0.3418085  -0.23151725 -0.09693142]\n",
      " [ 0.02405133  0.25213146 -0.30499615 -0.27249169  0.14980075 -0.25343178\n",
      "   0.23683011  0.10186507  0.23131753]\n",
      " [ 0.15173194  0.15259033  0.07366951 -0.12400228 -0.3261007   0.1826944\n",
      "  -0.03828654 -0.23409701 -0.26011409]\n",
      " [-0.10237591 -0.1696133   0.10716039 -0.01695827 -0.2252512   0.23903827\n",
      "   0.08920964  0.33850501  0.11453762]\n",
      " [-0.15855807  0.04628807 -0.33100342 -0.19490766 -0.13995772  0.00814778\n",
      "  -0.04149389 -0.19557029  0.11370526]\n",
      " [ 0.23720981 -0.04875168  0.33057722 -0.12932319 -0.08111844 -0.22412343\n",
      "  -0.0052946   0.31133    -0.11918178]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.11837843  0.42359526  0.07386189  0.10150718 -0.21305268  0.10731049\n",
      "   0.02933164]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:0 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.54489566]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 0 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.02344416 -0.16733207  0.24866697  0.01631997 -0.16294868  0.0144389\n",
      "   0.34960733 -0.23151725 -0.09693142]\n",
      " [ 0.01994529  0.25213146 -0.30910219 -0.27249169  0.14980075 -0.25753782\n",
      "   0.23272407  0.10186507  0.23131753]\n",
      " [ 0.15677459  0.15259033  0.07871216 -0.12400228 -0.3261007   0.18773705\n",
      "  -0.03324389 -0.23409701 -0.26011409]\n",
      " [-0.09780561 -0.1696133   0.1117307  -0.01695827 -0.2252512   0.24360857\n",
      "   0.09377995  0.33850501  0.11453762]\n",
      " [-0.16545277  0.04628807 -0.33789812 -0.19490766 -0.13995772  0.00125308\n",
      "  -0.04838859 -0.19557029  0.11370526]\n",
      " [ 0.24184923 -0.04875168  0.33521664 -0.12932319 -0.08111844 -0.21948401\n",
      "  -0.00065518  0.31133    -0.11918178]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.06194905  0.46009326  0.09790969  0.13488025 -0.18018273  0.12831196\n",
      "   0.0622748 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:0 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.56646705]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 1 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.02729926 -0.16347697  0.24866697  0.02017508 -0.16294868  0.0144389\n",
      "   0.34960733 -0.23151725 -0.09307632]\n",
      " [ 0.01598311  0.24816928 -0.30910219 -0.27645387  0.14980075 -0.25753782\n",
      "   0.23272407  0.10186507  0.22735534]\n",
      " [ 0.15807265  0.15388839  0.07871216 -0.12270422 -0.3261007   0.18773705\n",
      "  -0.03324389 -0.23409701 -0.25881603]\n",
      " [-0.09487342 -0.16668111  0.1117307  -0.01402608 -0.2252512   0.24360857\n",
      "   0.09377995  0.33850501  0.11746981]\n",
      " [-0.16200327  0.04973757 -0.33789812 -0.19145816 -0.13995772  0.00125308\n",
      "  -0.04838859 -0.19557029  0.11715476]\n",
      " [ 0.24281198 -0.04778893  0.33521664 -0.12836043 -0.08111844 -0.21948401\n",
      "  -0.00065518  0.31133    -0.11821902]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.13150614  0.42920228  0.05913366  0.10140097 -0.21201497  0.09700604\n",
      "   0.0284595 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:1 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.54351976]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 1 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.03534255 -0.16347697  0.25671026  0.02017508 -0.16294868  0.0144389\n",
      "   0.35765062 -0.23151725 -0.09307632]\n",
      " [ 0.01512888  0.24816928 -0.30995642 -0.27645387  0.14980075 -0.25753782\n",
      "   0.23186984  0.10186507  0.22735534]\n",
      " [ 0.16092452  0.15388839  0.08156403 -0.12270422 -0.3261007   0.18773705\n",
      "  -0.03039201 -0.23409701 -0.25881603]\n",
      " [-0.09331192 -0.16668111  0.1132922  -0.01402608 -0.2252512   0.24360857\n",
      "   0.09534145  0.33850501  0.11746981]\n",
      " [-0.16921004  0.04973757 -0.34510489 -0.19145816 -0.13995772  0.00125308\n",
      "  -0.05559536 -0.19557029  0.11715476]\n",
      " [ 0.25034075 -0.04778893  0.34274541 -0.12836043 -0.08111844 -0.21948401\n",
      "   0.00687359  0.31133    -0.11821902]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.07487839  0.46609441  0.08659279  0.13258645 -0.1821364   0.11774659\n",
      "   0.06472746]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:1 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.58311311]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 1 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.027217   -0.17160252  0.24858471  0.02017508 -0.16294868  0.0144389\n",
      "   0.34952507 -0.23151725 -0.09307632]\n",
      " [ 0.01187514  0.24491554 -0.31321016 -0.27645387  0.14980075 -0.25753782\n",
      "   0.2286161   0.10186507  0.22735534]\n",
      " [ 0.15465211  0.14761598  0.07529162 -0.12270422 -0.3261007   0.18773705\n",
      "  -0.03666443 -0.23409701 -0.25881603]\n",
      " [-0.09240249 -0.16577169  0.11420162 -0.01402608 -0.2252512   0.24360857\n",
      "   0.09625087  0.33850501  0.11746981]\n",
      " [-0.16058961  0.058358   -0.33648446 -0.19145816 -0.13995772  0.00125308\n",
      "  -0.04697493 -0.19557029  0.11715476]\n",
      " [ 0.24126638 -0.05686329  0.33367105 -0.12836043 -0.08111844 -0.21948401\n",
      "  -0.00220078  0.31133    -0.11821902]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.14575352  0.42220733  0.04788285  0.09073551 -0.21666414  0.0913235\n",
      "   0.01974734]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:1 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.51341021]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 1 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.02788957 -0.17160252  0.24925728  0.02017508 -0.16294868  0.0144389\n",
      "   0.34952507 -0.23084468 -0.09307632]\n",
      " [ 0.00887415  0.24491554 -0.31621115 -0.27645387  0.14980075 -0.25753782\n",
      "   0.2286161   0.09886407  0.22735534]\n",
      " [ 0.154589    0.14761598  0.07522851 -0.12270422 -0.3261007   0.18773705\n",
      "  -0.03666443 -0.23416012 -0.25881603]\n",
      " [-0.08710158 -0.16577169  0.11950253 -0.01402608 -0.2252512   0.24360857\n",
      "   0.09625087  0.34380592  0.11746981]\n",
      " [-0.16994649  0.058358   -0.34584134 -0.19145816 -0.13995772  0.00125308\n",
      "  -0.04697493 -0.20492717  0.11715476]\n",
      " [ 0.25240002 -0.05686329  0.34480468 -0.12836043 -0.08111844 -0.21948401\n",
      "  -0.00220078  0.32246364 -0.11821902]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.08497355  0.4532701   0.0752519   0.12106238 -0.18085781  0.11159028\n",
      "   0.06278681]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:1 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.5539035]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 1 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.02726969 -0.17160252  0.24925728  0.0195552  -0.16294868  0.0144389\n",
      "   0.34952507 -0.23084468 -0.0936962 ]\n",
      " [ 0.00832014  0.24491554 -0.31621115 -0.27700788  0.14980075 -0.25753782\n",
      "   0.2286161   0.09886407  0.22680134]\n",
      " [ 0.15150214  0.14761598  0.07522851 -0.12579108 -0.3261007   0.18773705\n",
      "  -0.03666443 -0.23416012 -0.26190289]\n",
      " [-0.08687642 -0.16577169  0.11950253 -0.01380092 -0.2252512   0.24360857\n",
      "   0.09625087  0.34380592  0.11769496]\n",
      " [-0.17326219  0.058358   -0.34584134 -0.19477386 -0.13995772  0.00125308\n",
      "  -0.04697493 -0.20492717  0.11383906]\n",
      " [ 0.25248022 -0.05686329  0.34480468 -0.12828023 -0.08111844 -0.21948401\n",
      "  -0.00220078  0.32246364 -0.11813883]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.02985957  0.480207    0.10225473  0.14550595 -0.15307565  0.13579851\n",
      "   0.090424  ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:1 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.58100649]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 1 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.03102618 -0.17160252  0.24925728  0.0195552  -0.15919218  0.01819539\n",
      "   0.34952507 -0.23084468 -0.0899397 ]\n",
      " [ 0.00607711  0.24491554 -0.31621115 -0.27700788  0.14755772 -0.25978085\n",
      "   0.2286161   0.09886407  0.22455831]\n",
      " [ 0.15583292  0.14761598  0.07522851 -0.12579108 -0.32176992  0.19206782\n",
      "  -0.03666443 -0.23416012 -0.25757211]\n",
      " [-0.08774532 -0.16577169  0.11950253 -0.01380092 -0.2261201   0.24273967\n",
      "   0.09625087  0.34380592  0.11682607]\n",
      " [-0.16979346  0.058358   -0.34584134 -0.19477386 -0.13648899  0.00472181\n",
      "  -0.04697493 -0.20492717  0.11730778]\n",
      " [ 0.25539947 -0.05686329  0.34480468 -0.12828023 -0.07819919 -0.21656475\n",
      "  -0.00220078  0.32246364 -0.11521957]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.10057908  0.44863273  0.06464587  0.11452178 -0.18930466  0.10393021\n",
      "   0.05799696]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:1 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.5402132]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 1 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 3.70472102e-02 -1.65581496e-01  2.49257279e-01  1.95551974e-02\n",
      "  -1.53171156e-01  2.42164197e-02  3.49525068e-01 -2.30844682e-01\n",
      "  -8.39186750e-02]\n",
      " [ 1.79976409e-04  2.39018403e-01 -3.16211155e-01 -2.77007876e-01\n",
      "   1.41660585e-01 -2.65677989e-01  2.28616102e-01  9.88640732e-02\n",
      "   2.18661171e-01]\n",
      " [ 1.57236411e-01  1.49019466e-01  7.52285102e-02 -1.25791077e-01\n",
      "  -3.20366430e-01  1.93471314e-01 -3.66644252e-02 -2.34160121e-01\n",
      "  -2.56168623e-01]\n",
      " [-8.57386713e-02 -1.63765035e-01  1.19502534e-01 -1.38009230e-02\n",
      "  -2.24113446e-01  2.44746323e-01  9.62508742e-02  3.43805919e-01\n",
      "   1.18832718e-01]\n",
      " [-1.67690241e-01  6.04612246e-02 -3.45841339e-01 -1.94773858e-01\n",
      "  -1.34385771e-01  6.82502909e-03 -4.69749293e-02 -2.04927171e-01\n",
      "   1.19411004e-01]\n",
      " [ 2.58906621e-01 -5.33561471e-02  3.44804683e-01 -1.28280233e-01\n",
      "  -7.46920441e-02 -2.13057606e-01 -2.20077568e-03  3.22463638e-01\n",
      "  -1.11712426e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.16766895  0.42124829  0.0250732   0.08238198 -0.22083811  0.07249406\n",
      "   0.02798537]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:1 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.51322319]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 1 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.03557527 -0.16705344  0.24925728  0.0195552  -0.1546431   0.02274448\n",
      "   0.34805312 -0.23084468 -0.08391867]\n",
      " [-0.0051704   0.23366803 -0.31621115 -0.27700788  0.13631021 -0.27102837\n",
      "   0.22326573  0.09886407  0.21866117]\n",
      " [ 0.15496102  0.14674407  0.07522851 -0.12579108 -0.32264182  0.19119592\n",
      "  -0.03893982 -0.23416012 -0.25616862]\n",
      " [-0.08362249 -0.16164885  0.11950253 -0.01380092 -0.22199726  0.24686251\n",
      "   0.09836706  0.34380592  0.11883272]\n",
      " [-0.16326285  0.06488861 -0.34584134 -0.19477386 -0.12995838  0.01125242\n",
      "  -0.04254754 -0.20492717  0.119411  ]\n",
      " [ 0.26025689 -0.05200587  0.34480468 -0.12828023 -0.07334177 -0.21170733\n",
      "  -0.0008505   0.32246364 -0.11171243]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.23177697  0.38772026 -0.01243721  0.04804484 -0.25076974  0.04492625\n",
      "  -0.00271677]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:1 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.47742641]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 1 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.04188605 -0.16705344  0.24925728  0.02586598 -0.1546431   0.02274448\n",
      "   0.35436391 -0.23084468 -0.08391867]\n",
      " [-0.00612967  0.23366803 -0.31621115 -0.27796715  0.13631021 -0.27102837\n",
      "   0.22230645  0.09886407  0.21866117]\n",
      " [ 0.1548018   0.14674407  0.07522851 -0.12595029 -0.32264182  0.19119592\n",
      "  -0.03909903 -0.23416012 -0.25616862]\n",
      " [-0.08360711 -0.16164885  0.11950253 -0.01378554 -0.22199726  0.24686251\n",
      "   0.09838244  0.34380592  0.11883272]\n",
      " [-0.16953618  0.06488861 -0.34584134 -0.20104719 -0.12995838  0.01125242\n",
      "  -0.04882087 -0.20492717  0.119411  ]\n",
      " [ 0.26238471 -0.05200587  0.34480468 -0.12615241 -0.07334177 -0.21170733\n",
      "   0.00127732  0.32246364 -0.11171243]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.16658842  0.42679769  0.01919724  0.0804799  -0.21816008  0.07107807\n",
      "   0.03201143]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:1 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.51473685]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 1 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.04582215 -0.16705344  0.24925728  0.02586598 -0.150707    0.02668058\n",
      "   0.3583     -0.23084468 -0.08391867]\n",
      " [-0.00489751  0.23366803 -0.31621115 -0.27796715  0.13754237 -0.26979621\n",
      "   0.22353861  0.09886407  0.21866117]\n",
      " [ 0.15456329  0.14674407  0.07522851 -0.12595029 -0.32288034  0.19095741\n",
      "  -0.03933755 -0.23416012 -0.25616862]\n",
      " [-0.08300674 -0.16164885  0.11950253 -0.01378554 -0.22139689  0.24746288\n",
      "   0.09898281  0.34380592  0.11883272]\n",
      " [-0.17450077  0.06488861 -0.34584134 -0.20104719 -0.13492298  0.00628782\n",
      "  -0.05378546 -0.20492717  0.119411  ]\n",
      " [ 0.26206071 -0.05200587  0.34480468 -0.12615241 -0.07366578 -0.21203134\n",
      "   0.00095331  0.32246364 -0.11171243]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.10598322  0.46108239  0.05073336  0.11054398 -0.18725695  0.09632154\n",
      "   0.06199   ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:1 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.54134348]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 1 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.04557938 -0.16705344  0.24925728  0.02562321 -0.150707    0.02643781\n",
      "   0.3583     -0.23084468 -0.08416144]\n",
      " [ 0.00056048  0.23366803 -0.31621115 -0.27250915  0.13754237 -0.26433821\n",
      "   0.22353861  0.09886407  0.22411917]\n",
      " [ 0.15517798  0.14674407  0.07522851 -0.1253356  -0.32288034  0.1915721\n",
      "  -0.03933755 -0.23416012 -0.25555393]\n",
      " [-0.08745352 -0.16164885  0.11950253 -0.01823232 -0.22139689  0.2430161\n",
      "   0.09898281  0.34380592  0.11438594]\n",
      " [-0.17036781  0.06488861 -0.34584134 -0.19691423 -0.13492298  0.01042078\n",
      "  -0.05378546 -0.20492717  0.12354396]\n",
      " [ 0.26518892 -0.05200587  0.34480468 -0.1230242  -0.07366578 -0.20890313\n",
      "   0.00095331  0.32246364 -0.10858422]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.1731885   0.42723697  0.02269076  0.07755617 -0.22536039  0.06689499\n",
      "   0.031534  ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:1 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.51318281]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 1 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.0498808  -0.16275202  0.24925728  0.02992463 -0.150707    0.03073923\n",
      "   0.36260143 -0.23084468 -0.08416144]\n",
      " [-0.00063985  0.23246769 -0.31621115 -0.27370949  0.13754237 -0.26553855\n",
      "   0.22233827  0.09886407  0.22411917]\n",
      " [ 0.16004414  0.15161024  0.07522851 -0.12046944 -0.32288034  0.19643826\n",
      "  -0.03447139 -0.23416012 -0.25555393]\n",
      " [-0.08632002 -0.16051535  0.11950253 -0.01709882 -0.22139689  0.2441496\n",
      "   0.10011631  0.34380592  0.11438594]\n",
      " [-0.17547017  0.05978626 -0.34584134 -0.20201658 -0.13492298  0.00531843\n",
      "  -0.05888782 -0.20492717  0.12354396]\n",
      " [ 0.2634044  -0.05379039  0.34480468 -0.12480872 -0.07366578 -0.21068764\n",
      "  -0.0008312   0.32246364 -0.10858422]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.11237865  0.4620034   0.0518941   0.11291542 -0.19382092  0.09209529\n",
      "   0.06015028]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:1 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.55470364]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 1 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 5.36515554e-02 -1.58981265e-01  2.49257279e-01  3.36953860e-02\n",
      "  -1.50706999e-01  3.07392322e-02  3.66372180e-01 -2.30844682e-01\n",
      "  -8.41614418e-02]\n",
      " [ 1.82114497e-03  2.34928688e-01 -3.16211155e-01 -2.71248490e-01\n",
      "   1.37542368e-01 -2.65538545e-01  2.24799273e-01  9.88640732e-02\n",
      "   2.24119168e-01]\n",
      " [ 1.62185613e-01  1.53751708e-01  7.52285102e-02 -1.18327968e-01\n",
      "  -3.22880337e-01  1.96438262e-01 -3.23299154e-02 -2.34160121e-01\n",
      "  -2.55553930e-01]\n",
      " [-8.85572966e-02 -1.62752633e-01  1.19502534e-01 -1.93361010e-02\n",
      "  -2.21396894e-01  2.44149598e-01  9.78790259e-02  3.43805919e-01\n",
      "   1.14385941e-01]\n",
      " [-1.80468540e-01  5.47878867e-02 -3.45841339e-01 -2.07014951e-01\n",
      "  -1.34922978e-01  5.31842784e-03 -6.38861883e-02 -2.04927171e-01\n",
      "   1.23543964e-01]\n",
      " [ 2.64556926e-01 -5.26378681e-02  3.44804683e-01 -1.23656195e-01\n",
      "  -7.36657764e-02 -2.10687644e-01  3.21320024e-04  3.22463638e-01\n",
      "  -1.08584216e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.05738288  0.49332138  0.08186636  0.14256355 -0.16857033  0.11447582\n",
      "   0.08880204]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:1 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.59524101]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 1 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.06121909 -0.15898126  0.25682481  0.03369539 -0.150707    0.03830677\n",
      "   0.37393971 -0.23084468 -0.08416144]\n",
      " [-0.00237413  0.23492869 -0.32040643 -0.27124849  0.13754237 -0.26973382\n",
      "   0.22060399  0.09886407  0.22411917]\n",
      " [ 0.16688799  0.15375171  0.07993089 -0.11832797 -0.32288034  0.20114064\n",
      "  -0.02762753 -0.23416012 -0.25555393]\n",
      " [-0.08416535 -0.16275263  0.12389448 -0.0193361  -0.22139689  0.24854155\n",
      "   0.10227098  0.34380592  0.11438594]\n",
      " [-0.1870215   0.05478789 -0.3523943  -0.20701495 -0.13492298 -0.00123453\n",
      "  -0.07043915 -0.20492717  0.12354396]\n",
      " [ 0.26923204 -0.05263787  0.3494798  -0.12365619 -0.07366578 -0.20601253\n",
      "   0.00499644  0.32246364 -0.10858422]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.00862376  0.52590185  0.1019619   0.17177287 -0.13969628  0.13192237\n",
      "   0.11798176]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:1 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.60516149]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 2 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.06388361 -0.15631674  0.25682481  0.03635991 -0.150707    0.03830677\n",
      "   0.37393971 -0.23084468 -0.08149692]\n",
      " [-0.005697    0.23160582 -0.32040643 -0.27457136  0.13754237 -0.26973382\n",
      "   0.22060399  0.09886407  0.2207963 ]\n",
      " [ 0.16784965  0.15471336  0.07993089 -0.11736631 -0.32288034  0.20114064\n",
      "  -0.02762753 -0.23416012 -0.25459227]\n",
      " [-0.08143614 -0.16002342  0.12389448 -0.01660689 -0.22139689  0.24854155\n",
      "   0.10227098  0.34380592  0.11711515]\n",
      " [-0.1831677   0.05864169 -0.3523943  -0.20316115 -0.13492298 -0.00123453\n",
      "  -0.07043915 -0.20492717  0.12739777]\n",
      " [ 0.26951483 -0.05235508  0.3494798  -0.12337341 -0.07366578 -0.20601253\n",
      "   0.00499644  0.32246364 -0.10830143]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.08092272  0.49242666  0.06247048  0.1365855  -0.17310605  0.09965665\n",
      "   0.08211507]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:2 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.58631024]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 2 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.07162434 -0.15631674  0.26456554  0.03635991 -0.150707    0.03830677\n",
      "   0.38168044 -0.23084468 -0.08149692]\n",
      " [-0.00701656  0.23160582 -0.32172599 -0.27457136  0.13754237 -0.26973382\n",
      "   0.21928443  0.09886407  0.2207963 ]\n",
      " [ 0.17057775  0.15471336  0.08265899 -0.11736631 -0.32288034  0.20114064\n",
      "  -0.02489944 -0.23416012 -0.25459227]\n",
      " [-0.07963033 -0.16002342  0.12570029 -0.01660689 -0.22139689  0.24854155\n",
      "   0.10407678  0.34380592  0.11711515]\n",
      " [-0.19011128  0.05864169 -0.35933788 -0.20316115 -0.13492298 -0.00123453\n",
      "  -0.07738273 -0.20492717  0.12739777]\n",
      " [ 0.27662629 -0.05235508  0.35659127 -0.12337341 -0.07366578 -0.20601253\n",
      "   0.0121079   0.32246364 -0.10830143]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.03075239  0.52589028  0.08623364  0.16442086 -0.14620877  0.11736537\n",
      "   0.11478227]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:2 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.6204082]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 2 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.06213632 -0.16580477  0.25507752  0.03635991 -0.150707    0.03830677\n",
      "   0.37219242 -0.23084468 -0.08149692]\n",
      " [-0.0092391   0.22938329 -0.32394853 -0.27457136  0.13754237 -0.26973382\n",
      "   0.2170619   0.09886407  0.2207963 ]\n",
      " [ 0.16383241  0.14796803  0.07591365 -0.11736631 -0.32288034  0.20114064\n",
      "  -0.03164478 -0.23416012 -0.25459227]\n",
      " [-0.07944995 -0.15984304  0.12588067 -0.01660689 -0.22139689  0.24854155\n",
      "   0.10425716  0.34380592  0.11711515]\n",
      " [-0.18052864  0.06822433 -0.34975524 -0.20316115 -0.13492298 -0.00123453\n",
      "  -0.06780009 -0.20492717  0.12739777]\n",
      " [ 0.26669547 -0.06228591  0.34666044 -0.12337341 -0.07366578 -0.20601253\n",
      "   0.00217708  0.32246364 -0.10830143]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.10380603  0.47936885  0.04747875  0.12098253 -0.18255521  0.09094519\n",
      "   0.06773233]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:2 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.54911824]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 2 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.06333927 -0.16580477  0.25628047  0.03635991 -0.150707    0.03830677\n",
      "   0.37219242 -0.22964172 -0.08149692]\n",
      " [-0.01246439  0.22938329 -0.32717382 -0.27457136  0.13754237 -0.26973382\n",
      "   0.2170619   0.09563878  0.2207963 ]\n",
      " [ 0.16391036  0.14796803  0.0759916  -0.11736631 -0.32288034  0.20114064\n",
      "  -0.03164478 -0.23408218 -0.25459227]\n",
      " [-0.07420672 -0.15984304  0.1311239  -0.01660689 -0.22139689  0.24854155\n",
      "   0.10425716  0.34904915  0.11711515]\n",
      " [-0.18951743  0.06822433 -0.35874403 -0.20316115 -0.13492298 -0.00123453\n",
      "  -0.06780009 -0.21391596  0.12739777]\n",
      " [ 0.2772658  -0.06228591  0.35723077 -0.12337341 -0.07366578 -0.20601253\n",
      "   0.00217708  0.33303397 -0.10830143]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.04798971  0.50848146  0.07213202  0.14896863 -0.14926972  0.10903259\n",
      "   0.10782265]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:2 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.5834742]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 2 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.06356958 -0.16580477  0.25628047  0.03659022 -0.150707    0.03830677\n",
      "   0.37219242 -0.22964172 -0.08126661]\n",
      " [-0.01330164  0.22938329 -0.32717382 -0.27540861  0.13754237 -0.26973382\n",
      "   0.2170619   0.09563878  0.21995905]\n",
      " [ 0.16130607  0.14796803  0.0759916  -0.11997059 -0.32288034  0.20114064\n",
      "  -0.03164478 -0.23408218 -0.25719656]\n",
      " [-0.07387397 -0.15984304  0.1311239  -0.01627414 -0.22139689  0.24854155\n",
      "   0.10425716  0.34904915  0.1174479 ]\n",
      " [-0.19281583  0.06822433 -0.35874403 -0.20645955 -0.13492298 -0.00123453\n",
      "  -0.06780009 -0.21391596  0.12409937]\n",
      " [ 0.27784239 -0.06228591  0.35723077 -0.12279682 -0.07366578 -0.20601253\n",
      "   0.00217708  0.33303397 -0.10772484]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.00262485  0.53401906  0.09660144  0.1716528  -0.12362965  0.13100265\n",
      "   0.13370672]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:2 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.60752547]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 2 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.06591533 -0.16580477  0.25628047  0.03659022 -0.14836125  0.04065252\n",
      "   0.37219242 -0.22964172 -0.07892086]\n",
      " [-0.01464814  0.22938329 -0.32717382 -0.27540861  0.13619587 -0.27108032\n",
      "   0.2170619   0.09563878  0.21861255]\n",
      " [ 0.16520044  0.14796803  0.0759916  -0.11997059 -0.31898597  0.20503501\n",
      "  -0.03164478 -0.23408218 -0.25330219]\n",
      " [-0.07515288 -0.15984304  0.1311239  -0.01627414 -0.22267581  0.24726263\n",
      "   0.10425716  0.34904915  0.11616899]\n",
      " [-0.1891448   0.06822433 -0.35874403 -0.20645955 -0.13125195  0.0024365\n",
      "  -0.06780009 -0.21391596  0.1277704 ]\n",
      " [ 0.27982029 -0.06228591  0.35723077 -0.12279682 -0.07168788 -0.20403463\n",
      "   0.00217708  0.33303397 -0.10574695]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.06980381  0.50015711  0.05903936  0.13936366 -0.16112396  0.09848508\n",
      "   0.09947424]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:2 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.56501142]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 2 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.07078807 -0.16093203  0.25628047  0.03659022 -0.14348851  0.04552526\n",
      "   0.37219242 -0.22964172 -0.07404812]\n",
      " [-0.01971521  0.22431621 -0.32717382 -0.27540861  0.1311288  -0.2761474\n",
      "   0.2170619   0.09563878  0.21354548]\n",
      " [ 0.16613856  0.14890615  0.0759916  -0.11997059 -0.31804784  0.20597314\n",
      "  -0.03164478 -0.23408218 -0.25236406]\n",
      " [-0.07352068 -0.15821084  0.1311239  -0.01627414 -0.2210436   0.24889484\n",
      "   0.10425716  0.34904915  0.11780119]\n",
      " [-0.18703556  0.07033357 -0.35874403 -0.20645955 -0.12914271  0.00454574\n",
      "  -0.06780009 -0.21391596  0.12987964]\n",
      " [ 0.28264686 -0.05945934  0.35723077 -0.12279682 -0.06886131 -0.20120806\n",
      "   0.00217708  0.33303397 -0.10292038]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.13923623  0.47038057  0.01918052  0.10558602 -0.19420555  0.06588334\n",
      "   0.06759728]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:2 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.53852785]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 2 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.06773444 -0.16398566  0.25628047  0.03659022 -0.14654214  0.04247162\n",
      "   0.36913879 -0.22964172 -0.07404812]\n",
      " [-0.02425578  0.21977564 -0.32717382 -0.27540861  0.12658822 -0.28068797\n",
      "   0.21252133  0.09563878  0.21354548]\n",
      " [ 0.16329338  0.14606096  0.0759916  -0.11997059 -0.32089303  0.20312795\n",
      "  -0.03448996 -0.23408218 -0.25236406]\n",
      " [-0.0718582  -0.15654837  0.1311239  -0.01627414 -0.21938113  0.25055731\n",
      "   0.10591963  0.34904915  0.11780119]\n",
      " [-0.18198619  0.07538294 -0.35874403 -0.20645955 -0.12409334  0.00959511\n",
      "  -0.06275072 -0.21391596  0.12987964]\n",
      " [ 0.28339435 -0.05871185  0.35723077 -0.12279682 -0.06811382 -0.20046057\n",
      "   0.00292457  0.33303397 -0.10292038]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.20615252  0.43385152 -0.01887634  0.06926876 -0.22599847  0.03755536\n",
      "   0.03488688]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:2 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.50160562]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 2 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.07471018 -0.16398566  0.25628047  0.04356596 -0.14654214  0.04247162\n",
      "   0.37611453 -0.22964172 -0.07404812]\n",
      " [-0.02561044  0.21977564 -0.32717382 -0.27676326  0.12658822 -0.28068797\n",
      "   0.21116668  0.09563878  0.21354548]\n",
      " [ 0.16343095  0.14606096  0.0759916  -0.11983303 -0.32089303  0.20312795\n",
      "  -0.03435239 -0.23408218 -0.25236406]\n",
      " [-0.0715812  -0.15654837  0.1311239  -0.01599713 -0.21938113  0.25055731\n",
      "   0.10619664  0.34904915  0.11780119]\n",
      " [-0.18866757  0.07538294 -0.35874403 -0.21314092 -0.12409334  0.00959511\n",
      "  -0.0694321  -0.21391596  0.12987964]\n",
      " [ 0.2859242  -0.05871185  0.35723077 -0.12026697 -0.06811382 -0.20046057\n",
      "   0.00545442  0.33303397 -0.10292038]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.14385386  0.47224015  0.01091662  0.10055565 -0.19457212  0.06179429\n",
      "   0.06857735]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:2 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.53700875]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 2 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.07955267 -0.16398566  0.25628047  0.04356596 -0.14169965  0.04731412\n",
      "   0.38095702 -0.22964172 -0.07404812]\n",
      " [-0.02515791  0.21977564 -0.32717382 -0.27676326  0.12704075 -0.28023544\n",
      "   0.2116192   0.09563878  0.21354548]\n",
      " [ 0.16359373  0.14606096  0.0759916  -0.11983303 -0.32073024  0.20329074\n",
      "  -0.03418961 -0.23408218 -0.25236406]\n",
      " [-0.07063553 -0.15654837  0.1311239  -0.01599713 -0.21843546  0.25150298\n",
      "   0.10714231  0.34904915  0.11780119]\n",
      " [-0.1938471   0.07538294 -0.35874403 -0.21314092 -0.12927287  0.00441557\n",
      "  -0.07461163 -0.21391596  0.12987964]\n",
      " [ 0.2862523  -0.05871185  0.35723077 -0.12026697 -0.06778573 -0.20013248\n",
      "   0.00578252  0.33303397 -0.10292038]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.08629702  0.50595869  0.04014764  0.12949686 -0.16484735  0.0852725\n",
      "   0.09768389]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:2 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.56173805]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 2 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.07789037 -0.16398566  0.25628047  0.04190365 -0.14169965  0.04565181\n",
      "   0.38095702 -0.22964172 -0.07571043]\n",
      " [-0.01899749  0.21977564 -0.32717382 -0.27060284  0.12704075 -0.27407502\n",
      "   0.2116192   0.09563878  0.2197059 ]\n",
      " [ 0.16368557  0.14606096  0.0759916  -0.11974119 -0.32073024  0.20338258\n",
      "  -0.03418961 -0.23408218 -0.25227223]\n",
      " [-0.07542565 -0.15654837  0.1311239  -0.02078725 -0.21843546  0.24671285\n",
      "   0.10714231  0.34904915  0.11301107]\n",
      " [-0.18921971  0.07538294 -0.35874403 -0.20851353 -0.12927287  0.00904297\n",
      "  -0.07461163 -0.21391596  0.13450703]\n",
      " [ 0.28861064 -0.05871185  0.35723077 -0.11790862 -0.06778573 -0.19777413\n",
      "   0.00578252  0.33303397 -0.10056203]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.15544372  0.46972046  0.01187517  0.09501535 -0.20427487  0.05538411\n",
      "   0.06547628]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:2 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.5329024]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 2 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.08324991 -0.15862611  0.25628047  0.0472632  -0.14169965  0.05101136\n",
      "   0.38631657 -0.22964172 -0.07571043]\n",
      " [-0.02091162  0.21786152 -0.32717382 -0.27251697  0.12704075 -0.27598915\n",
      "   0.20970508  0.09563878  0.2197059 ]\n",
      " [ 0.16874118  0.15111657  0.0759916  -0.11468558 -0.32073024  0.20843818\n",
      "  -0.029134   -0.23408218 -0.25227223]\n",
      " [-0.07396014 -0.15508286  0.1311239  -0.01932174 -0.21843546  0.24817836\n",
      "   0.10860782  0.34904915  0.11301107]\n",
      " [-0.19465068  0.06995197 -0.35874403 -0.2139445  -0.12927287  0.003612\n",
      "  -0.0800426  -0.21391596  0.13450703]\n",
      " [ 0.28744979 -0.0598727   0.35723077 -0.11906947 -0.06778573 -0.19893498\n",
      "   0.00462167  0.33303397 -0.10056203]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.09730936  0.50427878  0.03902264  0.12924756 -0.17373968  0.07888308\n",
      "   0.09338138]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:2 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.57210629]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 2 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.08779279 -0.15408323  0.25628047  0.05180608 -0.14169965  0.05101136\n",
      "   0.39085945 -0.22964172 -0.07571043]\n",
      " [-0.01916315  0.21960999 -0.32717382 -0.2707685   0.12704075 -0.27598915\n",
      "   0.21145355  0.09563878  0.2197059 ]\n",
      " [ 0.17102838  0.15340378  0.0759916  -0.11239838 -0.32073024  0.20843818\n",
      "  -0.02684679 -0.23408218 -0.25227223]\n",
      " [-0.07578116 -0.15690387  0.1311239  -0.02114276 -0.21843546  0.24817836\n",
      "   0.10678681  0.34904915  0.11301107]\n",
      " [-0.19989937  0.06470328 -0.35874403 -0.21919319 -0.12927287  0.003612\n",
      "  -0.08529129 -0.21391596  0.13450703]\n",
      " [ 0.28892633 -0.05839616  0.35723077 -0.11759293 -0.06778573 -0.19893498\n",
      "   0.00609821  0.33303397 -0.10056203]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.04493502  0.5351066   0.06696353  0.15773377 -0.14937946  0.09966686\n",
      "   0.12104823]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:2 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.61384134]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 2 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.09552816 -0.15408323  0.26401584  0.05180608 -0.14169965  0.05874672\n",
      "   0.39859481 -0.22964172 -0.07571043]\n",
      " [-0.02367135  0.21960999 -0.33168202 -0.2707685   0.12704075 -0.28049735\n",
      "   0.20694534  0.09563878  0.2197059 ]\n",
      " [ 0.175714    0.15340378  0.08067721 -0.11239838 -0.32073024  0.2131238\n",
      "  -0.02216118 -0.23408218 -0.25227223]\n",
      " [-0.07127864 -0.15690387  0.13562642 -0.02114276 -0.21843546  0.25268088\n",
      "   0.11128933  0.34904915  0.11301107]\n",
      " [-0.20652337  0.06470328 -0.36536803 -0.21919319 -0.12927287 -0.003012\n",
      "  -0.09191529 -0.21391596  0.13450703]\n",
      " [ 0.29385557 -0.05839616  0.36216001 -0.11759293 -0.06778573 -0.19400574\n",
      "   0.01102744  0.33303397 -0.10056203]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.00083253  0.56654707  0.08521118  0.18544794 -0.12186576  0.11546461\n",
      "   0.14903182]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:2 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.61746253]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 3 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.0970289  -0.15258249  0.26401584  0.05330683 -0.14169965  0.05874672\n",
      "   0.39859481 -0.22964172 -0.07420968]\n",
      " [-0.02629874  0.21698259 -0.33168202 -0.27339589  0.12704075 -0.28049735\n",
      "   0.20694534  0.09563878  0.2170785 ]\n",
      " [ 0.17636195  0.15405173  0.08067721 -0.11175043 -0.32073024  0.2131238\n",
      "  -0.02216118 -0.23408218 -0.25162428]\n",
      " [-0.06880503 -0.15443027  0.13562642 -0.01866915 -0.21843546  0.25268088\n",
      "   0.11128933  0.34904915  0.11548467]\n",
      " [-0.2024465   0.06878015 -0.36536803 -0.21511632 -0.12927287 -0.003012\n",
      "  -0.09191529 -0.21391596  0.1385839 ]\n",
      " [ 0.29354012 -0.05871161  0.36216001 -0.11790838 -0.06778573 -0.19400574\n",
      "   0.01102744  0.33303397 -0.10087748]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.07209058  0.53158796  0.04611303  0.14963447 -0.15584604  0.08311487\n",
      "   0.11225479]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:3 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.60171244]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 3 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.10490114 -0.15258249  0.27188808  0.05330683 -0.14169965  0.05874672\n",
      "   0.40646705 -0.22964172 -0.07420968]\n",
      " [-0.02809057  0.21698259 -0.33347385 -0.27339589  0.12704075 -0.28049735\n",
      "   0.20515351  0.09563878  0.2170785 ]\n",
      " [ 0.17912608  0.15405173  0.08344134 -0.11175043 -0.32073024  0.2131238\n",
      "  -0.01939705 -0.23408218 -0.25162428]\n",
      " [-0.06669668 -0.15443027  0.13773477 -0.01866915 -0.21843546  0.25268088\n",
      "   0.11339768  0.34904915  0.11548467]\n",
      " [-0.20952002  0.06878015 -0.37244155 -0.21511632 -0.12927287 -0.003012\n",
      "  -0.09898881 -0.21391596  0.1385839 ]\n",
      " [ 0.30067268 -0.05871161  0.36929257 -0.11790838 -0.06778573 -0.19400574\n",
      "   0.01816     0.33303397 -0.10087748]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.02436486  0.56410233  0.06817724  0.17628695 -0.12986367  0.09937981\n",
      "   0.14379052]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:3 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.63347902]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 3 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.0943866  -0.16309703  0.26137354  0.05330683 -0.14169965  0.05874672\n",
      "   0.39595251 -0.22964172 -0.07420968]\n",
      " [-0.02920319  0.21586998 -0.33458647 -0.27339589  0.12704075 -0.28049735\n",
      "   0.2040409   0.09563878  0.2170785 ]\n",
      " [ 0.17210366  0.14702931  0.07641892 -0.11175043 -0.32073024  0.2131238\n",
      "  -0.02641947 -0.23408218 -0.25162428]\n",
      " [-0.06724822 -0.1549818   0.13718323 -0.01866915 -0.21843546  0.25268088\n",
      "   0.11284614  0.34904915  0.11548467]\n",
      " [-0.19925695  0.07904321 -0.36217848 -0.21511632 -0.12927287 -0.003012\n",
      "  -0.08872575 -0.21391596  0.1385839 ]\n",
      " [ 0.29017511 -0.06920917  0.358795   -0.11790838 -0.06778573 -0.19400574\n",
      "   0.00766244  0.33303397 -0.10087748]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.0979065   0.51610595  0.03029313  0.13230757 -0.16718611  0.0735252\n",
      "   0.09581515]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:3 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: [1.] Net Result: [[0.56173186]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 3 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.09608083 -0.16309703  0.26306777  0.05330683 -0.14169965  0.05874672\n",
      "   0.39595251 -0.2279475  -0.07420968]\n",
      " [-0.03275553  0.21586998 -0.33813881 -0.27339589  0.12704075 -0.28049735\n",
      "   0.2040409   0.09208644  0.2170785 ]\n",
      " [ 0.17229841  0.14702931  0.07661367 -0.11175043 -0.32073024  0.2131238\n",
      "  -0.02641947 -0.23388743 -0.25162428]\n",
      " [-0.06183826 -0.1549818   0.1425932  -0.01866915 -0.21843546  0.25268088\n",
      "   0.11284614  0.35445912  0.11548467]\n",
      " [-0.2082876   0.07904321 -0.37120913 -0.21511632 -0.12927287 -0.003012\n",
      "  -0.08872575 -0.22294661  0.1385839 ]\n",
      " [ 0.30067745 -0.06920917  0.36929734 -0.11790838 -0.06778573 -0.19400574\n",
      "   0.00766244  0.34353631 -0.10087748]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.04395807  0.54477889  0.05367228  0.15947655 -0.13464225  0.09053636\n",
      "   0.13506294]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:3 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.59317635]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 3 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.09700209 -0.16309703  0.26306777  0.05422809 -0.14169965  0.05874672\n",
      "   0.39595251 -0.2279475  -0.07328842]\n",
      " [-0.03384644  0.21586998 -0.33813881 -0.2744868   0.12704075 -0.28049735\n",
      "   0.2040409   0.09208644  0.21598759]\n",
      " [ 0.16997484  0.14702931  0.07661367 -0.11407399 -0.32073024  0.2131238\n",
      "  -0.02641947 -0.23388743 -0.25394784]\n",
      " [-0.06140915 -0.1549818   0.1425932  -0.01824005 -0.21843546  0.25268088\n",
      "   0.11284614  0.35445912  0.11591377]\n",
      " [-0.21171289  0.07904321 -0.37120913 -0.21854162 -0.12927287 -0.003012\n",
      "  -0.08872575 -0.22294661  0.1351586 ]\n",
      " [ 0.30168072 -0.06920917  0.36929734 -0.11690511 -0.06778573 -0.19400574\n",
      "   0.00766244  0.34353631 -0.09987421]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.0051289   0.5702445   0.07712341  0.1816823  -0.10966957  0.11160805\n",
      "   0.16061082]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:3 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.61626345]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 3 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.09808031 -0.16309703  0.26306777  0.05422809 -0.14062144  0.05982493\n",
      "   0.39595251 -0.2279475  -0.07221021]\n",
      " [-0.03436888  0.21586998 -0.33813881 -0.2744868   0.12651831 -0.28101979\n",
      "   0.2040409   0.09208644  0.21546515]\n",
      " [ 0.17343301  0.14702931  0.07661367 -0.11407399 -0.31727207  0.21658196\n",
      "  -0.02641947 -0.23388743 -0.25048968]\n",
      " [-0.06302273 -0.1549818   0.1425932  -0.01824005 -0.22004904  0.25106731\n",
      "   0.11284614  0.35445912  0.1143002 ]\n",
      " [-0.20794966  0.07904321 -0.37120913 -0.21854162 -0.12550964  0.00075123\n",
      "  -0.08872575 -0.22294661  0.13892183]\n",
      " [ 0.30277248 -0.06920917  0.36929734 -0.11690511 -0.06669396 -0.19291398\n",
      "   0.00766244  0.34353631 -0.09878245]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.06773895  0.53488942  0.04016697  0.14872774 -0.1477192   0.07896477\n",
      "   0.12526931]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:3 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57228205]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 3 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.1018529  -0.15932443  0.26306777  0.05422809 -0.13684884  0.06359753\n",
      "   0.39595251 -0.2279475  -0.06843761]\n",
      " [-0.03855266  0.2116862  -0.33813881 -0.2744868   0.12233453 -0.28520357\n",
      "   0.2040409   0.09208644  0.21128137]\n",
      " [ 0.17397075  0.14756705  0.07661367 -0.11407399 -0.31673433  0.2171197\n",
      "  -0.02641947 -0.23388743 -0.24995194]\n",
      " [-0.06175167 -0.15371075  0.1425932  -0.01824005 -0.21877798  0.25233836\n",
      "   0.11284614  0.35445912  0.11557126]\n",
      " [-0.2059471   0.08104577 -0.37120913 -0.21854162 -0.12350708  0.00275379\n",
      "  -0.08872575 -0.22294661  0.14092439]\n",
      " [ 0.30494972 -0.06703193  0.36929734 -0.11690511 -0.06451673 -0.19073674\n",
      "   0.00766244  0.34353631 -0.09660521]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.13777921  0.50367184  0.00092195  0.11424543 -0.18146715  0.04595159\n",
      "   0.09243208]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:3 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.54686674]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 3 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.09743816 -0.16373918  0.26306777  0.05422809 -0.14126359  0.05918278\n",
      "   0.39153777 -0.2279475  -0.06843761]\n",
      " [-0.04214151  0.20809735 -0.33813881 -0.2744868   0.11874568 -0.28879242\n",
      "   0.20045205  0.09208644  0.21128137]\n",
      " [ 0.17069047  0.14428677  0.07661367 -0.11407399 -0.32001461  0.21383942\n",
      "  -0.02969975 -0.23388743 -0.24995194]\n",
      " [-0.0605833  -0.15254237  0.1425932  -0.01824005 -0.21760961  0.25350674\n",
      "   0.11401451  0.35445912  0.11557126]\n",
      " [-0.20043831  0.08655456 -0.37120913 -0.21854162 -0.11799829  0.00826258\n",
      "  -0.08321696 -0.22294661  0.14092439]\n",
      " [ 0.30511358 -0.06686808  0.36929734 -0.11690511 -0.06435287 -0.19057289\n",
      "   0.00782629  0.34353631 -0.09660521]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.20553696  0.46532628 -0.03657331  0.07706534 -0.21417672  0.01768474\n",
      "   0.05871706]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:3 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.51013771]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 3 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.10516599 -0.16373918  0.26306777  0.06195592 -0.14126359  0.05918278\n",
      "   0.3992656  -0.2279475  -0.06843761]\n",
      " [-0.04391324  0.20809735 -0.33813881 -0.27625854  0.11874568 -0.28879242\n",
      "   0.19868032  0.09208644  0.21128137]\n",
      " [ 0.17110227  0.14428677  0.07661367 -0.11366219 -0.32001461  0.21383942\n",
      "  -0.02928795 -0.23388743 -0.24995194]\n",
      " [-0.06004497 -0.15254237  0.1425932  -0.01770173 -0.21760961  0.25350674\n",
      "   0.11455284  0.35445912  0.11557126]\n",
      " [-0.20765803  0.08655456 -0.37120913 -0.22576134 -0.11799829  0.00826258\n",
      "  -0.09043667 -0.22294661  0.14092439]\n",
      " [ 0.30808464 -0.06686808  0.36929734 -0.11393404 -0.06435287 -0.19057289\n",
      "   0.01079736  0.34353631 -0.09660521]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.14432934  0.50404361 -0.00774523  0.108081   -0.18303448  0.04076151\n",
      "   0.092311  ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:3 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.54476917]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 3 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.11086808 -0.16373918  0.26306777  0.06195592 -0.1355615   0.06488487\n",
      "   0.40496769 -0.2279475  -0.06843761]\n",
      " [-0.04412885  0.20809735 -0.33813881 -0.27625854  0.11853007 -0.28900803\n",
      "   0.1984647   0.09208644  0.21128137]\n",
      " [ 0.17160505  0.14428677  0.07661367 -0.11366219 -0.31951184  0.2143422\n",
      "  -0.02878517 -0.23388743 -0.24995194]\n",
      " [-0.05877179 -0.15254237  0.1425932  -0.01770173 -0.21633642  0.25477992\n",
      "   0.11582602  0.35445912  0.11557126]\n",
      " [-0.21318046  0.08655456 -0.37120913 -0.22576134 -0.12352073  0.00274015\n",
      "  -0.09595911 -0.22294661  0.14092439]\n",
      " [ 0.30898627 -0.06686808  0.36929734 -0.11393404 -0.06345125 -0.18967127\n",
      "   0.01169898  0.34353631 -0.09660521]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.0878817   0.53814057  0.02026298  0.13680771 -0.15353573  0.06330854\n",
      "   0.12143707]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:3 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.56910827]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 3 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.10793622 -0.16373918  0.26306777  0.05902406 -0.1355615   0.06195301\n",
      "   0.40496769 -0.2279475  -0.07136948]\n",
      " [-0.03745188  0.20809735 -0.33813881 -0.26958156  0.11853007 -0.28233106\n",
      "   0.1984647   0.09208644  0.21795834]\n",
      " [ 0.1712155   0.14428677  0.07661367 -0.11405174 -0.31951184  0.21395265\n",
      "  -0.02878517 -0.23388743 -0.25034149]\n",
      " [-0.06378933 -0.15254237  0.1425932  -0.02271927 -0.21633642  0.24976238\n",
      "   0.11582602  0.35445912  0.11055371]\n",
      " [-0.20814005  0.08655456 -0.37120913 -0.22072093 -0.12352073  0.00778056\n",
      "  -0.09595911 -0.22294661  0.1459648 ]\n",
      " [ 0.31057436 -0.06686808  0.36929734 -0.11234595 -0.06345125 -0.18808318\n",
      "   0.01169898  0.34353631 -0.09501712]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.15766121  0.50030492 -0.00777203  0.10152837 -0.19351557  0.03353275\n",
      "   0.0881376 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:3 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.54005846]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 3 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.11429252 -0.15738288  0.26306777  0.06538036 -0.1355615   0.06830931\n",
      "   0.41132399 -0.2279475  -0.07136948]\n",
      " [-0.04004078  0.20550845 -0.33813881 -0.27217046  0.11853007 -0.28491996\n",
      "   0.19587581  0.09208644  0.21795834]\n",
      " [ 0.17653547  0.14960674  0.07661367 -0.10873177 -0.31951184  0.21927262\n",
      "  -0.0234652  -0.23388743 -0.25034149]\n",
      " [-0.06198948 -0.15074252  0.1425932  -0.02091941 -0.21633642  0.25156224\n",
      "   0.11762588  0.35445912  0.11055371]\n",
      " [-0.21401152  0.08068309 -0.37120913 -0.2265924  -0.12352073  0.00190908\n",
      "  -0.10183058 -0.22294661  0.1459648 ]\n",
      " [ 0.3099317  -0.06751074  0.36929734 -0.11298861 -0.06345125 -0.18872583\n",
      "   0.01105633  0.34353631 -0.09501712]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.10053755  0.53545982  0.01818646  0.1355437  -0.16314908  0.05604008\n",
      "   0.11605656]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:3 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.57812494]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 3 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.11961539 -0.15206     0.26306777  0.07070324 -0.1355615   0.06830931\n",
      "   0.41664687 -0.2279475  -0.07136948]\n",
      " [-0.03889614  0.20665309 -0.33813881 -0.27102582  0.11853007 -0.28491996\n",
      "   0.19702045  0.09208644  0.21795834]\n",
      " [ 0.17900663  0.1520779   0.07661367 -0.10626061 -0.31951184  0.21927262\n",
      "  -0.02099404 -0.23388743 -0.25034149]\n",
      " [-0.06347676 -0.15222979  0.1425932  -0.02240669 -0.21633642  0.25156224\n",
      "   0.1161386   0.35445912  0.11055371]\n",
      " [-0.2196448   0.07504982 -0.37120913 -0.23222568 -0.12352073  0.00190908\n",
      "  -0.10746385 -0.22294661  0.1459648 ]\n",
      " [ 0.31172974 -0.06571269  0.36929734 -0.11119057 -0.06345125 -0.18872583\n",
      "   0.01285437  0.34353631 -0.09501712]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.04909063  0.56667453  0.04505609  0.16375384 -0.13891623  0.07592794\n",
      "   0.14358398]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:3 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.6218958]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 3 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.12764876 -0.15206     0.27110114  0.07070324 -0.1355615   0.07634268\n",
      "   0.42468024 -0.2279475  -0.07136948]\n",
      " [-0.04379371  0.20665309 -0.34303638 -0.27102582  0.11853007 -0.28981753\n",
      "   0.19212287  0.09208644  0.21795834]\n",
      " [ 0.18379988  0.1520779   0.08140692 -0.10626061 -0.31951184  0.22406588\n",
      "  -0.01620078 -0.23388743 -0.25034149]\n",
      " [-0.05875088 -0.15222979  0.14731907 -0.02240669 -0.21633642  0.25628811\n",
      "   0.12086448  0.35445912  0.11055371]\n",
      " [-0.22651689  0.07504982 -0.37808122 -0.23222568 -0.12352073 -0.00496301\n",
      "  -0.11433595 -0.22294661  0.1459648 ]\n",
      " [ 0.31700033 -0.06571269  0.37456793 -0.11119057 -0.06345125 -0.18345524\n",
      "   0.01812496  0.34353631 -0.09501712]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.00463665  0.59798144  0.06220714  0.19094037 -0.11180454  0.09071373\n",
      "   0.1713086 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:3 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.62081237]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 4 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.1281068  -0.15160196  0.27110114  0.07116127 -0.1355615   0.07634268\n",
      "   0.42468024 -0.2279475  -0.07091144]\n",
      " [-0.04579333  0.20465348 -0.34303638 -0.27302544  0.11853007 -0.28981753\n",
      "   0.19212287  0.09208644  0.21595872]\n",
      " [ 0.18417843  0.15245645  0.08140692 -0.10588206 -0.31951184  0.22406588\n",
      "  -0.01620078 -0.23388743 -0.24996294]\n",
      " [-0.05651543 -0.14999434  0.14731907 -0.02017124 -0.21633642  0.25628811\n",
      "   0.12086448  0.35445912  0.11278916]\n",
      " [-0.22223492  0.07933179 -0.37808122 -0.22794371 -0.12352073 -0.00496301\n",
      "  -0.11433595 -0.22294661  0.15024677]\n",
      " [ 0.31617724 -0.06653578  0.37456793 -0.11201366 -0.06345125 -0.18345524\n",
      "   0.01812496  0.34353631 -0.09584021]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.07770762  0.56190403  0.02366801  0.15478346 -0.14609895  0.05850066\n",
      "   0.13394974]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:4 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.60796172]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 4 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.13626525 -0.15160196  0.27925958  0.07116127 -0.1355615   0.07634268\n",
      "   0.43283868 -0.2279475  -0.07091144]\n",
      " [-0.04806879  0.20465348 -0.34531184 -0.27302544  0.11853007 -0.28981753\n",
      "   0.18984742  0.09208644  0.21595872]\n",
      " [ 0.18704642  0.15245645  0.08427491 -0.10588206 -0.31951184  0.22406588\n",
      "  -0.01333279 -0.23388743 -0.24996294]\n",
      " [-0.05407063 -0.14999434  0.14976387 -0.02017124 -0.21633642  0.25628811\n",
      "   0.12330928  0.35445912  0.11278916]\n",
      " [-0.22960086  0.07933179 -0.38544715 -0.22794371 -0.12352073 -0.00496301\n",
      "  -0.12170188 -0.22294661  0.15024677]\n",
      " [ 0.32349802 -0.06653578  0.38188872 -0.11201366 -0.06345125 -0.18345524\n",
      "   0.02544574  0.34353631 -0.09584021]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.03098758  0.59437731  0.04473787  0.18104129 -0.12027583  0.07385154\n",
      "   0.16525925]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:4 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.6385478]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 4 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.12486918 -0.16299803  0.26786352  0.07116127 -0.1355615   0.07634268\n",
      "   0.42144262 -0.2279475  -0.07091144]\n",
      " [-0.04808942  0.20463284 -0.34533247 -0.27302544  0.11853007 -0.28981753\n",
      "   0.18982678  0.09208644  0.21595872]\n",
      " [ 0.17979474  0.14520476  0.07702323 -0.10588206 -0.31951184  0.22406588\n",
      "  -0.02058447 -0.23388743 -0.24996294]\n",
      " [-0.05534042 -0.15126413  0.14849408 -0.02017124 -0.21633642  0.25628811\n",
      "   0.12203949  0.35445912  0.11278916]\n",
      " [-0.21870941  0.09022323 -0.37455571 -0.22794371 -0.12352073 -0.00496301\n",
      "  -0.11081043 -0.22294661  0.15024677]\n",
      " [ 0.31251674 -0.07751707  0.37090743 -0.11201366 -0.06345125 -0.18345524\n",
      "   0.01446446  0.34353631 -0.09584021]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.10467744  0.54519159  0.00787231  0.13673935 -0.15839156  0.04869973\n",
      "   0.11660737]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:4 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.56635363]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 4 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.12704811 -0.16299803  0.27004244  0.07116127 -0.1355615   0.07634268\n",
      "   0.42144262 -0.22576857 -0.07091144]\n",
      " [-0.05201133  0.20463284 -0.34925438 -0.27302544  0.11853007 -0.28981753\n",
      "   0.18982678  0.08816453  0.21595872]\n",
      " [ 0.18009997  0.14520476  0.07732846 -0.10588206 -0.31951184  0.22406588\n",
      "  -0.02058447 -0.2335822  -0.24996294]\n",
      " [-0.04967023 -0.15126413  0.15416427 -0.02017124 -0.21633642  0.25628811\n",
      "   0.12203949  0.3601293   0.11278916]\n",
      " [-0.22794922  0.09022323 -0.38379552 -0.22794371 -0.12352073 -0.00496301\n",
      "  -0.11081043 -0.23218641  0.15024677]\n",
      " [ 0.32313418 -0.07751707  0.38152488 -0.11201366 -0.06345125 -0.18345524\n",
      "   0.01446446  0.35415376 -0.09584021]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.05142627  0.57400598  0.03051636  0.16367019 -0.12590454  0.06502485\n",
      "   0.1558176 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:4 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.5959412]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 4 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.12859004 -0.16299803  0.27004244  0.07270321 -0.1355615   0.07634268\n",
      "   0.42144262 -0.22576857 -0.06936951]\n",
      " [-0.05333399  0.20463284 -0.34925438 -0.2743481   0.11853007 -0.28981753\n",
      "   0.18982678  0.08816453  0.21463606]\n",
      " [ 0.17797899  0.14520476  0.07732846 -0.10800304 -0.31951184  0.22406588\n",
      "  -0.02058447 -0.2335822  -0.25208392]\n",
      " [-0.04914815 -0.15126413  0.15416427 -0.01964916 -0.21633642  0.25628811\n",
      "   0.12203949  0.3601293   0.11331125]\n",
      " [-0.23158099  0.09022323 -0.38379552 -0.23157547 -0.12352073 -0.00496301\n",
      "  -0.11081043 -0.23218641  0.14661501]\n",
      " [ 0.32453157 -0.07751707  0.38152488 -0.11061627 -0.06345125 -0.18345524\n",
      "   0.01446446  0.35415376 -0.09444283]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.00277854  0.59987594  0.05351493  0.18586214 -0.10105843  0.08566014\n",
      "   0.18154194]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:4 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.61868771]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 4 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.12859001 -0.16299803  0.27004244  0.07270321 -0.13556153  0.07634265\n",
      "   0.42144262 -0.22576857 -0.06936954]\n",
      " [-0.05315182  0.20463284 -0.34925438 -0.2743481   0.11871224 -0.28963536\n",
      "   0.18982678  0.08816453  0.21481824]\n",
      " [ 0.18105025  0.14520476  0.07732846 -0.10800304 -0.31644058  0.22713714\n",
      "  -0.02058447 -0.2335822  -0.24901266]\n",
      " [-0.05104254 -0.15126413  0.15416427 -0.01964916 -0.21823082  0.25439372\n",
      "   0.12203949  0.3601293   0.11141686]\n",
      " [-0.22773071  0.09022323 -0.38379552 -0.23157547 -0.11967045 -0.00111273\n",
      "  -0.11081043 -0.23218641  0.15046528]\n",
      " [ 0.32483838 -0.07751707  0.38152488 -0.11061627 -0.06314444 -0.18314843\n",
      "   0.01446446  0.35415376 -0.09413602]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.07575685  0.56338676  0.01720796  0.15245898 -0.1394454   0.05305057\n",
      "   0.14535961]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:4 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57317607]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 4 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.13142814 -0.1601599   0.27004244  0.07270321 -0.1327234   0.07918077\n",
      "   0.42144262 -0.22576857 -0.06653141]\n",
      " [-0.05654392  0.20124074 -0.34925438 -0.2743481   0.11532014 -0.29302746\n",
      "   0.18982678  0.08816453  0.21142614]\n",
      " [ 0.18126165  0.14541616  0.07732846 -0.10800304 -0.31622918  0.22734854\n",
      "  -0.02058447 -0.2335822  -0.24880126]\n",
      " [-0.050084   -0.15030559  0.15416427 -0.01964916 -0.21727228  0.25535226\n",
      "   0.12203949  0.3601293   0.1123754 ]\n",
      " [-0.22584622  0.09210772 -0.38379552 -0.23157547 -0.11778596  0.00077176\n",
      "  -0.11081043 -0.23218641  0.15234977]\n",
      " [ 0.32646685 -0.0758886   0.38152488 -0.11061627 -0.06151597 -0.18151996\n",
      "   0.01446446  0.35415376 -0.09250755]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.14586926  0.53118126 -0.02126196  0.11761418 -0.17354258  0.01988251\n",
      "   0.11193423]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:4 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.54928011]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 4 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.12582554 -0.1657625   0.27004244  0.07270321 -0.138326    0.07357818\n",
      "   0.41584002 -0.22576857 -0.06653141]\n",
      " [-0.0591932   0.19859146 -0.34925438 -0.2743481   0.11267086 -0.29567674\n",
      "   0.1871775   0.08816453  0.21142614]\n",
      " [ 0.17761262  0.14176714  0.07732846 -0.10800304 -0.3198782   0.22369951\n",
      "  -0.0242335  -0.2335822  -0.24880126]\n",
      " [-0.04939975 -0.14962135  0.15416427 -0.01964916 -0.21658803  0.2560365\n",
      "   0.12272373  0.3601293   0.1123754 ]\n",
      " [-0.21989684  0.0980571  -0.38379552 -0.23157547 -0.11183658  0.00672114\n",
      "  -0.10486105 -0.23218641  0.15234977]\n",
      " [ 0.32609281 -0.07626264  0.38152488 -0.11061627 -0.06189    -0.18189399\n",
      "   0.01409042  0.35415376 -0.09250755]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.2138623   0.49147411 -0.05791864  0.07993987 -0.20685467 -0.00803416\n",
      "   0.07756365]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:4 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.51329437]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 4 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.13433469 -0.1657625   0.27004244  0.08121236 -0.138326    0.07357818\n",
      "   0.42434917 -0.22576857 -0.06653141]\n",
      " [-0.06140588  0.19859146 -0.34925438 -0.27656078  0.11267086 -0.29567674\n",
      "   0.18496482  0.08816453  0.21142614]\n",
      " [ 0.17830193  0.14176714  0.07732846 -0.10731374 -0.3198782   0.22369951\n",
      "  -0.02354419 -0.2335822  -0.24880126]\n",
      " [-0.04858455 -0.14962135  0.15416427 -0.01883395 -0.21658803  0.2560365\n",
      "   0.12353894  0.3601293   0.1123754 ]\n",
      " [-0.2277305   0.0980571  -0.38379552 -0.23940913 -0.11183658  0.00672114\n",
      "  -0.11269471 -0.23218641  0.15234977]\n",
      " [ 0.32953639 -0.07626264  0.38152488 -0.1071727  -0.06189    -0.18189399\n",
      "   0.017534    0.35415376 -0.09250755]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.1530671   0.53092634 -0.02974164  0.11102701 -0.17564148  0.01411939\n",
      "   0.11143514]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:4 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.54751422]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 4 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.1408504  -0.1657625   0.27004244  0.08121236 -0.13181028  0.08009389\n",
      "   0.43086489 -0.22576857 -0.06653141]\n",
      " [-0.06223815  0.19859146 -0.34925438 -0.27656078  0.11183859 -0.296509\n",
      "   0.18413255  0.08816453  0.21142614]\n",
      " [ 0.17912206  0.14176714  0.07732846 -0.10731374 -0.31905807  0.22451964\n",
      "  -0.02272406 -0.2335822  -0.24880126]\n",
      " [-0.04698671 -0.14962135  0.15416427 -0.01883395 -0.2149902   0.25763434\n",
      "   0.12513677  0.3601293   0.1123754 ]\n",
      " [-0.23367378  0.0980571  -0.38379552 -0.23940913 -0.11777985  0.00077786\n",
      "  -0.11863799 -0.23218641  0.15234977]\n",
      " [ 0.33097983 -0.07626264  0.38152488 -0.1071727  -0.06044656 -0.18045055\n",
      "   0.01897744  0.35415376 -0.09250755]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.09701715  0.56573523 -0.00254941  0.13987259 -0.14601518  0.0360025\n",
      "   0.14090614]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:4 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57186218]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 4 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.13678331 -0.1657625   0.27004244  0.07714526 -0.13181028  0.0760268\n",
      "   0.43086489 -0.22576857 -0.0705985 ]\n",
      " [-0.05514307  0.19859146 -0.34925438 -0.2694657   0.11183859 -0.28941392\n",
      "   0.18413255  0.08816453  0.21852122]\n",
      " [ 0.17829074  0.14176714  0.07732846 -0.10814506 -0.31905807  0.22368832\n",
      "  -0.02272406 -0.2335822  -0.24963258]\n",
      " [-0.05218921 -0.14962135  0.15416427 -0.02403645 -0.2149902   0.25243184\n",
      "   0.12513677  0.3601293   0.1071729 ]\n",
      " [-0.22821497  0.0980571  -0.38379552 -0.23395032 -0.11777985  0.00623667\n",
      "  -0.11863799 -0.23218641  0.15780858]\n",
      " [ 0.33183953 -0.07626264  0.38152488 -0.106313   -0.06044656 -0.17959085\n",
      "   0.01897744  0.35415376 -0.09164785]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.16702332  0.52662731 -0.03024303  0.10403787 -0.18630137  0.00655183\n",
      "   0.10676309]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:4 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.54293385]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 4 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 1.44076602e-01 -1.58469204e-01  2.70042442e-01  8.44385551e-02\n",
      "  -1.31810285e-01  8.33200918e-02  4.38158179e-01 -2.25768572e-01\n",
      "  -7.05985045e-02]\n",
      " [-5.83789485e-02  1.95355575e-01 -3.49254384e-01 -2.72701581e-01\n",
      "   1.11838594e-01 -2.92649804e-01  1.80896670e-01  8.81645308e-02\n",
      "   2.18521220e-01]\n",
      " [ 1.83901963e-01  1.47378359e-01  7.73284598e-02 -1.02533835e-01\n",
      "  -3.19058070e-01  2.29299545e-01 -1.71128368e-02 -2.33582197e-01\n",
      "  -2.49632583e-01]\n",
      " [-5.00504341e-02 -1.47482569e-01  1.54164269e-01 -2.18976719e-02\n",
      "  -2.14990196e-01  2.54570617e-01  1.27275549e-01  3.60129301e-01\n",
      "   1.07172898e-01]\n",
      " [-2.34601475e-01  9.16705926e-02 -3.83795515e-01 -2.40336833e-01\n",
      "  -1.17779855e-01 -1.49838918e-04 -1.25024501e-01 -2.32186413e-01\n",
      "   1.57808584e-01]\n",
      " [ 3.31678624e-01 -7.64235440e-02  3.81524876e-01 -1.06473911e-01\n",
      "  -6.04465551e-02 -1.79751756e-01  1.88165348e-02  3.54153757e-01\n",
      "  -9.16478514e-02]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.11031131  0.56265692 -0.00515184  0.13816589 -0.15579837  0.02827688\n",
      "   0.13495819]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:4 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.58033875]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 4 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 1.50167519e-01 -1.52378287e-01  2.70042442e-01  9.05294725e-02\n",
      "  -1.31810285e-01  8.33200918e-02  4.44249096e-01 -2.25768572e-01\n",
      "  -7.05985045e-02]\n",
      " [-5.78021364e-02  1.95932387e-01 -3.49254384e-01 -2.72124768e-01\n",
      "   1.11838594e-01 -2.92649804e-01  1.81473482e-01  8.81645308e-02\n",
      "   2.18521220e-01]\n",
      " [ 1.86575710e-01  1.50052106e-01  7.73284598e-02 -9.98600883e-02\n",
      "  -3.19058070e-01  2.29299545e-01 -1.44390904e-02 -2.33582197e-01\n",
      "  -2.49632583e-01]\n",
      " [-5.12252967e-02 -1.48657431e-01  1.54164269e-01 -2.30725344e-02\n",
      "  -2.14990196e-01  2.54570617e-01  1.26100687e-01  3.60129301e-01\n",
      "   1.07172898e-01]\n",
      " [-2.40693315e-01  8.55787529e-02 -3.83795515e-01 -2.46428672e-01\n",
      "  -1.17779855e-01 -1.49838918e-04 -1.31116340e-01 -2.32186413e-01\n",
      "   1.57808584e-01]\n",
      " [ 3.33804859e-01 -7.42973092e-02  3.81524876e-01 -1.04347676e-01\n",
      "  -6.04465551e-02 -1.79751756e-01  2.09427696e-02  3.54153757e-01\n",
      "  -9.16478514e-02]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.05920797  0.5945651   0.02097684  0.16641131 -0.13142323  0.04747099\n",
      "   0.16264606]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:4 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.62647296]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 4 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.15850585 -0.15237829  0.27838077  0.09052947 -0.13181028  0.09165842\n",
      "   0.45258743 -0.22576857 -0.0705985 ]\n",
      " [-0.06310051  0.19593239 -0.35455276 -0.27212477  0.11183859 -0.29794818\n",
      "   0.17617511  0.08816453  0.21852122]\n",
      " [ 0.19151797  0.15005211  0.08227072 -0.09986009 -0.31905807  0.23424181\n",
      "  -0.00949683 -0.2335822  -0.24963258]\n",
      " [-0.04623872 -0.14865743  0.15915085 -0.02307253 -0.2149902   0.25955719\n",
      "   0.13108726  0.3601293   0.1071729 ]\n",
      " [-0.24787532  0.08557875 -0.39097752 -0.24642867 -0.11777985 -0.00733185\n",
      "  -0.13829835 -0.23218641  0.15780858]\n",
      " [ 0.33943781 -0.07429731  0.38715783 -0.10434768 -0.06044656 -0.1741188\n",
      "   0.02657572  0.35415376 -0.09164785]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.01550445  0.62606084  0.03728986  0.19339632 -0.10438823  0.06143726\n",
      "   0.19042608]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:4 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.62090358]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 5 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.15802988 -0.15285426  0.27838077  0.0900535  -0.13181028  0.09165842\n",
      "   0.45258743 -0.22576857 -0.07107448]\n",
      " [-0.06454564  0.19448726 -0.35455276 -0.2735699   0.11183859 -0.29794818\n",
      "   0.17617511  0.08816453  0.21707609]\n",
      " [ 0.19166271  0.15019684  0.08227072 -0.09971535 -0.31905807  0.23424181\n",
      "  -0.00949683 -0.2335822  -0.24948785]\n",
      " [-0.04422082 -0.14663953  0.15915085 -0.02105464 -0.2149902   0.25955719\n",
      "   0.13108726  0.3601293   0.1091908 ]\n",
      " [-0.2433628   0.09009128 -0.39097752 -0.24191615 -0.11777985 -0.00733185\n",
      "  -0.13829835 -0.23218641  0.16232111]\n",
      " [ 0.33817613 -0.07555899  0.38715783 -0.10560936 -0.06044656 -0.1741188\n",
      "   0.02657572  0.35415376 -0.09290953]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.08857931  0.58904738 -0.00069421  0.15700362 -0.13890363  0.02945986\n",
      "   0.15262596]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:5 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.61122467]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 5 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.16650774 -0.15285426  0.28685864  0.0900535  -0.13181028  0.09165842\n",
      "   0.4610653  -0.22576857 -0.07107448]\n",
      " [-0.06730995  0.19448726 -0.35731707 -0.2735699   0.11183859 -0.29794818\n",
      "   0.1734108   0.08816453  0.21707609]\n",
      " [ 0.19466366  0.15019684  0.08527168 -0.09971535 -0.31905807  0.23424181\n",
      "  -0.00649587 -0.2335822  -0.24948785]\n",
      " [-0.04142236 -0.14663953  0.16194931 -0.02105464 -0.2149902   0.25955719\n",
      "   0.13388572  0.3601293   0.1091908 ]\n",
      " [-0.25107574  0.09009128 -0.39869047 -0.24191615 -0.11777985 -0.00733185\n",
      "  -0.14601129 -0.23218641  0.16232111]\n",
      " [ 0.34573895 -0.07555899  0.39472065 -0.10560936 -0.06044656 -0.1741188\n",
      "   0.03413854  0.35415376 -0.09290953]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.04238715  0.621783    0.01961029  0.18313575 -0.11298078  0.04405236\n",
      "   0.18401791]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:5 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.64098343]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 5 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.15431995 -0.16504205  0.27467085  0.0900535  -0.13181028  0.09165842\n",
      "   0.44887751 -0.22576857 -0.07107448]\n",
      " [-0.06626481  0.19553239 -0.35627193 -0.2735699   0.11183859 -0.29794818\n",
      "   0.17445594  0.08816453  0.21707609]\n",
      " [ 0.18719282  0.14272599  0.07780083 -0.09971535 -0.31905807  0.23424181\n",
      "  -0.01396672 -0.2335822  -0.24948785]\n",
      " [-0.04340374 -0.14862092  0.15996792 -0.02105464 -0.2149902   0.25955719\n",
      "   0.13190434  0.3601293   0.1091908 ]\n",
      " [-0.23955854  0.10160848 -0.38717327 -0.24191615 -0.11777985 -0.00733185\n",
      "  -0.1344941  -0.23218641  0.16232111]\n",
      " [ 0.33430462 -0.08699332  0.38328631 -0.10560936 -0.06044656 -0.1741188\n",
      "   0.02270421  0.35415376 -0.09290953]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.11613988  0.57150605 -0.01622037  0.13856306 -0.15184236  0.01967319\n",
      "   0.13475295]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:5 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.5682478]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 5 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.1569832  -0.16504205  0.2773341   0.0900535  -0.13181028  0.09165842\n",
      "   0.44887751 -0.22310532 -0.07107448]\n",
      " [-0.07057071  0.19553239 -0.36057783 -0.2735699   0.11183859 -0.29794818\n",
      "   0.17445594  0.08385863  0.21707609]\n",
      " [ 0.18760863  0.14272599  0.07821665 -0.09971535 -0.31905807  0.23424181\n",
      "  -0.01396672 -0.23316638 -0.24948785]\n",
      " [-0.03743732 -0.14862092  0.16593435 -0.02105464 -0.2149902   0.25955719\n",
      "   0.13190434  0.36609573  0.1091908 ]\n",
      " [-0.24906715  0.10160848 -0.39668188 -0.24191615 -0.11777985 -0.00733185\n",
      "  -0.1344941  -0.24169502  0.16232111]\n",
      " [ 0.34509023 -0.08699332  0.39407192 -0.10560936 -0.06044656 -0.1741188\n",
      "   0.02270421  0.36493937 -0.09290953]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.06317636  0.60066943  0.0058748   0.16546071 -0.11916562  0.0354333\n",
      "   0.17420699]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:5 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.59617184]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 5 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.15910516 -0.16504205  0.2773341   0.09217545 -0.13181028  0.09165842\n",
      "   0.44887751 -0.22310532 -0.06895252]\n",
      " [-0.07210868  0.19553239 -0.36057783 -0.27510787  0.11183859 -0.29794818\n",
      "   0.17445594  0.08385863  0.21553812]\n",
      " [ 0.18565757  0.14272599  0.07821665 -0.10166641 -0.31905807  0.23424181\n",
      "  -0.01396672 -0.23316638 -0.2514389 ]\n",
      " [-0.03682158 -0.14862092  0.16593435 -0.0204389  -0.2149902   0.25955719\n",
      "   0.13190434  0.36609573  0.10980653]\n",
      " [-0.25295536  0.10160848 -0.39668188 -0.24580435 -0.11777985 -0.00733185\n",
      "  -0.1344941  -0.24169502  0.15843291]\n",
      " [ 0.34686194 -0.08699332  0.39407192 -0.10383764 -0.06044656 -0.1741188\n",
      "   0.02270421  0.36493937 -0.09113782]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.01456534  0.62710786  0.0286382   0.18780666 -0.09424412  0.05578023\n",
      "   0.20029056]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:5 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.61865448]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 5 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.15819351 -0.16504205  0.2773341   0.09217545 -0.13272194  0.09074677\n",
      "   0.44887751 -0.22310532 -0.06986418]\n",
      " [-0.07133037  0.19553239 -0.36057783 -0.27510787  0.1126169  -0.29716987\n",
      "   0.17445594  0.08385863  0.21631643]\n",
      " [ 0.18838959  0.14272599  0.07821665 -0.10166641 -0.31632606  0.23697382\n",
      "  -0.01396672 -0.23316638 -0.24870689]\n",
      " [-0.03895883 -0.14862092  0.16593435 -0.0204389  -0.21712745  0.25741994\n",
      "   0.13190434  0.36609573  0.10766928]\n",
      " [-0.24899624  0.10160848 -0.39668188 -0.24580435 -0.11382074 -0.00337273\n",
      "  -0.1344941  -0.24169502  0.16239202]\n",
      " [ 0.34647596 -0.08699332  0.39407192 -0.10383764 -0.06083254 -0.17450479\n",
      "   0.02270421  0.36493937 -0.0915238 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.08754217  0.58970741 -0.00707167  0.1540606  -0.13287471  0.02328284\n",
      "   0.16341614]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:5 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57151949]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 5 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.1602626  -0.16297295  0.2773341   0.09217545 -0.13065284  0.09281587\n",
      "   0.44887751 -0.22310532 -0.06779508]\n",
      " [-0.07404239  0.19282037 -0.36057783 -0.27510787  0.10990488 -0.29988189\n",
      "   0.17445594  0.08385863  0.21360441]\n",
      " [ 0.18833612  0.14267252  0.07821665 -0.10166641 -0.31637953  0.23692035\n",
      "  -0.01396672 -0.23316638 -0.24876036]\n",
      " [-0.03826601 -0.14792809  0.16593435 -0.0204389  -0.21643462  0.25811277\n",
      "   0.13190434  0.36609573  0.10836211]\n",
      " [-0.24721315  0.10339157 -0.39668188 -0.24580435 -0.11203765 -0.00158964\n",
      "  -0.1344941  -0.24169502  0.16417512]\n",
      " [ 0.34765338 -0.0858159   0.39407192 -0.10383764 -0.05965512 -0.17332737\n",
      "   0.02270421  0.36493937 -0.09034638]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.15752044  0.55679224 -0.04478383  0.11901799 -0.16717083 -0.00992009\n",
      "   0.12960532]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:5 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.54967312]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 5 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.15359945 -0.16963611  0.2773341   0.09217545 -0.137316    0.08615271\n",
      "   0.44221435 -0.22310532 -0.06779508]\n",
      " [-0.07579389  0.19106888 -0.36057783 -0.27510787  0.10815339 -0.30163339\n",
      "   0.17270444  0.08385863  0.21360441]\n",
      " [ 0.18435186  0.13868826  0.07821665 -0.10166641 -0.32036379  0.23293609\n",
      "  -0.01795098 -0.23316638 -0.24876036]\n",
      " [-0.03805152 -0.1477136   0.16593435 -0.0204389  -0.21622013  0.25832726\n",
      "   0.13211883  0.36609573  0.10836211]\n",
      " [-0.24079667  0.10980805 -0.39668188 -0.24580435 -0.10562117  0.00482684\n",
      "  -0.12807762 -0.24169502  0.16417512]\n",
      " [ 0.34677705 -0.08669223  0.39407192 -0.10383764 -0.06053144 -0.17420369\n",
      "   0.02182788  0.36493937 -0.09034638]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.22555144  0.51592687 -0.08055394  0.08098064 -0.20097184 -0.03735357\n",
      "   0.0947131 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:5 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.51479707]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 5 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.16287984 -0.16963611  0.2773341   0.10145584 -0.137316    0.08615271\n",
      "   0.45149474 -0.22310532 -0.06779508]\n",
      " [-0.07847214  0.19106888 -0.36057783 -0.27778612  0.10815339 -0.30163339\n",
      "   0.17002619  0.08385863  0.21360441]\n",
      " [ 0.18533151  0.13868826  0.07821665 -0.10068675 -0.32036379  0.23293609\n",
      "  -0.01697132 -0.23316638 -0.24876036]\n",
      " [-0.03693761 -0.1477136   0.16593435 -0.01932499 -0.21622013  0.25832726\n",
      "   0.13323274  0.36609573  0.10836211]\n",
      " [-0.24928161  0.10980805 -0.39668188 -0.25428929 -0.10562117  0.00482684\n",
      "  -0.13656256 -0.24169502  0.16417512]\n",
      " [ 0.35071861 -0.08669223  0.39407192 -0.09989608 -0.06053144 -0.17420369\n",
      "   0.02576944  0.36493937 -0.09034638]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.16495419  0.55625552 -0.05294776  0.11225961 -0.1695583  -0.01608438\n",
      "   0.1289995 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:5 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.54861274]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 5 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.17015682 -0.16963611  0.2773341   0.10145584 -0.13003901  0.0934297\n",
      "   0.45877172 -0.22310532 -0.06779508]\n",
      " [-0.07989261  0.19106888 -0.36057783 -0.27778612  0.10673292 -0.30305386\n",
      "   0.16860572  0.08385863  0.21360441]\n",
      " [ 0.18646049  0.13868826  0.07821665 -0.10068675 -0.31923481  0.23406507\n",
      "  -0.01584234 -0.23316638 -0.24876036]\n",
      " [-0.03501301 -0.1477136   0.16593435 -0.01932499 -0.21429554  0.26025185\n",
      "   0.13515733  0.36609573  0.10836211]\n",
      " [-0.25569398  0.10980805 -0.39668188 -0.25428929 -0.11203354 -0.00158554\n",
      "  -0.14297493 -0.24169502  0.16417512]\n",
      " [ 0.35268934 -0.08669223  0.39407192 -0.09989608 -0.05856071 -0.17223296\n",
      "   0.02774017  0.36493937 -0.09034638]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.10906414  0.59186839 -0.02642567  0.14133484 -0.13968253  0.00519216\n",
      "   0.15892186]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:5 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.5729501]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 5 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.16506138 -0.16963611  0.2773341   0.0963604  -0.13003901  0.08833425\n",
      "   0.45877172 -0.22310532 -0.07289052]\n",
      " [-0.07243626  0.19106888 -0.36057783 -0.27032977  0.10673292 -0.29559751\n",
      "   0.16860572  0.08385863  0.22106076]\n",
      " [ 0.18521652  0.13868826  0.07821665 -0.10193073 -0.31923481  0.2328211\n",
      "  -0.01584234 -0.23316638 -0.25000434]\n",
      " [-0.04038647 -0.1477136   0.16593435 -0.02469845 -0.21429554  0.2548784\n",
      "   0.13515733  0.36609573  0.10298865]\n",
      " [-0.24978644  0.10980805 -0.39668188 -0.24838175 -0.11203354  0.004322\n",
      "  -0.14297493 -0.24169502  0.17008266]\n",
      " [ 0.35286082 -0.08669223  0.39407192 -0.0997246  -0.05856071 -0.17206148\n",
      "   0.02774017  0.36493937 -0.0901749 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.17915837  0.55165047 -0.05376549  0.10504271 -0.18019199 -0.02382787\n",
      "   0.12404623]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:5 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.54448435]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 5 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.17322241 -0.16147507  0.2773341   0.10452143 -0.13003901  0.09649528\n",
      "   0.46693276 -0.22310532 -0.07289052]\n",
      " [-0.07629651  0.18720862 -0.36057783 -0.27419002  0.10673292 -0.29945776\n",
      "   0.16474547  0.08385863  0.22106076]\n",
      " [ 0.19112622  0.14459797  0.07821665 -0.09602102 -0.31923481  0.2387308\n",
      "  -0.00993264 -0.23316638 -0.25000434]\n",
      " [-0.03790304 -0.14523018  0.16593435 -0.02221502 -0.21429554  0.25736182\n",
      "   0.13764076  0.36609573  0.10298865]\n",
      " [-0.25673526  0.10285924 -0.39668188 -0.25533056 -0.11203354 -0.00262681\n",
      "  -0.14992374 -0.24169502  0.17008266]\n",
      " [ 0.35317321 -0.08637984  0.39407192 -0.09941222 -0.05856071 -0.1717491\n",
      "   0.02805256  0.36493937 -0.0901749 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.12266961  0.58862252 -0.02943153  0.13938841 -0.14945117 -0.00285846\n",
      "   0.15260302]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:5 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.58140585]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 5 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.18005173 -0.15464575  0.2773341   0.11135076 -0.13003901  0.09649528\n",
      "   0.47376208 -0.22310532 -0.07289052]\n",
      " [-0.07627782  0.18722731 -0.36057783 -0.27417133  0.10673292 -0.29945776\n",
      "   0.16476416  0.08385863  0.22106076]\n",
      " [ 0.19401391  0.14748566  0.07821665 -0.09313333 -0.31923481  0.2387308\n",
      "  -0.00704495 -0.23316638 -0.25000434]\n",
      " [-0.03876426 -0.1460914   0.16593435 -0.02307624 -0.21429554  0.25736182\n",
      "   0.13677954  0.36609573  0.10298865]\n",
      " [-0.26332667  0.09626782 -0.39668188 -0.26192198 -0.11203354 -0.00262681\n",
      "  -0.15651516 -0.24169502  0.17008266]\n",
      " [ 0.35563831 -0.08391474  0.39407192 -0.09694712 -0.05856071 -0.1717491\n",
      "   0.03051766  0.36493937 -0.0901749 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.07173234  0.62131426 -0.0039442   0.16777021 -0.12484441  0.01566991\n",
      "   0.18055248]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:5 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.63003936]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 5 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.18865153 -0.15464575  0.2859339   0.11135076 -0.13003901  0.10509508\n",
      "   0.48236188 -0.22310532 -0.07289052]\n",
      " [-0.08196174  0.18722731 -0.36626174 -0.27417133  0.10673292 -0.30514168\n",
      "   0.15908024  0.08385863  0.22106076]\n",
      " [ 0.19911501  0.14748566  0.08331774 -0.09313333 -0.31923481  0.2438319\n",
      "  -0.00194385 -0.23316638 -0.25000434]\n",
      " [-0.03351005 -0.1460914   0.17118856 -0.02307624 -0.21429554  0.26261604\n",
      "   0.14203375  0.36609573  0.10298865]\n",
      " [-0.27082646  0.09626782 -0.40418167 -0.26192198 -0.11203354 -0.01012659\n",
      "  -0.16401495 -0.24169502  0.17008266]\n",
      " [ 0.36162574 -0.08391474  0.40005935 -0.09694712 -0.05856071 -0.16576166\n",
      "   0.03650509  0.36493937 -0.0901749 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.02861532  0.65306816  0.01161584  0.19464847 -0.09779045  0.0288613\n",
      "   0.20847479]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:5 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.61968519]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 6 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.18733036 -0.15596692  0.2859339   0.11002958 -0.13003901  0.10509508\n",
      "   0.48236188 -0.22310532 -0.0742117 ]\n",
      " [-0.08291321  0.18627584 -0.36626174 -0.27512281  0.10673292 -0.30514168\n",
      "   0.15908024  0.08385863  0.22010929]\n",
      " [ 0.19905179  0.14742244  0.08331774 -0.09319655 -0.31923481  0.2438319\n",
      "  -0.00194385 -0.23316638 -0.25006756]\n",
      " [-0.03169469 -0.14427603  0.17118856 -0.02126088 -0.21429554  0.26261604\n",
      "   0.14203375  0.36609573  0.10480402]\n",
      " [-0.26604849  0.10104579 -0.40418167 -0.257144   -0.11203354 -0.01012659\n",
      "  -0.16401495 -0.24169502  0.17486063]\n",
      " [ 0.35997537 -0.08556511  0.40005935 -0.09859748 -0.05856071 -0.16576166\n",
      "   0.03650509  0.36493937 -0.09182526]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.10163761  0.61523468 -0.02584721  0.1580741  -0.13248322 -0.00281516\n",
      "   0.17031102]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:6 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.61366031]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 6 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.19610988 -0.15596692  0.29471342  0.11002958 -0.13003901  0.10509508\n",
      "   0.4911414  -0.22310532 -0.0742117 ]\n",
      " [-0.08616566  0.18627584 -0.3695142  -0.27512281  0.10673292 -0.30514168\n",
      "   0.15582779  0.08385863  0.22010929]\n",
      " [ 0.20220015  0.14742244  0.0864661  -0.09319655 -0.31923481  0.2438319\n",
      "   0.00120451 -0.23316638 -0.25006756]\n",
      " [-0.02853444 -0.14427603  0.17434881 -0.02126088 -0.21429554  0.26261604\n",
      "   0.14519399  0.36609573  0.10480402]\n",
      " [-0.27411343  0.10104579 -0.41224662 -0.257144   -0.11203354 -0.01012659\n",
      "  -0.1720799  -0.24169502  0.17486063]\n",
      " [ 0.36778885 -0.08556511  0.40787283 -0.09859748 -0.05856071 -0.16576166\n",
      "   0.04431857  0.36493937 -0.09182526]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.05584064  0.64831132 -0.00624699  0.18416237 -0.10638258  0.01104979\n",
      "   0.20187584]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:6 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.64264922]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 6 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.18320156 -0.16887524  0.2818051   0.11002958 -0.13003901  0.10509508\n",
      "   0.47823308 -0.22310532 -0.0742117 ]\n",
      " [-0.08407714  0.18836436 -0.36742567 -0.27512281  0.10673292 -0.30514168\n",
      "   0.15791632  0.08385863  0.22010929]\n",
      " [ 0.1945066   0.13972889  0.07877255 -0.09319655 -0.31923481  0.2438319\n",
      "  -0.00648904 -0.23316638 -0.25006756]\n",
      " [-0.03122686 -0.14696845  0.17165639 -0.02126088 -0.21429554  0.26261604\n",
      "   0.14250157  0.36609573  0.10480402]\n",
      " [-0.26196764  0.11319159 -0.40010082 -0.257144   -0.11203354 -0.01012659\n",
      "  -0.1599341  -0.24169502  0.17486063]\n",
      " [ 0.35591466 -0.0974393   0.39599864 -0.09859748 -0.05856071 -0.16576166\n",
      "   0.03244438  0.36493937 -0.09182526]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.12963323  0.59698799 -0.04105026  0.13932497 -0.14598096 -0.01250571\n",
      "   0.15200877]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:6 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.56927217]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 6 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.18634885 -0.16887524  0.28495239  0.11002958 -0.13003901  0.10509508\n",
      "   0.47823308 -0.21995804 -0.0742117 ]\n",
      " [-0.08877035  0.18836436 -0.37211888 -0.27512281  0.10673292 -0.30514168\n",
      "   0.15791632  0.07916541  0.22010929]\n",
      " [ 0.19503595  0.13972889  0.07930191 -0.09319655 -0.31923481  0.2438319\n",
      "  -0.00648904 -0.23263703 -0.25006756]\n",
      " [-0.024951   -0.14696845  0.17793225 -0.02126088 -0.21429554  0.26261604\n",
      "   0.14250157  0.37237159  0.10480402]\n",
      " [-0.27175966  0.11319159 -0.40989284 -0.257144   -0.11203354 -0.01012659\n",
      "  -0.1599341  -0.25148704  0.17486063]\n",
      " [ 0.366872   -0.0974393   0.40695598 -0.09859748 -0.05856071 -0.16576166\n",
      "   0.03244438  0.37589671 -0.09182526]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.0768257   0.62656982 -0.01944616  0.16625823 -0.11302951  0.00271753\n",
      "   0.1917942 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:6 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.59540567]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 6 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.18902243 -0.16887524  0.28495239  0.11270317 -0.13003901  0.10509508\n",
      "   0.47823308 -0.21995804 -0.07153811]\n",
      " [-0.09051308  0.18836436 -0.37211888 -0.27686554  0.10673292 -0.30514168\n",
      "   0.15791632  0.07916541  0.21836656]\n",
      " [ 0.19323994  0.13972889  0.07930191 -0.09499257 -0.31923481  0.2438319\n",
      "  -0.00648904 -0.23263703 -0.25186357]\n",
      " [-0.02423777 -0.14696845  0.17793225 -0.02054765 -0.21429554  0.26261604\n",
      "   0.14250157  0.37237159  0.10551725]\n",
      " [-0.27594065  0.11319159 -0.40989284 -0.261325   -0.11203354 -0.01012659\n",
      "  -0.1599341  -0.25148704  0.17067963]\n",
      " [ 0.36900508 -0.0974393   0.40695598 -0.09646441 -0.05856071 -0.16576166\n",
      "   0.03244438  0.37589671 -0.08969219]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.02809277  0.65363192  0.00317157  0.1888221  -0.08794941  0.0228151\n",
      "   0.21830483]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:6 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.61748959]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 6 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.18733893 -0.16887524  0.28495239  0.11270317 -0.13172252  0.10341158\n",
      "   0.47823308 -0.21995804 -0.07322162]\n",
      " [-0.08922838  0.18836436 -0.37211888 -0.27686554  0.10801762 -0.30385698\n",
      "   0.15791632  0.07916541  0.21965126]\n",
      " [ 0.19567245  0.13972889  0.07930191 -0.09499257 -0.3168023   0.24626441\n",
      "  -0.00648904 -0.23263703 -0.24943106]\n",
      " [-0.02659063 -0.14696845  0.17793225 -0.02054765 -0.21664839  0.26026318\n",
      "   0.14250157  0.37237159  0.1031644 ]\n",
      " [-0.27184767  0.11319159 -0.40989284 -0.261325   -0.10794056 -0.00603361\n",
      "  -0.1599341  -0.25148704  0.17477262]\n",
      " [ 0.3680033  -0.0974393   0.40695598 -0.09646441 -0.05956249 -0.16676344\n",
      "   0.03244438  0.37589671 -0.09069397]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.10101711  0.61548384 -0.03200484  0.15479972 -0.12677103 -0.00951871\n",
      "   0.18084037]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:6 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.56861907]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 6 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.18878471 -0.16742946  0.28495239  0.11270317 -0.13027674  0.10485736\n",
      "   0.47823308 -0.21995804 -0.07177584]\n",
      " [-0.09136386  0.18622889 -0.37211888 -0.27686554  0.10588214 -0.30599245\n",
      "   0.15791632  0.07916541  0.21751578]\n",
      " [ 0.19540341  0.13945984  0.07930191 -0.09499257 -0.31707134  0.24599537\n",
      "  -0.00648904 -0.23263703 -0.2497001 ]\n",
      " [-0.02612381 -0.14650164  0.17793225 -0.02054765 -0.21618158  0.26073\n",
      "   0.14250157  0.37237159  0.10363121]\n",
      " [-0.27014563  0.11489363 -0.40989284 -0.261325   -0.10623852 -0.00433157\n",
      "  -0.1599341  -0.25148704  0.17647466]\n",
      " [ 0.3688128  -0.09662979  0.40695598 -0.09646441 -0.05875298 -0.16595394\n",
      "   0.03244438  0.37589671 -0.08988446]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.1707558   0.58206194 -0.06901504  0.11966132 -0.1611735  -0.0426833\n",
      "   0.14678082]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:6 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: [0.] Net Result: [[0.54942977]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 6 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.18115964 -0.17505453  0.28495239  0.11270317 -0.13790181  0.09723229\n",
      "   0.47060801 -0.21995804 -0.07177584]\n",
      " [-0.09225874  0.185334   -0.37211888 -0.27686554  0.10498726 -0.30688734\n",
      "   0.15702143  0.07916541  0.21751578]\n",
      " [ 0.19110046  0.1351569   0.07930191 -0.09499257 -0.32137428  0.24169242\n",
      "  -0.01079198 -0.23263703 -0.2497001 ]\n",
      " [-0.02636904 -0.14674687  0.17793225 -0.02054765 -0.21642681  0.26048476\n",
      "   0.14225634  0.37237159  0.10363121]\n",
      " [-0.26322535  0.12181391 -0.40989284 -0.261325   -0.09931824  0.00258871\n",
      "  -0.15301382 -0.25148704  0.17647466]\n",
      " [ 0.36745617 -0.09798642  0.40695598 -0.09646441 -0.06010961 -0.16731057\n",
      "   0.03108775  0.37589671 -0.08988446]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.23876331  0.54014415 -0.1039141   0.08130699 -0.1954225  -0.0695558\n",
      "   0.11141899]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:6 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.5159888]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 6 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.19117478 -0.17505453  0.28495239  0.12271831 -0.13790181  0.09723229\n",
      "   0.48062316 -0.21995804 -0.07177584]\n",
      " [-0.09542781  0.185334   -0.37211888 -0.28003461  0.10498726 -0.30688734\n",
      "   0.15385236  0.07916541  0.21751578]\n",
      " [ 0.19238723  0.1351569   0.07930191 -0.0937058  -0.32137428  0.24169242\n",
      "  -0.00950521 -0.23263703 -0.2497001 ]\n",
      " [-0.02493174 -0.14674687  0.17793225 -0.01911035 -0.21642681  0.26048476\n",
      "   0.14369364  0.37237159  0.10363121]\n",
      " [-0.27237262  0.12181391 -0.40989284 -0.27047227 -0.09931824  0.00258871\n",
      "  -0.16216109 -0.25148704  0.17647466]\n",
      " [ 0.371918   -0.09798642  0.40695598 -0.09200258 -0.06010961 -0.16731057\n",
      "   0.03554957  0.37589671 -0.08988446]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.17832378  0.58138347 -0.07688722  0.11281509 -0.16376325 -0.04919946\n",
      "   0.14616875]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:6 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.54926341]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 6 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.19915668 -0.17505453  0.28495239  0.12271831 -0.12991992  0.10521418\n",
      "   0.48860505 -0.21995804 -0.07177584]\n",
      " [-0.09741886  0.185334   -0.37211888 -0.28003461  0.10299621 -0.30887838\n",
      "   0.15186131  0.07916541  0.21751578]\n",
      " [ 0.19382292  0.1351569   0.07930191 -0.0937058  -0.31993859  0.24312811\n",
      "  -0.00806952 -0.23263703 -0.2497001 ]\n",
      " [-0.02267559 -0.14674687  0.17793225 -0.01911035 -0.21417065  0.26274092\n",
      "   0.1459498   0.37237159  0.10363121]\n",
      " [-0.27928386  0.12181391 -0.40989284 -0.27047227 -0.10622948 -0.00432253\n",
      "  -0.16907233 -0.25148704  0.17647466]\n",
      " [ 0.37440919 -0.09798642  0.40695598 -0.09200258 -0.05761842 -0.16481937\n",
      "   0.03804077  0.37589671 -0.08988446]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.12252865  0.61780348 -0.05098753  0.1421509  -0.13359955 -0.02854285\n",
      "   0.176571  ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:6 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57341716]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 6 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.19311955 -0.17505453  0.28495239  0.11668118 -0.12991992  0.09917705\n",
      "   0.48860505 -0.21995804 -0.07781297]\n",
      " [-0.08963474  0.185334   -0.37211888 -0.27225049  0.10299621 -0.30109426\n",
      "   0.15186131  0.07916541  0.2252999 ]\n",
      " [ 0.19218639  0.1351569   0.07930191 -0.09534234 -0.31993859  0.24149158\n",
      "  -0.00806952 -0.23263703 -0.25133664]\n",
      " [-0.02821924 -0.14674687  0.17793225 -0.024654   -0.21417065  0.25719727\n",
      "   0.1459498   0.37237159  0.09808756]\n",
      " [-0.27289385  0.12181391 -0.40989284 -0.26408226 -0.10622948  0.00206748\n",
      "  -0.16907233 -0.25148704  0.18286467]\n",
      " [ 0.37392358 -0.09798642  0.40695598 -0.0924882  -0.05761842 -0.16530499\n",
      "   0.03804077  0.37589671 -0.09037008]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.19266041  0.57657263 -0.077981    0.10544609 -0.17430694 -0.05706579\n",
      "   0.14101943]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:6 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.54576941]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 6 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.20206985 -0.16610422  0.28495239  0.12563149 -0.12991992  0.10812735\n",
      "   0.49755536 -0.21995804 -0.07781297]\n",
      " [-0.09410085  0.1808679  -0.37211888 -0.2767166   0.10299621 -0.30556037\n",
      "   0.14739521  0.07916541  0.2252999 ]\n",
      " [ 0.19839522  0.14136573  0.07930191 -0.0891335  -0.31993859  0.24770041\n",
      "  -0.00186069 -0.23263703 -0.25133664]\n",
      " [-0.02538391 -0.14391154  0.17793225 -0.02181866 -0.21417065  0.2600326\n",
      "   0.14878513  0.37237159  0.09808756]\n",
      " [-0.2804314   0.11427636 -0.40989284 -0.27161981 -0.10622948 -0.00547007\n",
      "  -0.17660988 -0.25148704  0.18286467]\n",
      " [ 0.3747138  -0.0971962   0.40695598 -0.09169798 -0.05761842 -0.16451477\n",
      "   0.03883098  0.37589671 -0.09037008]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.13635736  0.6144738  -0.05437501  0.14003305 -0.14330046 -0.03688485\n",
      "   0.1699616 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:6 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.58227003]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 6 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.20959419 -0.15857988  0.28495239  0.13315583 -0.12991992  0.10812735\n",
      "   0.5050797  -0.21995804 -0.07781297]\n",
      " [-0.09464107  0.18032767 -0.37211888 -0.27725682  0.10299621 -0.30556037\n",
      "   0.14685498  0.07916541  0.2252999 ]\n",
      " [ 0.20150633  0.14447685  0.07930191 -0.08602239 -0.31993859  0.24770041\n",
      "   0.00125043 -0.23263703 -0.25133664]\n",
      " [-0.02592127 -0.1444489   0.17793225 -0.02235603 -0.21417065  0.2600326\n",
      "   0.14824776  0.37237159  0.09808756]\n",
      " [-0.28754209  0.10716567 -0.40989284 -0.2787305  -0.10622948 -0.00547007\n",
      "  -0.18372057 -0.25148704  0.18286467]\n",
      " [ 0.3775313  -0.09437869  0.40695598 -0.08888047 -0.05761842 -0.16451477\n",
      "   0.04164849  0.37589671 -0.09037008]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.08555478  0.64795625 -0.02951411  0.16857764 -0.1184367  -0.01905012\n",
      "   0.19820415]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:6 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.63347062]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 6 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.21839184 -0.15857988  0.29375004  0.13315583 -0.12991992  0.116925\n",
      "   0.51387735 -0.21995804 -0.07781297]\n",
      " [-0.10068414  0.18032767 -0.37816195 -0.27725682  0.10299621 -0.31160344\n",
      "   0.14081192  0.07916541  0.2252999 ]\n",
      " [ 0.20676423  0.14447685  0.0845598  -0.08602239 -0.31993859  0.2529583\n",
      "   0.00650832 -0.23263703 -0.25133664]\n",
      " [-0.02040532 -0.1444489   0.1834482  -0.02235603 -0.21417065  0.26554856\n",
      "   0.15376372  0.37237159  0.09808756]\n",
      " [-0.29533862  0.10716567 -0.41768937 -0.2787305  -0.10622948 -0.0132666\n",
      "  -0.1915171  -0.25148704  0.18286467]\n",
      " [ 0.3838522  -0.09437869  0.41327687 -0.08888047 -0.05761842 -0.15819388\n",
      "   0.04796939  0.37589671 -0.09037008]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.04300336  0.67994258 -0.01468326  0.19536065 -0.09135187 -0.00663333\n",
      "   0.22627211]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:6 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.61782113]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 7 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.21629896 -0.16067277  0.29375004  0.13106294 -0.12991992  0.116925\n",
      "   0.51387735 -0.21995804 -0.07990585]\n",
      " [-0.1011889   0.17982291 -0.37816195 -0.27776159  0.10299621 -0.31160344\n",
      "   0.14081192  0.07916541  0.22479514]\n",
      " [ 0.2065111   0.14422373  0.0845598  -0.08627551 -0.31993859  0.2529583\n",
      "   0.00650832 -0.23263703 -0.25158977]\n",
      " [-0.0187834  -0.14282699  0.1834482  -0.02073411 -0.21417065  0.26554856\n",
      "   0.15376372  0.37237159  0.09970948]\n",
      " [-0.29026229  0.11224199 -0.41768937 -0.27365417 -0.10622948 -0.0132666\n",
      "  -0.1915171  -0.25148704  0.18794099]\n",
      " [ 0.38184839 -0.0963825   0.41327687 -0.09088428 -0.05761842 -0.15819388\n",
      "   0.04796939  0.37589671 -0.09237388]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.11594276  0.64137537 -0.05165779  0.15863781 -0.12619751 -0.03795817\n",
      "   0.18779454]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:7 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.61602977]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 7 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.22533951 -0.16067277  0.30279059  0.13106294 -0.12991992  0.116925\n",
      "   0.5229179  -0.21995804 -0.07990585]\n",
      " [-0.10492425  0.17982291 -0.3818973  -0.27776159  0.10299621 -0.31160344\n",
      "   0.13707657  0.07916541  0.22479514]\n",
      " [ 0.20981579  0.14422373  0.08786448 -0.08627551 -0.31993859  0.2529583\n",
      "   0.00981301 -0.23263703 -0.25158977]\n",
      " [-0.01525843 -0.14282699  0.18697317 -0.02073411 -0.21417065  0.26554856\n",
      "   0.15728869  0.37237159  0.09970948]\n",
      " [-0.2986581   0.11224199 -0.42608518 -0.27365417 -0.10622948 -0.0132666\n",
      "  -0.19991291 -0.25148704  0.18794099]\n",
      " [ 0.38990201 -0.0963825   0.42133049 -0.09088428 -0.05761842 -0.15819388\n",
      "   0.056023    0.37589671 -0.09237388]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.07053116  0.67478635 -0.03275909  0.18469729 -0.09990686 -0.02482696\n",
      "   0.21954254]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:7 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.64419532]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 7 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.21177645 -0.17423583  0.28922753  0.13106294 -0.12991992  0.116925\n",
      "   0.50935484 -0.21995804 -0.07990585]\n",
      " [-0.10181056  0.1829366  -0.37878361 -0.27776159  0.10299621 -0.31160344\n",
      "   0.14019026  0.07916541  0.22479514]\n",
      " [ 0.20188977  0.13629771  0.07993847 -0.08627551 -0.31993859  0.2529583\n",
      "   0.00188699 -0.23263703 -0.25158977]\n",
      " [-0.01866505 -0.1462336   0.18356655 -0.02073411 -0.21417065  0.26554856\n",
      "   0.15388207  0.37237159  0.09970948]\n",
      " [-0.28588895  0.12501115 -0.41331603 -0.27365417 -0.10622948 -0.0132666\n",
      "  -0.18714376 -0.25148704  0.18794099]\n",
      " [ 0.37759487 -0.10868964  0.40902335 -0.09088428 -0.05761842 -0.15819388\n",
      "   0.04371586  0.37589671 -0.09237388]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.14435843  0.62244521 -0.06654403  0.13958533 -0.14024683 -0.04751972\n",
      "   0.16906612]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:7 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.57007704]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 7 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.21540601 -0.17423583  0.2928571   0.13106294 -0.12991992  0.116925\n",
      "   0.50935484 -0.21632847 -0.07990585]\n",
      " [-0.10689044  0.1829366  -0.38386349 -0.27776159  0.10299621 -0.31160344\n",
      "   0.14019026  0.07408553  0.22479514]\n",
      " [ 0.20253729  0.13629771  0.08058598 -0.08627551 -0.31993859  0.2529583\n",
      "   0.00188699 -0.23198952 -0.25158977]\n",
      " [-0.0120756  -0.1462336   0.190156   -0.02073411 -0.21417065  0.26554856\n",
      "   0.15388207  0.37896103  0.09970948]\n",
      " [-0.29595876  0.12501115 -0.42338584 -0.27365417 -0.10622948 -0.0132666\n",
      "  -0.18714376 -0.26155685  0.18794099]\n",
      " [ 0.3887091  -0.10868964  0.42013758 -0.09088428 -0.05761842 -0.15819388\n",
      "   0.04371586  0.38701094 -0.09237388]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.09167369  0.65246511 -0.04541908  0.16657548 -0.10699339 -0.03283407\n",
      "   0.2092008 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:7 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.59417391]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 7 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.21860942 -0.17423583  0.2928571   0.13426634 -0.12991992  0.116925\n",
      "   0.50935484 -0.21632847 -0.07670245]\n",
      " [-0.1088334   0.1829366  -0.38386349 -0.27970454  0.10299621 -0.31160344\n",
      "   0.14019026  0.07408553  0.22285218]\n",
      " [ 0.20088949  0.13629771  0.08058598 -0.08792331 -0.31993859  0.2529583\n",
      "   0.00188699 -0.23198952 -0.25323757]\n",
      " [-0.01125819 -0.1462336   0.190156   -0.0199167  -0.21417065  0.26554856\n",
      "   0.15388207  0.37896103  0.10052689]\n",
      " [-0.30046146  0.12501115 -0.42338584 -0.27815687 -0.10622948 -0.0132666\n",
      "  -0.18714376 -0.26155685  0.1834383 ]\n",
      " [ 0.39119587 -0.10868964  0.42013758 -0.0883975  -0.05761842 -0.15819388\n",
      "   0.04371586  0.38701094 -0.08988711]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.04274501  0.68017092 -0.02290598  0.18938698 -0.08171102 -0.01298255\n",
      "   0.23616945]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:7 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.61563174]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 7 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.21627171 -0.17423583  0.2928571   0.13426634 -0.13225762  0.1145873\n",
      "   0.50935484 -0.21632847 -0.07904016]\n",
      " [-0.10711483  0.1829366  -0.38386349 -0.27970454  0.10471478 -0.30988487\n",
      "   0.14019026  0.07408553  0.22457075]\n",
      " [ 0.20305469  0.13629771  0.08058598 -0.08792331 -0.31777339  0.2551235\n",
      "   0.00188699 -0.23198952 -0.25107237]\n",
      " [-0.01380668 -0.1462336   0.190156   -0.0199167  -0.21671914  0.26300007\n",
      "   0.15388207  0.37896103  0.0979784 ]\n",
      " [-0.29621422  0.12501115 -0.42338584 -0.27815687 -0.10198224 -0.00901936\n",
      "  -0.18714376 -0.26155685  0.18768553]\n",
      " [ 0.38964186 -0.10868964  0.42013758 -0.0883975  -0.05917243 -0.15974789\n",
      "   0.04371586  0.38701094 -0.09144112]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.11558326  0.64140761 -0.05760397  0.1551382  -0.12068705 -0.04511473\n",
      "   0.19819441]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:7 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.56488874]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 7 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.2172199  -0.17328763  0.2928571   0.13426634 -0.13130943  0.11553549\n",
      "   0.50935484 -0.21632847 -0.07809196]\n",
      " [-0.10876372  0.18128771 -0.38386349 -0.27970454  0.10306589 -0.31153376\n",
      "   0.14019026  0.07408553  0.22292186]\n",
      " [ 0.20260994  0.13585296  0.08058598 -0.08792331 -0.31821814  0.25467875\n",
      "   0.00188699 -0.23198952 -0.25151712]\n",
      " [-0.01353281 -0.14595974  0.190156   -0.0199167  -0.21644527  0.26327393\n",
      "   0.15388207  0.37896103  0.09825227]\n",
      " [-0.29457746  0.12664791 -0.42338584 -0.27815687 -0.10034548 -0.0073826\n",
      "  -0.18714376 -0.26155685  0.1893223 ]\n",
      " [ 0.39015216 -0.10817934  0.42013758 -0.0883975  -0.05866213 -0.15923759\n",
      "   0.04371586  0.38701094 -0.09093082]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.18500511  0.60764535 -0.09396628  0.11998248 -0.15512409 -0.07818645\n",
      "   0.16399387]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:7 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.5490335]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 7 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 2.08715526e-01 -1.81792008e-01  2.92857097e-01  1.34266345e-01\n",
      "  -1.39813804e-01  1.07031116e-01  5.00850463e-01 -2.16328474e-01\n",
      "  -7.80919628e-02]\n",
      " [-1.08835872e-01  1.81215555e-01 -3.83863486e-01 -2.79704543e-01\n",
      "   1.02993735e-01 -3.11605914e-01  1.40118106e-01  7.40855347e-02\n",
      "   2.22921857e-01]\n",
      " [ 1.97995260e-01  1.31238282e-01  8.05859838e-02 -8.79233104e-02\n",
      "  -3.22832822e-01  2.50064072e-01 -2.72768644e-03 -2.31989518e-01\n",
      "  -2.51517116e-01]\n",
      " [-1.42329033e-02 -1.46659830e-01  1.90155997e-01 -1.99166956e-02\n",
      "  -2.17145369e-01  2.62573841e-01  1.53181978e-01  3.78961031e-01\n",
      "   9.82522705e-02]\n",
      " [-2.87119903e-01  1.34105464e-01 -4.23385840e-01 -2.78156866e-01\n",
      "  -9.28879241e-02  7.49542544e-05 -1.79686204e-01 -2.61556852e-01\n",
      "   1.89322298e-01]\n",
      " [ 3.88325889e-01 -1.10005615e-01  4.20137579e-01 -8.83975043e-02\n",
      "  -6.04884003e-02 -1.61063859e-01  4.18895895e-02  3.87010936e-01\n",
      "  -9.09308198e-02]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.25297428  0.56474092 -0.12802302  0.08132405 -0.18980897 -0.1044444\n",
      "   0.12817947]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:7 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.51735381]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 7 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 2.19408335e-01 -1.81792008e-01  2.92857097e-01  1.44959154e-01\n",
      "  -1.39813804e-01  1.07031116e-01  5.11543272e-01 -2.16328474e-01\n",
      "  -7.80919628e-02]\n",
      " [-1.12521086e-01  1.81215555e-01 -3.83863486e-01 -2.83389757e-01\n",
      "   1.02993735e-01 -3.11605914e-01  1.36432892e-01  7.40855347e-02\n",
      "   2.22921857e-01]\n",
      " [ 1.99607700e-01  1.31238282e-01  8.05859838e-02 -8.63108696e-02\n",
      "  -3.22832822e-01  2.50064072e-01 -1.11524559e-03 -2.31989518e-01\n",
      "  -2.51517116e-01]\n",
      " [-1.24460738e-02 -1.46659830e-01  1.90155997e-01 -1.81298661e-02\n",
      "  -2.17145369e-01  2.62573841e-01  1.54968808e-01  3.78961031e-01\n",
      "   9.82522705e-02]\n",
      " [-2.96918838e-01  1.34105464e-01 -4.23385840e-01 -2.87955801e-01\n",
      "  -9.28879241e-02  7.49542544e-05 -1.89485139e-01 -2.61556852e-01\n",
      "   1.89322298e-01]\n",
      " [ 3.93327684e-01 -1.10005615e-01  4.20137579e-01 -8.33957090e-02\n",
      "  -6.04884003e-02 -1.61063859e-01  4.68913847e-02  3.87010936e-01\n",
      "  -9.09308198e-02]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.19271618  0.60687771 -0.10161721  0.11306864 -0.15788887 -0.08504612\n",
      "   0.16340829]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:7 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.54988807]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 7 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.22803621 -0.18179201  0.2928571   0.14495915 -0.13118593  0.11565899\n",
      "   0.52017115 -0.21632847 -0.07809196]\n",
      " [-0.11507141  0.18121555 -0.38386349 -0.28338976  0.10044342 -0.31415623\n",
      "   0.13388257  0.07408553  0.22292186]\n",
      " [ 0.20135162  0.13123828  0.08058598 -0.08631087 -0.3210889   0.251808\n",
      "   0.00062868 -0.23198952 -0.25151712]\n",
      " [-0.00985165 -0.14665983  0.190156   -0.01812987 -0.21455095  0.26516826\n",
      "   0.15756323  0.37896103  0.09825227]\n",
      " [-0.30434457  0.13410546 -0.42338584 -0.2879558  -0.10031365 -0.00735077\n",
      "  -0.19691087 -0.26155685  0.1893223 ]\n",
      " [ 0.39633672 -0.11000562  0.42013758 -0.08339571 -0.05747937 -0.15805483\n",
      "   0.04990042  0.38701094 -0.09093082]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.13701232  0.64407573 -0.07633012  0.1426691  -0.12742721 -0.06504215\n",
      "   0.19429329]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:7 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57362876]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 7 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 2.21131196e-01 -1.81792008e-01  2.92857097e-01  1.38054137e-01\n",
      "  -1.31185927e-01  1.08753977e-01  5.20171149e-01 -2.16328474e-01\n",
      "  -8.49969790e-02]\n",
      " [-1.06978475e-01  1.81215555e-01 -3.83863486e-01 -2.75296826e-01\n",
      "   1.00443416e-01 -3.06063302e-01  1.33882573e-01  7.40855347e-02\n",
      "   2.31014787e-01]\n",
      " [ 1.99335743e-01  1.31238282e-01  8.05859838e-02 -8.83267502e-02\n",
      "  -3.21088898e-01  2.49792115e-01  6.28678183e-04 -2.31989518e-01\n",
      "  -2.53532997e-01]\n",
      " [-1.55718822e-02 -1.46659830e-01  1.90155997e-01 -2.38500968e-02\n",
      "  -2.14550947e-01  2.59448032e-01  1.57563230e-01  3.78961031e-01\n",
      "   9.25320398e-02]\n",
      " [-2.97443152e-01  1.34105464e-01 -4.23385840e-01 -2.81054387e-01\n",
      "  -1.00313653e-01 -4.49360286e-04 -1.96910868e-01 -2.61556852e-01\n",
      "   1.96223713e-01]\n",
      " [ 3.95216268e-01 -1.10005615e-01  4.20137579e-01 -8.45161596e-02\n",
      "  -5.74793665e-02 -1.59175276e-01  4.99004186e-02  3.87010936e-01\n",
      "  -9.20512704e-02]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.20716104  0.60190072 -0.1029842   0.10557439 -0.16832968 -0.09301979\n",
      "   0.15809772]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:7 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.54716695]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 7 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.23078407 -0.17213913  0.2928571   0.14770702 -0.13118593  0.11840686\n",
      "   0.52982403 -0.21632847 -0.08499698]\n",
      " [-0.11203505  0.17615898 -0.38386349 -0.2803534   0.10044342 -0.31111988\n",
      "   0.128826    0.07408553  0.23101479]\n",
      " [ 0.20584258  0.13774511  0.08058598 -0.08181992 -0.3210889   0.25629895\n",
      "   0.00713551 -0.23198952 -0.253533  ]\n",
      " [-0.01237587 -0.14346382  0.190156   -0.02065409 -0.21455095  0.26264404\n",
      "   0.16075924  0.37896103  0.09253204]\n",
      " [-0.30557716  0.12597145 -0.42338584 -0.2891884  -0.10031365 -0.00858337\n",
      "  -0.20504488 -0.26155685  0.19622371]\n",
      " [ 0.39649576 -0.10872612  0.42013758 -0.08323666 -0.05747937 -0.15789578\n",
      "   0.05117991  0.38701094 -0.09205127]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.15106062  0.64068431 -0.08010879  0.14039786 -0.13705498 -0.07367353\n",
      "   0.1874292 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:7 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.58326427]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 7 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.23894838 -0.16397483  0.2928571   0.15587132 -0.13118593  0.11840686\n",
      "   0.53798833 -0.21632847 -0.08499698]\n",
      " [-0.11313963  0.1750544  -0.38386349 -0.28145798  0.10044342 -0.31111988\n",
      "   0.12772142  0.07408553  0.23101479]\n",
      " [ 0.20918657  0.14108911  0.08058598 -0.07847592 -0.3210889   0.25629895\n",
      "   0.01047951 -0.23198952 -0.253533  ]\n",
      " [-0.01257509 -0.14366304  0.190156   -0.0208533  -0.21455095  0.26264404\n",
      "   0.16056002  0.37896103  0.09253204]\n",
      " [-0.31320954  0.11833908 -0.42338584 -0.29682077 -0.10031365 -0.00858337\n",
      "  -0.21267725 -0.26155685  0.19622371]\n",
      " [ 0.3996812  -0.10554068  0.42013758 -0.08005123 -0.05747937 -0.15789578\n",
      "   0.05436535  0.38701094 -0.09205127]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.10041325  0.67492999 -0.05589109  0.16910599 -0.11193052 -0.05657307\n",
      "   0.21597315]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:7 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.63707275]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 7 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.24787304 -0.16397483  0.30178176  0.15587132 -0.13118593  0.12733151\n",
      "   0.54691299 -0.21632847 -0.08499698]\n",
      " [-0.11951023  0.1750544  -0.39023409 -0.28145798  0.10044342 -0.31749048\n",
      "   0.12135082  0.07408553  0.23101479]\n",
      " [ 0.21459459  0.14108911  0.085994   -0.07847592 -0.3210889   0.26170696\n",
      "   0.01588752 -0.23198952 -0.253533  ]\n",
      " [-0.00680949 -0.14366304  0.19592159 -0.0208533  -0.21455095  0.26840964\n",
      "   0.16632562  0.37896103  0.09253204]\n",
      " [-0.32126358  0.11833908 -0.43143988 -0.29682077 -0.10031365 -0.01663741\n",
      "  -0.22073129 -0.26155685  0.19622371]\n",
      " [ 0.40630751 -0.10554068  0.42676389 -0.08005123 -0.05747937 -0.15126947\n",
      "   0.06099167  0.38701094 -0.09205127]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.05845685  0.70708617 -0.04178654  0.19577575 -0.08483357 -0.04493824\n",
      "   0.24415901]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:7 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.61552144]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 8 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.24507171 -0.16677616  0.30178176  0.15306999 -0.13118593  0.12733151\n",
      "   0.54691299 -0.21632847 -0.08779831]\n",
      " [-0.11960311  0.17496152 -0.39023409 -0.28155086  0.10044342 -0.31749048\n",
      "   0.12135082  0.07408553  0.23092191]\n",
      " [ 0.21416357  0.14065809  0.085994   -0.07890694 -0.3210889   0.26170696\n",
      "   0.01588752 -0.23198952 -0.25396401]\n",
      " [-0.00537702 -0.14223056  0.19592159 -0.01942083 -0.21455095  0.26840964\n",
      "   0.16632562  0.37896103  0.09396451]\n",
      " [-0.31586233  0.12374032 -0.43143988 -0.29141953 -0.10031365 -0.01663741\n",
      "  -0.22073129 -0.26155685  0.20162496]\n",
      " [ 0.40397443 -0.10787377  0.42676389 -0.08238431 -0.05747937 -0.15126947\n",
      "   0.06099167  0.38701094 -0.09438436]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.1312899   0.66785712 -0.07829594  0.15892816 -0.11981614 -0.07587021\n",
      "   0.20540295]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:8 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.61859426]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 8 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.25432138 -0.16677616  0.31103143  0.15306999 -0.13118593  0.12733151\n",
      "   0.55616267 -0.21632847 -0.08779831]\n",
      " [-0.12381214  0.17496152 -0.39444312 -0.28155086  0.10044342 -0.31749048\n",
      "   0.11714179  0.07408553  0.23092191]\n",
      " [ 0.21763125  0.14065809  0.08946168 -0.07890694 -0.3210889   0.26170696\n",
      "   0.0193552  -0.23198952 -0.25396401]\n",
      " [-0.00148795 -0.14223056  0.19981066 -0.01942083 -0.21455095  0.26840964\n",
      "   0.17021469  0.37896103  0.09396451]\n",
      " [-0.3245515   0.12374032 -0.44012905 -0.29141953 -0.10031365 -0.01663741\n",
      "  -0.22942047 -0.26155685  0.20162496]\n",
      " [ 0.41224824 -0.10787377  0.4350377  -0.08238431 -0.05747937 -0.15126947\n",
      "   0.06926547  0.38701094 -0.09438436]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.08629635  0.70156136 -0.06011487  0.18495064 -0.09334722 -0.06348471\n",
      "   0.2373145 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:8 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.64583998]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 8 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.24016926 -0.18092828  0.29687931  0.15306999 -0.13118593  0.12733151\n",
      "   0.54201055 -0.21632847 -0.08779831]\n",
      " [-0.11968909  0.17908457 -0.39032007 -0.28155086  0.10044342 -0.31749048\n",
      "   0.12126484  0.07408553  0.23092191]\n",
      " [ 0.20945983  0.13248667  0.08129026 -0.07890694 -0.3210889   0.26170696\n",
      "   0.01118378 -0.23198952 -0.25396401]\n",
      " [-0.00561375 -0.14635636  0.19568486 -0.01942083 -0.21455095  0.26840964\n",
      "   0.16608889  0.37896103  0.09396451]\n",
      " [-0.31117683  0.137115   -0.42675438 -0.29141953 -0.10031365 -0.01663741\n",
      "  -0.21604579 -0.26155685  0.20162496]\n",
      " [ 0.3995136  -0.1206084   0.42230307 -0.08238431 -0.05747937 -0.15126947\n",
      "   0.05653084  0.38701094 -0.09438436]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.16015806  0.64822823 -0.09288744  0.13954795 -0.13443918 -0.08528716\n",
      "   0.18621493]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:8 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.57088118]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 8 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.24427713 -0.18092828  0.30098718  0.15306999 -0.13118593  0.12733151\n",
      "   0.54201055 -0.21222061 -0.08779831]\n",
      " [-0.12515359  0.17908457 -0.39578457 -0.28155086  0.10044342 -0.31749048\n",
      "   0.12126484  0.06862103  0.23092191]\n",
      " [ 0.2102313   0.13248667  0.08206173 -0.07890694 -0.3210889   0.26170696\n",
      "   0.01118378 -0.23121804 -0.25396401]\n",
      " [ 0.00128954 -0.14635636  0.20258815 -0.01942083 -0.21455095  0.26840964\n",
      "   0.16608889  0.38586432  0.09396451]\n",
      " [-0.32150828  0.137115   -0.43708582 -0.29141953 -0.10031365 -0.01663741\n",
      "  -0.21604579 -0.2718883   0.20162496]\n",
      " [ 0.41076309 -0.1206084   0.43355255 -0.08238431 -0.05747937 -0.15126947\n",
      "   0.05653084  0.39826042 -0.09438436]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.10759619  0.67868782 -0.07224572  0.16660081 -0.10087633 -0.0711458\n",
      "   0.22669238]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:8 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.59265105]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 8 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.24799256 -0.18092828  0.30098718  0.15678542 -0.13118593  0.12733151\n",
      "   0.54201055 -0.21222061 -0.08408288]\n",
      " [-0.1272978   0.17908457 -0.39578457 -0.28369507  0.10044342 -0.31749048\n",
      "   0.12126484  0.06862103  0.2287777 ]\n",
      " [ 0.2087294   0.13248667  0.08206173 -0.08040884 -0.3210889   0.26170696\n",
      "   0.01118378 -0.23121804 -0.25546591]\n",
      " [ 0.00222038 -0.14635636  0.20258815 -0.01848998 -0.21455095  0.26840964\n",
      "   0.16608889  0.38586432  0.09489536]\n",
      " [-0.3263563   0.137115   -0.43708582 -0.29626755 -0.10031365 -0.01663741\n",
      "  -0.21604579 -0.2718883   0.19677693]\n",
      " [ 0.41360046 -0.1206084   0.43355255 -0.07954694 -0.05747937 -0.15126947\n",
      "   0.05653084  0.39826042 -0.09154698]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.05842595  0.70704799 -0.04981588  0.18968026 -0.07535948 -0.05154656\n",
      "   0.25414083]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:8 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.61320453]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 8 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 2.45101259e-01 -1.80928277e-01  3.00987180e-01  1.56785420e-01\n",
      "  -1.34077230e-01  1.24440211e-01  5.42010546e-01 -2.12220605e-01\n",
      "  -8.69741815e-02]\n",
      " [-1.25203750e-01  1.79084570e-01 -3.95784567e-01 -2.83695071e-01\n",
      "   1.02537467e-01 -3.15396428e-01  1.21264836e-01  6.86210349e-02\n",
      "   2.30871749e-01]\n",
      " [ 2.10653250e-01  1.32486668e-01  8.20617312e-02 -8.04088394e-02\n",
      "  -3.19165049e-01  2.63630811e-01  1.11837780e-02 -2.31218043e-01\n",
      "  -2.53542065e-01]\n",
      " [-5.08802263e-04 -1.46356363e-01  2.02588149e-01 -1.84899833e-02\n",
      "  -2.17280132e-01  2.65680450e-01  1.66088887e-01  3.85864320e-01\n",
      "   9.21661730e-02]\n",
      " [-3.21941708e-01  1.37114997e-01 -4.37085824e-01 -2.96267550e-01\n",
      "  -9.58990621e-02 -1.22228209e-02 -2.16045792e-01 -2.71888297e-01\n",
      "   2.01191526e-01]\n",
      " [ 4.11547143e-01 -1.20608399e-01  4.33552553e-01 -7.95469377e-02\n",
      "  -5.95326868e-02 -1.53322788e-01  5.65308399e-02  3.98260422e-01\n",
      "  -9.36003021e-02]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.13114733  0.66778364 -0.08407785  0.15524703 -0.11445973 -0.0834478\n",
      "   0.21572243]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:8 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.56042555]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 8 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 2.45660915e-01 -1.80368621e-01  3.00987180e-01  1.56785420e-01\n",
      "  -1.33517574e-01  1.24999867e-01  5.42010546e-01 -2.12220605e-01\n",
      "  -8.64145256e-02]\n",
      " [-1.26442853e-01  1.77845467e-01 -3.95784567e-01 -2.83695071e-01\n",
      "   1.01298364e-01 -3.16635532e-01  1.21264836e-01  6.86210349e-02\n",
      "   2.29632645e-01]\n",
      " [ 2.10065567e-01  1.31898985e-01  8.20617312e-02 -8.04088394e-02\n",
      "  -3.19752732e-01  2.63043128e-01  1.11837780e-02 -2.31218043e-01\n",
      "  -2.54129748e-01]\n",
      " [-4.00103851e-04 -1.46247665e-01  2.02588149e-01 -1.84899833e-02\n",
      "  -2.17171434e-01  2.65789148e-01  1.66088887e-01  3.85864320e-01\n",
      "   9.22748714e-02]\n",
      " [-3.20361538e-01  1.38695168e-01 -4.37085824e-01 -2.96267550e-01\n",
      "  -9.43188913e-02 -1.06426501e-02 -2.16045792e-01 -2.71888297e-01\n",
      "   2.02771696e-01]\n",
      " [ 4.11814912e-01 -1.20340630e-01  4.33552553e-01 -7.95469377e-02\n",
      "  -5.92649174e-02 -1.53055019e-01  5.65308399e-02  3.98260422e-01\n",
      "  -9.33325327e-02]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.2001774   0.63382836 -0.11983306  0.1201442  -0.14886607 -0.11638044\n",
      "   0.18147517]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:8 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.54864772]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 8 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.23635148 -0.18967806  0.30098718  0.15678542 -0.14282701  0.11569043\n",
      "   0.53270111 -0.21222061 -0.08641453]\n",
      " [-0.12571853  0.17856979 -0.39578457 -0.28369507  0.10202269 -0.3159112\n",
      "   0.12198916  0.06862103  0.22963265]\n",
      " [ 0.20514017  0.12697359  0.08206173 -0.08040884 -0.32467813  0.25811773\n",
      "   0.00625838 -0.23121804 -0.25412975]\n",
      " [-0.00155461 -0.14740217  0.20258815 -0.01848998 -0.21832594  0.26463465\n",
      "   0.16493439  0.38586432  0.09227487]\n",
      " [-0.31234201  0.14671469 -0.43708582 -0.29626755 -0.08629937 -0.00262313\n",
      "  -0.20802627 -0.2718883   0.2027717 ]\n",
      " [ 0.40952115 -0.12263439  0.43355255 -0.07954694 -0.06155868 -0.15534878\n",
      "   0.05423708  0.39826042 -0.09333253]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.26810915  0.58998668 -0.15307438  0.08118047 -0.18398733 -0.14198477\n",
      "   0.14520849]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:8 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.51906828]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 8 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.24764723 -0.18967806  0.30098718  0.16808117 -0.14282701  0.11569043\n",
      "   0.54399686 -0.21222061 -0.08641453]\n",
      " [-0.1299441   0.17856979 -0.39578457 -0.28792065  0.10202269 -0.3159112\n",
      "   0.11776359  0.06862103  0.22963265]\n",
      " [ 0.20709756  0.12697359  0.08206173 -0.07845145 -0.32467813  0.25811773\n",
      "   0.00821577 -0.23121804 -0.25412975]\n",
      " [ 0.00060842 -0.14740217  0.20258815 -0.01632696 -0.21832594  0.26463465\n",
      "   0.16709741  0.38586432  0.09227487]\n",
      " [-0.32276158  0.14671469 -0.43708582 -0.30668711 -0.08629937 -0.00262313\n",
      "  -0.21844583 -0.2718883   0.2027717 ]\n",
      " [ 0.41507945 -0.12263439  0.43355255 -0.07398864 -0.06155868 -0.15534878\n",
      "   0.05979538  0.39826042 -0.09333253]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.20808011  0.63298128 -0.12734387  0.11315798 -0.15180221 -0.12358718\n",
      "   0.18091907]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:8 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.55063179]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 8 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.25685994 -0.18967806  0.30098718  0.16808117 -0.13361431  0.12490314\n",
      "   0.55320957 -0.21222061 -0.08641453]\n",
      " [-0.13304638  0.17856979 -0.39578457 -0.28792065  0.09892042 -0.31901348\n",
      "   0.11466131  0.06862103  0.22963265]\n",
      " [ 0.20915364  0.12697359  0.08206173 -0.07845145 -0.32262205  0.26017381\n",
      "   0.01027185 -0.23121804 -0.25412975]\n",
      " [ 0.00354916 -0.14740217  0.20258815 -0.01632696 -0.2153852   0.26757538\n",
      "   0.17003815  0.38586432  0.09227487]\n",
      " [-0.33070468  0.14671469 -0.43708582 -0.30668711 -0.09424247 -0.01056623\n",
      "  -0.22638894 -0.2718883   0.2027717 ]\n",
      " [ 0.41860588 -0.12263439  0.43355255 -0.07398864 -0.05803225 -0.15182235\n",
      "   0.06332181  0.39826042 -0.09333253]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.15248508  0.67091553 -0.10267512  0.14301917 -0.12104146 -0.10426896\n",
      "   0.21228226]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:8 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57370813]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 8 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.2491532  -0.18967806  0.30098718  0.16037443 -0.13361431  0.1171964\n",
      "   0.55320957 -0.21222061 -0.09412126]\n",
      " [-0.12465393  0.17856979 -0.39578457 -0.2795282   0.09892042 -0.31062103\n",
      "   0.11466131  0.06862103  0.23802509]\n",
      " [ 0.20676646  0.12697359  0.08206173 -0.08083864 -0.32262205  0.25778663\n",
      "   0.01027185 -0.23121804 -0.25651693]\n",
      " [-0.00235837 -0.14740217  0.20258815 -0.02223448 -0.2153852   0.26166786\n",
      "   0.17003815  0.38586432  0.08636735]\n",
      " [-0.3232711   0.14671469 -0.43708582 -0.29925353 -0.09424247 -0.00313265\n",
      "  -0.22638894 -0.2718883   0.21020528]\n",
      " [ 0.41686574 -0.12263439  0.43355255 -0.07572878 -0.05803225 -0.15356249\n",
      "   0.06332181  0.39826042 -0.09507267]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.22264015  0.62785242 -0.12899112  0.105547   -0.16214584 -0.13166493\n",
      "   0.17546171]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:8 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.5488103]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 8 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.25941479 -0.17941647  0.30098718  0.17063603 -0.13361431  0.12745799\n",
      "   0.56347116 -0.21222061 -0.09412126]\n",
      " [-0.13028729  0.17293643 -0.39578457 -0.28516157  0.09892042 -0.31625439\n",
      "   0.10902795  0.06862103  0.23802509]\n",
      " [ 0.21356969  0.13377682  0.08206173 -0.0740354  -0.32262205  0.26458986\n",
      "   0.01707509 -0.23121804 -0.25651693]\n",
      " [ 0.0012081  -0.1438357   0.20258815 -0.01866802 -0.2153852   0.26523432\n",
      "   0.17360461  0.38586432  0.08636735]\n",
      " [-0.33199104  0.13799476 -0.43708582 -0.30797347 -0.09424247 -0.01185259\n",
      "  -0.23510887 -0.2718883   0.21020528]\n",
      " [ 0.41864961 -0.12085052  0.43355255 -0.07394491 -0.05803225 -0.15177862\n",
      "   0.06510568  0.39826042 -0.09507267]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.16677891  0.66745485 -0.10686222  0.14059279 -0.13060853 -0.11319545\n",
      "   0.2051811 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:8 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.58450321]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 8 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.2681537  -0.17067755  0.30098718  0.17937494 -0.13361431  0.12745799\n",
      "   0.57221007 -0.21222061 -0.09412126]\n",
      " [-0.13196351  0.17126022 -0.39578457 -0.28683778  0.09892042 -0.31625439\n",
      "   0.10735174  0.06862103  0.23802509]\n",
      " [ 0.21715631  0.13736344  0.08206173 -0.07044879 -0.32262205  0.26458986\n",
      "   0.0206617  -0.23121804 -0.25651693]\n",
      " [ 0.00136335 -0.14368045  0.20258815 -0.01851276 -0.2153852   0.26523432\n",
      "   0.17375987  0.38586432  0.08636735]\n",
      " [-0.34013149  0.12985431 -0.43708582 -0.31611392 -0.09424247 -0.01185259\n",
      "  -0.24324932 -0.2718883   0.21020528]\n",
      " [ 0.42221934 -0.11728079  0.43355255 -0.07037518 -0.05803225 -0.15177862\n",
      "   0.06867541  0.39826042 -0.09507267]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.1163253   0.70241838 -0.08331661  0.16945683 -0.10522646 -0.09686648\n",
      "   0.23402752]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:8 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.64094408]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 8 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.27713342 -0.17067755  0.3099669   0.17937494 -0.13361431  0.13643771\n",
      "   0.58118979 -0.21222061 -0.09412126]\n",
      " [-0.13862673  0.17126022 -0.40244778 -0.28683778  0.09892042 -0.32291761\n",
      "   0.10068852  0.06862103  0.23802509]\n",
      " [ 0.22270567  0.13736344  0.0876111  -0.07044879 -0.32262205  0.27013923\n",
      "   0.02621107 -0.23121804 -0.25651693]\n",
      " [ 0.00736263 -0.14368045  0.20858743 -0.01851276 -0.2153852   0.2712336\n",
      "   0.17975914  0.38586432  0.08636735]\n",
      " [-0.34839117  0.12985431 -0.4453455  -0.31611392 -0.09424247 -0.02011227\n",
      "  -0.251509   -0.2718883   0.21020528]\n",
      " [ 0.42911822 -0.11728079  0.44045143 -0.07037518 -0.05803225 -0.14487974\n",
      "   0.07557429  0.39826042 -0.09507267]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.07500967  0.73466683 -0.06994121  0.1959854  -0.07814742 -0.08601556\n",
      "   0.26229146]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:8 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.61283704]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 9 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.27368077 -0.17413021  0.3099669   0.17592228 -0.13361431  0.13643771\n",
      "   0.58118979 -0.21222061 -0.09757392]\n",
      " [-0.13833267  0.17155427 -0.40244778 -0.28654373  0.09892042 -0.32291761\n",
      "   0.10068852  0.06862103  0.23831914]\n",
      " [ 0.22210416  0.13676192  0.0876111  -0.0710503  -0.32262205  0.27013923\n",
      "   0.02621107 -0.23121804 -0.25711845]\n",
      " [ 0.00860554 -0.14243753  0.20858743 -0.01726985 -0.2153852   0.2712336\n",
      "   0.17975914  0.38586432  0.08761026]\n",
      " [-0.34264661  0.13559886 -0.4453455  -0.31036936 -0.09424247 -0.02011227\n",
      "  -0.251509   -0.2718883   0.21594983]\n",
      " [ 0.42647167 -0.11992733  0.44045143 -0.07302173 -0.05803225 -0.14487974\n",
      "   0.07557429  0.39826042 -0.09771922]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.14771292  0.69484136 -0.10599877  0.15903215 -0.11325515 -0.11652132\n",
      "   0.22328382]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:9 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.62143466]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 9 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.2830819  -0.17413021  0.31936803  0.17592228 -0.13361431  0.13643771\n",
      "   0.59059092 -0.21222061 -0.09757392]\n",
      " [-0.14300216  0.17155427 -0.40711727 -0.28654373  0.09892042 -0.32291761\n",
      "   0.09601903  0.06862103  0.23831914]\n",
      " [ 0.22574026  0.13676192  0.0912472  -0.0710503  -0.32262205  0.27013923\n",
      "   0.02984717 -0.23121804 -0.25711845]\n",
      " [ 0.01285495 -0.14243753  0.21283683 -0.01726985 -0.2153852   0.2712336\n",
      "   0.18400855  0.38586432  0.08761026]\n",
      " [-0.35158012  0.13559886 -0.45427901 -0.31036936 -0.09424247 -0.02011227\n",
      "  -0.26044251 -0.2718883   0.21594983]\n",
      " [ 0.43493991 -0.11992733  0.44891966 -0.07302173 -0.05803225 -0.14487974\n",
      "   0.08404252  0.39826042 -0.09771922]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.10318349  0.72878217 -0.08855574  0.18500174 -0.08662855 -0.10488817\n",
      "   0.25532738]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:9 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.64764673]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 9 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.26840802 -0.18880409  0.30469415  0.17592228 -0.13361431  0.13643771\n",
      "   0.57591704 -0.21222061 -0.09757392]\n",
      " [-0.13788517  0.17667126 -0.40200028 -0.28654373  0.09892042 -0.32291761\n",
      "   0.10113602  0.06862103  0.23831914]\n",
      " [ 0.21730886  0.12833052  0.0828158  -0.0710503  -0.32262205  0.27013923\n",
      "   0.02141577 -0.23121804 -0.25711845]\n",
      " [ 0.00800463 -0.14728785  0.20798651 -0.01726985 -0.2153852   0.2712336\n",
      "   0.17915823  0.38586432  0.08761026]\n",
      " [-0.33763126  0.14954772 -0.44033015 -0.31036936 -0.09424247 -0.02011227\n",
      "  -0.24649365 -0.2718883   0.21594983]\n",
      " [ 0.42178435 -0.13308289  0.43576411 -0.07302173 -0.05803225 -0.14487974\n",
      "   0.07088696  0.39826042 -0.09771922]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.17708012  0.67448585 -0.12031871  0.13928952 -0.12848514 -0.12578482\n",
      "   0.20358923]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:9 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.57175088]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 9 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.27298744 -0.18880409  0.30927357  0.17592228 -0.13361431  0.13643771\n",
      "   0.57591704 -0.20764119 -0.09757392]\n",
      " [-0.14373147  0.17667126 -0.40784658 -0.28654373  0.09892042 -0.32291761\n",
      "   0.10113602  0.06277473  0.23831914]\n",
      " [ 0.21821096  0.12833052  0.0837179  -0.0710503  -0.32262205  0.27013923\n",
      "   0.02141577 -0.23031594 -0.25711845]\n",
      " [ 0.0152199  -0.14728785  0.21520179 -0.01726985 -0.2153852   0.2712336\n",
      "   0.17915823  0.3930796   0.08761026]\n",
      " [-0.34820166  0.14954772 -0.45090055 -0.31036936 -0.09424247 -0.02011227\n",
      "  -0.24649365 -0.2824587   0.21594983]\n",
      " [ 0.43314494 -0.13308289  0.44712469 -0.07302173 -0.05803225 -0.14487974\n",
      "   0.07088696  0.40962101 -0.09771922]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.12465134  0.70537971 -0.10016944  0.16640673 -0.09461191 -0.11219245\n",
      "   0.24439475]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:9 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.59088767]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 9 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.27719944 -0.18880409  0.30927357  0.18013429 -0.13361431  0.13643771\n",
      "   0.57591704 -0.20764119 -0.09336192]\n",
      " [-0.14608277  0.17667126 -0.40784658 -0.28889503  0.09892042 -0.32291761\n",
      "   0.10113602  0.06277473  0.23596784]\n",
      " [ 0.21685572  0.12833052  0.0837179  -0.07240553 -0.32262205  0.27013923\n",
      "   0.02141577 -0.23031594 -0.25847368]\n",
      " [ 0.0162757  -0.14728785  0.21520179 -0.01621405 -0.2153852   0.2712336\n",
      "   0.17915823  0.3930796   0.08866605]\n",
      " [-0.353414    0.14954772 -0.45090055 -0.3155817  -0.09424247 -0.02011227\n",
      "  -0.24649365 -0.2824587   0.21073749]\n",
      " [ 0.43633365 -0.13308289  0.44712469 -0.06983301 -0.05803225 -0.14487974\n",
      "   0.07088696  0.40962101 -0.0945305 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.07520204  0.73440355 -0.07781055  0.18977341 -0.06883019 -0.09285201\n",
      "   0.27234483]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:9 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.61022131]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 9 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.27384223 -0.18880409  0.30927357  0.18013429 -0.13697152  0.1330805\n",
      "   0.57591704 -0.20764119 -0.09671913]\n",
      " [-0.14366051  0.17667126 -0.40784658 -0.28889503  0.10134267 -0.32049536\n",
      "   0.10113602  0.06277473  0.2383901 ]\n",
      " [ 0.2185592   0.12833052  0.0837179  -0.07240553 -0.32091857  0.2718427\n",
      "   0.02141577 -0.23031594 -0.25677021]\n",
      " [ 0.0133773  -0.14728785  0.21520179 -0.01621405 -0.2182836   0.2683352\n",
      "   0.17915823  0.3930796   0.08576765]\n",
      " [-0.34882693  0.14954772 -0.45090055 -0.3155817  -0.0896554  -0.0155252\n",
      "  -0.24649365 -0.2824587   0.21532456]\n",
      " [ 0.4338259  -0.13308289  0.44712469 -0.06983301 -0.06054    -0.14738749\n",
      "   0.07088696  0.40962101 -0.09703825]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.147773    0.69474133 -0.11166651  0.15519392 -0.10802657 -0.12449974\n",
      "   0.23354353]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:9 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.55521731]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 9 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.2741091  -0.18853722  0.30927357  0.18013429 -0.13670465  0.13334737\n",
      "   0.57591704 -0.20764119 -0.09645226]\n",
      " [-0.14455538  0.1757764  -0.40784658 -0.28889503  0.1004478  -0.32139022\n",
      "   0.10113602  0.06277473  0.23749523]\n",
      " [ 0.21785605  0.12762737  0.0837179  -0.07240553 -0.32162172  0.27113955\n",
      "   0.02141577 -0.23031594 -0.25747336]\n",
      " [ 0.01334458 -0.14732057  0.21520179 -0.01621405 -0.21831631  0.26830249\n",
      "   0.17915823  0.3930796   0.08573494]\n",
      " [-0.34730228  0.15107237 -0.45090055 -0.3155817  -0.08813075 -0.01400054\n",
      "  -0.24649365 -0.2824587   0.21684922]\n",
      " [ 0.43389827 -0.13301052  0.44712469 -0.06983301 -0.06046763 -0.14731512\n",
      "   0.07088696  0.40962101 -0.09696588]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.21632875  0.66073034 -0.14683966  0.1202127  -0.14233716 -0.15725094\n",
      "   0.19933803]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:9 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.54832485]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 9 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.26406474 -0.19858158  0.30927357  0.18013429 -0.14674901  0.123303\n",
      "   0.56587268 -0.20764119 -0.09645226]\n",
      " [-0.14305458  0.17727719 -0.40784658 -0.28889503  0.1019486  -0.31988943\n",
      "   0.10263682  0.06277473  0.23749523]\n",
      " [ 0.21261707  0.12238839  0.0837179  -0.07240553 -0.3268607   0.26590057\n",
      "   0.01617679 -0.23031594 -0.25747336]\n",
      " [ 0.01173274 -0.14893241  0.21520179 -0.01621405 -0.21992815  0.26669065\n",
      "   0.17754639  0.3930796   0.08573494]\n",
      " [-0.33870716  0.15966749 -0.45090055 -0.3155817  -0.07953563 -0.00540542\n",
      "  -0.23789853 -0.2824587   0.21684922]\n",
      " [ 0.43113313 -0.13577566  0.44712469 -0.06983301 -0.06323277 -0.15008026\n",
      "   0.06812182  0.40962101 -0.09696588]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.2842291   0.61599483 -0.17928707  0.08093569 -0.17790161 -0.18217438\n",
      "   0.1626103 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:9 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.52119886]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 9 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.27587381 -0.19858158  0.30927357  0.19194336 -0.14674901  0.123303\n",
      "   0.57768175 -0.20764119 -0.09645226]\n",
      " [-0.1478421   0.17727719 -0.40784658 -0.29368255  0.1019486  -0.31988943\n",
      "   0.0978493   0.06277473  0.23749523]\n",
      " [ 0.21493861  0.12238839  0.0837179  -0.070084   -0.3268607   0.26590057\n",
      "   0.01849832 -0.23031594 -0.25747336]\n",
      " [ 0.01429832 -0.14893241  0.21520179 -0.01364848 -0.21992815  0.26669065\n",
      "   0.18011196  0.3930796   0.08573494]\n",
      " [-0.34969667  0.15966749 -0.45090055 -0.32657122 -0.07953563 -0.00540542\n",
      "  -0.24888804 -0.2824587   0.21684922]\n",
      " [ 0.43726    -0.13577566  0.44712469 -0.06370614 -0.06323277 -0.15008026\n",
      "   0.07424869  0.40962101 -0.09696588]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.22448655  0.65978817 -0.15429032  0.11313797 -0.14545193 -0.16480969\n",
      "   0.19879849]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:9 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.55154295]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 9 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.28560811 -0.19858158  0.30927357  0.19194336 -0.13701471  0.1330373\n",
      "   0.58741605 -0.20764119 -0.09645226]\n",
      " [-0.15149122  0.17727719 -0.40784658 -0.29368255  0.09829949 -0.32353854\n",
      "   0.09420019  0.06277473  0.23749523]\n",
      " [ 0.21731236  0.12238839  0.0837179  -0.070084   -0.32448695  0.26827433\n",
      "   0.02087208 -0.23031594 -0.25747336]\n",
      " [ 0.01759411 -0.14893241  0.21520179 -0.01364848 -0.21663236  0.26998644\n",
      "   0.18340775  0.3930796   0.08573494]\n",
      " [-0.35814785  0.15966749 -0.45090055 -0.32657122 -0.08798681 -0.0138566\n",
      "  -0.25733922 -0.2824587   0.21684922]\n",
      " [ 0.44130402 -0.13577566  0.44712469 -0.06370614 -0.05918875 -0.14603624\n",
      "   0.07829271  0.40962101 -0.09696588]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.16902512  0.69841029 -0.13025254  0.14325423 -0.11439338 -0.14620377\n",
      "   0.23063343]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:9 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.5736942]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 9 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.27716152 -0.19858158  0.30927357  0.18349678 -0.13701471  0.12459071\n",
      "   0.58741605 -0.20764119 -0.10489884]\n",
      " [-0.14280203  0.17727719 -0.40784658 -0.28499336  0.09829949 -0.31484935\n",
      "   0.09420019  0.06277473  0.24618442]\n",
      " [ 0.214558    0.12238839  0.0837179  -0.07283836 -0.32448695  0.26551997\n",
      "   0.02087208 -0.23031594 -0.26022772]\n",
      " [ 0.01148578 -0.14893241  0.21520179 -0.01975681 -0.21663236  0.2638781\n",
      "   0.18340775  0.3930796   0.07962661]\n",
      " [-0.3501709   0.15966749 -0.45090055 -0.31859426 -0.08798681 -0.00587965\n",
      "  -0.25733922 -0.2824587   0.22482617]\n",
      " [ 0.43895407 -0.13577566  0.44712469 -0.06605609 -0.05918875 -0.14838619\n",
      "   0.07829271  0.40962101 -0.09931583]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Theta two: \n",
      "[[-0.23917907  0.65450965 -0.15622585  0.10541141 -0.15571124 -0.17299168\n",
      "   0.1931994 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:9 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.55074516]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 9 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.28793252 -0.18781058  0.30927357  0.19426778 -0.13701471  0.13536171\n",
      "   0.59818705 -0.20764119 -0.10489884]\n",
      " [-0.14899841  0.17108082 -0.40784658 -0.29118974  0.09829949 -0.32104573\n",
      "   0.08800381  0.06277473  0.24618442]\n",
      " [ 0.22165553  0.12948593  0.0837179  -0.06574083 -0.32448695  0.2726175\n",
      "   0.02796962 -0.23031594 -0.26022772]\n",
      " [ 0.01543274 -0.14498544  0.21520179 -0.01580984 -0.21663236  0.26782507\n",
      "   0.18735472  0.3930796   0.07962661]\n",
      " [-0.35944835  0.15039004 -0.45090055 -0.32787171 -0.08798681 -0.0151571\n",
      "  -0.26661667 -0.2824587   0.22482617]\n",
      " [ 0.44125876 -0.13347098  0.44712469 -0.0637514  -0.05918875 -0.1460815\n",
      "   0.0805974   0.40962101 -0.09931583]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.18360065  0.69485585 -0.13486484  0.14066212 -0.12391948 -0.15543011\n",
      "   0.22330403]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:9 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.58602367]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 9 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.29717204 -0.17857106  0.30927357  0.2035073  -0.13701471  0.13536171\n",
      "   0.60742657 -0.20764119 -0.10489884]\n",
      " [-0.15125341  0.16882581 -0.40784658 -0.29344474  0.09829949 -0.32104573\n",
      "   0.08574881  0.06277473  0.24618442]\n",
      " [ 0.22549451  0.1333249   0.0837179  -0.06190185 -0.32448695  0.2726175\n",
      "   0.03180859 -0.23031594 -0.26022772]\n",
      " [ 0.01595967 -0.14445851  0.21520179 -0.01528291 -0.21663236  0.26782507\n",
      "   0.18788165  0.3930796   0.07962661]\n",
      " [-0.36806803  0.14177036 -0.45090055 -0.3364914  -0.08798681 -0.0151571\n",
      "  -0.27523635 -0.2824587   0.22482617]\n",
      " [ 0.44522864 -0.12950109  0.44712469 -0.05978152 -0.05918875 -0.1460815\n",
      "   0.08456728  0.40962101 -0.09931583]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.13338534  0.73047968 -0.11202453  0.16967189 -0.09828473 -0.13990015\n",
      "   0.25245167]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:9 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.64510543]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 9 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.3061374  -0.17857106  0.31823893  0.2035073  -0.13701471  0.14432707\n",
      "   0.61639193 -0.20764119 -0.10489884]\n",
      " [-0.15817154  0.16882581 -0.41476472 -0.29344474  0.09829949 -0.32796386\n",
      "   0.07883067  0.06277473  0.24618442]\n",
      " [ 0.23117511  0.1333249   0.0893985  -0.06190185 -0.32448695  0.2782981\n",
      "   0.03748919 -0.23031594 -0.26022772]\n",
      " [ 0.02217359 -0.14445851  0.2214157  -0.01528291 -0.21663236  0.27403899\n",
      "   0.19409557  0.3930796   0.07962661]\n",
      " [-0.37647289  0.14177036 -0.45930541 -0.3364914  -0.08798681 -0.02356196\n",
      "  -0.28364121 -0.2824587   0.22482617]\n",
      " [ 0.45236325 -0.12950109  0.4542593  -0.05978152 -0.05918875 -0.13894689\n",
      "   0.09170189  0.40962101 -0.09931583]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.09275977  0.76273655 -0.09938083  0.19602855 -0.07125777 -0.12982542\n",
      "   0.28074869]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:9 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.60976557]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 10 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.30208732 -0.18262114  0.31823893  0.19945722 -0.13701471  0.14432707\n",
      "   0.61639193 -0.20764119 -0.10894892]\n",
      " [-0.15750785  0.16948951 -0.41476472 -0.29278105  0.09829949 -0.32796386\n",
      "   0.07883067  0.06277473  0.24684811]\n",
      " [ 0.23040699  0.13255678  0.0893985  -0.06266997 -0.32448695  0.2782981\n",
      "   0.03748919 -0.23031594 -0.26099584]\n",
      " [ 0.02322358 -0.14340852  0.2214157  -0.01423293 -0.21663236  0.27403899\n",
      "   0.19409557  0.3930796   0.0806766 ]\n",
      " [-0.37037559  0.14786766 -0.45930541 -0.3303941  -0.08798681 -0.02356196\n",
      "  -0.28364121 -0.2824587   0.23092347]\n",
      " [ 0.4494129  -0.13245144  0.4542593  -0.06273187 -0.05918875 -0.13894689\n",
      "   0.09170189  0.40962101 -0.10226618]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.16530709  0.7223782  -0.13499064  0.15898654 -0.10648085 -0.15987913\n",
      "   0.24151147]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:10 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.62456639]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 10 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.31157982 -0.18262114  0.32773143  0.19945722 -0.13701471  0.14432707\n",
      "   0.62588443 -0.20764119 -0.10894892]\n",
      " [-0.16262018  0.16948951 -0.41987705 -0.29278105  0.09829949 -0.32796386\n",
      "   0.07371834  0.06277473  0.24684811]\n",
      " [ 0.23421584  0.13255678  0.09320735 -0.06266997 -0.32448695  0.2782981\n",
      "   0.04129804 -0.23031594 -0.26099584]\n",
      " [ 0.02782639 -0.14340852  0.22601852 -0.01423293 -0.21663236  0.27403899\n",
      "   0.19869838  0.3930796   0.0806766 ]\n",
      " [-0.37949623  0.14786766 -0.46842605 -0.3303941  -0.08798681 -0.02356196\n",
      "  -0.29276185 -0.2824587   0.23092347]\n",
      " [ 0.45804542 -0.13245144  0.46289182 -0.06273187 -0.05918875 -0.13894689\n",
      "   0.10033441  0.40962101 -0.10226618]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.12129065  0.7564906  -0.11830475  0.18488517 -0.07972073 -0.14899597\n",
      "   0.27364999]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:10 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.64962381]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 10 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.29645295 -0.19774801  0.31260455  0.19945722 -0.13701471  0.14432707\n",
      "   0.61075756 -0.20764119 -0.10894892]\n",
      " [-0.15652622  0.17558347 -0.41378309 -0.29278105  0.09829949 -0.32796386\n",
      "   0.0798123   0.06277473  0.24684811]\n",
      " [ 0.2255093   0.12385024  0.08450081 -0.06266997 -0.32448695  0.2782981\n",
      "   0.0325915  -0.23031594 -0.26099584]\n",
      " [ 0.02224707 -0.14898785  0.22043919 -0.01423293 -0.21663236  0.27403899\n",
      "   0.19311906  0.3930796   0.0806766 ]\n",
      " [-0.36501702  0.16234686 -0.45394685 -0.3303941  -0.08798681 -0.02356196\n",
      "  -0.27828265 -0.2824587   0.23092347]\n",
      " [ 0.44447816 -0.1460187   0.44932456 -0.06273187 -0.05918875 -0.13894689\n",
      "   0.08676715  0.40962101 -0.10226618]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.19522197  0.70126531 -0.14905886  0.13884373 -0.122355   -0.16898376\n",
      "   0.22125881]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:10 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.57270062]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 10 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.30149397 -0.19774801  0.31764557  0.19945722 -0.13701471  0.14432707\n",
      "   0.61075756 -0.20260017 -0.10894892]\n",
      " [-0.16275059  0.17558347 -0.42000746 -0.29278105  0.09829949 -0.32796386\n",
      "   0.0798123   0.05655036  0.24684811]\n",
      " [ 0.22654931  0.12385024  0.08554082 -0.06266997 -0.32448695  0.2782981\n",
      "   0.0325915  -0.22927593 -0.26099584]\n",
      " [ 0.02977081 -0.14898785  0.22796294 -0.01423293 -0.21663236  0.27403899\n",
      "   0.19311906  0.40060334  0.0806766 ]\n",
      " [-0.3757992   0.16234686 -0.46472903 -0.3303941  -0.08798681 -0.02356196\n",
      "  -0.27828265 -0.29324088  0.23092347]\n",
      " [ 0.45592477 -0.1460187   0.46077117 -0.06273187 -0.05918875 -0.13894689\n",
      "   0.08676715  0.42106762 -0.10226618]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.14293877  0.73258441 -0.12941232  0.16602645 -0.08817247 -0.15594049\n",
      "   0.26237488]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:10 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.58889439]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 10 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.3061882  -0.19774801  0.31764557  0.20415145 -0.13701471  0.14432707\n",
      "   0.61075756 -0.20260017 -0.10425469]\n",
      " [-0.16531875  0.17558347 -0.42000746 -0.29534921  0.09829949 -0.32796386\n",
      "   0.0798123   0.05655036  0.24427995]\n",
      " [ 0.22534393  0.12385024  0.08554082 -0.06387535 -0.32448695  0.2782981\n",
      "   0.0325915  -0.22927593 -0.26220122]\n",
      " [ 0.03096505 -0.14898785  0.22796294 -0.01303869 -0.21663236  0.27403899\n",
      "   0.19311906  0.40060334  0.08187083]\n",
      " [-0.3813903   0.16234686 -0.46472903 -0.33598519 -0.08798681 -0.02356196\n",
      "  -0.27828265 -0.29324088  0.22533237]\n",
      " [ 0.45946866 -0.1460187   0.46077117 -0.05918797 -0.05918875 -0.13894689\n",
      "   0.08676715  0.42106762 -0.09872228]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.09317489  0.76228173 -0.10711722  0.18970111 -0.06209445 -0.13686253\n",
      "   0.29085091]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:10 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.60665938]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 10 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.30244295 -0.19774801  0.31764557  0.20415145 -0.14075996  0.14058182\n",
      "   0.61075756 -0.20260017 -0.10799994]\n",
      " [-0.16260714  0.17558347 -0.42000746 -0.29534921  0.1010111  -0.32525225\n",
      "   0.0798123   0.05655036  0.24699156]\n",
      " [ 0.2268441   0.12385024  0.08554082 -0.06387535 -0.32298678  0.27979827\n",
      "   0.0325915  -0.22927593 -0.26070105]\n",
      " [ 0.02790665 -0.14898785  0.22796294 -0.01303869 -0.21969076  0.27098059\n",
      "   0.19311906  0.40060334  0.07881244]\n",
      " [-0.37663352  0.16234686 -0.46472903 -0.33598519 -0.08323002 -0.01880517\n",
      "  -0.27828265 -0.29324088  0.23008916]\n",
      " [ 0.45654552 -0.1460187   0.46077117 -0.05918797 -0.06211189 -0.14187004\n",
      "   0.08676715  0.42106762 -0.10164543]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.16555657  0.72231826 -0.14058617  0.15501216 -0.10135848 -0.1682396\n",
      "   0.25172403]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:10 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.54921809]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 10 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.30250216 -0.19768881  0.31764557  0.20415145 -0.14070076  0.14064103\n",
      "   0.61075756 -0.20260017 -0.10794073]\n",
      " [-0.16321418  0.17497642 -0.42000746 -0.29534921  0.10040406 -0.3258593\n",
      "   0.0798123   0.05655036  0.24638452]\n",
      " [ 0.226049    0.12305514  0.08554082 -0.06387535 -0.32378187  0.27900317\n",
      "   0.0325915  -0.22927593 -0.26149615]\n",
      " [ 0.02775332 -0.14914117  0.22796294 -0.01303869 -0.21984409  0.27082726\n",
      "   0.19311906  0.40060334  0.07865911]\n",
      " [-0.37517056  0.16380982 -0.46472903 -0.33598519 -0.08176707 -0.01734222\n",
      "  -0.27828265 -0.29324088  0.23155211]\n",
      " [ 0.45646225 -0.14610198  0.46077117 -0.05918797 -0.06219517 -0.14195331\n",
      "   0.08676715  0.42106762 -0.1017287 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.23354361  0.68838394 -0.17518687  0.12022326 -0.13550533 -0.20076835\n",
      "   0.21764724]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:10 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.54808105]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 10 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.29179095 -0.20840001  0.31764557  0.20415145 -0.15141196  0.12992982\n",
      "   0.60004635 -0.20260017 -0.10794073]\n",
      " [-0.16095249  0.17723812 -0.42000746 -0.29534921  0.10266575 -0.3235976\n",
      "   0.08207399  0.05655036  0.24638452]\n",
      " [ 0.22049103  0.11749717  0.08554082 -0.06387535 -0.32933984  0.2734452\n",
      "   0.02703353 -0.22927593 -0.26149615]\n",
      " [ 0.02567879 -0.15121571  0.22796294 -0.01303869 -0.22191862  0.26875272\n",
      "   0.19104452  0.40060334  0.07865911]\n",
      " [-0.36599803  0.17298236 -0.46472903 -0.33598519 -0.07259453 -0.00816968\n",
      "  -0.26911011 -0.29324088  0.23155211]\n",
      " [ 0.45321786 -0.14934637  0.46077117 -0.05918797 -0.06543956 -0.1451977\n",
      "   0.08352276  0.42106762 -0.1017287 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.30142022  0.6427968  -0.20685672  0.08062123 -0.17152338 -0.22499495\n",
      "   0.18044437]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:10 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.52377352]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 10 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.30401224 -0.20840001  0.31764557  0.21637274 -0.15141196  0.12992982\n",
      "   0.61226764 -0.20260017 -0.10794073]\n",
      " [-0.16631909  0.17723812 -0.42000746 -0.30071581  0.10266575 -0.3235976\n",
      "   0.0767074   0.05655036  0.24638452]\n",
      " [ 0.22319507  0.11749717  0.08554082 -0.06117131 -0.32933984  0.2734452\n",
      "   0.02973757 -0.22927593 -0.26149615]\n",
      " [ 0.02867203 -0.15121571  0.22796294 -0.01004545 -0.22191862  0.26875272\n",
      "   0.19403776  0.40060334  0.07865911]\n",
      " [-0.37748859  0.17298236 -0.46472903 -0.34747575 -0.07259453 -0.00816968\n",
      "  -0.28060067 -0.29324088  0.23155211]\n",
      " [ 0.45991937 -0.14934637  0.46077117 -0.05248646 -0.06543956 -0.1451977\n",
      "   0.09022427  0.42106762 -0.1017287 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.24202648  0.68731287 -0.18265259  0.11303737 -0.13881253 -0.20868171\n",
      "   0.21710039]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:10 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.55263757]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 10 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.31420326 -0.20840001  0.31764557  0.21637274 -0.14122095  0.14012084\n",
      "   0.62245866 -0.20260017 -0.10794073]\n",
      " [-0.17051053  0.17723812 -0.42000746 -0.30071581  0.09847431 -0.32778904\n",
      "   0.07251596  0.05655036  0.24638452]\n",
      " [ 0.22589288  0.11749717  0.08554082 -0.06117131 -0.32664204  0.27614301\n",
      "   0.03243537 -0.22927593 -0.26149615]\n",
      " [ 0.03233164 -0.15121571  0.22796294 -0.01004545 -0.21825902  0.27241233\n",
      "   0.19769737  0.40060334  0.07865911]\n",
      " [-0.38642695  0.17298236 -0.46472903 -0.34747575 -0.08153289 -0.01710804\n",
      "  -0.28953903 -0.29324088  0.23155211]\n",
      " [ 0.46448039 -0.14934637  0.46077117 -0.05248646 -0.06087853 -0.14063668\n",
      "   0.09478529  0.42106762 -0.1017287 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.18672594  0.72656958 -0.15926145  0.14340294 -0.10745817 -0.19080553\n",
      "   0.24939981]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:10 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57359853]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 10 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.30507633 -0.20840001  0.31764557  0.20724581 -0.14122095  0.13099391\n",
      "   0.62245866 -0.20260017 -0.11706766]\n",
      " [-0.16152311  0.17723812 -0.42000746 -0.29172839  0.09847431 -0.31880162\n",
      "   0.07251596  0.05655036  0.25537194]\n",
      " [ 0.22277254  0.11749717  0.08554082 -0.06429165 -0.32664204  0.27302267\n",
      "   0.03243537 -0.22927593 -0.26461649]\n",
      " [ 0.02600719 -0.15121571  0.22796294 -0.01636989 -0.21825902  0.26608789\n",
      "   0.19769737  0.40060334  0.07233467]\n",
      " [-0.37790533  0.17298236 -0.46472903 -0.33895414 -0.08153289 -0.00858642\n",
      "  -0.28953903 -0.29324088  0.24007373]\n",
      " [ 0.46152698 -0.14934637  0.46077117 -0.05543987 -0.06087853 -0.14359009\n",
      "   0.09478529  0.42106762 -0.10468211]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.25687223  0.68188007 -0.18488268  0.10519269 -0.14900373 -0.21696862\n",
      "   0.21135907]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:10 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.55298552]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 10 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.31625454 -0.1972218   0.31764557  0.21842403 -0.14122095  0.14217212\n",
      "   0.63363687 -0.20260017 -0.11706766]\n",
      " [-0.16826681  0.17049441 -0.42000746 -0.29847209  0.09847431 -0.32554533\n",
      "   0.06577225  0.05655036  0.25537194]\n",
      " [ 0.23016126  0.12488589  0.08554082 -0.05690293 -0.32664204  0.28041139\n",
      "   0.0398241  -0.22927593 -0.26461649]\n",
      " [ 0.03034408 -0.14687882  0.22796294 -0.012033   -0.21825902  0.27042478\n",
      "   0.20203426  0.40060334  0.07233467]\n",
      " [-0.3876952   0.16319249 -0.46472903 -0.34874401 -0.08153289 -0.0183763\n",
      "  -0.29932891 -0.29324088  0.24007373]\n",
      " [ 0.46436824 -0.14650511  0.46077117 -0.05259861 -0.06087853 -0.14074883\n",
      "   0.09762655  0.42106762 -0.10468211]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.20162291  0.72288549 -0.16431254  0.14062948 -0.11696675 -0.20033232\n",
      "   0.24184551]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:10 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.58783463]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 10 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.32591436 -0.18756199  0.31764557  0.22808384 -0.14122095  0.14217212\n",
      "   0.64329669 -0.20260017 -0.11706766]\n",
      " [-0.17110586  0.16765537 -0.42000746 -0.30131114  0.09847431 -0.32554533\n",
      "   0.06293321  0.05655036  0.25537194]\n",
      " [ 0.2342618   0.12898644  0.08554082 -0.05280239 -0.32664204  0.28041139\n",
      "   0.04392464 -0.22927593 -0.26461649]\n",
      " [ 0.03125991 -0.145963    0.22796294 -0.01111717 -0.21825902  0.27042478\n",
      "   0.20295008  0.40060334  0.07233467]\n",
      " [-0.3967513   0.15413638 -0.46472903 -0.35780011 -0.08153289 -0.0183763\n",
      "  -0.30838501 -0.29324088  0.24007373]\n",
      " [ 0.46875218 -0.14212117  0.46077117 -0.04821468 -0.06087853 -0.14074883\n",
      "   0.10201049  0.42106762 -0.10468211]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.15169215  0.75910213 -0.14221141  0.16977391 -0.09108472 -0.18561679\n",
      "   0.27129168]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:10 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.64954708]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 10 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.33480104 -0.18756199  0.32653225  0.22808384 -0.14122095  0.1510588\n",
      "   0.65218337 -0.20260017 -0.11706766]\n",
      " [-0.17823861  0.16765537 -0.42714021 -0.30131114  0.09847431 -0.33267808\n",
      "   0.05580046  0.05655036  0.25537194]\n",
      " [ 0.24006234  0.12898644  0.09134136 -0.05280239 -0.32664204  0.28621193\n",
      "   0.04972518 -0.22927593 -0.26461649]\n",
      " [ 0.03766658 -0.145963    0.23436961 -0.01111717 -0.21825902  0.27683145\n",
      "   0.20935675  0.40060334  0.07233467]\n",
      " [-0.40523605  0.15413638 -0.47321377 -0.35780011 -0.08153289 -0.02686104\n",
      "  -0.31686975 -0.29324088  0.24007373]\n",
      " [ 0.47608222 -0.14212117  0.46810121 -0.04821468 -0.06087853 -0.13341879\n",
      "   0.10934053  0.42106762 -0.10468211]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.11180436  0.79128161 -0.13029884  0.19592787 -0.0641455  -0.17630006\n",
      "   0.2995742 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:10 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.60628983]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 11 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.33020615 -0.19215688  0.32653225  0.22348895 -0.14122095  0.1510588\n",
      "   0.65218337 -0.20260017 -0.12166255]\n",
      " [-0.17721692  0.16867706 -0.42714021 -0.30028944  0.09847431 -0.33267808\n",
      "   0.05580046  0.05655036  0.25639363]\n",
      " [ 0.23912896  0.12805305  0.09134136 -0.05373577 -0.32664204  0.28621193\n",
      "   0.04972518 -0.22927593 -0.26554987]\n",
      " [ 0.03851778 -0.1451118   0.23436961 -0.01026597 -0.21825902  0.27683145\n",
      "   0.20935675  0.40060334  0.07318587]\n",
      " [-0.39878572  0.16058672 -0.47321377 -0.35134977 -0.08153289 -0.02686104\n",
      "  -0.31686975 -0.29324088  0.24652406]\n",
      " [ 0.4728334  -0.14536999  0.46810121 -0.05146349 -0.06087853 -0.13341879\n",
      "   0.10934053  0.42106762 -0.10793093]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.1841658   0.75045475 -0.16545732  0.15881336 -0.0994747  -0.20588321\n",
      "   0.26012689]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:11 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.62798085]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 11 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.33973016 -0.19215688  0.33605626  0.22348895 -0.14122095  0.1510588\n",
      "   0.66170738 -0.20260017 -0.12166255]\n",
      " [-0.18274964  0.16867706 -0.43267293 -0.30028944  0.09847431 -0.33267808\n",
      "   0.05026774  0.05655036  0.25639363]\n",
      " [ 0.24311364  0.12805305  0.09532604 -0.05373577 -0.32664204  0.28621193\n",
      "   0.05370986 -0.22927593 -0.26554987]\n",
      " [ 0.04346368 -0.1451118   0.2393155  -0.01026597 -0.21825902  0.27683145\n",
      "   0.21430265  0.40060334  0.07318587]\n",
      " [-0.40803121  0.16058672 -0.48245927 -0.35134977 -0.08153289 -0.02686104\n",
      "  -0.32611525 -0.29324088  0.24652406]\n",
      " [ 0.48159646 -0.14536999  0.47686427 -0.05146349 -0.06087853 -0.13341879\n",
      "   0.11810359  0.42106762 -0.10793093]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.14071008  0.78466886 -0.14954387  0.18462259 -0.072607   -0.19573798\n",
      "   0.29232009]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:11 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.65175993]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 11 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.32421929 -0.20766774  0.3205454   0.22348895 -0.14122095  0.1510588\n",
      "   0.64619651 -0.20260017 -0.12166255]\n",
      " [-0.17569903  0.17572767 -0.42562232 -0.30028944  0.09847431 -0.33267808\n",
      "   0.05731834  0.05655036  0.25639363]\n",
      " [ 0.23411699  0.1190564   0.08632939 -0.05373577 -0.32664204  0.28621193\n",
      "   0.04471321 -0.22927593 -0.26554987]\n",
      " [ 0.03715282 -0.15142265  0.23300465 -0.01026597 -0.21825902  0.27683145\n",
      "   0.20799179  0.40060334  0.07318587]\n",
      " [-0.39307583  0.17554209 -0.46750389 -0.35134977 -0.08153289 -0.02686104\n",
      "  -0.31115987 -0.29324088  0.24652406]\n",
      " [ 0.46763037 -0.15933608  0.46289819 -0.05146349 -0.06087853 -0.13341879\n",
      "   0.1041375   0.42106762 -0.10793093]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.2146747   0.728555   -0.1792894   0.13823234 -0.11603127 -0.2148261\n",
      "   0.23926397]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:11 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.57372932]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 11 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.32970857 -0.20766774  0.32603468  0.22348895 -0.14122095  0.1510588\n",
      "   0.64619651 -0.19711089 -0.12166255]\n",
      " [-0.1822964   0.17572767 -0.43221969 -0.30028944  0.09847431 -0.33267808\n",
      "   0.05731834  0.049953    0.25639363]\n",
      " [ 0.2353026   0.1190564   0.087515   -0.05373577 -0.32664204  0.28621193\n",
      "   0.04471321 -0.22809032 -0.26554987]\n",
      " [ 0.04497983 -0.15142265  0.24083165 -0.01026597 -0.21825902  0.27683145\n",
      "   0.20799179  0.40843034  0.07318587]\n",
      " [-0.40403944  0.17554209 -0.47846749 -0.35134977 -0.08153289 -0.02686104\n",
      "  -0.31115987 -0.30420448  0.24652406]\n",
      " [ 0.47913764 -0.15933608  0.47440545 -0.05146349 -0.06087853 -0.13341879\n",
      "   0.1041375   0.43257488 -0.10793093]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.16254947  0.76028752 -0.16015535  0.1654822  -0.08154139 -0.20232689\n",
      "   0.28067175]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:11 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.58667217]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 11 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.33487092 -0.20766774  0.32603468  0.2286513  -0.14122095  0.1510588\n",
      "   0.64619651 -0.19711089 -0.11650021]\n",
      " [-0.18509428  0.17572767 -0.43221969 -0.30308732  0.09847431 -0.33267808\n",
      "   0.05731834  0.049953    0.25359575]\n",
      " [ 0.23425228  0.1190564   0.087515   -0.05478609 -0.32664204  0.28621193\n",
      "   0.04471321 -0.22809032 -0.26660019]\n",
      " [ 0.04632771 -0.15142265  0.24083165 -0.00891809 -0.21825902  0.27683145\n",
      "   0.20799179  0.40843034  0.07453375]\n",
      " [-0.41001916  0.17554209 -0.47846749 -0.3573295  -0.08153289 -0.02686104\n",
      "  -0.31115987 -0.30420448  0.24054434]\n",
      " [ 0.48304284 -0.15933608  0.47440545 -0.0475583  -0.06087853 -0.13341879\n",
      "   0.1041375   0.43257488 -0.10402573]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.11243597  0.79066834 -0.1379204   0.1894874  -0.05513414 -0.18351129\n",
      "   0.30970059]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:11 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.60248727]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 11 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.33080792 -0.20766774  0.32603468  0.2286513  -0.14528394  0.1469958\n",
      "   0.64619651 -0.19711089 -0.12056321]\n",
      " [-0.18212598  0.17572767 -0.43221969 -0.30308732  0.10144261 -0.32970978\n",
      "   0.05731834  0.049953    0.25656405]\n",
      " [ 0.23556321  0.1190564   0.087515   -0.05478609 -0.3253311   0.28752287\n",
      "   0.04471321 -0.22809032 -0.26528925]\n",
      " [ 0.04311723 -0.15142265  0.24083165 -0.00891809 -0.2214695   0.27362097\n",
      "   0.20799179  0.40843034  0.07132327]\n",
      " [-0.40510284  0.17554209 -0.47846749 -0.3573295  -0.07661657 -0.02194472\n",
      "  -0.31115987 -0.30420448  0.24546066]\n",
      " [ 0.47973937 -0.15933608  0.47440545 -0.0475583  -0.06418201 -0.13672226\n",
      "   0.1041375   0.43257488 -0.1073292 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.18458272  0.7504966  -0.17101187  0.15472612 -0.09443525 -0.21460484\n",
      "   0.27030492]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:11 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.54237628]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 11 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.33073575 -0.20773991  0.32603468  0.2286513  -0.14535611  0.14692363\n",
      "   0.64619651 -0.19711089 -0.12063538]\n",
      " [-0.18249444  0.17535921 -0.43221969 -0.30308732  0.10107416 -0.33007823\n",
      "   0.05731834  0.049953    0.25619559]\n",
      " [ 0.2346968   0.11818999  0.087515   -0.05478609 -0.32619751  0.28665645\n",
      "   0.04471321 -0.22809032 -0.26615567]\n",
      " [ 0.04286198 -0.1516779   0.24083165 -0.00891809 -0.22172475  0.27336572\n",
      "   0.20799179  0.40843034  0.07106802]\n",
      " [-0.40371423  0.17693071 -0.47846749 -0.3573295  -0.07522796 -0.02055611\n",
      "  -0.31115987 -0.30420448  0.24684927]\n",
      " [ 0.47953459 -0.15954086  0.47440545 -0.0475583  -0.06438679 -0.13692704\n",
      "   0.1041375   0.43257488 -0.10753398]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.25189277  0.71676941 -0.20503538  0.1202043  -0.12834554 -0.24686967\n",
      "   0.23644511]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:11 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.54792245]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 11 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.31942436 -0.2190513   0.32603468  0.2286513  -0.1566675   0.13561225\n",
      "   0.63488512 -0.19711089 -0.12063538]\n",
      " [-0.17948481  0.17836884 -0.43221969 -0.30308732  0.10408378 -0.32706861\n",
      "   0.06032796  0.049953    0.25619559]\n",
      " [ 0.22881284  0.11230603  0.087515   -0.05478609 -0.33208147  0.2807725\n",
      "   0.03882925 -0.22809032 -0.26615567]\n",
      " [ 0.04031782 -0.15422206  0.24083165 -0.00891809 -0.2242689   0.27082157\n",
      "   0.20544764  0.40843034  0.07106802]\n",
      " [-0.39397398  0.18667095 -0.47846749 -0.3573295  -0.06548771 -0.01081586\n",
      "  -0.30141962 -0.30420448  0.24684927]\n",
      " [ 0.47580096 -0.16327448  0.47440545 -0.0475583  -0.06812041 -0.14066067\n",
      "   0.10040387  0.43257488 -0.10753398]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.31975391  0.67037375 -0.23594026  0.08026323 -0.16482992 -0.27039481\n",
      "   0.19874994]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:11 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.52680637]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 11 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.33194952 -0.2190513   0.32603468  0.24117645 -0.1566675   0.13561225\n",
      "   0.64741028 -0.19711089 -0.12063538]\n",
      " [-0.18544126  0.17836884 -0.43221969 -0.30904377  0.10408378 -0.32706861\n",
      "   0.05437152  0.049953    0.25619559]\n",
      " [ 0.23191608  0.11230603  0.087515   -0.05168285 -0.33208147  0.2807725\n",
      "   0.04193249 -0.22809032 -0.26615567]\n",
      " [ 0.04376157 -0.15422206  0.24083165 -0.00547434 -0.2242689   0.27082157\n",
      "   0.20889138  0.40843034  0.07106802]\n",
      " [-0.40588108  0.18667095 -0.47846749 -0.3692366  -0.06548771 -0.01081586\n",
      "  -0.31332673 -0.30420448  0.24684927]\n",
      " [ 0.48307555 -0.16327448  0.47440545 -0.04028371 -0.06812041 -0.14066067\n",
      "   0.10767846  0.43257488 -0.10753398]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.26077472  0.71552113 -0.21258569  0.11287955 -0.13186429 -0.25513674\n",
      "   0.23585772]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:11 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.55392195]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 11 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.34253163 -0.2190513   0.32603468  0.24117645 -0.14608539  0.14619436\n",
      "   0.65799239 -0.19711089 -0.12063538]\n",
      " [-0.19016962  0.17836884 -0.43221969 -0.30904377  0.09935542 -0.33179697\n",
      "   0.04964316  0.049953    0.25619559]\n",
      " [ 0.23494454  0.11230603  0.087515   -0.05168285 -0.32905301  0.28380096\n",
      "   0.04496095 -0.22809032 -0.26615567]\n",
      " [ 0.04779306 -0.15422206  0.24083165 -0.00547434 -0.22023741  0.27485306\n",
      "   0.21292288  0.40843034  0.07106802]\n",
      " [-0.4152752   0.18667095 -0.47846749 -0.3692366  -0.07488183 -0.02020998\n",
      "  -0.32272084 -0.30420448  0.24684927]\n",
      " [ 0.48815091 -0.16327448  0.47440545 -0.04028371 -0.06304505 -0.13558531\n",
      "   0.11275382  0.43257488 -0.10753398]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.20566346  0.75535456 -0.18985783  0.1434887  -0.10021675 -0.23799789\n",
      "   0.26861304]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:11 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57342575]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 11 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.33278237 -0.2190513   0.32603468  0.23142719 -0.14608539  0.13644509\n",
      "   0.65799239 -0.19711089 -0.13038464]\n",
      " [-0.18088004  0.17836884 -0.43221969 -0.29975418  0.09935542 -0.32250739\n",
      "   0.04964316  0.049953    0.26548518]\n",
      " [ 0.23145732  0.11230603  0.087515   -0.05517007 -0.32905301  0.28031374\n",
      "   0.04496095 -0.22809032 -0.26964288]\n",
      " [ 0.04123621 -0.15422206  0.24083165 -0.0120312  -0.22023741  0.26829621\n",
      "   0.21292288  0.40843034  0.06451117]\n",
      " [-0.40621719  0.18667095 -0.47846749 -0.36017859 -0.07488183 -0.01115197\n",
      "  -0.32272084 -0.30420448  0.25590728]\n",
      " [ 0.4845984  -0.16327448  0.47440545 -0.04383622 -0.06304505 -0.13913781\n",
      "   0.11275382  0.43257488 -0.11108649]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.27579592  0.70992461 -0.21511439  0.10491175 -0.14200579 -0.26352889\n",
      "   0.22996943]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:11 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.55553333]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 11 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.34426589 -0.20756777  0.32603468  0.24291072 -0.14608539  0.14792862\n",
      "   0.66947592 -0.19711089 -0.13038464]\n",
      " [-0.18815173  0.17109715 -0.43221969 -0.30702587  0.09935542 -0.32977908\n",
      "   0.04237147  0.049953    0.26548518]\n",
      " [ 0.23913245  0.11998117  0.087515   -0.04749494 -0.32905301  0.28798887\n",
      "   0.05263608 -0.22809032 -0.26964288]\n",
      " [ 0.0459709  -0.14948737  0.24083165 -0.00729651 -0.22023741  0.27303089\n",
      "   0.21765757  0.40843034  0.06451117]\n",
      " [-0.41645997  0.17642817 -0.47846749 -0.37042138 -0.07488183 -0.02139475\n",
      "  -0.33296363 -0.30420448  0.25590728]\n",
      " [ 0.48798937 -0.15988352  0.47440545 -0.04044525 -0.06304505 -0.13574685\n",
      "   0.11614479  0.43257488 -0.11108649]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.22092294  0.75149667 -0.19535716  0.14051457 -0.10973386 -0.24782073\n",
      "   0.26083263]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:11 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.5899349]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 11 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.35426232 -0.19757134  0.32603468  0.25290715 -0.14608539  0.14792862\n",
      "   0.67947235 -0.19711089 -0.13038464]\n",
      " [-0.1915765   0.16767237 -0.43221969 -0.31045065  0.09935542 -0.32977908\n",
      "   0.03894669  0.049953    0.26548518]\n",
      " [ 0.24350262  0.12435133  0.087515   -0.04312477 -0.32905301  0.28798887\n",
      "   0.05700625 -0.22809032 -0.26964288]\n",
      " [ 0.04729199 -0.14816627  0.24083165 -0.00597542 -0.22023741  0.27303089\n",
      "   0.21897866  0.40843034  0.06451117]\n",
      " [-0.42589783  0.16699031 -0.47846749 -0.37985923 -0.07488183 -0.02139475\n",
      "  -0.34240148 -0.30420448  0.25590728]\n",
      " [ 0.49279784 -0.15507505  0.47440545 -0.03563678 -0.06304505 -0.13574685\n",
      "   0.12095326  0.43257488 -0.11108649]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.17132316  0.78823022 -0.17402753  0.16978191 -0.08361036 -0.23392234\n",
      "   0.29057274]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:11 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.65424623]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 11 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.36301307 -0.19757134  0.33478542  0.25290715 -0.14608539  0.15667937\n",
      "   0.6882231  -0.19711089 -0.13038464]\n",
      " [-0.19888121  0.16767237 -0.4395244  -0.31045065  0.09935542 -0.33708379\n",
      "   0.03164198  0.049953    0.26548518]\n",
      " [ 0.24941061  0.12435133  0.09342298 -0.04312477 -0.32905301  0.29389686\n",
      "   0.06291424 -0.22809032 -0.26964288]\n",
      " [ 0.05386684 -0.14816627  0.2474065  -0.00597542 -0.22023741  0.27960574\n",
      "   0.22555351  0.40843034  0.06451117]\n",
      " [-0.43439608  0.16699031 -0.48696575 -0.37985923 -0.07488183 -0.029893\n",
      "  -0.35089974 -0.30420448  0.25590728]\n",
      " [ 0.50028014 -0.15507505  0.48188775 -0.03563678 -0.06304505 -0.12826455\n",
      "   0.12843556  0.43257488 -0.11108649]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.13221701  0.8202472  -0.16284107  0.19570336 -0.05679493 -0.22533572\n",
      "   0.31879172]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:11 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.60239182]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 12 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.35792574 -0.20265867  0.33478542  0.24781982 -0.14608539  0.15667937\n",
      "   0.6882231  -0.19711089 -0.13547197]\n",
      " [-0.19750938  0.16904421 -0.4395244  -0.30907881  0.09935542 -0.33708379\n",
      "   0.03164198  0.049953    0.26685701]\n",
      " [ 0.24831158  0.12325231  0.09342298 -0.0442238  -0.32905301  0.29389686\n",
      "   0.06291424 -0.22809032 -0.27074191]\n",
      " [ 0.05451164 -0.14752147  0.2474065  -0.00533062 -0.22023741  0.27960574\n",
      "   0.22555351  0.40843034  0.06515597]\n",
      " [-0.42760134  0.17378506 -0.48696575 -0.37306449 -0.07488183 -0.029893\n",
      "  -0.35089974 -0.30420448  0.26270203]\n",
      " [ 0.49673548 -0.1586197   0.48188775 -0.03918143 -0.06304505 -0.12826455\n",
      "   0.12843556  0.43257488 -0.11463114]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.20435823  0.77901867 -0.19753851  0.15853304 -0.09222059 -0.2544368\n",
      "   0.27915314]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:12 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.63166063]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 12 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.36742387 -0.20265867  0.34428355  0.24781982 -0.14608539  0.15667937\n",
      "   0.69772122 -0.19711089 -0.13547197]\n",
      " [-0.20343491  0.16904421 -0.44544994 -0.30907881  0.09935542 -0.33708379\n",
      "   0.02571645  0.049953    0.26685701]\n",
      " [ 0.25247366  0.12325231  0.09758506 -0.0442238  -0.32905301  0.29389686\n",
      "   0.06707632 -0.22809032 -0.27074191]\n",
      " [ 0.05978667 -0.14752147  0.25268153 -0.00533062 -0.22023741  0.27960574\n",
      "   0.23082854  0.40843034  0.06515597]\n",
      " [-0.43690742  0.17378506 -0.49627182 -0.37306449 -0.07488183 -0.029893\n",
      "  -0.36020581 -0.30420448  0.26270203]\n",
      " [ 0.50559239 -0.1586197   0.49074466 -0.03918143 -0.06304505 -0.12826455\n",
      "   0.13729247  0.43257488 -0.11463114]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.1615083   0.8132618  -0.18240773  0.1842346  -0.06527257 -0.24500835\n",
      "   0.3113583 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:12 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.65403739]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 12 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.3515965  -0.21848603  0.32845618  0.24781982 -0.14608539  0.15667937\n",
      "   0.68189385 -0.19711089 -0.13547197]\n",
      " [-0.1954528   0.17702632 -0.43746783 -0.30907881  0.09935542 -0.33708379\n",
      "   0.03369856  0.049953    0.26685701]\n",
      " [ 0.24317282  0.11395146  0.08828422 -0.0442238  -0.32905301  0.29389686\n",
      "   0.05777547 -0.22809032 -0.27074191]\n",
      " [ 0.05274467 -0.15456347  0.24563953 -0.00533062 -0.22023741  0.27960574\n",
      "   0.22378654  0.40843034  0.06515597]\n",
      " [-0.42153736  0.18915511 -0.48090177 -0.37306449 -0.07488183 -0.029893\n",
      "  -0.34483576 -0.30420448  0.26270203]\n",
      " [ 0.4912446  -0.17296749  0.47639687 -0.03918143 -0.06304505 -0.12826455\n",
      "   0.12294468  0.43257488 -0.11463114]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.23550363  0.7563056  -0.21114608  0.13747676 -0.10949752 -0.26321752\n",
      "   0.25762904]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:12 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.57483342]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 12 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.35751736 -0.21848603  0.33437703  0.24781982 -0.14608539  0.15667937\n",
      "   0.68189385 -0.19119003 -0.13547197]\n",
      " [-0.20241629  0.17702632 -0.44443131 -0.30907881  0.09935542 -0.33708379\n",
      "   0.03369856  0.04298951  0.26685701]\n",
      " [ 0.24451185  0.11395146  0.08962325 -0.0442238  -0.32905301  0.29389686\n",
      "   0.05777547 -0.22675129 -0.27074191]\n",
      " [ 0.06086786 -0.15456347  0.25376272 -0.00533062 -0.22023741  0.27960574\n",
      "   0.22378654  0.41655353  0.06515597]\n",
      " [-0.43265004  0.18915511 -0.49201445 -0.37306449 -0.07488183 -0.029893\n",
      "  -0.34483576 -0.31531716  0.26270203]\n",
      " [ 0.50278721 -0.17296749  0.48793948 -0.03918143 -0.06304505 -0.12826455\n",
      "   0.12294468  0.4441175  -0.11463114]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.18354828  0.78843699 -0.19253308  0.16479586 -0.07470318 -0.25125244\n",
      "   0.29930856]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:12 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.58422332]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 12 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.36313339 -0.21848603  0.33437703  0.25343585 -0.14608539  0.15667937\n",
      "   0.68189385 -0.19119003 -0.12985594]\n",
      " [-0.20545893  0.17702632 -0.44443131 -0.31212146  0.09935542 -0.33708379\n",
      "   0.03369856  0.04298951  0.26381437]\n",
      " [ 0.24362352  0.11395146  0.08962325 -0.04511213 -0.32905301  0.29389686\n",
      "   0.05777547 -0.22675129 -0.27163024]\n",
      " [ 0.062386   -0.15456347  0.25376272 -0.00381248 -0.22023741  0.27960574\n",
      "   0.22378654  0.41655353  0.06667411]\n",
      " [-0.43902372  0.18915511 -0.49201445 -0.37943817 -0.07488183 -0.029893\n",
      "  -0.34483576 -0.31531716  0.25632835]\n",
      " [ 0.50706134 -0.17296749  0.48793948 -0.03490731 -0.06304505 -0.12826455\n",
      "   0.12294468  0.4441175  -0.11035702]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.13305087  0.81951066 -0.17035746  0.1891555  -0.04793265 -0.2326953\n",
      "   0.32891868]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:12 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.59767518]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 12 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.35881684 -0.21848603  0.33437703  0.25343585 -0.15040194  0.15236282\n",
      "   0.68189385 -0.19119003 -0.13417249]\n",
      " [-0.20226235  0.17702632 -0.44443131 -0.31212146  0.102552   -0.33388721\n",
      "   0.03369856  0.04298951  0.26701095]\n",
      " [ 0.24475708  0.11395146  0.08962325 -0.04511213 -0.32791945  0.29503042\n",
      "   0.05777547 -0.22675129 -0.27049668]\n",
      " [ 0.05903083 -0.15456347  0.25376272 -0.00381248 -0.22359258  0.27625057\n",
      "   0.22378654  0.41655353  0.06331894]\n",
      " [-0.43396468  0.18915511 -0.49201445 -0.37943817 -0.06982279 -0.02483396\n",
      "  -0.34483576 -0.31531716  0.26138739]\n",
      " [ 0.50341015 -0.17296749  0.48793948 -0.03490731 -0.06669624 -0.13191574\n",
      "   0.12294468  0.4441175  -0.11400821]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.20490922  0.77922202 -0.20307289  0.15436064 -0.08723689 -0.26349546\n",
      "   0.28931259]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:12 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.53464582]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 12 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.35868184 -0.21862103  0.33437703  0.25343585 -0.15053694  0.15222782\n",
      "   0.68189385 -0.19119003 -0.13430749]\n",
      " [-0.20243594  0.17685274 -0.44443131 -0.31212146  0.10237842 -0.33406079\n",
      "   0.03369856  0.04298951  0.26683737]\n",
      " [ 0.24383791  0.11303229  0.08962325 -0.04511213 -0.32883862  0.29411125\n",
      "   0.05777547 -0.22675129 -0.27141585]\n",
      " [ 0.05869093 -0.15490337  0.25376272 -0.00381248 -0.22393248  0.27591067\n",
      "   0.22378654  0.41655353  0.06297904]\n",
      " [-0.4326684   0.19045139 -0.49201445 -0.37943817 -0.06852651 -0.02353768\n",
      "  -0.34483576 -0.31531716  0.26268367]\n",
      " [ 0.50311383 -0.17326381  0.48793948 -0.03490731 -0.06699256 -0.13221205\n",
      "   0.12294468  0.4441175  -0.11430453]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.27141907  0.74583209 -0.2365014   0.12018607 -0.12083174 -0.29545279\n",
      "   0.25576133]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:12 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.54785394]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 12 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.34683519 -0.23046769  0.33437703  0.25343585 -0.16238359  0.14038116\n",
      "   0.6700472  -0.19119003 -0.13430749]\n",
      " [-0.19869043  0.18059825 -0.44443131 -0.31212146  0.10612392 -0.33031528\n",
      "   0.03744407  0.04298951  0.26683737]\n",
      " [ 0.2376202   0.10681457  0.08962325 -0.04511213 -0.33505634  0.28789354\n",
      "   0.05155776 -0.22675129 -0.27141585]\n",
      " [ 0.05566949 -0.15792481  0.25376272 -0.00381248 -0.22695392  0.27288923\n",
      "   0.2207651   0.41655353  0.06297904]\n",
      " [-0.4223806   0.20073919 -0.49201445 -0.37943817 -0.05823871 -0.01324988\n",
      "  -0.33454796 -0.31531716  0.26268367]\n",
      " [ 0.49888045 -0.17749719  0.48793948 -0.03490731 -0.07122594 -0.13644543\n",
      "   0.1187113   0.4441175  -0.11430453]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.33927352  0.6986726  -0.26665183  0.07989047 -0.15779666 -0.31828299\n",
      "   0.2175552 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:12 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.53030624]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 12 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 3.59553313e-01 -2.30467686e-01  3.34377035e-01  2.66153981e-01\n",
      "  -1.62383592e-01  1.40381165e-01  6.82765330e-01 -1.91190030e-01\n",
      "  -1.34307487e-01]\n",
      " [-2.05239203e-01  1.80598245e-01 -4.44431314e-01 -3.18670234e-01\n",
      "   1.06123923e-01 -3.30315282e-01  3.08952916e-02  4.29895093e-02\n",
      "   2.66837365e-01]\n",
      " [ 2.41136785e-01  1.06814574e-01  8.96232508e-02 -4.15955377e-02\n",
      "  -3.35056336e-01  2.87893537e-01  5.50743481e-02 -2.26751289e-01\n",
      "  -2.71415854e-01]\n",
      " [ 5.95831475e-02 -1.57924812e-01  2.53762721e-01  1.01179865e-04\n",
      "  -2.26953918e-01  2.72889234e-01  2.24678757e-01  4.16553532e-01\n",
      "   6.29790406e-02]\n",
      " [-4.34607958e-01  2.00739187e-01 -4.92014450e-01 -3.91665526e-01\n",
      "  -5.82387086e-02 -1.32498838e-02 -3.46775318e-01 -3.15317162e-01\n",
      "   2.62683667e-01]\n",
      " [ 5.06717396e-01 -1.77497190e-01  4.87939483e-01 -2.70703649e-02\n",
      "  -7.12259365e-02 -1.36445433e-01  1.26548241e-01  4.44117499e-01\n",
      "  -1.14304527e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.2807775   0.74434613 -0.24419993  0.11269009 -0.12458633 -0.30406916\n",
      "   0.25509121]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:12 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.55540044]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 12 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 3.70461346e-01 -2.30467686e-01  3.34377035e-01  2.66153981e-01\n",
      "  -1.51475559e-01  1.51289198e-01  6.93673363e-01 -1.91190030e-01\n",
      "  -1.34307487e-01]\n",
      " [-2.10496823e-01  1.80598245e-01 -4.44431314e-01 -3.18670234e-01\n",
      "   1.00866303e-01 -3.35572902e-01  2.56376716e-02  4.29895093e-02\n",
      "   2.66837365e-01]\n",
      " [ 2.44502063e-01  1.06814574e-01  8.96232508e-02 -4.15955377e-02\n",
      "  -3.31691058e-01  2.91258815e-01  5.84396263e-02 -2.26751289e-01\n",
      "  -2.71415854e-01]\n",
      " [ 6.39931952e-02 -1.57924812e-01  2.53762721e-01  1.01179865e-04\n",
      "  -2.22543870e-01  2.77299281e-01  2.29088805e-01  4.16553532e-01\n",
      "   6.29790406e-02]\n",
      " [-4.44417443e-01  2.00739187e-01 -4.92014450e-01 -3.91665526e-01\n",
      "  -6.80481940e-02 -2.30593692e-02 -3.56584803e-01 -3.15317162e-01\n",
      "   2.62683667e-01]\n",
      " [ 5.12301208e-01 -1.77497190e-01  4.87939483e-01 -2.70703649e-02\n",
      "  -6.56421255e-02 -1.30861622e-01  1.32132052e-01  4.44117499e-01\n",
      "  -1.14304527e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.22588484  0.78469411 -0.2221515   0.1435366  -0.09264938 -0.28766554\n",
      "   0.28829146]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:12 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57318096]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 12 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.36014636 -0.23046769  0.33437703  0.25583899 -0.15147556  0.14097421\n",
      "   0.69367336 -0.19119003 -0.14462247]\n",
      " [-0.20090019  0.18059825 -0.44443131 -0.3090736   0.1008663  -0.32597627\n",
      "   0.02563767  0.04298951  0.276434  ]\n",
      " [ 0.2406457   0.10681457  0.08962325 -0.0454519  -0.33169106  0.28740245\n",
      "   0.05843963 -0.22675129 -0.27527222]\n",
      " [ 0.0571873  -0.15792481  0.25376272 -0.00670471 -0.22254387  0.27049339\n",
      "   0.2290888   0.41655353  0.05617315]\n",
      " [-0.43483998  0.20073919 -0.49201445 -0.38208806 -0.06804819 -0.0134819\n",
      "  -0.3565848  -0.31531716  0.27226113]\n",
      " [ 0.50815335 -0.17749719  0.48793948 -0.03121822 -0.06564213 -0.13500947\n",
      "   0.13213205  0.4441175  -0.11845238]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.29599764  0.73857247 -0.24702918  0.10459186 -0.13469867 -0.31256641\n",
      "   0.24904725]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:12 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.55838528]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 12 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.37183698 -0.21877707  0.33437703  0.26752961 -0.15147556  0.15266483\n",
      "   0.70536398 -0.19119003 -0.14462247]\n",
      " [-0.20867543  0.172823   -0.44443131 -0.31684884  0.1008663  -0.33375151\n",
      "   0.01786243  0.04298951  0.276434  ]\n",
      " [ 0.24860018  0.11476905  0.08962325 -0.03749742 -0.33169106  0.29535693\n",
      "   0.0663941  -0.22675129 -0.27527222]\n",
      " [ 0.06232516 -0.15278695  0.25376272 -0.00156685 -0.22254387  0.27563125\n",
      "   0.23422667  0.41655353  0.05617315]\n",
      " [-0.44546507  0.1901141  -0.49201445 -0.39271315 -0.06804819 -0.024107\n",
      "  -0.36720989 -0.31531716  0.27226113]\n",
      " [ 0.51210275 -0.1735478   0.48793948 -0.02726882 -0.06564213 -0.13106008\n",
      "   0.13608144  0.4441175  -0.11845238]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.2415485   0.78061225 -0.228104    0.14033914 -0.10220355 -0.29777517\n",
      "   0.28027936]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:12 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.59231931]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 12 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 3.82086183e-01 -2.08527862e-01  3.34377035e-01  2.77778818e-01\n",
      "  -1.51475559e-01  1.52664828e-01  7.15613187e-01 -1.91190030e-01\n",
      "  -1.44622474e-01]\n",
      " [-2.12682491e-01  1.68815948e-01 -4.44431314e-01 -3.20855901e-01\n",
      "   1.00866303e-01 -3.33751512e-01  1.38553742e-02  4.29895093e-02\n",
      "   2.76433995e-01]\n",
      " [ 2.53246253e-01  1.19415128e-01  8.96232508e-02 -3.28513480e-02\n",
      "  -3.31691058e-01  2.95356929e-01  7.10401797e-02 -2.26751289e-01\n",
      "  -2.75272217e-01]\n",
      " [ 6.40661921e-02 -1.51045922e-01  2.53762721e-01  1.74176756e-04\n",
      "  -2.22543870e-01  2.75631249e-01  2.35967695e-01  4.16553532e-01\n",
      "   5.61731475e-02]\n",
      " [-4.55221120e-01  1.80358045e-01 -4.92014450e-01 -4.02469203e-01\n",
      "  -6.80481940e-02 -2.41069959e-02 -3.76965945e-01 -3.15317162e-01\n",
      "   2.72261131e-01]\n",
      " [ 5.17341403e-01 -1.68309141e-01  4.87939483e-01 -2.20301697e-02\n",
      "  -6.56421255e-02 -1.31060082e-01  1.41320100e-01  4.44117499e-01\n",
      "  -1.18452380e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.19232571  0.81777995 -0.20757497  0.16971667 -0.07584525 -0.28468442\n",
      "   0.3103059 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:12 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.6591739]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 12 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 3.90652122e-01 -2.08527862e-01  3.42942974e-01  2.77778818e-01\n",
      "  -1.51475559e-01  1.61230767e-01  7.24179126e-01 -1.91190030e-01\n",
      "  -1.44622474e-01]\n",
      " [-2.20114620e-01  1.68815948e-01 -4.51863443e-01 -3.20855901e-01\n",
      "   1.00866303e-01 -3.41183641e-01  6.42324507e-03  4.29895093e-02\n",
      "   2.76433995e-01]\n",
      " [ 2.59247951e-01  1.19415128e-01  9.56249488e-02 -3.28513480e-02\n",
      "  -3.31691058e-01  3.01358627e-01  7.70418776e-02 -2.26751289e-01\n",
      "  -2.75272217e-01]\n",
      " [ 7.07821621e-02 -1.51045922e-01  2.60478691e-01  1.74176756e-04\n",
      "  -2.22543870e-01  2.82347219e-01  2.42683665e-01  4.16553532e-01\n",
      "   5.61731475e-02]\n",
      " [-4.63668892e-01  1.80358045e-01 -5.00462222e-01 -4.02469203e-01\n",
      "  -6.80481940e-02 -3.25547673e-02 -3.85413716e-01 -3.15317162e-01\n",
      "   2.72261131e-01]\n",
      " [ 5.24930733e-01 -1.68309141e-01  4.95528813e-01 -2.20301697e-02\n",
      "  -6.56421255e-02 -1.23470752e-01  1.48909430e-01  4.44117499e-01\n",
      "  -1.18452380e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.15404009  0.84955223 -0.19710441  0.19537719 -0.0491895  -0.27679195\n",
      "   0.3384115 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:12 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.59805807]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 13 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 3.85124870e-01 -2.14055114e-01  3.42942974e-01  2.72251565e-01\n",
      "  -1.51475559e-01  1.61230767e-01  7.24179126e-01 -1.91190030e-01\n",
      "  -1.50149726e-01]\n",
      " [-2.18398408e-01  1.70532160e-01 -4.51863443e-01 -3.19139689e-01\n",
      "   1.00866303e-01 -3.41183641e-01  6.42324507e-03  4.29895093e-02\n",
      "   2.78150207e-01]\n",
      " [ 2.57981897e-01  1.18149073e-01  9.56249488e-02 -3.41174024e-02\n",
      "  -3.31691058e-01  3.01358627e-01  7.70418776e-02 -2.26751289e-01\n",
      "  -2.76538272e-01]\n",
      " [ 7.12118908e-02 -1.50616193e-01  2.60478691e-01  6.03905480e-04\n",
      "  -2.22543870e-01  2.82347219e-01  2.42683665e-01  4.16553532e-01\n",
      "   5.66028762e-02]\n",
      " [-4.56546590e-01  1.87480347e-01 -5.00462222e-01 -3.95346902e-01\n",
      "  -6.80481940e-02 -3.25547673e-02 -3.85413716e-01 -3.15317162e-01\n",
      "   2.79383433e-01]\n",
      " [ 5.21091619e-01 -1.72148255e-01  4.95528813e-01 -2.58692833e-02\n",
      "  -6.56421255e-02 -1.23470752e-01  1.48909430e-01  4.44117499e-01\n",
      "  -1.22291493e-01]]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta two: \n",
      "[[-0.22592207  0.80799196 -0.23132656  0.1581691  -0.08470072 -0.30540592\n",
      "   0.29860143]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:13 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.63558531]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 13 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 3.94544028e-01 -2.14055114e-01  3.52362132e-01  2.72251565e-01\n",
      "  -1.51475559e-01  1.61230767e-01  7.33598284e-01 -1.91190030e-01\n",
      "  -1.50149726e-01]\n",
      " [-2.24683986e-01  1.70532160e-01 -4.58149022e-01 -3.19139689e-01\n",
      "   1.00866303e-01 -3.41183641e-01  1.37666325e-04  4.29895093e-02\n",
      "   2.78150207e-01]\n",
      " [ 2.62321166e-01  1.18149073e-01  9.99642180e-02 -3.41174024e-02\n",
      "  -3.31691058e-01  3.01358627e-01  8.13811468e-02 -2.26751289e-01\n",
      "  -2.76538272e-01]\n",
      " [ 7.67983073e-02 -1.50616193e-01  2.66065108e-01  6.03905480e-04\n",
      "  -2.22543870e-01  2.82347219e-01  2.48270081e-01  4.16553532e-01\n",
      "   5.66028762e-02]\n",
      " [-4.65849994e-01  1.87480347e-01 -5.09765626e-01 -3.95346902e-01\n",
      "  -6.80481940e-02 -3.25547673e-02 -3.94717120e-01 -3.15317162e-01\n",
      "   2.79383433e-01]\n",
      " [ 5.30003487e-01 -1.72148255e-01  5.04440681e-01 -2.58692833e-02\n",
      "  -6.56421255e-02 -1.23470752e-01  1.57821298e-01  4.44117499e-01\n",
      "  -1.22291493e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.18371982  0.8421903  -0.21698295  0.18374487 -0.05770091 -0.29666533\n",
      "   0.33077373]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:13 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.65643703]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 13 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 3.78464358e-01 -2.30134784e-01  3.36282462e-01  2.72251565e-01\n",
      "  -1.51475559e-01  1.61230767e-01  7.17518614e-01 -1.91190030e-01\n",
      "  -1.50149726e-01]\n",
      " [-2.15801455e-01  1.79414691e-01 -4.49266491e-01 -3.19139689e-01\n",
      "   1.00866303e-01 -3.41183641e-01  9.02019731e-03  4.29895093e-02\n",
      "   2.78150207e-01]\n",
      " [ 2.52703539e-01  1.08531447e-01  9.03465913e-02 -3.41174024e-02\n",
      "  -3.31691058e-01  3.01358627e-01  7.17635202e-02 -2.26751289e-01\n",
      "  -2.76538272e-01]\n",
      " [ 6.90293210e-02 -1.58385179e-01  2.58296121e-01  6.03905480e-04\n",
      "  -2.22543870e-01  2.82347219e-01  2.40501095e-01  4.16553532e-01\n",
      "   5.66028762e-02]\n",
      " [-4.50130734e-01  2.03199607e-01 -4.94046365e-01 -3.95346902e-01\n",
      "  -6.80481940e-02 -3.25547673e-02 -3.78997860e-01 -3.15317162e-01\n",
      "   2.79383433e-01]\n",
      " [ 5.15295590e-01 -1.86856152e-01  4.89732785e-01 -2.58692833e-02\n",
      "  -6.56421255e-02 -1.23470752e-01  1.43113401e-01  4.44117499e-01\n",
      "  -1.22291493e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.25774211  0.78444297 -0.24471811  0.13660211 -0.10273476 -0.31402649\n",
      "   0.27636769]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:13 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.57601159]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 13 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 3.84797008e-01 -2.30134784e-01  3.42615112e-01  2.72251565e-01\n",
      "  -1.51475559e-01  1.61230767e-01  7.17518614e-01 -1.84857379e-01\n",
      "  -1.50149726e-01]\n",
      " [-2.23121924e-01  1.79414691e-01 -4.56586959e-01 -3.19139689e-01\n",
      "   1.00866303e-01 -3.41183641e-01  9.02019731e-03  3.56690412e-02\n",
      "   2.78150207e-01]\n",
      " [ 2.54203766e-01  1.08531447e-01  9.18468181e-02 -3.41174024e-02\n",
      "  -3.31691058e-01  3.01358627e-01  7.17635202e-02 -2.25251063e-01\n",
      "  -2.76538272e-01]\n",
      " [ 7.74395498e-02 -1.58385179e-01  2.66706350e-01  6.03905480e-04\n",
      "  -2.22543870e-01  2.82347219e-01  2.40501095e-01  4.24963761e-01\n",
      "   5.66028762e-02]\n",
      " [-4.61359212e-01  2.03199607e-01 -5.05274844e-01 -3.95346902e-01\n",
      "  -6.80481940e-02 -3.25547673e-02 -3.78997860e-01 -3.26545640e-01\n",
      "   2.79383433e-01]\n",
      " [ 5.26848532e-01 -1.86856152e-01  5.01285727e-01 -2.58692833e-02\n",
      "  -6.56421255e-02 -1.23470752e-01  1.43113401e-01  4.55670441e-01\n",
      "  -1.22291493e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.20596841  0.81695577 -0.22663309  0.16399257 -0.06764029 -0.30258135\n",
      "   0.31829749]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:13 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.58155535]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 13 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.39085173 -0.23013478  0.34261511  0.27830629 -0.15147556  0.16123077\n",
      "   0.71751861 -0.18485738 -0.144095  ]\n",
      " [-0.22642572  0.17941469 -0.45658696 -0.32244348  0.1008663  -0.34118364\n",
      "   0.0090202   0.03566904  0.27484642]\n",
      " [ 0.25348579  0.10853145  0.09184682 -0.03483538 -0.33169106  0.30135863\n",
      "   0.07176352 -0.22525106 -0.27725625]\n",
      " [ 0.07914565 -0.15838518  0.26670635  0.00231001 -0.22254387  0.28234722\n",
      "   0.2405011   0.42496376  0.05830898]\n",
      " [-0.46812782  0.20319961 -0.50527484 -0.40211551 -0.06804819 -0.03255477\n",
      "  -0.37899786 -0.32654564  0.27261482]\n",
      " [ 0.5314999  -0.18685615  0.50128573 -0.02121792 -0.06564213 -0.12347075\n",
      "   0.1431134   0.45567044 -0.11764013]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.15505443  0.84873013 -0.20451844  0.1887312  -0.04047204 -0.28427528\n",
      "   0.34851782]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:13 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.5921995]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 13 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.38634069 -0.23013478  0.34261511  0.27830629 -0.1559866   0.15671972\n",
      "   0.71751861 -0.18485738 -0.14860604]\n",
      " [-0.22302657  0.17941469 -0.45658696 -0.32244348  0.10426544 -0.3377845\n",
      "   0.0090202   0.03566904  0.27824556]\n",
      " [ 0.25445228  0.10853145  0.09184682 -0.03483538 -0.33072457  0.30232512\n",
      "   0.07176352 -0.22525106 -0.27628976]\n",
      " [ 0.07565336 -0.15838518  0.26670635  0.00231001 -0.22603616  0.27885493\n",
      "   0.2405011   0.42496376  0.05481669]\n",
      " [-0.46294854  0.20319961 -0.50527484 -0.40211551 -0.06286891 -0.02737548\n",
      "  -0.37899786 -0.32654564  0.27779411]\n",
      " [ 0.52753242 -0.18685615  0.50128573 -0.02121792 -0.06960961 -0.12743823\n",
      "   0.1431134   0.45567044 -0.12160761]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.22656229  0.80841552 -0.23685233  0.15394423 -0.07974096 -0.3147739\n",
      "   0.30876292]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:13 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.52599128]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 13 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.38620409 -0.23027138  0.34261511  0.27830629 -0.1561232   0.15658313\n",
      "   0.71751861 -0.18485738 -0.14874264]\n",
      " [-0.22304485  0.17939642 -0.45658696 -0.32244348  0.10424717 -0.33780277\n",
      "   0.0090202   0.03566904  0.27822728]\n",
      " [ 0.25349748  0.10757665  0.09184682 -0.03483538 -0.33167937  0.30137032\n",
      "   0.07176352 -0.22525106 -0.27724456]\n",
      " [ 0.07524518 -0.15879336  0.26670635  0.00231001 -0.22644434  0.27844675\n",
      "   0.2405011   0.42496376  0.05440851]\n",
      " [-0.46176653  0.20438161 -0.50527484 -0.40211551 -0.0616869  -0.02619347\n",
      "  -0.37899786 -0.32654564  0.27897611]\n",
      " [ 0.52717148 -0.18721709  0.50128573 -0.02121792 -0.06997055 -0.12779917\n",
      "   0.1431134   0.45567044 -0.12196855]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.29213354  0.7754933  -0.26965622  0.12020327 -0.1129348  -0.34637649\n",
      "   0.27561633]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:13 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.54788175]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 13 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.37388454 -0.24259093  0.34261511  0.27830629 -0.16844275  0.14426358\n",
      "   0.70519907 -0.18485738 -0.14874264]\n",
      " [-0.21857613  0.18386513 -0.45658696 -0.32244348  0.10871589 -0.33333406\n",
      "   0.01348891  0.03566904  0.27822728]\n",
      " [ 0.24693817  0.10101734  0.09184682 -0.03483538 -0.33823867  0.29481101\n",
      "   0.06520421 -0.22525106 -0.27724456]\n",
      " [ 0.07173882 -0.16229972  0.26670635  0.00231001 -0.2299507   0.27494038\n",
      "   0.23699473  0.42496376  0.05440851]\n",
      " [-0.45096011  0.21518804 -0.50527484 -0.40211551 -0.05088048 -0.01538705\n",
      "  -0.36819144 -0.32654564  0.27897611]\n",
      " [ 0.52242876 -0.19195981  0.50128573 -0.02121792 -0.07471326 -0.13254189\n",
      "   0.13837069  0.45567044 -0.12196855]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.3599907   0.72761607 -0.29906233  0.0795368  -0.15039532 -0.3685288\n",
      "   0.23688044]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:13 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.53427941]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 13 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.38668683 -0.24259093  0.34261511  0.29110857 -0.16844275  0.14426358\n",
      "   0.71800135 -0.18485738 -0.14874264]\n",
      " [-0.22570969  0.18386513 -0.45658696 -0.32957704  0.10871589 -0.33333406\n",
      "   0.00635535  0.03566904  0.27822728]\n",
      " [ 0.25087882  0.10101734  0.09184682 -0.03089473 -0.33823867  0.29481101\n",
      "   0.06914486 -0.22525106 -0.27724456]\n",
      " [ 0.07613715 -0.16229972  0.26670635  0.00670834 -0.2299507   0.27494038\n",
      "   0.24139306  0.42496376  0.05440851]\n",
      " [-0.46340431  0.21518804 -0.50527484 -0.41455972 -0.05088048 -0.01538705\n",
      "  -0.38063564 -0.32654564  0.27897611]\n",
      " [ 0.53080692 -0.19195981  0.50128573 -0.01283976 -0.07471326 -0.13254189\n",
      "   0.14674885  0.45567044 -0.12196855]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.30204926  0.77369894 -0.27756067  0.11249887 -0.116955   -0.35533474\n",
      "   0.27481232]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:13 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.5570777]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 13 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 3.97857383e-01 -2.42590930e-01  3.42615112e-01  2.91108572e-01\n",
      "  -1.57272190e-01  1.55434136e-01  7.29171905e-01 -1.84857379e-01\n",
      "  -1.48742642e-01]\n",
      " [-2.31485452e-01  1.83865135e-01 -4.56586959e-01 -3.29577039e-01\n",
      "   1.02940125e-01 -3.39109819e-01  5.79591608e-04  3.56690412e-02\n",
      "   2.78227285e-01]\n",
      " [ 2.54586026e-01  1.01017342e-01  9.18468181e-02 -3.08947308e-02\n",
      "  -3.34531469e-01  2.98518216e-01  7.28520677e-02 -2.25251063e-01\n",
      "  -2.77244559e-01]\n",
      " [ 8.09303045e-02 -1.62299722e-01  2.66706350e-01  6.70834144e-03\n",
      "  -2.25157551e-01  2.79733537e-01  2.46186218e-01  4.24963761e-01\n",
      "   5.44085078e-02]\n",
      " [-4.73581867e-01  2.15188037e-01 -5.05274844e-01 -4.14559722e-01\n",
      "  -6.10580293e-02 -2.55646026e-02 -3.90813197e-01 -3.26545640e-01\n",
      "   2.78976114e-01]\n",
      " [ 5.36889112e-01 -1.91959806e-01  5.01285727e-01 -1.28397575e-02\n",
      "  -6.86310715e-02 -1.26459698e-01  1.52831034e-01  4.55670441e-01\n",
      "  -1.21968547e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.24740546  0.81449547 -0.25620615  0.14357543 -0.08473427 -0.33965542\n",
      "   0.30844311]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:13 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57287219]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 13 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 3.87031527e-01 -2.42590930e-01  3.42615112e-01  2.80282716e-01\n",
      "  -1.57272190e-01  1.44608280e-01  7.29171905e-01 -1.84857379e-01\n",
      "  -1.59568498e-01]\n",
      " [-2.21577194e-01  1.83865135e-01 -4.56586959e-01 -3.19668782e-01\n",
      "   1.02940125e-01 -3.29201561e-01  5.79591608e-04  3.56690412e-02\n",
      "   2.88135543e-01]\n",
      " [ 2.50357508e-01  1.01017342e-01  9.18468181e-02 -3.51232495e-02\n",
      "  -3.34531469e-01  2.94289697e-01  7.28520677e-02 -2.25251063e-01\n",
      "  -2.81473078e-01]\n",
      " [ 7.38590512e-02 -1.62299722e-01  2.66706350e-01 -3.62911916e-04\n",
      "  -2.25157551e-01  2.72662284e-01  2.46186218e-01  4.24963761e-01\n",
      "   4.73372545e-02]\n",
      " [-4.63509184e-01  2.15188037e-01 -5.05274844e-01 -4.04487038e-01\n",
      "  -6.10580293e-02 -1.54919191e-02 -3.90813197e-01 -3.26545640e-01\n",
      "   2.89048798e-01]\n",
      " [ 5.32150253e-01 -1.91959806e-01  5.01285727e-01 -1.75786164e-02\n",
      "  -6.86310715e-02 -1.31198557e-01  1.52831034e-01  4.55670441e-01\n",
      "  -1.26707406e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.31749341  0.76773109 -0.28069062  0.10426052 -0.12706103 -0.3639366\n",
      "   0.26860005]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:13 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.56153502]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 13 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.39883775 -0.2307847   0.34261511  0.29208894 -0.15727219  0.15641451\n",
      "   0.74097813 -0.18485738 -0.1595685 ]\n",
      " [-0.2298254   0.17561693 -0.45658696 -0.32791699  0.10294012 -0.33744976\n",
      "  -0.00766861  0.03566904  0.28813554]\n",
      " [ 0.25858141  0.10924124  0.09184682 -0.02689935 -0.33453147  0.3025136\n",
      "   0.08107597 -0.22525106 -0.28147308]\n",
      " [ 0.07940204 -0.15675673  0.26670635  0.00518008 -0.22515755  0.27820527\n",
      "   0.25172921  0.42496376  0.04733725]\n",
      " [-0.47443892  0.2042583  -0.50527484 -0.41541677 -0.06105803 -0.02642165\n",
      "  -0.40174293 -0.32654564  0.2890488 ]\n",
      " [ 0.53666071 -0.18744935  0.50128573 -0.01306816 -0.06863107 -0.1266881\n",
      "   0.15734149  0.45567044 -0.12670741]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.26351542  0.81013528 -0.26261216  0.14012866 -0.09435661 -0.35003841\n",
      "   0.30018911]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:13 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.59498082]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 13 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.40925881 -0.22036365  0.34261511  0.30250999 -0.15727219  0.15641451\n",
      "   0.75139918 -0.18485738 -0.1595685 ]\n",
      " [-0.23440477  0.17103756 -0.45658696 -0.33249636  0.10294012 -0.33744976\n",
      "  -0.01224799  0.03566904  0.28813554]\n",
      " [ 0.26350727  0.11416711  0.09184682 -0.02197349 -0.33453147  0.3025136\n",
      "   0.08600183 -0.22525106 -0.28147308]\n",
      " [ 0.08157506 -0.15458371  0.26670635  0.0073531  -0.22515755  0.27820527\n",
      "   0.25390223  0.42496376  0.04733725]\n",
      " [-0.48444417  0.19425305 -0.50527484 -0.42542203 -0.06105803 -0.02642165\n",
      "  -0.41174819 -0.32654564  0.2890488 ]\n",
      " [ 0.54232911 -0.18178094  0.50128573 -0.00739975 -0.06863107 -0.1266881\n",
      "   0.1630099   0.45567044 -0.12670741]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.21471494  0.84764938 -0.24290814  0.16960227 -0.06777164 -0.33773486\n",
      "   0.33049065]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:13 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.66429802]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 13 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.41760013 -0.22036365  0.35095644  0.30250999 -0.15727219  0.16475583\n",
      "   0.75974051 -0.18485738 -0.1595685 ]\n",
      " [-0.24191867  0.17103756 -0.46410085 -0.33249636  0.10294012 -0.34496366\n",
      "  -0.01976188  0.03566904  0.28813554]\n",
      " [ 0.26958772  0.11416711  0.09792726 -0.02197349 -0.33453147  0.30859404\n",
      "   0.09208228 -0.22525106 -0.28147308]\n",
      " [ 0.08840296 -0.15458371  0.27353424  0.0073531  -0.22515755  0.28503316\n",
      "   0.26073012  0.42496376  0.04733725]\n",
      " [-0.49278286  0.19425305 -0.51361353 -0.42542203 -0.06105803 -0.03476033\n",
      "  -0.42008687 -0.32654564  0.2890488 ]\n",
      " [ 0.54997922 -0.18178094  0.50893583 -0.00739975 -0.06863107 -0.119038\n",
      "   0.17066     0.45567044 -0.12670741]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.17728313  0.87909944 -0.2331378   0.1949751  -0.04131099 -0.33049451\n",
      "   0.35843281]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:13 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.59328154]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 14 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.4116855  -0.22627828  0.35095644  0.29659536 -0.15727219  0.16475583\n",
      "   0.75974051 -0.18485738 -0.16548313]\n",
      " [-0.23986327  0.17309295 -0.46410085 -0.33044097  0.10294012 -0.34496366\n",
      "  -0.01976188  0.03566904  0.29019094]\n",
      " [ 0.26815293  0.11273232  0.09792726 -0.02340827 -0.33453147  0.30859404\n",
      "   0.09208228 -0.22525106 -0.28290786]\n",
      " [ 0.08860857 -0.1543781   0.27353424  0.00755871 -0.22515755  0.28503316\n",
      "   0.26073012  0.42496376  0.04754287]\n",
      " [-0.48535707  0.20167883 -0.51361353 -0.41799625 -0.06105803 -0.03476033\n",
      "  -0.42008687 -0.32654564  0.29647458]\n",
      " [ 0.54584708 -0.18591308  0.50893583 -0.01153189 -0.06863107 -0.119038\n",
      "   0.17066     0.45567044 -0.13083954]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.24886212  0.83728058 -0.26686735  0.15774928 -0.07689487 -0.35862191\n",
      "   0.31847334]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:14 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.63973383]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 14 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.42097831 -0.22627828  0.36024924  0.29659536 -0.15727219  0.16475583\n",
      "   0.76903331 -0.18485738 -0.16548313]\n",
      " [-0.24647114  0.17309295 -0.47070872 -0.33044097  0.10294012 -0.34496366\n",
      "  -0.02636975  0.03566904  0.29019094]\n",
      " [ 0.27266714  0.11273232  0.10244147 -0.02340827 -0.33453147  0.30859404\n",
      "   0.09659648 -0.22525106 -0.28290786]\n",
      " [ 0.09448476 -0.1543781   0.27941044  0.00755871 -0.22515755  0.28503316\n",
      "   0.26660632  0.42496376  0.04754287]\n",
      " [-0.4945982   0.20167883 -0.52285465 -0.41799625 -0.06105803 -0.03476033\n",
      "  -0.42932799 -0.32654564  0.29647458]\n",
      " [ 0.55477372 -0.18591308  0.51786246 -0.01153189 -0.06863107 -0.119038\n",
      "   0.17958663  0.45567044 -0.13083954]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.20734604  0.87136067 -0.25330932  0.18318113 -0.04987324 -0.35053419\n",
      "   0.3505661 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:14 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.65894007]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 14 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.40470564 -0.24255094  0.34397658  0.29659536 -0.15727219  0.16475583\n",
      "   0.75276065 -0.18485738 -0.16548313]\n",
      " [-0.23672591  0.18283818 -0.46096349 -0.33044097  0.10294012 -0.34496366\n",
      "  -0.01662452  0.03566904  0.29019094]\n",
      " [ 0.26272222  0.1027874   0.09249655 -0.02340827 -0.33453147  0.30859404\n",
      "   0.08665156 -0.22525106 -0.28290786]\n",
      " [ 0.08599745 -0.16286541  0.27092313  0.00755871 -0.22515755  0.28503316\n",
      "   0.25811901  0.42496376  0.04754287]\n",
      " [-0.47859582  0.2176812  -0.50685227 -0.41799625 -0.06105803 -0.03476033\n",
      "  -0.41332562 -0.32654564  0.29647458]\n",
      " [ 0.5397317  -0.2009551   0.50282044 -0.01153189 -0.06863107 -0.119038\n",
      "   0.16454461  0.45567044 -0.13083954]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.2813905   0.81287713 -0.2800491   0.13563808 -0.09572103 -0.36708676\n",
      "   0.29548484]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:14 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.57726608]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 14 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.41142762 -0.24255094  0.35069855  0.29659536 -0.15727219  0.16475583\n",
      "   0.75276065 -0.17813541 -0.16548313]\n",
      " [-0.2443916   0.18283818 -0.46862918 -0.33044097  0.10294012 -0.34496366\n",
      "  -0.01662452  0.02800335  0.29019094]\n",
      " [ 0.26439109  0.1027874   0.09416542 -0.02340827 -0.33453147  0.30859404\n",
      "   0.08665156 -0.22358219 -0.28290786]\n",
      " [ 0.09468329 -0.16286541  0.27960897  0.00755871 -0.22515755  0.28503316\n",
      "   0.25811901  0.4336496   0.04754287]\n",
      " [-0.48990689  0.2176812  -0.51816334 -0.41799625 -0.06105803 -0.03476033\n",
      "  -0.41332562 -0.33785671  0.29647458]\n",
      " [ 0.55127047 -0.2009551   0.51435921 -0.01153189 -0.06863107 -0.119038\n",
      "   0.16454461  0.46720921 -0.13083954]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.22981063  0.8457509  -0.2624972   0.16310159 -0.0603328  -0.35614392\n",
      "   0.33764154]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:14 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.5786822]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 14 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.41790541 -0.24255094  0.35069855  0.30307316 -0.15727219  0.16475583\n",
      "   0.75276065 -0.17813541 -0.15900533]\n",
      " [-0.2479734   0.18283818 -0.46862918 -0.33402276  0.10294012 -0.34496366\n",
      "  -0.01662452  0.02800335  0.28660914]\n",
      " [ 0.263853    0.1027874   0.09416542 -0.02394635 -0.33453147  0.30859404\n",
      "   0.08665156 -0.22358219 -0.28344595]\n",
      " [ 0.09659581 -0.16286541  0.27960897  0.00947122 -0.22515755  0.28503316\n",
      "   0.25811901  0.4336496   0.04945538]\n",
      " [-0.49706738  0.2176812  -0.51816334 -0.42515674 -0.06105803 -0.03476033\n",
      "  -0.41332562 -0.33785671  0.28931409]\n",
      " [ 0.5563073  -0.2009551   0.51435921 -0.00649506 -0.06863107 -0.119038\n",
      "   0.16454461  0.46720921 -0.12580271]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.17845007  0.87823147 -0.24044728  0.18824363 -0.03273285 -0.33807869\n",
      "   0.36850019]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:14 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.58604477]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 14 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.41325436 -0.24255094  0.35069855  0.30307316 -0.16192324  0.16010478\n",
      "   0.75276065 -0.17813541 -0.16365638]\n",
      " [-0.24439601  0.18283818 -0.46862918 -0.33402276  0.10651751 -0.34138627\n",
      "  -0.01662452  0.02800335  0.29018653]\n",
      " [ 0.26466173  0.1027874   0.09416542 -0.02394635 -0.33372274  0.30940277\n",
      "   0.08665156 -0.22358219 -0.28263722]\n",
      " [ 0.0929747  -0.16286541  0.27960897  0.00947122 -0.22877866  0.28141205\n",
      "   0.25811901  0.4336496   0.04583427]\n",
      " [-0.49179486  0.2176812  -0.51816334 -0.42515674 -0.0557855  -0.02948781\n",
      "  -0.41332562 -0.33785671  0.29458662]\n",
      " [ 0.55205474 -0.2009551   0.51435921 -0.00649506 -0.07288363 -0.12329056\n",
      "   0.16454461  0.46720921 -0.13005527]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.24953622  0.83798215 -0.27238825  0.15350956 -0.07192268 -0.36826788\n",
      "   0.32866264]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:14 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.51639065]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 14 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.41317008 -0.24263523  0.35069855  0.30307316 -0.16200752  0.1600205\n",
      "   0.75276065 -0.17813541 -0.16374067]\n",
      " [-0.24429542  0.18293877 -0.46862918 -0.33402276  0.1066181  -0.34128568\n",
      "  -0.01662452  0.02800335  0.29028712]\n",
      " [ 0.2636875   0.10181317  0.09416542 -0.02394635 -0.33469697  0.30842854\n",
      "   0.08665156 -0.22358219 -0.28361145]\n",
      " [ 0.09251413 -0.16332597  0.27960897  0.00947122 -0.22923922  0.28095149\n",
      "   0.25811901  0.4336496   0.0453737 ]\n",
      " [-0.49075138  0.21872468 -0.51816334 -0.42515674 -0.05474203 -0.02844433\n",
      "  -0.41332562 -0.33785671  0.29563009]\n",
      " [ 0.5516539  -0.20135594  0.51435921 -0.00649506 -0.07328447 -0.1236914\n",
      "   0.16454461  0.46720921 -0.13045611]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.31401568  0.80565813 -0.30452739  0.120295   -0.10462304 -0.39946341\n",
      "   0.29602202]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:14 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.54801351]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 14 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.40043649 -0.25536882  0.35069855  0.30307316 -0.17474111  0.14728691\n",
      "   0.74002706 -0.17813541 -0.16374067]\n",
      " [-0.23911811  0.18811608 -0.46862918 -0.33402276  0.11179542 -0.33610837\n",
      "  -0.0114472   0.02800335  0.29028712]\n",
      " [ 0.25677932  0.09490499  0.09416542 -0.02394635 -0.34160515  0.30152036\n",
      "   0.07974338 -0.22358219 -0.28361145]\n",
      " [ 0.08851595 -0.16732416  0.27960897  0.00947122 -0.23323741  0.2769533\n",
      "   0.25412082  0.4336496   0.0453737 ]\n",
      " [-0.47946192  0.23001414 -0.51816334 -0.42515674 -0.04345256 -0.01715487\n",
      "  -0.40203615 -0.33785671  0.29563009]\n",
      " [ 0.54639447 -0.20661537  0.51435921 -0.00649506 -0.0785439  -0.12895083\n",
      "   0.15928518  0.46720921 -0.13045611]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.38188571  0.75711012 -0.33320033  0.07924108 -0.14259441 -0.42096431\n",
      "   0.25673857]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:14 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.53873032]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 14 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.41322023 -0.25536882  0.35069855  0.3158569  -0.17474111  0.14728691\n",
      "   0.7528108  -0.17813541 -0.16374067]\n",
      " [-0.24681745  0.18811608 -0.46862918 -0.3417221   0.11179542 -0.33610837\n",
      "  -0.01914655  0.02800335  0.29028712]\n",
      " [ 0.26115033  0.09490499  0.09416542 -0.01957534 -0.34160515  0.30152036\n",
      "   0.0841144  -0.22358219 -0.28361145]\n",
      " [ 0.09340782 -0.16732416  0.27960897  0.0143631  -0.23323741  0.2769533\n",
      "   0.25901269  0.4336496   0.0453737 ]\n",
      " [-0.49201749  0.23001414 -0.51816334 -0.43771231 -0.04345256 -0.01715487\n",
      "  -0.41459173 -0.33785671  0.29563009]\n",
      " [ 0.55528143 -0.20661537  0.51435921  0.0023919  -0.0785439  -0.12895083\n",
      "   0.16817214  0.46720921 -0.13045611]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.32457296  0.80347623 -0.31268938  0.11234    -0.10894445 -0.408754\n",
      "   0.29502388]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:14 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.55895895]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 14 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.42459295 -0.25536882  0.35069855  0.3158569  -0.1633684   0.15865963\n",
      "   0.76418352 -0.17813541 -0.16374067]\n",
      " [-0.25309584  0.18811608 -0.46862918 -0.3417221   0.10551702 -0.34238676\n",
      "  -0.02542494  0.02800335  0.29028712]\n",
      " [ 0.26520291  0.09490499  0.09416542 -0.01957534 -0.33755258  0.30557293\n",
      "   0.08816697 -0.22358219 -0.28361145]\n",
      " [ 0.09858586 -0.16732416  0.27960897  0.0143631  -0.22805937  0.28213134\n",
      "   0.26419073  0.4336496   0.0453737 ]\n",
      " [-0.50251119  0.23001414 -0.51816334 -0.43771231 -0.05394627 -0.02764857\n",
      "  -0.42508543 -0.33785671  0.29563009]\n",
      " [ 0.56184702 -0.20661537  0.51435921  0.0023919  -0.07197831 -0.12238524\n",
      "   0.17473773  0.46720921 -0.13045611]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.27020939  0.84465219 -0.29204056  0.14363755 -0.07644814 -0.39378033\n",
      "   0.32906647]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:14 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57251092]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 14 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.41330872 -0.25536882  0.35069855  0.30457267 -0.1633684   0.1473754\n",
      "   0.76418352 -0.17813541 -0.17502489]\n",
      " [-0.24287271  0.18811608 -0.46862918 -0.33149897  0.10551702 -0.33216363\n",
      "  -0.02542494  0.02800335  0.30051025]\n",
      " [ 0.26059907  0.09490499  0.09416542 -0.02417918 -0.33755258  0.3009691\n",
      "   0.08816697 -0.22358219 -0.28821529]\n",
      " [ 0.09123381 -0.16732416  0.27960897  0.00701105 -0.22805937  0.2747793\n",
      "   0.26419073  0.4336496   0.03802166]\n",
      " [-0.49197316  0.23001414 -0.51816334 -0.42717427 -0.05394627 -0.01711053\n",
      "  -0.42508543 -0.33785671  0.30616813]\n",
      " [ 0.5565231  -0.20661537  0.51435921 -0.00293202 -0.07197831 -0.12770916\n",
      "   0.17473773  0.46720921 -0.13578003]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.34026817  0.79729369 -0.31611868  0.10394919 -0.11906961 -0.41745985\n",
      "   0.2886267 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:14 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.56497377]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 14 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.42514821 -0.24352932  0.35069855  0.31641216 -0.1633684   0.15921489\n",
      "   0.77602301 -0.17813541 -0.17502489]\n",
      " [-0.25155657  0.17943222 -0.46862918 -0.34018283  0.10551702 -0.34084749\n",
      "  -0.0341088   0.02800335  0.30051025]\n",
      " [ 0.26907918  0.1033851   0.09416542 -0.01569907 -0.33755258  0.3094492\n",
      "   0.09664708 -0.22358219 -0.28821529]\n",
      " [ 0.09717962 -0.16137835  0.27960897  0.01295686 -0.22805937  0.2807251\n",
      "   0.27013654  0.4336496   0.03802166]\n",
      " [-0.5031271   0.2188602  -0.51816334 -0.43832821 -0.05394627 -0.02826448\n",
      "  -0.43623937 -0.33785671  0.30616813]\n",
      " [ 0.56158978 -0.20154869  0.51435921  0.00213465 -0.07197831 -0.12264249\n",
      "   0.17980441  0.46720921 -0.13578003]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.28680814  0.83995678 -0.29889577  0.1399122  -0.08617256 -0.4044201\n",
      "   0.3205555 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:14 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.59791098]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 14 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.43566567 -0.23301186  0.35069855  0.32692963 -0.1633684   0.15921489\n",
      "   0.78654047 -0.17813541 -0.17502489]\n",
      " [-0.25669074  0.17429805 -0.46862918 -0.345317    0.10551702 -0.34084749\n",
      "  -0.03924298  0.02800335  0.30051025]\n",
      " [ 0.27428575  0.10859167  0.09416542 -0.0104925  -0.33755258  0.3094492\n",
      "   0.10185365 -0.22358219 -0.28821529]\n",
      " [ 0.09979319 -0.15876478  0.27960897  0.01557043 -0.22805937  0.2807251\n",
      "   0.27275011  0.4336496   0.03802166]\n",
      " [-0.51331077  0.20867654 -0.51816334 -0.44851188 -0.05394627 -0.02826448\n",
      "  -0.44642304 -0.33785671  0.30616813]\n",
      " [ 0.56768044 -0.19545803  0.51435921  0.00822532 -0.07197831 -0.12264249\n",
      "   0.18589507  0.46720921 -0.13578003]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.23847434  0.87772655 -0.28003528  0.16946592 -0.05937116 -0.39287416\n",
      "   0.35111565]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:14 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.66958506]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 14 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.44375174 -0.23301186  0.35878462  0.32692963 -0.1633684   0.16730096\n",
      "   0.79462654 -0.17813541 -0.17502489]\n",
      " [-0.26424062  0.17429805 -0.47617906 -0.345317    0.10551702 -0.34839737\n",
      "  -0.04679286  0.02800335  0.30051025]\n",
      " [ 0.28042883  0.10859167  0.10030849 -0.0104925  -0.33755258  0.31559228\n",
      "   0.10799672 -0.22358219 -0.28821529]\n",
      " [ 0.10670214 -0.15876478  0.28651792  0.01557043 -0.22805937  0.28763405\n",
      "   0.27965906  0.4336496   0.03802166]\n",
      " [-0.52148934  0.20867654 -0.52634192 -0.44851188 -0.05394627 -0.03644305\n",
      "  -0.45460161 -0.33785671  0.30616813]\n",
      " [ 0.57534523 -0.19545803  0.522024    0.00822532 -0.07197831 -0.1149777\n",
      "   0.19355986  0.46720921 -0.13578003]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.20192369  0.90878303 -0.27094413  0.19452604 -0.03314027 -0.38623992\n",
      "   0.3788448 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:14 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.58806222]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 15 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.43750191 -0.23926169  0.35878462  0.32067979 -0.1633684   0.16730096\n",
      "   0.79462654 -0.17813541 -0.18127473]\n",
      " [-0.26185201  0.17668665 -0.47617906 -0.3429284   0.10551702 -0.34839737\n",
      "  -0.04679286  0.02800335  0.30289886]\n",
      " [ 0.27882391  0.10698676  0.10030849 -0.01209741 -0.33755258  0.31559228\n",
      "   0.10799672 -0.22358219 -0.2898202 ]\n",
      " [ 0.1066749  -0.15879202  0.28651792  0.0155432  -0.22805937  0.28763405\n",
      "   0.27965906  0.4336496   0.03799443]\n",
      " [-0.5137901   0.21637577 -0.52634192 -0.44081265 -0.05394627 -0.03644305\n",
      "  -0.45460161 -0.33785671  0.31386737]\n",
      " [ 0.57092274 -0.19988052  0.522024    0.00380283 -0.07197831 -0.1149777\n",
      "   0.19355986  0.46720921 -0.14020252]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.27315127  0.86678152 -0.30416208  0.15730515 -0.0687813  -0.41388587\n",
      "   0.33876145]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:15 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.64408538]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 15 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.44662751 -0.23926169  0.36791023  0.32067979 -0.1633684   0.16730096\n",
      "   0.80375215 -0.17813541 -0.18127473]\n",
      " [-0.26873997  0.17668665 -0.48306702 -0.3429284   0.10551702 -0.34839737\n",
      "  -0.05368082  0.02800335  0.30289886]\n",
      " [ 0.28350855  0.10698676  0.10499313 -0.01209741 -0.33755258  0.31559228\n",
      "   0.11268136 -0.22358219 -0.2898202 ]\n",
      " [ 0.1128155  -0.15879202  0.29265851  0.0155432  -0.22805937  0.28763405\n",
      "   0.28579965  0.4336496   0.03799443]\n",
      " [-0.5229151   0.21637577 -0.53546691 -0.44081265 -0.05394627 -0.03644305\n",
      "  -0.46372661 -0.33785671  0.31386737]\n",
      " [ 0.57982364 -0.19988052  0.5309249   0.00380283 -0.07197831 -0.1149777\n",
      "   0.20246076  0.46720921 -0.14020252]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.23235644  0.9006716  -0.29138183  0.18257469 -0.04176935 -0.40641175\n",
      "   0.37072645]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:15 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.66152877]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 15 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.43021511 -0.25567409  0.35149783  0.32067979 -0.1633684   0.16730096\n",
      "   0.78733975 -0.17813541 -0.18127473]\n",
      " [-0.2581766   0.18725003 -0.47250365 -0.3429284   0.10551702 -0.34839737\n",
      "  -0.04311744  0.02800335  0.30289886]\n",
      " [ 0.27322839  0.0967066   0.09471297 -0.01209741 -0.33755258  0.31559228\n",
      "   0.1024012  -0.22358219 -0.2898202 ]\n",
      " [ 0.10362358 -0.16798394  0.28346659  0.0155432  -0.22805937  0.28763405\n",
      "   0.27660773  0.4336496   0.03799443]\n",
      " [-0.50669335  0.23259752 -0.51924516 -0.44081265 -0.05394627 -0.03644305\n",
      "  -0.44750486 -0.33785671  0.31386737]\n",
      " [ 0.56447744 -0.21522671  0.5155787   0.00380283 -0.07197831 -0.1149777\n",
      "   0.18711456  0.46720921 -0.14020252]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.30641738  0.84150913 -0.31713892  0.13461851 -0.08843231 -0.42220168\n",
      "   0.31497714]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:15 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.57860286]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 15 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.43730173 -0.25567409  0.35858444  0.32067979 -0.1633684   0.16730096\n",
      "   0.78733975 -0.17104879 -0.18127473]\n",
      " [-0.26617289  0.18725003 -0.48049994 -0.3429284   0.10551702 -0.34839737\n",
      "  -0.04311744  0.02000706  0.30289886]\n",
      " [ 0.2750728   0.0967066   0.09655738 -0.01209741 -0.33755258  0.31559228\n",
      "   0.1024012  -0.22173778 -0.2898202 ]\n",
      " [ 0.1125712  -0.16798394  0.29241421  0.0155432  -0.22805937  0.28763405\n",
      "   0.27660773  0.44259722  0.03799443]\n",
      " [-0.51805471  0.23259752 -0.53060652 -0.44081265 -0.05394627 -0.03644305\n",
      "  -0.44750486 -0.34921807  0.31386737]\n",
      " [ 0.57597831 -0.21522671  0.52707957  0.00380283 -0.07197831 -0.1149777\n",
      "   0.18711456  0.47871008 -0.14020252]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.25504452  0.87472037 -0.30012325  0.16215576 -0.05275934 -0.41174092\n",
      "   0.35733501]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:15 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.57562418]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 15 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.44418642 -0.25567409  0.35858444  0.32756448 -0.1633684   0.16730096\n",
      "   0.78733975 -0.17104879 -0.17439004]\n",
      " [-0.2700492   0.18725003 -0.48049994 -0.34680471  0.10551702 -0.34839737\n",
      "  -0.04311744  0.02000706  0.29902255]\n",
      " [ 0.274725    0.0967066   0.09655738 -0.01244521 -0.33755258  0.31559228\n",
      "   0.1024012  -0.22173778 -0.290168  ]\n",
      " [ 0.11470892 -0.16798394  0.29241421  0.01768091 -0.22805937  0.28763405\n",
      "   0.27660773  0.44259722  0.04013215]\n",
      " [-0.52560046  0.23259752 -0.53060652 -0.4483584  -0.05394627 -0.03644305\n",
      "  -0.44750486 -0.34921807  0.30632161]\n",
      " [ 0.58140798 -0.21522671  0.52707957  0.00923251 -0.07197831 -0.1149777\n",
      "   0.18711456  0.47871008 -0.13477284]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.20321105  0.90790983 -0.27814369  0.18772465 -0.02469504 -0.39390413\n",
      "   0.38885798]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:15 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57920485]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 15 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.43944557 -0.25567409  0.35858444  0.32756448 -0.16810924  0.16256011\n",
      "   0.78733975 -0.17104879 -0.17913088]\n",
      " [-0.26631746  0.18725003 -0.48049994 -0.34680471  0.10924876 -0.34466562\n",
      "  -0.04311744  0.02000706  0.30275429]\n",
      " [ 0.27538478  0.0967066   0.09655738 -0.01244521 -0.33689279  0.31625207\n",
      "   0.1024012  -0.22173778 -0.28950822]\n",
      " [ 0.1109685  -0.16798394  0.29241421  0.01768091 -0.23179979  0.28389364\n",
      "   0.27660773  0.44259722  0.03639173]\n",
      " [-0.52026501  0.23259752 -0.53060652 -0.4483584  -0.04861081 -0.03110759\n",
      "  -0.44750486 -0.34921807  0.31165707]\n",
      " [ 0.57690209 -0.21522671  0.52707957  0.00923251 -0.0764842  -0.11948359\n",
      "   0.18711456  0.47871008 -0.13927874]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.27379486  0.86781765 -0.30967533  0.15309269 -0.06375609 -0.4237749\n",
      "   0.34900937]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:15 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.50583715]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 15 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.43945993 -0.25565973  0.35858444  0.32756448 -0.16809488  0.16257447\n",
      "   0.78733975 -0.17104879 -0.17911652]\n",
      " [-0.26613207  0.18743542 -0.48049994 -0.34680471  0.10943415 -0.34448023\n",
      "  -0.04311744  0.02000706  0.30293968]\n",
      " [ 0.27440671  0.09572852  0.09655738 -0.01244521 -0.33787087  0.31527399\n",
      "   0.1024012  -0.22173778 -0.29048629]\n",
      " [ 0.11047123 -0.16848121  0.29241421  0.01768091 -0.23229706  0.28339637\n",
      "   0.27660773  0.44259722  0.03589446]\n",
      " [-0.51938488  0.23347765 -0.53060652 -0.4483584  -0.04773069 -0.03022747\n",
      "  -0.44750486 -0.34921807  0.31253719]\n",
      " [ 0.57648444 -0.21564436  0.52707957  0.00923251 -0.07690184 -0.11990123\n",
      "   0.18711456  0.47871008 -0.13969638]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.33701589  0.83622149 -0.34110045  0.12050347 -0.09586396 -0.45450483\n",
      "   0.31698116]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:15 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.5482573]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 15 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.42636678 -0.26875289  0.35858444  0.32756448 -0.18118804  0.14948131\n",
      "   0.77424659 -0.17104879 -0.17911652]\n",
      " [-0.26026374  0.19330374 -0.48049994 -0.34680471  0.11530248 -0.33861191\n",
      "  -0.03724912  0.02000706  0.30293968]\n",
      " [ 0.26714353  0.08846534  0.09655738 -0.01244521 -0.34513405  0.30801081\n",
      "   0.09513802 -0.22173778 -0.29048629]\n",
      " [ 0.10597579 -0.17297665  0.29241421  0.01768091 -0.23679251  0.27890092\n",
      "   0.27211228  0.44259722  0.03589446]\n",
      " [-0.50765236  0.24521017 -0.53060652 -0.4483584  -0.03599817 -0.01849495\n",
      "  -0.43577234 -0.34921807  0.31253719]\n",
      " [ 0.57070422 -0.22142458  0.52707957  0.00923251 -0.08268207 -0.12568146\n",
      "   0.18133434  0.47871008 -0.13969638]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.40490967  0.78704973 -0.36905366  0.07904584 -0.13436099 -0.47538886\n",
      "   0.27713452]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:15 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.54366119]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 15 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.43903851 -0.26875289  0.35858444  0.34023622 -0.18118804  0.14948131\n",
      "   0.78691833 -0.17104879 -0.17911652]\n",
      " [-0.2684975   0.19330374 -0.48049994 -0.35503846  0.11530248 -0.33861191\n",
      "  -0.04548287  0.02000706  0.30293968]\n",
      " [ 0.27194592  0.08846534  0.09655738 -0.00764282 -0.34513405  0.30801081\n",
      "   0.09994041 -0.22173778 -0.29048629]\n",
      " [ 0.11136293 -0.17297665  0.29241421  0.02306806 -0.23679251  0.27890092\n",
      "   0.27749943  0.44259722  0.03589446]\n",
      " [-0.52021649  0.24521017 -0.53060652 -0.46092253 -0.03599817 -0.01849495\n",
      "  -0.44833647 -0.34921807  0.31253719]\n",
      " [ 0.58005598 -0.22142458  0.52707957  0.01858427 -0.08268207 -0.12568146\n",
      "   0.1906861   0.47871008 -0.13969638]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.34830228  0.83356626 -0.34956537  0.11225048 -0.10052841 -0.46411702\n",
      "   0.31571979]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:15 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.56104934]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 15 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.45055709 -0.26875289  0.35858444  0.34023622 -0.16966947  0.16099988\n",
      "   0.7984369  -0.17104879 -0.17911652]\n",
      " [-0.27525799  0.19330374 -0.48049994 -0.35503846  0.10854199 -0.3453724\n",
      "  -0.05224336  0.02000706  0.30293968]\n",
      " [ 0.27634507  0.08846534  0.09655738 -0.00764282 -0.3407349   0.31240996\n",
      "   0.10433956 -0.22173778 -0.29048629]\n",
      " [ 0.11692426 -0.17297665  0.29241421  0.02306806 -0.23123118  0.28446225\n",
      "   0.28306075  0.44259722  0.03589446]\n",
      " [-0.53097215  0.24521017 -0.53060652 -0.46092253 -0.04675383 -0.02925061\n",
      "  -0.45909213 -0.34921807  0.31253719]\n",
      " [ 0.5870847  -0.22142458  0.52707957  0.01858427 -0.07565336 -0.11865275\n",
      "   0.19771482  0.47871008 -0.13969638]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.29425143  0.8750503  -0.32963043  0.14375769 -0.0677679  -0.44982411\n",
      "   0.35015033]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:15 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57211169]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 15 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.438864   -0.26875289  0.35858444  0.32854313 -0.16966947  0.14930679\n",
      "   0.7984369  -0.17104879 -0.19080961]\n",
      " [-0.26471885  0.19330374 -0.48049994 -0.34449932  0.10854199 -0.33483325\n",
      "  -0.05224336  0.02000706  0.31347882]\n",
      " [ 0.27136313  0.08846534  0.09655738 -0.01262476 -0.3407349   0.30742802\n",
      "   0.10433956 -0.22173778 -0.29546823]\n",
      " [ 0.10927744 -0.17297665  0.29241421  0.01542124 -0.23123118  0.27681544\n",
      "   0.28306075  0.44259722  0.02824765]\n",
      " [-0.52000246  0.24521017 -0.53060652 -0.44995284 -0.04675383 -0.01828092\n",
      "  -0.45909213 -0.34921807  0.32350688]\n",
      " [ 0.58118405 -0.22142458  0.52707957  0.01268362 -0.07565336 -0.12455339\n",
      "   0.19771482  0.47871008 -0.14559703]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.36427788  0.82714524 -0.35329139  0.1036921  -0.1107009  -0.47292629\n",
      "   0.30911711]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:15 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.56869014]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 15 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.4506651  -0.25695179  0.35858444  0.34034424 -0.16966947  0.1611079\n",
      "   0.810238   -0.17104879 -0.19080961]\n",
      " [-0.27379436  0.18422823 -0.48049994 -0.35357484  0.10854199 -0.34390877\n",
      "  -0.06131887  0.02000706  0.31347882]\n",
      " [ 0.28008267  0.09718487  0.09655738 -0.00390522 -0.3407349   0.31614756\n",
      "   0.1130591  -0.22173778 -0.29546823]\n",
      " [ 0.11561878 -0.16663532  0.29241421  0.02176258 -0.23123118  0.28315677\n",
      "   0.28940209  0.44259722  0.02824765]\n",
      " [-0.53130145  0.23391118 -0.53060652 -0.46125183 -0.04675383 -0.02957991\n",
      "  -0.47039112 -0.34921807  0.32350688]\n",
      " [ 0.58679364 -0.21581499  0.52707957  0.01829322 -0.07565336 -0.1189438\n",
      "   0.20332441  0.47871008 -0.14559703]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.31138168  0.86996172 -0.33692604  0.13972131 -0.07763119 -0.46070205\n",
      "   0.3413623 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:15 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.60109971]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 15 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.46121092 -0.24640597  0.35858444  0.35089005 -0.16966947  0.1611079\n",
      "   0.82078382 -0.17104879 -0.19080961]\n",
      " [-0.27945767  0.17856492 -0.48049994 -0.35923814  0.10854199 -0.34390877\n",
      "  -0.06698218  0.02000706  0.31347882]\n",
      " [ 0.28556743  0.10266964  0.09655738  0.00157954 -0.3407349   0.31614756\n",
      "   0.11854386 -0.22173778 -0.29546823]\n",
      " [ 0.11867707 -0.16357703  0.29241421  0.02482087 -0.23123118  0.28315677\n",
      "   0.29246038  0.44259722  0.02824765]\n",
      " [-0.54159428  0.22361835 -0.53060652 -0.47154466 -0.04675383 -0.02957991\n",
      "  -0.48068395 -0.34921807  0.32350688]\n",
      " [ 0.59329137 -0.20931726  0.52707957  0.02479094 -0.07565336 -0.1189438\n",
      "   0.20982214  0.47871008 -0.14559703]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.26355775  0.90789544 -0.31892066  0.1693369  -0.05062633 -0.44987707\n",
      "   0.37215902]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:15 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.67500102]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 15 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.46901979 -0.24640597  0.36639331  0.35089005 -0.16966947  0.16891677\n",
      "   0.82859269 -0.17104879 -0.19080961]\n",
      " [-0.28699877  0.17856492 -0.48804104 -0.35923814  0.10854199 -0.35144987\n",
      "  -0.07452328  0.02000706  0.31347882]\n",
      " [ 0.29175603  0.10266964  0.10274597  0.00157954 -0.3407349   0.32233616\n",
      "   0.12473246 -0.22173778 -0.29546823]\n",
      " [ 0.12563515 -0.16357703  0.29937229  0.02482087 -0.23123118  0.29011486\n",
      "   0.29941846  0.44259722  0.02824765]\n",
      " [-0.54957065  0.22361835 -0.53858289 -0.47154466 -0.04675383 -0.03755628\n",
      "  -0.48866032 -0.34921807  0.32350688]\n",
      " [ 0.60092623 -0.20931726  0.53471442  0.02479094 -0.07565336 -0.11130895\n",
      "   0.21745699  0.47871008 -0.14559703]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.22790948  0.93849428 -0.31048265  0.19406111 -0.0246588  -0.44380094\n",
      "   0.39962672]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:15 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.5824069]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 16 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 4.62486026e-01 -2.52939731e-01  3.66393312e-01  3.44356294e-01\n",
      "  -1.69669469e-01  1.68916766e-01  8.28592690e-01 -1.71048788e-01\n",
      "  -1.97343376e-01]\n",
      " [-2.84284814e-01  1.81278874e-01 -4.88041039e-01 -3.56524188e-01\n",
      "   1.08541992e-01 -3.51449870e-01 -7.45232832e-02  2.00070594e-02\n",
      "   3.16192779e-01]\n",
      " [ 2.89980430e-01  1.00894037e-01  1.02745975e-01 -1.96057413e-04\n",
      "  -3.40734896e-01  3.22336155e-01  1.24732458e-01 -2.21737781e-01\n",
      "  -2.97243828e-01]\n",
      " [ 1.25367336e-01 -1.63844842e-01  2.99372295e-01  2.45530554e-02\n",
      "  -2.31231178e-01  2.90114855e-01  2.99418464e-01  4.42597224e-01\n",
      "   2.79798313e-02]\n",
      " [-5.41632529e-01  2.31556469e-01 -5.38582893e-01 -4.63606536e-01\n",
      "  -4.67538280e-02 -3.75562797e-02 -4.88660320e-01 -3.49218070e-01\n",
      "   3.31445005e-01]\n",
      " [ 5.96218278e-01 -2.14025210e-01  5.34714423e-01  2.00829962e-02\n",
      "  -7.56533564e-02 -1.11308945e-01  2.17456992e-01  4.78710076e-01\n",
      "  -1.50304974e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.29873281  0.8963882  -0.3431696   0.15687086 -0.06033829 -0.47097389\n",
      "   0.35944936]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:16 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.64861971]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 16 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 4.71410521e-01 -2.52939731e-01  3.75317807e-01  3.44356294e-01\n",
      "  -1.69669469e-01  1.68916766e-01  8.37517185e-01 -1.71048788e-01\n",
      "  -1.97343376e-01]\n",
      " [-2.91407080e-01  1.81278874e-01 -4.95163305e-01 -3.56524188e-01\n",
      "   1.08541992e-01 -3.51449870e-01 -8.16455495e-02  2.00070594e-02\n",
      "   3.16192779e-01]\n",
      " [ 2.94828594e-01  1.00894037e-01  1.07594140e-01 -1.96057413e-04\n",
      "  -3.40734896e-01  3.22336155e-01  1.29580623e-01 -2.21737781e-01\n",
      "  -2.97243828e-01]\n",
      " [ 1.31743420e-01 -1.63844842e-01  3.05748379e-01  2.45530554e-02\n",
      "  -2.31231178e-01  2.90114855e-01  3.05794548e-01  4.42597224e-01\n",
      "   2.79798313e-02]\n",
      " [-5.50594733e-01  2.31556469e-01 -5.47545097e-01 -4.63606536e-01\n",
      "  -4.67538280e-02 -3.75562797e-02 -4.97622523e-01 -3.49218070e-01\n",
      "   3.31445005e-01]\n",
      " [ 6.05053690e-01 -2.14025210e-01  5.43549836e-01  2.00829962e-02\n",
      "  -7.56533564e-02 -1.11308945e-01  2.26292404e-01  4.78710076e-01\n",
      "  -1.50304974e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.25869089  0.93001934 -0.33115325  0.18195926 -0.03336917 -0.46407159\n",
      "   0.39123729]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:16 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.66418647]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 16 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 4.54904887e-01 -2.69445365e-01  3.58812173e-01  3.44356294e-01\n",
      "  -1.69669469e-01  1.68916766e-01  8.21011551e-01 -1.71048788e-01\n",
      "  -1.97343376e-01]\n",
      " [-2.80076609e-01  1.92609345e-01 -4.83832834e-01 -3.56524188e-01\n",
      "   1.08541992e-01 -3.51449870e-01 -7.03150786e-02  2.00070594e-02\n",
      "   3.16192779e-01]\n",
      " [ 2.84208254e-01  9.02736966e-02  9.69737992e-02 -1.96057413e-04\n",
      "  -3.40734896e-01  3.22336155e-01  1.18960283e-01 -2.21737781e-01\n",
      "  -2.97243828e-01]\n",
      " [ 1.21866016e-01 -1.73722246e-01  2.95870974e-01  2.45530554e-02\n",
      "  -2.31231178e-01  2.90114855e-01  2.95917143e-01  4.42597224e-01\n",
      "   2.79798313e-02]\n",
      " [-5.34212623e-01  2.47938579e-01 -5.31162987e-01 -4.63606536e-01\n",
      "  -4.67538280e-02 -3.75562797e-02 -4.81240413e-01 -3.49218070e-01\n",
      "   3.31445005e-01]\n",
      " [ 5.89436540e-01 -2.29642360e-01  5.27932686e-01  2.00829962e-02\n",
      "  -7.56533564e-02 -1.11308945e-01  2.10675254e-01  4.78710076e-01\n",
      "  -1.50304974e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.33276189  0.87023629 -0.35594591  0.13358007 -0.08084416 -0.47914931\n",
      "   0.33483289]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:16 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.58003108]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 16 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 4.62329780e-01 -2.69445365e-01  3.66237066e-01  3.44356294e-01\n",
      "  -1.69669469e-01  1.68916766e-01  8.21011551e-01 -1.63623895e-01\n",
      "  -1.97343376e-01]\n",
      " [-2.88385905e-01  1.92609345e-01 -4.92142131e-01 -3.56524188e-01\n",
      "   1.08541992e-01 -3.51449870e-01 -7.03150786e-02  1.16977631e-02\n",
      "   3.16192779e-01]\n",
      " [ 2.86234290e-01  9.02736966e-02  9.89998354e-02 -1.96057413e-04\n",
      "  -3.40734896e-01  3.22336155e-01  1.18960283e-01 -2.19711745e-01\n",
      "  -2.97243828e-01]\n",
      " [ 1.31059115e-01 -1.73722246e-01  3.05064073e-01  2.45530554e-02\n",
      "  -2.31231178e-01  2.90114855e-01  2.95917143e-01  4.51790323e-01\n",
      "   2.79798313e-02]\n",
      " [-5.45593554e-01  2.47938579e-01 -5.42543918e-01 -4.63606536e-01\n",
      "  -4.67538280e-02 -3.75562797e-02 -4.81240413e-01 -3.60599001e-01\n",
      "   3.31445005e-01]\n",
      " [ 6.00876762e-01 -2.29642360e-01  5.39372908e-01  2.00829962e-02\n",
      "  -7.56533564e-02 -1.11308945e-01  2.10675254e-01  4.90150299e-01\n",
      "  -1.50304974e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.28161072  0.90375853 -0.33946749  0.16119028 -0.04489868 -0.46914871\n",
      "   0.37736354]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:16 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.57240741]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 16 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 4.69604774e-01 -2.69445365e-01  3.66237066e-01  3.51631288e-01\n",
      "  -1.69669469e-01  1.68916766e-01  8.21011551e-01 -1.63623895e-01\n",
      "  -1.90068382e-01]\n",
      " [-2.92572093e-01  1.92609345e-01 -4.92142131e-01 -3.60710376e-01\n",
      "   1.08541992e-01 -3.51449870e-01 -7.03150786e-02  1.16977631e-02\n",
      "   3.12006592e-01]\n",
      " [ 2.86087703e-01  9.02736966e-02  9.89998354e-02 -3.42644901e-04\n",
      "  -3.40734896e-01  3.22336155e-01  1.18960283e-01 -2.19711745e-01\n",
      "  -2.97390416e-01]\n",
      " [ 1.33440748e-01 -1.73722246e-01  3.05064073e-01  2.69346887e-02\n",
      "  -2.31231178e-01  2.90114855e-01  2.95917143e-01  4.51790323e-01\n",
      "   3.03614647e-02]\n",
      " [-5.53514940e-01  2.47938579e-01 -5.42543918e-01 -4.71527922e-01\n",
      "  -4.67538280e-02 -3.75562797e-02 -4.81240413e-01 -3.60599001e-01\n",
      "   3.23523618e-01]\n",
      " [ 6.06705118e-01 -2.29642360e-01  5.39372908e-01  2.59113519e-02\n",
      "  -7.56533564e-02 -1.11308945e-01  2.10675254e-01  4.90150299e-01\n",
      "  -1.44476618e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.22928255  0.93765625 -0.31756539  0.18720778 -0.01633955 -0.45152655\n",
      "   0.40957356]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:16 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.5716833]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 16 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 4.64820212e-01 -2.69445365e-01  3.66237066e-01  3.51631288e-01\n",
      "  -1.74454031e-01  1.64132203e-01  8.21011551e-01 -1.63623895e-01\n",
      "  -1.94852944e-01]\n",
      " [-2.88710194e-01  1.92609345e-01 -4.92142131e-01 -3.60710376e-01\n",
      "   1.12403890e-01 -3.47587971e-01 -7.03150786e-02  1.16977631e-02\n",
      "   3.15868490e-01]\n",
      " [ 2.86607301e-01  9.02736966e-02  9.89998354e-02 -3.42644901e-04\n",
      "  -3.40215297e-01  3.22855754e-01  1.18960283e-01 -2.19711745e-01\n",
      "  -2.96870817e-01]\n",
      " [ 1.29592122e-01 -1.73722246e-01  3.05064073e-01  2.69346887e-02\n",
      "  -2.35079804e-01  2.86266229e-01  2.95917143e-01  4.51790323e-01\n",
      "   2.65128382e-02]\n",
      " [-5.48148939e-01  2.47938579e-01 -5.42543918e-01 -4.71527922e-01\n",
      "  -4.13878269e-02 -3.21902786e-02 -4.81240413e-01 -3.60599001e-01\n",
      "   3.28889620e-01]\n",
      " [ 6.01978636e-01 -2.29642360e-01  5.39372908e-01  2.59113519e-02\n",
      "  -8.03798380e-02 -1.16035427e-01  2.10675254e-01  4.90150299e-01\n",
      "  -1.49203100e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.29927417  0.89781366 -0.34866707  0.15273165 -0.05521587 -0.48106757\n",
      "   0.36979135]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:16 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.49434023]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 16 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 4.64971580e-01 -2.69293998e-01  3.66237066e-01  3.51631288e-01\n",
      "  -1.74302664e-01  1.64283571e-01  8.21011551e-01 -1.63623895e-01\n",
      "  -1.94701577e-01]\n",
      " [-2.88472084e-01  1.92847456e-01 -4.92142131e-01 -3.60710376e-01\n",
      "   1.12642001e-01 -3.47349861e-01 -7.03150786e-02  1.16977631e-02\n",
      "   3.16106601e-01]\n",
      " [ 2.85640539e-01  8.93069342e-02  9.89998354e-02 -3.42644901e-04\n",
      "  -3.41182060e-01  3.21888992e-01  1.18960283e-01 -2.19711745e-01\n",
      "  -2.97837579e-01]\n",
      " [ 1.29073754e-01 -1.74240614e-01  3.05064073e-01  2.69346887e-02\n",
      "  -2.35598172e-01  2.85747861e-01  2.95917143e-01  4.51790323e-01\n",
      "   2.59944702e-02]\n",
      " [-5.47455774e-01  2.48631744e-01 -5.42543918e-01 -4.71527922e-01\n",
      "  -4.06946618e-02 -3.14971135e-02 -4.81240413e-01 -3.60599001e-01\n",
      "   3.29582785e-01]\n",
      " [ 6.01566021e-01 -2.30054976e-01  5.39372908e-01  2.59113519e-02\n",
      "  -8.07924533e-02 -1.16448042e-01  2.10675254e-01  4.90150299e-01\n",
      "  -1.49615715e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.36105878  0.86707272 -0.37932126  0.12087195 -0.08662665 -0.51126648\n",
      "   0.33848638]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:16 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.54862017]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 16 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 4.51568325e-01 -2.82697252e-01  3.66237066e-01  3.51631288e-01\n",
      "  -1.87705918e-01  1.50880316e-01  8.07608297e-01 -1.63623895e-01\n",
      "  -1.94701577e-01]\n",
      " [-2.81934076e-01  1.99385464e-01 -4.92142131e-01 -3.60710376e-01\n",
      "   1.19180009e-01 -3.40811852e-01 -6.37770703e-02  1.16977631e-02\n",
      "   3.16106601e-01]\n",
      " [ 2.78017891e-01  8.16842867e-02  9.89998354e-02 -3.42644901e-04\n",
      "  -3.48804707e-01  3.14266344e-01  1.11337635e-01 -2.19711745e-01\n",
      "  -2.97837579e-01]\n",
      " [ 1.24077732e-01 -1.79236636e-01  3.05064073e-01  2.69346887e-02\n",
      "  -2.40594195e-01  2.80751839e-01  2.90921121e-01  4.51790323e-01\n",
      "   2.59944702e-02]\n",
      " [-5.35322401e-01  2.60765117e-01 -5.42543918e-01 -4.71527922e-01\n",
      "  -2.85612883e-02 -1.93637400e-02 -4.69107040e-01 -3.60599001e-01\n",
      "   3.29582785e-01]\n",
      " [ 5.95265088e-01 -2.36355909e-01  5.39372908e-01  2.59113519e-02\n",
      "  -8.70933863e-02 -1.22748975e-01  2.04374321e-01  4.90150299e-01\n",
      "  -1.49615715e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.42898785  0.81732365 -0.40657146  0.07899534 -0.12566294 -0.53157454\n",
      "   0.29806414]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:16 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.54907129]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 16 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.46404587 -0.28269725  0.36623707  0.36410883 -0.18770592  0.15088032\n",
      "   0.82008584 -0.16362389 -0.19470158]\n",
      " [-0.29065831  0.19938546 -0.49214213 -0.36943461  0.11918001 -0.34081185\n",
      "  -0.0725013   0.01169776  0.3161066 ]\n",
      " [ 0.28324655  0.08168429  0.09899984  0.00488602 -0.34880471  0.31426634\n",
      "   0.1165663  -0.21971174 -0.29783758]\n",
      " [ 0.12995366 -0.17923664  0.30506407  0.03281062 -0.24059419  0.28075184\n",
      "   0.29679705  0.45179032  0.02599447]\n",
      " [-0.54779898  0.26076512 -0.54254392 -0.4840045  -0.02856129 -0.01936374\n",
      "  -0.48158362 -0.360599    0.32958278]\n",
      " [ 0.60502649 -0.23635591  0.53937291  0.03567275 -0.08709339 -0.12274898\n",
      "   0.21413572  0.4901503  -0.14961572]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.37316468  0.86385357 -0.38812804  0.11226847 -0.09168228 -0.52118926\n",
      "   0.33688432]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:16 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.56335292]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 16 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.4756588  -0.28269725  0.36623707  0.36410883 -0.17609298  0.16249325\n",
      "   0.83169878 -0.16362389 -0.19470158]\n",
      " [-0.29787503  0.19938546 -0.49214213 -0.36943461  0.11196329 -0.34802857\n",
      "  -0.07971802  0.01169776  0.3161066 ]\n",
      " [ 0.28799077  0.08168429  0.09899984  0.00488602 -0.34406049  0.31901056\n",
      "   0.12131051 -0.21971174 -0.29783758]\n",
      " [ 0.13589284 -0.17923664  0.30506407  0.03281062 -0.23465502  0.28669101\n",
      "   0.30273623  0.45179032  0.02599447]\n",
      " [-0.55876229  0.26076512 -0.54254392 -0.4840045  -0.0395246  -0.03032705\n",
      "  -0.49254693 -0.360599    0.32958278]\n",
      " [ 0.61249262 -0.23635591  0.53937291  0.03567275 -0.07962726 -0.11528285\n",
      "   0.22160185  0.4901503  -0.14961572]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.31946005  0.90557311 -0.36891087  0.1439712  -0.05867263 -0.50754743\n",
      "   0.3716734 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:16 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57169121]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 16 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 4.63602831e-01 -2.82697252e-01  3.66237066e-01  3.52052856e-01\n",
      "  -1.76092980e-01  1.50437281e-01  8.31698775e-01 -1.63623895e-01\n",
      "  -2.06757549e-01]\n",
      " [-2.87021421e-01  1.99385464e-01 -4.92142131e-01 -3.58581000e-01\n",
      "   1.11963289e-01 -3.37174964e-01 -7.97180245e-02  1.16977631e-02\n",
      "   3.26960210e-01]\n",
      " [ 2.82628804e-01  8.16842867e-02  9.89998354e-02 -4.75947663e-04\n",
      "  -3.44060492e-01  3.13648595e-01  1.21310511e-01 -2.19711745e-01\n",
      "  -3.03199543e-01]\n",
      " [ 1.27939212e-01 -1.79236636e-01  3.05064073e-01  2.48569979e-02\n",
      "  -2.34655023e-01  2.78737386e-01  3.02736225e-01  4.51790323e-01\n",
      "   1.80408464e-02]\n",
      " [-5.47396741e-01  2.60765117e-01 -5.42543918e-01 -4.72638948e-01\n",
      "  -3.95246018e-02 -1.89614994e-02 -4.92546934e-01 -3.60599001e-01\n",
      "   3.40948339e-01]\n",
      " [ 6.06026565e-01 -2.36355909e-01  5.39372908e-01  2.92067022e-02\n",
      "  -7.96272592e-02 -1.21748899e-01  2.21601850e-01  4.90150299e-01\n",
      "  -1.56081767e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.38945232  0.85716722 -0.39214709  0.10352453 -0.10193316 -0.53010152\n",
      "   0.33005175]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:16 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.57266966]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 16 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.47530526 -0.27099483  0.36623707  0.36375528 -0.17609298  0.16213971\n",
      "   0.8434012  -0.16362389 -0.20675755]\n",
      " [-0.29643853  0.18996836 -0.49214213 -0.36799811  0.11196329 -0.34659207\n",
      "  -0.08913513  0.01169776  0.32696021]\n",
      " [ 0.29156734  0.09062283  0.09899984  0.00846259 -0.34406049  0.32258714\n",
      "   0.13024905 -0.21971174 -0.30319954]\n",
      " [ 0.13466331 -0.17251254  0.30506407  0.03158109 -0.23465502  0.28546148\n",
      "   0.30946032  0.45179032  0.01804085]\n",
      " [-0.5587663   0.24939556 -0.54254392 -0.48400851 -0.0395246  -0.03033106\n",
      "  -0.50391649 -0.360599    0.34094834]\n",
      " [ 0.6121568  -0.23022568  0.53937291  0.03533694 -0.07962726 -0.11561867\n",
      "   0.22773208  0.4901503  -0.15608177]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.33716436  0.90003366 -0.37663377  0.13958842 -0.06871449 -0.51864398\n",
      "   0.36258327]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:16 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.60453474]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 16 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.48581992 -0.26048016  0.36623707  0.37426995 -0.17609298  0.16213971\n",
      "   0.85391587 -0.16362389 -0.20675755]\n",
      " [-0.3025971   0.18380978 -0.49214213 -0.37415668  0.11196329 -0.34659207\n",
      "  -0.09529371  0.01169776  0.32696021]\n",
      " [ 0.29732401  0.09637949  0.09899984  0.01421926 -0.34406049  0.32258714\n",
      "   0.13600571 -0.21971174 -0.30319954]\n",
      " [ 0.13816537 -0.16901048  0.30506407  0.03508315 -0.23465502  0.28546148\n",
      "   0.31296238  0.45179032  0.01804085]\n",
      " [-0.56910337  0.23905848 -0.54254392 -0.49434558 -0.0395246  -0.03033106\n",
      "  -0.51425357 -0.360599    0.34094834]\n",
      " [ 0.61903857 -0.2233439   0.53937291  0.04221871 -0.07962726 -0.11561867\n",
      "   0.23461386  0.4901503  -0.15608177]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.28989193  0.93804057 -0.3594874   0.16924519 -0.04152231 -0.50849835\n",
      "   0.39358845]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:16 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.68051203]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 16 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.49333759 -0.26048016  0.37375473  0.37426995 -0.17609298  0.16965737\n",
      "   0.86143353 -0.16362389 -0.20675755]\n",
      " [-0.3100869   0.18380978 -0.49963193 -0.37415668  0.11196329 -0.35408187\n",
      "  -0.1027835   0.01169776  0.32696021]\n",
      " [ 0.30354027  0.09637949  0.10521609  0.01421926 -0.34406049  0.32880339\n",
      "   0.14222197 -0.21971174 -0.30319954]\n",
      " [ 0.14514032 -0.16901048  0.31203902  0.03508315 -0.23465502  0.29243643\n",
      "   0.31993733  0.45179032  0.01804085]\n",
      " [-0.57684486  0.23905848 -0.55028541 -0.49434558 -0.0395246  -0.03807255\n",
      "  -0.52199506 -0.360599    0.34094834]\n",
      " [ 0.6266016  -0.2233439   0.54693594  0.04221871 -0.07962726 -0.10805564\n",
      "   0.24217688  0.4901503  -0.15608177]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.25516113  0.96812577 -0.35167213  0.19361223 -0.01585038 -0.50293209\n",
      "   0.42074822]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:16 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57632863]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 17 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.48656972 -0.26724803  0.37375473  0.36750208 -0.17609298  0.16965737\n",
      "   0.86143353 -0.16362389 -0.21352542]\n",
      " [-0.30705824  0.18683844 -0.49963193 -0.37112803  0.11196329 -0.35408187\n",
      "  -0.1027835   0.01169776  0.32998886]\n",
      " [ 0.30159477  0.094434    0.10521609  0.01227376 -0.34406049  0.32880339\n",
      "   0.14222197 -0.21971174 -0.30514504]\n",
      " [ 0.14462583 -0.16952496  0.31203902  0.03456867 -0.23465502  0.29243643\n",
      "   0.31993733  0.45179032  0.01752637]\n",
      " [-0.56870555  0.2471978  -0.55028541 -0.48620627 -0.0395246  -0.03807255\n",
      "  -0.52199506 -0.360599    0.34908766]\n",
      " [ 0.6216161  -0.2283294   0.54693594  0.03723321 -0.07962726 -0.10805564\n",
      "   0.24217688  0.4901503  -0.16106727]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.32552334  0.92599453 -0.38380937  0.15648164 -0.05154605 -0.52964263\n",
      "   0.38051173]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:17 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.65331711]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 17 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.49526606 -0.26724803  0.38245107  0.36750208 -0.17609298  0.16965737\n",
      "   0.87012987 -0.16362389 -0.21352542]\n",
      " [-0.31436656  0.18683844 -0.50694025 -0.37112803  0.11196329 -0.35408187\n",
      "  -0.11009182  0.01169776  0.32998886]\n",
      " [ 0.3065971   0.094434    0.11021842  0.01227376 -0.34406049  0.32880339\n",
      "   0.1472243  -0.21971174 -0.30514504]\n",
      " [ 0.15120541 -0.16952496  0.3186186   0.03456867 -0.23465502  0.29243643\n",
      "   0.3265169   0.45179032  0.01752637]\n",
      " [-0.57746622  0.2471978  -0.55904608 -0.48620627 -0.0395246  -0.03807255\n",
      "  -0.53075573 -0.360599    0.34908766]\n",
      " [ 0.63034811 -0.2283294   0.55566794  0.03723321 -0.07962726 -0.10805564\n",
      "   0.25090889  0.4901503  -0.16106727]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.28626257  0.95930161 -0.37253731  0.18136948 -0.02465448 -0.52326939\n",
      "   0.41207271]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:17 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.6668974]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 17 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.47870669 -0.28380739  0.36589171  0.36750208 -0.17609298  0.16965737\n",
      "   0.85357051 -0.16362389 -0.21352542]\n",
      " [-0.30232572  0.19887928 -0.4948994  -0.37112803  0.11196329 -0.35408187\n",
      "  -0.09805098  0.01169776  0.32998886]\n",
      " [ 0.29563496  0.08347186  0.09925629  0.01227376 -0.34406049  0.32880339\n",
      "   0.13626217 -0.21971174 -0.30514504]\n",
      " [ 0.14066716 -0.18006321  0.30808035  0.03456867 -0.23465502  0.29243643\n",
      "   0.31597866  0.45179032  0.01752637]\n",
      " [-0.5609764   0.26368762 -0.54255626 -0.48620627 -0.0395246  -0.03807255\n",
      "  -0.51426591 -0.360599    0.34908766]\n",
      " [ 0.61449557 -0.24418194  0.5398154   0.03723321 -0.07962726 -0.10805564\n",
      "   0.23505635  0.4901503  -0.16106727]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.36033662  0.89895622 -0.39638973  0.13256079 -0.07293351 -0.53768795\n",
      "   0.35503197]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:17 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.58156229]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 17 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.4864423  -0.28380739  0.37362731  0.36750208 -0.17609298  0.16965737\n",
      "   0.85357051 -0.15588829 -0.21352542]\n",
      " [-0.3109275   0.19887928 -0.50350118 -0.37112803  0.11196329 -0.35408187\n",
      "  -0.09805098  0.00309598  0.32998886]\n",
      " [ 0.29784767  0.08347186  0.10146899  0.01227376 -0.34406049  0.32880339\n",
      "   0.13626217 -0.21749904 -0.30514504]\n",
      " [ 0.15008701 -0.18006321  0.3175002   0.03456867 -0.23465502  0.29243643\n",
      "   0.31597866  0.46121017  0.01752637]\n",
      " [-0.57234825  0.26368762 -0.55392811 -0.48620627 -0.0395246  -0.03807255\n",
      "  -0.51426591 -0.37197085  0.34908766]\n",
      " [ 0.62585366 -0.24418194  0.55117349  0.03723321 -0.07962726 -0.10805564\n",
      "   0.23505635  0.50150839 -0.16106727]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.30942371  0.93276016 -0.38044738  0.16024129 -0.03673138 -0.52812466\n",
      "   0.39770416]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:17 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.56906289]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 17 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.49409074 -0.28380739  0.37362731  0.37515052 -0.17609298  0.16965737\n",
      "   0.85357051 -0.15588829 -0.20587698]\n",
      " [-0.31543706  0.19887928 -0.50350118 -0.37563759  0.11196329 -0.35408187\n",
      "  -0.09805098  0.00309598  0.3254793 ]\n",
      " [ 0.29791341  0.08347186  0.10146899  0.0123395  -0.34406049  0.32880339\n",
      "   0.13626217 -0.21749904 -0.3050793 ]\n",
      " [ 0.15273069 -0.18006321  0.3175002   0.03721236 -0.23465502  0.29243643\n",
      "   0.31597866  0.46121017  0.02017005]\n",
      " [-0.58063319  0.26368762 -0.55392811 -0.49449121 -0.0395246  -0.03807255\n",
      "  -0.51426591 -0.37197085  0.34080271]\n",
      " [ 0.63208438 -0.24418194  0.55117349  0.04346393 -0.07962726 -0.10805564\n",
      "   0.23505635  0.50150839 -0.15483655]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.25658429  0.96736195 -0.35863097  0.18672673 -0.00764994 -0.51070262\n",
      "   0.43061963]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:17 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.56349354]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 17 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.48930451 -0.28380739  0.37362731  0.37515052 -0.18087921  0.16487114\n",
      "   0.85357051 -0.15588829 -0.21066321]\n",
      " [-0.31146994  0.19887928 -0.50350118 -0.37563759  0.1159304  -0.35011475\n",
      "  -0.09805098  0.00309598  0.32944642]\n",
      " [ 0.29830184  0.08347186  0.10146899  0.0123395  -0.34367206  0.32919183\n",
      "   0.13626217 -0.21749904 -0.30469087]\n",
      " [ 0.14878678 -0.18006321  0.3175002   0.03721236 -0.23859894  0.28849252\n",
      "   0.31597866  0.46121017  0.01622614]\n",
      " [-0.57526993  0.26368762 -0.55392811 -0.49449121 -0.03416134 -0.03270929\n",
      "  -0.51426591 -0.37197085  0.34616598]\n",
      " [ 0.62717131 -0.24418194  0.55117349  0.04346393 -0.08454032 -0.1129687\n",
      "   0.23505635  0.50150839 -0.15974961]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.32588514  0.92786171 -0.38927857  0.15246478 -0.04627935 -0.53989926\n",
      "   0.39098719]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:17 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.48192608]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 17 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.48962294 -0.28348896  0.37362731  0.37515052 -0.18056078  0.16518958\n",
      "   0.85357051 -0.15588829 -0.21034477]\n",
      " [-0.31120933  0.19913989 -0.50350118 -0.37563759  0.11619101 -0.34985414\n",
      "  -0.09805098  0.00309598  0.32970703]\n",
      " [ 0.29736119  0.08253121  0.10146899  0.0123395  -0.34461271  0.32825118\n",
      "   0.13626217 -0.21749904 -0.30563152]\n",
      " [ 0.14826288 -0.18058711  0.3175002   0.03721236 -0.23912284  0.28796861\n",
      "   0.31597866  0.46121017  0.01570223]\n",
      " [-0.57478444  0.26417311 -0.55392811 -0.49449121 -0.03367585 -0.0322238\n",
      "  -0.51426591 -0.37197085  0.34665146]\n",
      " [ 0.62678437 -0.24456888  0.55117349  0.04346393 -0.08492726 -0.11335564\n",
      "   0.23505635  0.50150839 -0.16013655]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.38604719  0.89809914 -0.41909897  0.12144249 -0.07688438 -0.56949472\n",
      "   0.36051919]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:17 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.54910642]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 17 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.47595378 -0.29715812  0.37362731  0.37515052 -0.19422994  0.15152041\n",
      "   0.83990134 -0.15588829 -0.21034477]\n",
      " [-0.30402711  0.20632211 -0.50350118 -0.37563759  0.12337323 -0.34267192\n",
      "  -0.09086876  0.00309598  0.32970703]\n",
      " [ 0.28937676  0.07454678  0.10146899  0.0123395  -0.35259714  0.32026674\n",
      "   0.12827774 -0.21749904 -0.30563152]\n",
      " [ 0.14276569 -0.1860843   0.3175002   0.03721236 -0.24462003  0.28247143\n",
      "   0.31048147  0.46121017  0.01570223]\n",
      " [-0.56229276  0.27666479 -0.55392811 -0.49449121 -0.02118417 -0.01973212\n",
      "  -0.50177423 -0.37197085  0.34665146]\n",
      " [ 0.61996762 -0.25138563  0.55117349  0.04346393 -0.09174401 -0.12017239\n",
      "   0.2282396   0.50150839 -0.16013655]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.45402342  0.84781813 -0.44566678  0.07913335 -0.11647154 -0.58927237\n",
      "   0.31951312]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:17 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.5549557]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 17 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.48816719 -0.29715812  0.37362731  0.38736393 -0.19422994  0.15152041\n",
      "   0.85211476 -0.15588829 -0.21034477]\n",
      " [-0.3131859   0.20632211 -0.50350118 -0.38479638  0.12337323 -0.34267192\n",
      "  -0.10002755  0.00309598  0.32970703]\n",
      " [ 0.29501982  0.07454678  0.10146899  0.01798257 -0.35259714  0.32026674\n",
      "   0.1339208  -0.21749904 -0.30563152]\n",
      " [ 0.14911488 -0.1860843   0.3175002   0.04356155 -0.24462003  0.28247143\n",
      "   0.31683066  0.46121017  0.01570223]\n",
      " [-0.57459541  0.27666479 -0.55392811 -0.50679386 -0.02118417 -0.01973212\n",
      "  -0.51407688 -0.37197085  0.34665146]\n",
      " [ 0.63007345 -0.25138563  0.55117349  0.05356976 -0.09174401 -0.12017239\n",
      "   0.23834543  0.50150839 -0.16013655]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.39906493  0.89422268 -0.42827991  0.11243118 -0.08238547 -0.57971764\n",
      "   0.3584916 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:17 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.56587151]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 17 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.49982825 -0.29715812  0.37362731  0.38736393 -0.18256888  0.16318148\n",
      "   0.86377582 -0.15588829 -0.21034477]\n",
      " [-0.32082781  0.20632211 -0.50350118 -0.38479638  0.11573133 -0.35031382\n",
      "  -0.10766945  0.00309598  0.32970703]\n",
      " [ 0.30010447  0.07454678  0.10146899  0.01798257 -0.3475125   0.32535139\n",
      "   0.13900545 -0.21749904 -0.30563152]\n",
      " [ 0.15542226 -0.1860843   0.3175002   0.04356155 -0.23831264  0.28877881\n",
      "   0.32313804  0.46121017  0.01570223]\n",
      " [-0.5857138   0.27666479 -0.55392811 -0.50679386 -0.03230256 -0.03085051\n",
      "  -0.52519527 -0.37197085  0.34665146]\n",
      " [ 0.63794614 -0.25138563  0.55117349  0.05356976 -0.08387132 -0.1122997\n",
      "   0.24621812  0.50150839 -0.16013655]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.34574072  0.93610491 -0.40977955  0.14431228 -0.04914576 -0.56669389\n",
      "   0.39360422]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:17 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57126724]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 17 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.4874515  -0.29715812  0.37362731  0.37498718 -0.18256888  0.15080472\n",
      "   0.86377582 -0.15588829 -0.22272153]\n",
      " [-0.30966424  0.20632211 -0.50350118 -0.37363281  0.11573133 -0.33915026\n",
      "  -0.10766945  0.00309598  0.3408706 ]\n",
      " [ 0.29436181  0.07454678  0.10146899  0.01223991 -0.3475125   0.31960873\n",
      "   0.13900545 -0.21749904 -0.31137417]\n",
      " [ 0.14715219 -0.1860843   0.3175002   0.03529147 -0.23831264  0.28050874\n",
      "   0.32313804  0.46121017  0.00743216]\n",
      " [-0.57398871  0.27666479 -0.55392811 -0.49506877 -0.03230256 -0.01912543\n",
      "  -0.52519527 -0.37197085  0.35837654]\n",
      " [ 0.6309293  -0.25138563  0.55117349  0.04655292 -0.08387132 -0.11931654\n",
      "   0.24621812  0.50150839 -0.16715339]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.41569839  0.88724148 -0.43258731  0.10348095 -0.0927485  -0.58873266\n",
      "   0.35140134]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:17 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.57689422]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 17 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.49900626 -0.28560337  0.37362731  0.38654193 -0.18256888  0.16235948\n",
      "   0.87533058 -0.15588829 -0.22272153]\n",
      " [-0.31936804  0.19661831 -0.50350118 -0.38333661  0.11573133 -0.34885405\n",
      "  -0.11737325  0.00309598  0.3408706 ]\n",
      " [ 0.30349545  0.08368041  0.10146899  0.02137355 -0.3475125   0.32874237\n",
      "   0.14813908 -0.21749904 -0.31137417]\n",
      " [ 0.15424054 -0.17899595  0.3175002   0.04237982 -0.23831264  0.28759709\n",
      "   0.33022639  0.46121017  0.00743216]\n",
      " [-0.58536157  0.26529193 -0.55392811 -0.50644163 -0.03230256 -0.03049828\n",
      "  -0.53656812 -0.37197085  0.35837654]\n",
      " [ 0.63754903 -0.24476591  0.55117349  0.05317265 -0.08387132 -0.11269681\n",
      "   0.25283784  0.50150839 -0.16715339]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.36406102  0.93005841 -0.41791273  0.13954527 -0.05940844 -0.57798945\n",
      "   0.38418234]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:17 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.60820094]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 17 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.50943928 -0.27517034  0.37362731  0.39697496 -0.18256888  0.16235948\n",
      "   0.8857636  -0.15588829 -0.22272153]\n",
      " [-0.32598039  0.19000596 -0.50350118 -0.38994896  0.11573133 -0.34885405\n",
      "  -0.1239856   0.00309598  0.3408706 ]\n",
      " [ 0.30951376  0.08969873  0.10146899  0.02739186 -0.3475125   0.32874237\n",
      "   0.1541574  -0.21749904 -0.31137417]\n",
      " [ 0.15817971 -0.17505677  0.3175002   0.04631899 -0.23831264  0.28759709\n",
      "   0.33416557  0.46121017  0.00743216]\n",
      " [-0.59568435  0.25496915 -0.55392811 -0.51676441 -0.03230256 -0.03049828\n",
      "  -0.5468909  -0.37197085  0.35837654]\n",
      " [ 0.64478438 -0.23753055  0.55117349  0.060408   -0.08387132 -0.11269681\n",
      "   0.26007319  0.50150839 -0.16715339]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.31737962  0.96805043 -0.40162115  0.16921998 -0.03204864 -0.56847857\n",
      "   0.41536179]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:17 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.68608489]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 17 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.51665866 -0.27517034  0.38084669  0.39697496 -0.18256888  0.16957886\n",
      "   0.89298298 -0.15588829 -0.22272153]\n",
      " [-0.33337974  0.19000596 -0.51090053 -0.38994896  0.11573133 -0.35625341\n",
      "  -0.13138495  0.00309598  0.3408706 ]\n",
      " [ 0.31573939  0.08969873  0.10769462  0.02739186 -0.3475125   0.33496799\n",
      "   0.16038302 -0.21749904 -0.31137417]\n",
      " [ 0.16513971 -0.17505677  0.3244602   0.04631899 -0.23831264  0.29455709\n",
      "   0.34112557  0.46121017  0.00743216]\n",
      " [-0.60316746  0.25496915 -0.56141122 -0.51676441 -0.03230256 -0.03798139\n",
      "  -0.55437401 -0.37197085  0.35837654]\n",
      " [ 0.65223756 -0.23753055  0.55862667  0.060408   -0.08387132 -0.10524364\n",
      "   0.26752637  0.50150839 -0.16715339]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.28357529  0.99757449 -0.39439465  0.1932106  -0.00670281 -0.56337508\n",
      "   0.4421699 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:17 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.56984592]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 18 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.50970457 -0.28212443  0.38084669  0.39002087 -0.18256888  0.16957886\n",
      "   0.89298298 -0.15588829 -0.22967561]\n",
      " [-0.33005041  0.19333529 -0.51090053 -0.38661963  0.11573133 -0.35625341\n",
      "  -0.13138495  0.00309598  0.34419993]\n",
      " [ 0.31362655  0.08758589  0.10769462  0.02527902 -0.3475125   0.33496799\n",
      "   0.16038302 -0.21749904 -0.31348701]\n",
      " [ 0.16437473 -0.17582176  0.3244602   0.04555401 -0.23831264  0.29455709\n",
      "   0.34112557  0.46121017  0.00666718]\n",
      " [-0.59486643  0.26327018 -0.56141122 -0.50846338 -0.03230256 -0.03798139\n",
      "  -0.55437401 -0.37197085  0.36667757]\n",
      " [ 0.64698599 -0.24278212  0.55862667  0.05515643 -0.08387132 -0.10524364\n",
      "   0.26752637  0.50150839 -0.17240495]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.35341605  0.95549796 -0.42596511  0.15617218 -0.04238842 -0.5896348\n",
      "   0.40191455]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:18 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.65815818]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 18 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.51815221 -0.28212443  0.38929432  0.39002087 -0.18256888  0.16957886\n",
      "   0.90143061 -0.15588829 -0.22967561]\n",
      " [-0.33749539  0.19333529 -0.51834551 -0.38661963  0.11573133 -0.35625341\n",
      "  -0.13882992  0.00309598  0.34419993]\n",
      " [ 0.31877125  0.08758589  0.11283932  0.02527902 -0.3475125   0.33496799\n",
      "   0.16552772 -0.21749904 -0.31348701]\n",
      " [ 0.1711233  -0.17582176  0.33120877  0.04555401 -0.23831264  0.29455709\n",
      "   0.34787414  0.46121017  0.00666718]\n",
      " [-0.6033949   0.26327018 -0.56993969 -0.50846338 -0.03230256 -0.03798139\n",
      "  -0.56290249 -0.37197085  0.36667757]\n",
      " [ 0.65557949 -0.24278212  0.56722016  0.05515643 -0.08387132 -0.10524364\n",
      "   0.27611986  0.50150839 -0.17240495]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.31496124  0.98842038 -0.41541252  0.18083939 -0.01561055 -0.58374816\n",
      "   0.43319885]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:18 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.66964634]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 18 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.50157176 -0.29870487  0.37271388  0.39002087 -0.18256888  0.16957886\n",
      "   0.88485017 -0.15588829 -0.22967561]\n",
      " [-0.3248053   0.20602537 -0.50565543 -0.38661963  0.11573133 -0.35625341\n",
      "  -0.12613984  0.00309598  0.34419993]\n",
      " [ 0.30746924  0.07628387  0.10153731  0.02527902 -0.3475125   0.33496799\n",
      "   0.15422571 -0.21749904 -0.31348701]\n",
      " [ 0.15995422 -0.18699083  0.32003969  0.04555401 -0.23831264  0.29455709\n",
      "   0.33670506  0.46121017  0.00666718]\n",
      " [-0.58684272  0.27982237 -0.55338751 -0.50846338 -0.03230256 -0.03798139\n",
      "  -0.5463503  -0.37197085  0.36667757]\n",
      " [ 0.63952837 -0.25883324  0.55116904  0.05515643 -0.08387132 -0.10524364\n",
      "   0.26006875  0.50150839 -0.17240495]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.38903086  0.92756991 -0.43835484  0.13159847 -0.0646805  -0.59756156\n",
      "   0.37554598]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:18 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.5832095]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 18 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.50958976 -0.29870487  0.38073188  0.39002087 -0.18256888  0.16957886\n",
      "   0.88485017 -0.1478703  -0.22967561]\n",
      " [-0.33367631  0.20602537 -0.51452643 -0.38661963  0.11573133 -0.35625341\n",
      "  -0.12613984 -0.00577502  0.34419993]\n",
      " [ 0.30987238  0.07628387  0.10394045  0.02527902 -0.3475125   0.33496799\n",
      "   0.15422571 -0.21509589 -0.31348701]\n",
      " [ 0.16957978 -0.18699083  0.32966525  0.04555401 -0.23831264  0.29455709\n",
      "   0.33670506  0.47083573  0.00666718]\n",
      " [-0.59817918  0.27982237 -0.56472397 -0.50846338 -0.03230256 -0.03798139\n",
      "  -0.5463503  -0.38330731  0.36667757]\n",
      " [ 0.6507843  -0.25883324  0.56242498  0.05515643 -0.08387132 -0.10524364\n",
      "   0.26006875  0.51276432 -0.17240495]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.33837494  0.96162356 -0.42294518  0.1593443  -0.02824153 -0.58841245\n",
      "   0.41832562]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:18 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.56562537]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 18 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.51759466 -0.29870487  0.38073188  0.39802577 -0.18256888  0.16957886\n",
      "   0.88485017 -0.1478703  -0.22167071]\n",
      " [-0.33852025  0.20602537 -0.51452643 -0.39146357  0.11573133 -0.35625341\n",
      "  -0.12613984 -0.00577502  0.33935599]\n",
      " [ 0.31016136  0.07628387  0.10394045  0.025568   -0.3475125   0.33496799\n",
      "   0.15422571 -0.21509589 -0.31319804]\n",
      " [ 0.17250259 -0.18699083  0.32966525  0.04847682 -0.23831264  0.29455709\n",
      "   0.33670506  0.47083573  0.00958999]\n",
      " [-0.60681371  0.27982237 -0.56472397 -0.5170979  -0.03230256 -0.03798139\n",
      "  -0.5463503  -0.38330731  0.35804305]\n",
      " [ 0.65741844 -0.25883324  0.56242498  0.06179057 -0.08387132 -0.10524364\n",
      "   0.26006875  0.51276432 -0.16577081]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.28501347  0.99692156 -0.40122339  0.18631403  0.00138604 -0.57117607\n",
      "   0.45195975]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:18 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.55465865]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 18 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.51284484 -0.29870487  0.38073188  0.39802577 -0.18731869  0.16482904\n",
      "   0.88485017 -0.1478703  -0.22642053]\n",
      " [-0.33447379  0.20602537 -0.51452643 -0.39146357  0.11977779 -0.35220695\n",
      "  -0.12613984 -0.00577502  0.34340245]\n",
      " [ 0.31042819  0.07628387  0.10394045  0.025568   -0.34724567  0.33523482\n",
      "   0.15422571 -0.21509589 -0.31293121]\n",
      " [ 0.16847827 -0.18699083  0.32966525  0.04847682 -0.24233697  0.29053276\n",
      "   0.33670506  0.47083573  0.00556566]\n",
      " [-0.60148632  0.27982237 -0.56472397 -0.5170979  -0.02697518 -0.03265401\n",
      "  -0.5463503  -0.38330731  0.36337043]\n",
      " [ 0.65235405 -0.25883324  0.56242498  0.06179057 -0.08893571 -0.11030803\n",
      "   0.26006875  0.51276432 -0.1708352 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.35351726  0.95785623 -0.43138997  0.15232897 -0.03692839 -0.60000974\n",
      "   0.41256585]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:18 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.46863766]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 18 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.51335189 -0.29819782  0.38073188  0.39802577 -0.18681164  0.16533609\n",
      "   0.88485017 -0.1478703  -0.22591348]\n",
      " [-0.33421889  0.20628027 -0.51452643 -0.39146357  0.12003269 -0.35195205\n",
      "  -0.12613984 -0.00577502  0.34365734]\n",
      " [ 0.30952799  0.07538367  0.10394045  0.025568   -0.34814587  0.33433462\n",
      "   0.15422571 -0.21509589 -0.31383141]\n",
      " [ 0.16796424 -0.18750486  0.32966525  0.04847682 -0.242851    0.29001874\n",
      "   0.33670506  0.47083573  0.00505163]\n",
      " [-0.6012249   0.28008379 -0.56472397 -0.5170979  -0.02671376 -0.03239259\n",
      "  -0.5463503  -0.38330731  0.36363185]\n",
      " [ 0.65201214 -0.25917514  0.56242498  0.06179057 -0.08927762 -0.11064993\n",
      "   0.26006875  0.51276432 -0.17117711]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.41186649  0.92918876 -0.46030968  0.12225359 -0.06661714 -0.62892291\n",
      "   0.3830493 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:18 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.54971584]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 18 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.49945575 -0.31209397  0.38073188  0.39802577 -0.20070779  0.15143995\n",
      "   0.87095403 -0.1478703  -0.22591348]\n",
      " [-0.32642218  0.21407698 -0.51452643 -0.39146357  0.1278294  -0.34415534\n",
      "  -0.11834313 -0.00577502  0.34365734]\n",
      " [ 0.301182    0.06703769  0.10394045  0.025568   -0.35649186  0.32598863\n",
      "   0.14587972 -0.21509589 -0.31383141]\n",
      " [ 0.16196857 -0.19350053  0.32966525  0.04847682 -0.24884667  0.28402306\n",
      "   0.33070939  0.47083573  0.00505163]\n",
      " [-0.58841633  0.29289236 -0.56472397 -0.5170979  -0.01390519 -0.01958402\n",
      "  -0.53354174 -0.38330731  0.36363185]\n",
      " [ 0.64468963 -0.26649766  0.56242498  0.06179057 -0.09660014 -0.11797245\n",
      "   0.25274623  0.51276432 -0.17117711]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.47990162  0.8784199  -0.48621996  0.07950083 -0.1067639  -0.64821873\n",
      "   0.34145623]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:18 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.56130407]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 18 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.51134751 -0.31209397  0.38073188  0.40991753 -0.20070779  0.15143995\n",
      "   0.88284578 -0.1478703  -0.22591348]\n",
      " [-0.33594908  0.21407698 -0.51452643 -0.40099047  0.1278294  -0.34415534\n",
      "  -0.12787003 -0.00577502  0.34365734]\n",
      " [ 0.30722045  0.06703769  0.10394045  0.03160645 -0.35649186  0.32598863\n",
      "   0.15191818 -0.21509589 -0.31383141]\n",
      " [ 0.16876595 -0.19350053  0.32966525  0.05527421 -0.24884667  0.28402306\n",
      "   0.33750677  0.47083573  0.00505163]\n",
      " [-0.60047027  0.29289236 -0.56472397 -0.52915184 -0.01390519 -0.01958402\n",
      "  -0.54559567 -0.38330731  0.36363185]\n",
      " [ 0.6550665  -0.26649766  0.56242498  0.07216745 -0.09660014 -0.11797245\n",
      "   0.26312311  0.51276432 -0.17117711]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.42588898  0.9245609  -0.46989034  0.11277294 -0.07262353 -0.63943662\n",
      "   0.38050558]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:18 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.56860351]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 18 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.52301589 -0.31209397  0.38073188  0.40991753 -0.1890394   0.16310833\n",
      "   0.89451417 -0.1478703  -0.22591348]\n",
      " [-0.34398041  0.21407698 -0.51452643 -0.40099047  0.11979807 -0.35218667\n",
      "  -0.13590136 -0.00577502  0.34365734]\n",
      " [ 0.31263751  0.06703769  0.10394045  0.03160645 -0.3510748   0.33140569\n",
      "   0.15733524 -0.21509589 -0.31383141]\n",
      " [ 0.1754276  -0.19350053  0.32966525  0.05527421 -0.24218502  0.29068471\n",
      "   0.34416842  0.47083573  0.00505163]\n",
      " [-0.61169432  0.29289236 -0.56472397 -0.52915184 -0.02512924 -0.03080807\n",
      "  -0.55681972 -0.38330731  0.36363185]\n",
      " [ 0.66331036 -0.26649766  0.56242498  0.07216745 -0.08835628 -0.10972859\n",
      "   0.27136696  0.51276432 -0.17117711]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.37297959  0.96653386 -0.45210067  0.14481211 -0.03917689 -0.62699592\n",
      "   0.41590139]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:18 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57085743]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 18 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.51035641 -0.31209397  0.38073188  0.39725804 -0.1890394   0.15044885\n",
      "   0.89451417 -0.1478703  -0.23857296]\n",
      " [-0.33251442  0.21407698 -0.51452643 -0.38952448  0.11979807 -0.34072068\n",
      "  -0.13590136 -0.00577502  0.35512333]\n",
      " [ 0.30651513  0.06703769  0.10394045  0.02548407 -0.3510748   0.32528331\n",
      "   0.15733524 -0.21509589 -0.31995379]\n",
      " [ 0.16683419 -0.19350053  0.32966525  0.04668079 -0.24218502  0.2820913\n",
      "   0.34416842  0.47083573 -0.00354178]\n",
      " [-0.59964531  0.29289236 -0.56472397 -0.51710284 -0.02512924 -0.01875907\n",
      "  -0.55681972 -0.38330731  0.37568086]\n",
      " [ 0.65576074 -0.26649766  0.56242498  0.06461783 -0.08835628 -0.11727821\n",
      "   0.27136696  0.51276432 -0.17872673]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.44290369  0.91725328 -0.47448052  0.10359321 -0.08313482 -0.64855437\n",
      "   0.37312701]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:18 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.58134158]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 18 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.52172519 -0.30072518  0.38073188  0.40862683 -0.1890394   0.16181763\n",
      "   0.90588296 -0.1478703  -0.23857296]\n",
      " [-0.34244689  0.20414451 -0.51452643 -0.39945695  0.11979807 -0.35065315\n",
      "  -0.14583383 -0.00577502  0.35512333]\n",
      " [ 0.31581686  0.07633941  0.10394045  0.0347858  -0.3510748   0.33458504\n",
      "   0.16663696 -0.21509589 -0.31995379]\n",
      " [ 0.17426263 -0.18607209  0.32966525  0.05410924 -0.24218502  0.28951974\n",
      "   0.35159686  0.47083573 -0.00354178]\n",
      " [-0.610963    0.28157467 -0.56472397 -0.52842052 -0.02512924 -0.03007675\n",
      "  -0.5681374  -0.38330731  0.37568086]\n",
      " [ 0.66283066 -0.25942774  0.56242498  0.07168775 -0.08835628 -0.11020829\n",
      "   0.27843688  0.51276432 -0.17872673]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.39195641  0.95992676 -0.46062371  0.13962135 -0.04970472 -0.6384716\n",
      "   0.40611423]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:18 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.61207977]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 18 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.53203504 -0.29041533  0.38073188  0.41893668 -0.1890394   0.16181763\n",
      "   0.9161928  -0.1478703  -0.23857296]\n",
      " [-0.34946504  0.19712637 -0.51452643 -0.40647509  0.11979807 -0.35065315\n",
      "  -0.15285198 -0.00577502  0.35512333]\n",
      " [ 0.32208265  0.0826052   0.10394045  0.04105158 -0.3510748   0.33458504\n",
      "   0.17290275 -0.21509589 -0.31995379]\n",
      " [ 0.17862625 -0.18170847  0.32966525  0.05847286 -0.24218502  0.28951974\n",
      "   0.35596049  0.47083573 -0.00354178]\n",
      " [-0.62122059  0.27131708 -0.56472397 -0.53867811 -0.02512924 -0.03007675\n",
      "  -0.57839499 -0.38330731  0.37568086]\n",
      " [ 0.67038261 -0.25187579  0.56242498  0.0792397  -0.08835628 -0.11020829\n",
      "   0.28598883  0.51276432 -0.17872673]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.34590288  0.99782011 -0.4451746   0.16928839 -0.02220062 -0.6295496\n",
      "   0.43742818]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:18 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.69168756]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 18 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.53895485 -0.29041533  0.38765168  0.41893668 -0.1890394   0.16873744\n",
      "   0.92311261 -0.1478703  -0.23857296]\n",
      " [-0.35673918  0.19712637 -0.52180058 -0.40647509  0.11979807 -0.35792729\n",
      "  -0.16012612 -0.00577502  0.35512333]\n",
      " [ 0.32829926  0.0826052   0.11015707  0.04105158 -0.3510748   0.34080165\n",
      "   0.17911937 -0.21509589 -0.31995379]\n",
      " [ 0.18554074 -0.18170847  0.33657974  0.05847286 -0.24218502  0.29643423\n",
      "   0.36287497  0.47083573 -0.00354178]\n",
      " [-0.62843027  0.27131708 -0.57193365 -0.53867811 -0.02512924 -0.03728643\n",
      "  -0.58560467 -0.38330731  0.37568086]\n",
      " [ 0.67769269 -0.25187579  0.56973506  0.0792397  -0.08835628 -0.10289821\n",
      "   0.29329892  0.51276432 -0.17872673]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.31302816  1.02674412 -0.43850023  0.19288544  0.00279072 -0.6248639\n",
      "   0.46384444]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:18 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.56298182]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 19 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.53186012 -0.29751006  0.38765168  0.41184195 -0.1890394   0.16873744\n",
      "   0.92311261 -0.1478703  -0.24566769]\n",
      " [-0.35312687  0.20073867 -0.52180058 -0.40286279  0.11979807 -0.35792729\n",
      "  -0.16012612 -0.00577502  0.35873564]\n",
      " [ 0.3260237   0.08032964  0.11015707  0.03877602 -0.3510748   0.34080165\n",
      "   0.17911937 -0.21509589 -0.32222936]\n",
      " [ 0.18452419 -0.18272502  0.33657974  0.05745631 -0.24218502  0.29643423\n",
      "   0.36287497  0.47083573 -0.00455833]\n",
      " [-0.62000763  0.27973972 -0.57193365 -0.53025547 -0.02512924 -0.03728643\n",
      "  -0.58560467 -0.38330731  0.3841035 ]\n",
      " [ 0.6721904  -0.25737808  0.56973506  0.0737374  -0.08835628 -0.10289821\n",
      "   0.29329892  0.51276432 -0.18422903]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.38228429  0.98480179 -0.46948913  0.15597519 -0.03285449 -0.65068441\n",
      "   0.4236159 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:19 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.66312368]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 19 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.54004444 -0.29751006  0.39583601  0.41184195 -0.1890394   0.16873744\n",
      "   0.93129694 -0.1478703  -0.24566769]\n",
      " [-0.36065937  0.20073867 -0.52933307 -0.40286279  0.11979807 -0.35792729\n",
      "  -0.16765861 -0.00577502  0.35873564]\n",
      " [ 0.33129672  0.08032964  0.11543009  0.03877602 -0.3510748   0.34080165\n",
      "   0.18439239 -0.21509589 -0.32222936]\n",
      " [ 0.19140552 -0.18272502  0.34346107  0.05745631 -0.24218502  0.29643423\n",
      "   0.3697563   0.47083573 -0.00455833]\n",
      " [-0.62828095  0.27973972 -0.58020697 -0.53025547 -0.02512924 -0.03728643\n",
      "  -0.59387799 -0.38330731  0.3841035 ]\n",
      " [ 0.68061393 -0.25737808  0.57815859  0.0737374  -0.08835628 -0.10289821\n",
      "   0.30172245  0.51276432 -0.18422903]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.34465678  1.01728407 -0.45962675  0.18040105 -0.00622752 -0.64524323\n",
      "   0.45457467]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:19 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.67241842]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 19 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.52346911 -0.3140854   0.37926067  0.41184195 -0.1890394   0.16873744\n",
      "   0.9147216  -0.1478703  -0.24566769]\n",
      " [-0.34738402  0.21401402 -0.51605773 -0.40286279  0.11979807 -0.35792729\n",
      "  -0.15438327 -0.00577502  0.35873564]\n",
      " [ 0.31966033  0.06869325  0.10379371  0.03877602 -0.3510748   0.34080165\n",
      "   0.172756   -0.21509589 -0.32222936]\n",
      " [ 0.17964056 -0.19448998  0.33169611  0.05745631 -0.24218502  0.29643423\n",
      "   0.35799135  0.47083573 -0.00455833]\n",
      " [-0.61170415  0.29631652 -0.56363017 -0.53025547 -0.02512924 -0.03728643\n",
      "  -0.57730119 -0.38330731  0.3841035 ]\n",
      " [ 0.66440115 -0.27359086  0.56194582  0.0737374  -0.08835628 -0.10289821\n",
      "   0.28550967  0.51276432 -0.18422903]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.41871422  0.95598405 -0.48169475  0.13072915 -0.05607002 -0.65850508\n",
      "   0.39633893]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:19 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.58498627]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 19 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.53174082 -0.3140854   0.38753238  0.41184195 -0.1890394   0.16873744\n",
      "   0.9147216  -0.13959859 -0.24566769]\n",
      " [-0.3564986   0.21401402 -0.52517231 -0.40286279  0.11979807 -0.35792729\n",
      "  -0.15438327 -0.0148896   0.35873564]\n",
      " [ 0.32225622  0.06869325  0.10638959  0.03877602 -0.3510748   0.34080165\n",
      "   0.172756   -0.2125     -0.32222936]\n",
      " [ 0.18944875 -0.19448998  0.34150431  0.05745631 -0.24218502  0.29643423\n",
      "   0.35799135  0.48064392 -0.00455833]\n",
      " [-0.62298142  0.29631652 -0.57490744 -0.53025547 -0.02512924 -0.03728643\n",
      "  -0.57730119 -0.39458458  0.3841035 ]\n",
      " [ 0.67553655 -0.27359086  0.57308122  0.0737374  -0.08835628 -0.10289821\n",
      "   0.28550967  0.52389972 -0.18422903]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.36833626  0.99025299 -0.46681223  0.15853285 -0.01941808 -0.64974729\n",
      "   0.43918912]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:19 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.56213216]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 19 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.54008516 -0.3140854   0.38753238  0.42018629 -0.1890394   0.16873744\n",
      "   0.9147216  -0.13959859 -0.23732335]\n",
      " [-0.36168496  0.21401402 -0.52517231 -0.40804914  0.11979807 -0.35792729\n",
      "  -0.15438327 -0.0148896   0.35354928]\n",
      " [ 0.32277878  0.06869325  0.10638959  0.03929858 -0.3510748   0.34080165\n",
      "   0.172756   -0.2125     -0.3217068 ]\n",
      " [ 0.19266619 -0.19448998  0.34150431  0.06067375 -0.24218502  0.29643423\n",
      "   0.35799135  0.48064392 -0.00134089]\n",
      " [-0.63195016  0.29631652 -0.57490744 -0.5392242  -0.02512924 -0.03728643\n",
      "  -0.57730119 -0.39458458  0.37513476]\n",
      " [ 0.6825722  -0.27359086  0.57308122  0.08077305 -0.08835628 -0.10289821\n",
      "   0.28550967  0.52389972 -0.17719338]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.31444795  1.02623558 -0.44519428  0.18599969  0.0107751  -0.63268266\n",
      "   0.47354933]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:19 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.54521085]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 19 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.53540596 -0.3140854   0.38753238  0.42018629 -0.19371861  0.16405824\n",
      "   0.9147216  -0.13959859 -0.24200255]\n",
      " [-0.35758591  0.21401402 -0.52517231 -0.40804914  0.12389712 -0.35382824\n",
      "  -0.15438327 -0.0148896   0.35764834]\n",
      " [ 0.32293426  0.06869325  0.10638959  0.03929858 -0.35091932  0.34095713\n",
      "   0.172756   -0.2125     -0.32155132]\n",
      " [ 0.18857829 -0.19448998  0.34150431  0.06067375 -0.24627293  0.29234632\n",
      "   0.35799135  0.48064392 -0.0054288 ]\n",
      " [-0.62669075  0.29631652 -0.57490744 -0.5392242  -0.01986984 -0.03202703\n",
      "  -0.57730119 -0.39458458  0.38039416]\n",
      " [ 0.67739278 -0.27359086  0.57308122  0.08077305 -0.0935357  -0.10807763\n",
      "   0.28550967  0.52389972 -0.1823728 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.38204209  0.98769683 -0.47485075  0.1523581  -0.02715109 -0.66113055\n",
      "   0.43448724]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:19 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.4545341]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 19 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.53611467 -0.31337669  0.38753238  0.42018629 -0.1930099   0.16476695\n",
      "   0.9147216  -0.13959859 -0.24129384]\n",
      " [-0.35736258  0.21423734 -0.52517231 -0.40804914  0.12412045 -0.35360491\n",
      "  -0.15438327 -0.0148896   0.35787166]\n",
      " [ 0.32208822  0.06784721  0.10638959  0.03929858 -0.35176536  0.34011109\n",
      "   0.172756   -0.2125     -0.32239736]\n",
      " [ 0.18808916 -0.1949791   0.34150431  0.06067375 -0.24676205  0.29185719\n",
      "   0.35799135  0.48064392 -0.00591792]\n",
      " [-0.62666431  0.29634296 -0.57490744 -0.5392242  -0.0198434  -0.03200059\n",
      "  -0.57730119 -0.39458458  0.3804206 ]\n",
      " [ 0.67711367 -0.27386997  0.57308122  0.08077305 -0.09381481 -0.10835674\n",
      "   0.28550967  0.52389972 -0.18265191]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.43838906  0.96023236 -0.50280089  0.12333806 -0.0558138  -0.68927759\n",
      "   0.40603462]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:19 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.55044211]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 19 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.52202552 -0.32746584  0.38753238  0.42018629 -0.20709904  0.1506778\n",
      "   0.90063245 -0.13959859 -0.24129384]\n",
      " [-0.34898511  0.22261482 -0.52517231 -0.40804914  0.13249792 -0.34522744\n",
      "  -0.1460058  -0.0148896   0.35787166]\n",
      " [ 0.31338377  0.05914276  0.10638959  0.03929858 -0.36046981  0.33140664\n",
      "   0.16405156 -0.2125     -0.32239736]\n",
      " [ 0.18160137 -0.2014669   0.34150431  0.06067375 -0.25324985  0.2853694\n",
      "   0.35150356  0.48064392 -0.00591792]\n",
      " [-0.61357815  0.30942912 -0.57490744 -0.5392242  -0.00675723 -0.01891443\n",
      "  -0.56421503 -0.39458458  0.3804206 ]\n",
      " [ 0.66930066 -0.28168298  0.57308122  0.08077305 -0.10162782 -0.11616975\n",
      "   0.27769666  0.52389972 -0.18265191]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.50649405  0.90901856 -0.52808273  0.08013388 -0.09652516 -0.70814165\n",
      "   0.36385715]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:19 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.56809943]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 19 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.53355001 -0.32746584  0.38753238  0.43171078 -0.20709904  0.1506778\n",
      "   0.91215694 -0.13959859 -0.24129384]\n",
      " [-0.35880538  0.22261482 -0.52517231 -0.41786942  0.13249792 -0.34522744\n",
      "  -0.15582607 -0.0148896   0.35787166]\n",
      " [ 0.31979137  0.05914276  0.10638959  0.04570617 -0.36046981  0.33140664\n",
      "   0.17045915 -0.2125     -0.32239736]\n",
      " [ 0.18881238 -0.2014669   0.34150431  0.06788475 -0.25324985  0.2853694\n",
      "   0.35871456  0.48064392 -0.00591792]\n",
      " [-0.62532104  0.30942912 -0.57490744 -0.5509671  -0.00675723 -0.01891443\n",
      "  -0.57595793 -0.39458458  0.3804206 ]\n",
      " [ 0.67986954 -0.28168298  0.57308122  0.09134193 -0.10162782 -0.11616975\n",
      "   0.28826554  0.52389972 -0.18265191]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.45350796  0.95476065 -0.51280015  0.11332351 -0.06238984 -0.70007422\n",
      "   0.40288053]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:19 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.57154308]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 19 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.54519032 -0.32746584  0.38753238  0.43171078 -0.19545873  0.16231811\n",
      "   0.92379725 -0.13959859 -0.24129384]\n",
      " [-0.36718656  0.22261482 -0.52517231 -0.41786942  0.12411674 -0.35360862\n",
      "  -0.16420725 -0.0148896   0.35787166]\n",
      " [ 0.32552935  0.05914276  0.10638959  0.04570617 -0.35473182  0.33714463\n",
      "   0.17619714 -0.2125     -0.32239736]\n",
      " [ 0.19581009 -0.2014669   0.34150431  0.06788475 -0.24625213  0.29236712\n",
      "   0.36571228  0.48064392 -0.00591792]\n",
      " [-0.63660551  0.30942912 -0.57490744 -0.5509671  -0.0180417  -0.03019889\n",
      "  -0.58724239 -0.39458458  0.3804206 ]\n",
      " [ 0.68844554 -0.28168298  0.57308122  0.09134193 -0.09305182 -0.10759375\n",
      "   0.29684154  0.52389972 -0.18265191]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.40104735  0.99675422 -0.49570984  0.14549738 -0.02876332 -0.68818058\n",
      "   0.43851451]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:19 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.5704782]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 19 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.53228212 -0.32746584  0.38753238  0.41880258 -0.19545873  0.14940991\n",
      "   0.92379725 -0.13959859 -0.25420205]\n",
      " [-0.35542853  0.22261482 -0.52517231 -0.40611138  0.12411674 -0.34185059\n",
      "  -0.16420725 -0.0148896   0.36962969]\n",
      " [ 0.31903008  0.05914276  0.10638959  0.0392069  -0.35473182  0.33064535\n",
      "   0.17619714 -0.2125     -0.32889664]\n",
      " [ 0.18688947 -0.2014669   0.34150431  0.05896413 -0.24625213  0.2834465\n",
      "   0.36571228  0.48064392 -0.01483855]\n",
      " [-0.62426654  0.30942912 -0.57490744 -0.53862812 -0.0180417  -0.01785992\n",
      "  -0.58724239 -0.39458458  0.39275958]\n",
      " [ 0.68038435 -0.28168298  0.57308122  0.08328074 -0.09305182 -0.11565494\n",
      "   0.29684154  0.52389972 -0.1907131 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.47094029  0.94709368 -0.51766664  0.10388904 -0.07308726 -0.70929475\n",
      "   0.39518099]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:19 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.58598519]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 19 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.54343636 -0.3163116   0.38753238  0.42995682 -0.19545873  0.16056415\n",
      "   0.9349515  -0.13959859 -0.25420205]\n",
      " [-0.36553054  0.2125128  -0.52517231 -0.4162134   0.12411674 -0.3519526\n",
      "  -0.17430927 -0.0148896   0.36962969]\n",
      " [ 0.3284704   0.06858308  0.10638959  0.04864722 -0.35473182  0.34008567\n",
      "   0.18563746 -0.2125     -0.32889664]\n",
      " [ 0.19462862 -0.19372775  0.34150431  0.06670328 -0.24625213  0.29118564\n",
      "   0.37345143  0.48064392 -0.01483855]\n",
      " [-0.63548012  0.29821554 -0.57490744 -0.54984171 -0.0180417  -0.02907351\n",
      "  -0.59845598 -0.39458458  0.39275958]\n",
      " [ 0.68785834 -0.27420899  0.57308122  0.09075474 -0.09305182 -0.10818095\n",
      "   0.30431553  0.52389972 -0.1907131 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.42071894  0.98953673 -0.50459956  0.13984259 -0.03960191 -0.69981866\n",
      "   0.42832567]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:19 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.61614903]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 19 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.55359    -0.30615796  0.38753238  0.44011045 -0.19545873  0.16056415\n",
      "   0.94510513 -0.13959859 -0.25420205]\n",
      " [-0.37290168  0.20514167 -0.52517231 -0.42358453  0.12411674 -0.3519526\n",
      "  -0.1816804  -0.0148896   0.36962969]\n",
      " [ 0.33496582  0.0750785   0.10638959  0.05514264 -0.35473182  0.34008567\n",
      "   0.19213288 -0.2125     -0.32889664]\n",
      " [ 0.199398   -0.18895836  0.34150431  0.07147266 -0.24625213  0.29118564\n",
      "   0.37822081  0.48064392 -0.01483855]\n",
      " [-0.64562984  0.28806582 -0.57490744 -0.55999142 -0.0180417  -0.02907351\n",
      "  -0.60860569 -0.39458458  0.39275958]\n",
      " [ 0.69568477 -0.26638256  0.57308122  0.09858116 -0.09305182 -0.10818095\n",
      "   0.31214196  0.52389972 -0.1907131 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.37532676  1.02725318 -0.48997282  0.16947428 -0.01198029 -0.69143977\n",
      "   0.45972964]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:19 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.6972896]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 19 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.56021367 -0.30615796  0.39415606  0.44011045 -0.19545873  0.16718783\n",
      "   0.9517288  -0.13959859 -0.25420205]\n",
      " [-0.38002097  0.20514167 -0.53229159 -0.42358453  0.12411674 -0.35907189\n",
      "  -0.18879969 -0.0148896   0.36962969]\n",
      " [ 0.34115539  0.0750785   0.11257916  0.05514264 -0.35473182  0.34627524\n",
      "   0.19832245 -0.2125     -0.32889664]\n",
      " [ 0.20623844 -0.18895836  0.34834474  0.07147266 -0.24625213  0.29802608\n",
      "   0.38506125  0.48064392 -0.01483855]\n",
      " [-0.65255839  0.28806582 -0.581836   -0.55999142 -0.0180417  -0.03600206\n",
      "  -0.61553424 -0.39458458  0.39275958]\n",
      " [ 0.70282394 -0.26638256  0.58022039  0.09858116 -0.09305182 -0.10104178\n",
      "   0.31928113  0.52389972 -0.1907131 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.34337919  1.05554669 -0.48381228  0.19266283  0.01263064 -0.68712969\n",
      "   0.48571808]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:19 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.55576304]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 20 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.5530213  -0.31335033  0.39415606  0.43291808 -0.19545873  0.16718783\n",
      "   0.9517288  -0.13959859 -0.26139442]\n",
      " [-0.37614704  0.2090156  -0.53229159 -0.4197106   0.12411674 -0.35907189\n",
      "  -0.18879969 -0.0148896   0.37350362]\n",
      " [ 0.33872399  0.07264711  0.11257916  0.05271125 -0.35473182  0.34627524\n",
      "   0.19832245 -0.2125     -0.33132803]\n",
      " [ 0.20497243 -0.19022438  0.34834474  0.07020665 -0.24625213  0.29802608\n",
      "   0.38506125  0.48064392 -0.01610456]\n",
      " [-0.6440539   0.29657032 -0.581836   -0.55148693 -0.0180417  -0.03600206\n",
      "  -0.61553424 -0.39458458  0.40126407]\n",
      " [ 0.69709011 -0.27211639  0.58022039  0.09284734 -0.09305182 -0.10104178\n",
      "   0.31928113  0.52389972 -0.19644693]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.41198549  1.01381688 -0.5142076   0.15592005 -0.02293968 -0.7125219\n",
      "   0.44556714]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:20 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.66819448]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 20 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.56093292 -0.31335033  0.40206768  0.43291808 -0.19545873  0.16718783\n",
      "   0.95964043 -0.13959859 -0.26139442]\n",
      " [-0.38371956  0.2090156  -0.53986412 -0.4197106   0.12411674 -0.35907189\n",
      "  -0.19637221 -0.0148896   0.37350362]\n",
      " [ 0.34410926  0.07264711  0.11796443  0.05271125 -0.35473182  0.34627524\n",
      "   0.20370772 -0.2125     -0.33132803]\n",
      " [ 0.21194941 -0.19022438  0.35532173  0.07020665 -0.24625213  0.29802608\n",
      "   0.39203824  0.48064392 -0.01610456]\n",
      " [-0.65205616  0.29657032 -0.58983826 -0.55148693 -0.0180417  -0.03600206\n",
      "  -0.62353651 -0.39458458  0.40126407]\n",
      " [ 0.70531653 -0.27211639  0.58844681  0.09284734 -0.09305182 -0.10104178\n",
      "   0.32750755  0.52389972 -0.19644693]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.37520309  1.04580902 -0.50500252  0.18008337  0.00349848 -0.70748702\n",
      "   0.47615323]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:20 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.67519898]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 20 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.54438302 -0.32990024  0.38551778  0.43291808 -0.19545873  0.16718783\n",
      "   0.94309052 -0.13959859 -0.26139442]\n",
      " [-0.36992404  0.22281112 -0.5260686  -0.4197106   0.12411674 -0.35907189\n",
      "  -0.18257669 -0.0148896   0.37350362]\n",
      " [ 0.33214751  0.06068535  0.10600268  0.05271125 -0.35473182  0.34627524\n",
      "   0.19174597 -0.2125     -0.33132803]\n",
      " [ 0.19962779 -0.202546    0.34300011  0.07020665 -0.24625213  0.29802608\n",
      "   0.37971661  0.48064392 -0.01610456]\n",
      " [-0.63548508  0.3131414  -0.57326718 -0.55148693 -0.0180417  -0.03600206\n",
      "  -0.60696543 -0.39458458  0.40126407]\n",
      " [ 0.68897797 -0.28845495  0.57210824  0.09284734 -0.09305182 -0.10104178\n",
      "   0.31116899  0.52389972 -0.19644693]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.44924045  0.98411273 -0.52623699  0.12998583 -0.04709295 -0.72024949\n",
      "   0.41736821]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:20 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.58690584]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 20 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.55287974 -0.32990024  0.3940145   0.43291808 -0.19545873  0.16718783\n",
      "   0.94309052 -0.13110186 -0.26139442]\n",
      " [-0.37925458  0.22281112 -0.53539914 -0.4197106   0.12411674 -0.35907189\n",
      "  -0.18257669 -0.02422014  0.37350362]\n",
      " [ 0.33493681  0.06068535  0.10879198  0.05271125 -0.35473182  0.34627524\n",
      "   0.19174597 -0.2097107  -0.33132803]\n",
      " [ 0.20959379 -0.202546    0.3529661   0.07020665 -0.24625213  0.29802608\n",
      "   0.37971661  0.49060992 -0.01610456]\n",
      " [-0.64668185  0.3131414  -0.58446395 -0.55148693 -0.0180417  -0.03600206\n",
      "  -0.60696543 -0.40578136  0.40126407]\n",
      " [ 0.69997626 -0.28845495  0.58310654  0.09284734 -0.09305182 -0.10104178\n",
      "   0.31116899  0.53489802 -0.19644693]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.39916365  1.01856037 -0.511874    0.15783726 -0.01025599 -0.71186084\n",
      "   0.46024954]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:20 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.55862188]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 20 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.56154654 -0.32990024  0.3940145   0.44158488 -0.19545873  0.16718783\n",
      "   0.94309052 -0.13110186 -0.25272762]\n",
      " [-0.38478811  0.22281112 -0.53539914 -0.42524414  0.12411674 -0.35907189\n",
      "  -0.18257669 -0.02422014  0.36797009]\n",
      " [ 0.33570235  0.06068535  0.10879198  0.05347679 -0.35473182  0.34627524\n",
      "   0.19174597 -0.2097107  -0.33056249]\n",
      " [ 0.21311932 -0.202546    0.3529661   0.07373218 -0.24625213  0.29802608\n",
      "   0.37971661  0.49060992 -0.01257903]\n",
      " [-0.65596844  0.3131414  -0.58446395 -0.56077352 -0.0180417  -0.03600206\n",
      "  -0.60696543 -0.40578136  0.39197748]\n",
      " [ 0.70740839 -0.28845495  0.58310654  0.10027947 -0.09305182 -0.10104178\n",
      "   0.31116899  0.53489802 -0.18901479]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.34474979  1.05521224 -0.49036906  0.18581014  0.02051748 -0.69495511\n",
      "   0.49533706]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:20 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.53519076]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 20 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.55696838 -0.32990024  0.3940145   0.44158488 -0.20003689  0.16260966\n",
      "   0.94309052 -0.13110186 -0.25730579]\n",
      " [-0.38066384  0.22281112 -0.53539914 -0.42524414  0.12824101 -0.35494762\n",
      "  -0.18257669 -0.02422014  0.37209436]\n",
      " [ 0.33575755  0.06068535  0.10879198  0.05347679 -0.35467663  0.34633044\n",
      "   0.19174597 -0.2097107  -0.3305073 ]\n",
      " [ 0.20898647 -0.202546    0.3529661   0.07373218 -0.25038498  0.29389323\n",
      "   0.37971661  0.49060992 -0.01671188]\n",
      " [-0.65080736  0.3131414  -0.58446395 -0.56077352 -0.01288062 -0.03084098\n",
      "  -0.60696543 -0.40578136  0.39713856]\n",
      " [ 0.70215089 -0.28845495  0.58310654  0.10027947 -0.09830933 -0.10629928\n",
      "   0.31116899  0.53489802 -0.1942723 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.41131725  1.01729005 -0.51948481  0.1525816  -0.01694309 -0.72299031\n",
      "   0.45670318]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:20 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.43968951]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 20 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.55788354 -0.32898508  0.3940145   0.44158488 -0.19912173  0.16352482\n",
      "   0.94309052 -0.13110186 -0.25639063]\n",
      " [-0.38049507  0.22297989 -0.53539914 -0.42524414  0.12840978 -0.35477885\n",
      "  -0.18257669 -0.02422014  0.37226313]\n",
      " [ 0.33497841  0.05990622  0.10879198  0.05347679 -0.35545576  0.3455513\n",
      "   0.19174597 -0.2097107  -0.33128643]\n",
      " [ 0.20853655 -0.20299591  0.3529661   0.07373218 -0.2508349   0.29344331\n",
      "   0.37971661  0.49060992 -0.0171618 ]\n",
      " [-0.65102063  0.31292814 -0.58446395 -0.56077352 -0.01309388 -0.03105424\n",
      "  -0.60696543 -0.40578136  0.3969253 ]\n",
      " [ 0.7019503  -0.28865554  0.58310654  0.10027947 -0.09850992 -0.10649987\n",
      "   0.31116899  0.53489802 -0.19447289]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.46547878  0.99112514 -0.5463968   0.12472127 -0.04447386 -0.75028435\n",
      "   0.42942182]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:20 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.55127154]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 20 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 5.43630909e-01 -3.43237709e-01  3.94014504e-01  4.41584877e-01\n",
      "  -2.13374362e-01  1.49272193e-01  9.28837893e-01 -1.31101859e-01\n",
      "  -2.56390625e-01]\n",
      " [-3.71574101e-01  2.31900859e-01 -5.35399136e-01 -4.25244138e-01\n",
      "   1.37330750e-01 -3.45857880e-01 -1.73655723e-01 -2.42201422e-02\n",
      "   3.72263134e-01]\n",
      " [ 3.25921689e-01  5.08494945e-02  1.08791981e-01  5.34767857e-02\n",
      "  -3.64512485e-01  3.36494581e-01  1.82689246e-01 -2.09710702e-01\n",
      "  -3.31286434e-01]\n",
      " [ 2.01566991e-01 -2.09965476e-01  3.52966104e-01  7.37321829e-02\n",
      "  -2.57804460e-01  2.86473750e-01  3.72747052e-01  4.90609919e-01\n",
      "  -1.71617977e-02]\n",
      " [-6.37693462e-01  3.26255303e-01 -5.84463954e-01 -5.60773520e-01\n",
      "   2.33284677e-04 -1.77270751e-02 -5.93638263e-01 -4.05781355e-01\n",
      "   3.96925301e-01]\n",
      " [ 6.93667086e-01 -2.96938752e-01  5.83106537e-01  1.00279467e-01\n",
      "  -1.06793132e-01 -1.14783085e-01  3.02885777e-01  5.34898017e-01\n",
      "  -1.94472891e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.53366314  0.93950845 -0.57108321  0.08106189 -0.08575021 -0.76876701\n",
      "   0.38666886]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:20 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.57531735]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 20 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 5.54753511e-01 -3.43237709e-01  3.94014504e-01  4.52707480e-01\n",
      "  -2.13374362e-01  1.49272193e-01  9.39960496e-01 -1.31101859e-01\n",
      "  -2.56390625e-01]\n",
      " [-3.81607573e-01  2.31900859e-01 -5.35399136e-01 -4.35277609e-01\n",
      "   1.37330750e-01 -3.45857880e-01 -1.83689195e-01 -2.42201422e-02\n",
      "   3.72263134e-01]\n",
      " [ 3.32665259e-01  5.08494945e-02  1.08791981e-01  6.02203556e-02\n",
      "  -3.64512485e-01  3.36494581e-01  1.89432816e-01 -2.09710702e-01\n",
      "  -3.31286434e-01]\n",
      " [ 2.09148082e-01 -2.09965476e-01  3.52966104e-01  8.13132742e-02\n",
      "  -2.57804460e-01  2.86473750e-01  3.80328143e-01  4.90609919e-01\n",
      "  -1.71617977e-02]\n",
      " [-6.49075445e-01  3.26255303e-01 -5.84463954e-01 -5.72155503e-01\n",
      "   2.33284677e-04 -1.77270751e-02 -6.05020245e-01 -4.05781355e-01\n",
      "   3.96925301e-01]\n",
      " [ 7.04346177e-01 -2.96938752e-01  5.83106537e-01  1.10958558e-01\n",
      "  -1.06793132e-01 -1.14783085e-01  3.13564868e-01  5.34898017e-01\n",
      "  -1.94472891e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.48178236  0.9847212  -0.55682709  0.1141066  -0.05168684 -0.76135766\n",
      "   0.42556212]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:20 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.57467947]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 20 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.5663356  -0.34323771  0.3940145   0.45270748 -0.20179228  0.16085428\n",
      "   0.95154258 -0.13110186 -0.25639063]\n",
      " [-0.39029631  0.23190086 -0.53539914 -0.43527761  0.12864202 -0.35454661\n",
      "  -0.19237793 -0.02422014  0.37226313]\n",
      " [ 0.33870929  0.05084949  0.10879198  0.06022036 -0.35846846  0.34253861\n",
      "   0.19547685 -0.2097107  -0.33128643]\n",
      " [ 0.21645976 -0.20996548  0.3529661   0.08131327 -0.25049279  0.29378542\n",
      "   0.38763982  0.49060992 -0.0171618 ]\n",
      " [-0.66037989  0.3262553  -0.58446395 -0.5721555  -0.01107117 -0.02903153\n",
      "  -0.6163247  -0.40578136  0.3969253 ]\n",
      " [ 0.7132128  -0.29693875  0.58310654  0.11095856 -0.09792651 -0.10591647\n",
      "   0.32243149  0.53489802 -0.19447289]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.42980331  1.02666812 -0.54041978  0.14638908 -0.01791102 -0.74997502\n",
      "   0.46138556]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:20 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57014383]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 20 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.55320881 -0.34323771  0.3940145   0.4395807  -0.20179228  0.1477275\n",
      "   0.95154258 -0.13110186 -0.26951741]\n",
      " [-0.37825912  0.23190086 -0.53539914 -0.42324042  0.12864202 -0.34250943\n",
      "  -0.19237793 -0.02422014  0.38430032]\n",
      " [ 0.33183801  0.05084949  0.10879198  0.05334908 -0.35846846  0.33566733\n",
      "   0.19547685 -0.2097107  -0.33815771]\n",
      " [ 0.2072112  -0.20996548  0.3529661   0.07206472 -0.25049279  0.28453687\n",
      "   0.38763982  0.49060992 -0.02641035]\n",
      " [-0.6477826   0.3262553  -0.58446395 -0.55955821 -0.01107117 -0.01643423\n",
      "  -0.6163247  -0.40578136  0.40952259]\n",
      " [ 0.7046641  -0.29693875  0.58310654  0.10240986 -0.09792651 -0.11446517\n",
      "   0.32243149  0.53489802 -0.20302159]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.49966869  0.97666155 -0.56196257  0.1043908  -0.06260929 -0.77068105\n",
      "   0.41750789]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:20 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.59079435]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 20 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.56412852 -0.332318    0.3940145   0.4505004  -0.20179228  0.1586472\n",
      "   0.96246229 -0.13110186 -0.26951741]\n",
      " [-0.38847258  0.2216874  -0.53539914 -0.43345388  0.12864202 -0.35272289\n",
      "  -0.20259139 -0.02422014  0.38430032]\n",
      " [ 0.34138577  0.06039725  0.10879198  0.06289683 -0.35846846  0.34521509\n",
      "   0.2050246  -0.2097107  -0.33815771]\n",
      " [ 0.21522721 -0.20194946  0.3529661   0.08008073 -0.25049279  0.29255288\n",
      "   0.39565583  0.49060992 -0.02641035]\n",
      " [-0.65885273  0.31518518 -0.58446395 -0.57062834 -0.01107117 -0.02750436\n",
      "  -0.62739482 -0.40578136  0.40952259]\n",
      " [ 0.71249101 -0.28911184  0.58310654  0.11023677 -0.09792651 -0.10663825\n",
      "   0.3302584   0.53489802 -0.20302159]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.45020465  1.01879522 -0.54965096  0.14023033 -0.02910626 -0.76175924\n",
      "   0.45075706]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:20 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.62038302]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 20 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.57410068 -0.32234584  0.3940145   0.46047257 -0.20179228  0.1586472\n",
      "   0.97243445 -0.13110186 -0.26951741]\n",
      " [-0.39614103  0.21401895 -0.53539914 -0.44112233  0.12864202 -0.35272289\n",
      "  -0.21025984 -0.02422014  0.38430032]\n",
      " [ 0.34808974  0.06710123  0.10879198  0.06960081 -0.35846846  0.34521509\n",
      "   0.21172858 -0.2097107  -0.33815771]\n",
      " [ 0.220378   -0.19679867  0.3529661   0.08523152 -0.25049279  0.29255288\n",
      "   0.40080662  0.49060992 -0.02641035]\n",
      " [-0.66886009  0.30517781 -0.58446395 -0.5806357  -0.01107117 -0.02750436\n",
      "  -0.63740218 -0.40578136  0.40952259]\n",
      " [ 0.7205464  -0.28105645  0.58310654  0.11829216 -0.09792651 -0.10663825\n",
      "   0.33831379  0.53489802 -0.20302159]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.40550324  1.0562632  -0.53581943  0.16979749 -0.00139701 -0.75387884\n",
      "   0.48220314]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:20 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.70286271]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 20 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.58043534 -0.32234584  0.40034916  0.46047257 -0.20179228  0.16498186\n",
      "   0.97876911 -0.13110186 -0.26951741]\n",
      " [-0.40308135  0.21401895 -0.54233945 -0.44112233  0.12864202 -0.3596632\n",
      "  -0.21720015 -0.02422014  0.38430032]\n",
      " [ 0.35423495  0.06710123  0.11493719  0.06960081 -0.35846846  0.3513603\n",
      "   0.21787379 -0.2097107  -0.33815771]\n",
      " [ 0.22711854 -0.19679867  0.35970665  0.08523152 -0.25049279  0.29929342\n",
      "   0.40754716  0.49060992 -0.02641035]\n",
      " [-0.67550598  0.30517781 -0.59110984 -0.5806357  -0.01107117 -0.03415024\n",
      "  -0.64404807 -0.40578136  0.40952259]\n",
      " [ 0.72749254 -0.28105645  0.59005268  0.11829216 -0.09792651 -0.09969211\n",
      "   0.34525993  0.53489802 -0.20302159]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.37447517  1.08390378 -0.53013373  0.1925649   0.02281041 -0.74990542\n",
      "   0.50773256]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:20 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.54821899]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 21 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.57318557 -0.32959561  0.40034916  0.4532228  -0.20179228  0.16498186\n",
      "   0.97876911 -0.13110186 -0.27676718]\n",
      " [-0.39897052  0.21812978 -0.54233945 -0.43701151  0.12864202 -0.3596632\n",
      "  -0.21720015 -0.02422014  0.38841115]\n",
      " [ 0.35165698  0.06452325  0.11493719  0.06702283 -0.35846846  0.3513603\n",
      "   0.21787379 -0.2097107  -0.34073569]\n",
      " [ 0.2256086  -0.19830862  0.35970665  0.08372157 -0.25049279  0.29929342\n",
      "   0.40754716  0.49060992 -0.0279203 ]\n",
      " [-0.66695827  0.31372552 -0.59110984 -0.572088   -0.01107117 -0.03415024\n",
      "  -0.64404807 -0.40578136  0.4180703 ]\n",
      " [ 0.72154997 -0.28699903  0.59005268  0.11234958 -0.09792651 -0.09969211\n",
      "   0.34525993  0.53489802 -0.20896416]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.44236522  1.04246288 -0.55992638  0.15603186 -0.01264657 -0.77487893\n",
      "   0.46771453]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:21 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.67335155]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 21 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.58081961 -0.32959561  0.4079832   0.4532228  -0.20179228  0.16498186\n",
      "   0.98640314 -0.13110186 -0.27676718]\n",
      " [-0.40653847  0.21812978 -0.5499074  -0.43701151  0.12864202 -0.3596632\n",
      "  -0.2247681  -0.02422014  0.38841115]\n",
      " [ 0.35713675  0.06452325  0.12041697  0.06702283 -0.35846846  0.3513603\n",
      "   0.22335357 -0.2097107  -0.34073569]\n",
      " [ 0.23264418 -0.19830862  0.36674223  0.08372157 -0.25049279  0.29929342\n",
      "   0.41458274  0.49060992 -0.0279203 ]\n",
      " [-0.67467976  0.31372552 -0.59883132 -0.572088   -0.01107117 -0.03415024\n",
      "  -0.65176955 -0.40578136  0.4180703 ]\n",
      " [ 0.72955682 -0.28699903  0.59805954  0.11234958 -0.09792651 -0.09969211\n",
      "   0.35326679  0.53489802 -0.20896416]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.40644218  1.0739206  -0.551343    0.17991117  0.01356479 -0.77021364\n",
      "   0.49788329]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:21 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.67797352]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 21 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.56431024 -0.34610497  0.39147384  0.4532228  -0.20179228  0.16498186\n",
      "   0.96989378 -0.13110186 -0.27676718]\n",
      " [-0.39228722  0.23238103 -0.53565615 -0.43701151  0.12864202 -0.3596632\n",
      "  -0.21051685 -0.02422014  0.38841115]\n",
      " [ 0.34486191  0.0522484   0.10814212  0.06702283 -0.35846846  0.3513603\n",
      "   0.21107872 -0.2097107  -0.34073569]\n",
      " [ 0.21980847 -0.21114434  0.35390651  0.08372157 -0.25049279  0.29929342\n",
      "   0.40174703  0.49060992 -0.0279203 ]\n",
      " [-0.65813781  0.33026746 -0.58228938 -0.572088   -0.01107117 -0.03415024\n",
      "  -0.63522761 -0.40578136  0.4180703 ]\n",
      " [ 0.71312628 -0.30342957  0.58162899  0.11234958 -0.09792651 -0.09969211\n",
      "   0.33683624  0.53489802 -0.20896416]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.48045161  1.01187873 -0.57178892  0.12939739 -0.03774702 -0.7825268\n",
      "   0.43858608]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:21 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.58898029]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 21 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.57300353 -0.34610497  0.40016712  0.4532228  -0.20179228  0.16498186\n",
      "   0.96989378 -0.12240857 -0.27676718]\n",
      " [-0.40180468  0.23238103 -0.54517361 -0.43701151  0.12864202 -0.3596632\n",
      "  -0.21051685 -0.0337376   0.38841115]\n",
      " [ 0.34784355  0.0522484   0.11112376  0.06702283 -0.35846846  0.3513603\n",
      "   0.21107872 -0.20672906 -0.34073569]\n",
      " [ 0.22990612 -0.21114434  0.36400417  0.08372157 -0.25049279  0.29929342\n",
      "   0.40174703  0.50070758 -0.0279203 ]\n",
      " [-0.66923522  0.33026746 -0.59338678 -0.572088   -0.01107117 -0.03415024\n",
      "  -0.63522761 -0.41687876  0.4180703 ]\n",
      " [ 0.72397277 -0.30342957  0.59247548  0.11234958 -0.09792651 -0.09969211\n",
      "   0.33683624  0.54574451 -0.20896416]]\n",
      "\n",
      "Theta two: \n",
      "[[-4.30701265e-01  1.04646662e+00 -5.57935945e-01  1.57283707e-01\n",
      "  -7.56835723e-04 -7.74486045e-01  4.81456960e-01]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:21 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.55513334]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 21 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.58197588 -0.34610497  0.40016712  0.46219515 -0.20179228  0.16498186\n",
      "   0.96989378 -0.12240857 -0.26779483]\n",
      " [-0.40768672  0.23238103 -0.54517361 -0.44289355  0.12864202 -0.3596632\n",
      "  -0.21051685 -0.0337376   0.3825291 ]\n",
      " [ 0.34886019  0.0522484   0.11112376  0.06803947 -0.35846846  0.3513603\n",
      "   0.21107872 -0.20672906 -0.33971905]\n",
      " [ 0.23375076 -0.21114434  0.36400417  0.08756621 -0.25049279  0.29929342\n",
      "   0.40174703  0.50070758 -0.02407566]\n",
      " [-0.67882268  0.33026746 -0.59338678 -0.58167546 -0.01107117 -0.03415024\n",
      "  -0.63522761 -0.41687876  0.40848283]\n",
      " [ 0.73179325 -0.30342957  0.59247548  0.12017006 -0.09792651 -0.09969211\n",
      "   0.33683624  0.54574451 -0.20114369]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.37576906  1.08376893 -0.53655279  0.18576738  0.03060643 -0.75772768\n",
      "   0.51726673]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:21 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.52464641]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 21 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.57752553 -0.34610497  0.40016712  0.46219515 -0.20624263  0.16053151\n",
      "   0.96989378 -0.12240857 -0.27224518]\n",
      " [-0.40356482  0.23238103 -0.54517361 -0.44289355  0.13276392 -0.3555413\n",
      "  -0.21051685 -0.0337376   0.386651  ]\n",
      " [ 0.34882694  0.0522484   0.11112376  0.06803947 -0.35850171  0.35132705\n",
      "   0.21107872 -0.20672906 -0.3397523 ]\n",
      " [ 0.22959313 -0.21114434  0.36400417  0.08756621 -0.25465042  0.29513579\n",
      "   0.40174703  0.50070758 -0.02823329]\n",
      " [-0.67378794  0.33026746 -0.59338678 -0.58167546 -0.00603643 -0.02911551\n",
      "  -0.63522761 -0.41687876  0.41351757]\n",
      " [ 0.72649468 -0.30342957  0.59247548  0.12017006 -0.10322508 -0.10499067\n",
      "   0.33683624  0.54574451 -0.20644225]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.44119051  1.04655057 -0.56509638  0.15302341 -0.00630838 -0.78531969\n",
      "   0.47915895]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:21 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.42419123]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 21 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.57864418 -0.34498632  0.40016712  0.46219515 -0.20512398  0.16165016\n",
      "   0.96989378 -0.12240857 -0.27112653]\n",
      " [-0.40347015  0.2324757  -0.54517361 -0.44289355  0.13285859 -0.35544663\n",
      "  -0.21051685 -0.0337376   0.38674568]\n",
      " [ 0.34812616  0.05154763  0.11112376  0.06803947 -0.35920248  0.35062627\n",
      "   0.21107872 -0.20672906 -0.34045307]\n",
      " [ 0.22919561 -0.21154186  0.36400417  0.08756621 -0.25504794  0.29473827\n",
      "   0.40174703  0.50070758 -0.02863081]\n",
      " [-0.6742391   0.32981631 -0.59338678 -0.58167546 -0.00648758 -0.02956666\n",
      "  -0.63522761 -0.41687876  0.41306642]\n",
      " [ 0.7263858  -0.30353845  0.59247548  0.12017006 -0.10333396 -0.10509955\n",
      "   0.33683624  0.54574451 -0.20655113]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.49299551  1.02176812 -0.59090421  0.12641979 -0.03260847 -0.81167343\n",
      "   0.45314757]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:21 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.55218236]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 21 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.56425377 -0.35937673  0.40016712  0.46219515 -0.21951438  0.14725975\n",
      "   0.95550337 -0.12240857 -0.27112653]\n",
      " [-0.39404582  0.24190003 -0.54517361 -0.44289355  0.14228292 -0.3460223\n",
      "  -0.20109252 -0.0337376   0.38674568]\n",
      " [ 0.33872655  0.04214801  0.11112376  0.06803947 -0.3686021   0.34122666\n",
      "   0.2016791  -0.20672906 -0.34045307]\n",
      " [ 0.22175875 -0.21897872  0.36400417  0.08756621 -0.26248481  0.2873014\n",
      "   0.39431016  0.50070758 -0.02863081]\n",
      " [-0.66070464  0.34335076 -0.59338678 -0.58167546  0.00704687 -0.01603221\n",
      "  -0.62169316 -0.41687876  0.41306642]\n",
      " [ 0.71765727 -0.31226699  0.59247548  0.12017006 -0.11206249 -0.11382809\n",
      "   0.3281077   0.54574451 -0.20655113]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.56126651  0.96979021 -0.61503161  0.08230621 -0.07444483 -0.8298243\n",
      "   0.4098347 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:21 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.58292562]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 21 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.57494976 -0.35937673  0.40016712  0.47289113 -0.21951438  0.14725975\n",
      "   0.96619935 -0.12240857 -0.27112653]\n",
      " [-0.40421007  0.24190003 -0.54517361 -0.45305779  0.14228292 -0.3460223\n",
      "  -0.21125677 -0.0337376   0.38674568]\n",
      " [ 0.3457667   0.04214801  0.11112376  0.07507963 -0.3686021   0.34122666\n",
      "   0.20871925 -0.20672906 -0.34045307]\n",
      " [ 0.22965855 -0.21897872  0.36400417  0.09546602 -0.26248481  0.2873014\n",
      "   0.40220997  0.50070758 -0.02863081]\n",
      " [-0.67168768  0.34335076 -0.59338678 -0.5926585   0.00704687 -0.01603221\n",
      "  -0.6326762  -0.41687876  0.41306642]\n",
      " [ 0.72836511 -0.31226699  0.59247548  0.1308779  -0.11206249 -0.11382809\n",
      "   0.33881555  0.54574451 -0.20655113]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.51056625  1.01435011 -0.60177198  0.11513899 -0.04052671 -0.82301885\n",
      "   0.44848906]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:21 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.57799684]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 21 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.58644842 -0.35937673  0.40016712  0.47289113 -0.20801572  0.15875842\n",
      "   0.97769802 -0.12240857 -0.27112653]\n",
      " [-0.41316263  0.24190003 -0.54517361 -0.45305779  0.13333036 -0.35497486\n",
      "  -0.22020933 -0.0337376   0.38674568]\n",
      " [ 0.35209876  0.04214801  0.11112376  0.07507963 -0.36227004  0.34755871\n",
      "   0.21505131 -0.20672906 -0.34045307]\n",
      " [ 0.23725868 -0.21897872  0.36400417  0.09546602 -0.25488467  0.29490154\n",
      "   0.4098101   0.50070758 -0.02863081]\n",
      " [-0.68297679  0.34335076 -0.59338678 -0.5926585  -0.00424224 -0.02732132\n",
      "  -0.64396531 -0.41687876  0.41306642]\n",
      " [ 0.7374796  -0.31226699  0.59247548  0.1308779  -0.102948   -0.1047136\n",
      "   0.34793003  0.54574451 -0.20655113]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.45909949  1.05618689 -0.58602664  0.14750175 -0.00663505 -0.81211166\n",
      "   0.48445078]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:21 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.56986579]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 21 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.57312958 -0.35937673  0.40016712  0.45957229 -0.20801572  0.14543958\n",
      "   0.97769802 -0.12240857 -0.28444537]\n",
      " [-0.4008612   0.24190003 -0.54517361 -0.44075637  0.13333036 -0.34267343\n",
      "  -0.22020933 -0.0337376   0.3990471 ]\n",
      " [ 0.34486249  0.04214801  0.11112376  0.06784336 -0.36227004  0.34032244\n",
      "   0.21505131 -0.20672906 -0.34768934]\n",
      " [ 0.22768463 -0.21897872  0.36400417  0.08589197 -0.25488467  0.28532748\n",
      "   0.4098101   0.50070758 -0.03820487]\n",
      " [-0.6701502   0.34335076 -0.59338678 -0.5798319  -0.00424224 -0.01449472\n",
      "  -0.64396531 -0.41687876  0.42589301]\n",
      " [ 0.72846975 -0.31226699  0.59247548  0.12186805 -0.102948   -0.11372345\n",
      "   0.34793003  0.54574451 -0.21556098]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.52894189  1.00586505 -0.60716829  0.10511466 -0.05171318 -0.83244505\n",
      "   0.44004641]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:21 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.59573474]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 21 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.58380215 -0.34870416  0.40016712  0.47024486 -0.20801572  0.15611215\n",
      "   0.98837059 -0.12240857 -0.28444537]\n",
      " [-0.41113102  0.23163021 -0.54517361 -0.45102619  0.13333036 -0.35294325\n",
      "  -0.23047915 -0.0337376   0.3990471 ]\n",
      " [ 0.35448577  0.05177129  0.11112376  0.07746664 -0.36227004  0.34994573\n",
      "   0.22467459 -0.20672906 -0.34768934]\n",
      " [ 0.23594029 -0.21072306  0.36400417  0.09414763 -0.25488467  0.29358314\n",
      "   0.41806576  0.50070758 -0.03820487]\n",
      " [-0.68104659  0.33245437 -0.59338678 -0.5907283  -0.00424224 -0.02539112\n",
      "  -0.6548617  -0.41687876  0.42589301]\n",
      " [ 0.73659548 -0.30414126  0.59247548  0.12999379 -0.102948   -0.10559772\n",
      "   0.35605576  0.54574451 -0.21556098]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.4802613   1.04761925 -0.59557281  0.14080068 -0.01823192 -0.82402734\n",
      "   0.47334458]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:21 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.62475305]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 21 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.59357451 -0.3389318   0.40016712  0.48001722 -0.20801572  0.15611215\n",
      "   0.99814295 -0.12240857 -0.28444537]\n",
      " [-0.41904035  0.22372088 -0.54517361 -0.45893552  0.13333036 -0.35294325\n",
      "  -0.23838848 -0.0337376   0.3990471 ]\n",
      " [ 0.36137463  0.05866016  0.11112376  0.08435551 -0.36227004  0.34994573\n",
      "   0.23156346 -0.20672906 -0.34768934]\n",
      " [ 0.24144312 -0.20522023  0.36400417  0.09965045 -0.25488467  0.29358314\n",
      "   0.42356859  0.50070758 -0.03820487]\n",
      " [-0.69088492  0.32261604 -0.59338678 -0.60056662 -0.00424224 -0.02539112\n",
      "  -0.66470003 -0.41687876  0.42589301]\n",
      " [ 0.74483278 -0.29590395  0.59247548  0.13823109 -0.102948   -0.10559772\n",
      "   0.36429307  0.54574451 -0.21556098]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.43627548  1.0847746  -0.58250334  0.17027325  0.00953257 -0.81660269\n",
      "   0.50478288]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:21 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.70838118]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 21 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.59963004 -0.3389318   0.40622265  0.48001722 -0.20801572  0.16216767\n",
      "   1.00419848 -0.12240857 -0.28444537]\n",
      " [-0.42578322  0.22372088 -0.55191648 -0.45893552  0.13333036 -0.35968612\n",
      "  -0.24513135 -0.0337376   0.3990471 ]\n",
      " [ 0.36745929  0.05866016  0.11720841  0.08435551 -0.36227004  0.35603038\n",
      "   0.23764811 -0.20672906 -0.34768934]\n",
      " [ 0.24806114 -0.20522023  0.3706222   0.09965045 -0.25488467  0.30020117\n",
      "   0.43018661  0.50070758 -0.03820487]\n",
      " [-0.69725151  0.32261604 -0.59975338 -0.60056662 -0.00424224 -0.03175771\n",
      "  -0.67106662 -0.41687876  0.42589301]\n",
      " [ 0.7515695  -0.29590395  0.5992122   0.13823109 -0.102948   -0.09886101\n",
      "   0.37102979  0.54574451 -0.21556098]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.40615457  1.11174732 -0.57725366  0.19260923  0.03331646 -0.81293032\n",
      "   0.52982718]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:21 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.54038095]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 22 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.59236027 -0.34620156  0.40622265  0.47274745 -0.20801572  0.16216767\n",
      "   1.00419848 -0.12240857 -0.29171513]\n",
      " [-0.42146307  0.22804103 -0.55191648 -0.45461537  0.13333036 -0.35968612\n",
      "  -0.24513135 -0.0337376   0.40336725]\n",
      " [ 0.36474628  0.05594715  0.11720841  0.0816425  -0.36227004  0.35603038\n",
      "   0.23764811 -0.20672906 -0.35040234]\n",
      " [ 0.2463163  -0.20696507  0.3706222   0.09790561 -0.25488467  0.30020117\n",
      "   0.43018661  0.50070758 -0.03994971]\n",
      " [-0.68869754  0.33117001 -0.59975338 -0.59201265 -0.00424224 -0.03175771\n",
      "  -0.67106662 -0.41687876  0.43444698]\n",
      " [ 0.74544407 -0.30202938  0.5992122   0.13210567 -0.102948   -0.09886101\n",
      "   0.37102979  0.54574451 -0.2216864 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.47326161  1.07066915 -0.60643746  0.15633071 -0.00198507 -0.83749316\n",
      "   0.49000106]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:22 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.67857607]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 22 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.59971558 -0.34620156  0.41357795  0.47274745 -0.20801572  0.16216767\n",
      "   1.01155379 -0.12240857 -0.29171513]\n",
      " [-0.42898575  0.22804103 -0.55943917 -0.45461537  0.13333036 -0.35968612\n",
      "  -0.25265403 -0.0337376   0.40336725]\n",
      " [ 0.37030159  0.05594715  0.12276372  0.0816425  -0.36227004  0.35603038\n",
      "   0.24320342 -0.20672906 -0.35040234]\n",
      " [ 0.25337439 -0.20696507  0.37768029  0.09790561 -0.25488467  0.30020117\n",
      "   0.43724471  0.50070758 -0.03994971]\n",
      " [-0.69613375  0.33117001 -0.60718958 -0.59201265 -0.00424224 -0.03175771\n",
      "  -0.67850282 -0.41687876  0.43444698]\n",
      " [ 0.75321376 -0.30202938  0.60698188  0.13210567 -0.102948   -0.09886101\n",
      "   0.37879947  0.54574451 -0.2216864 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.43820863  1.10155394 -0.59843841  0.17990452  0.02396194 -0.83316344\n",
      "   0.519711  ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:22 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.68072781]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 22 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.58325738 -0.36265977  0.39711975  0.47274745 -0.20801572  0.16216767\n",
      "   0.99509558 -0.12240857 -0.29171513]\n",
      " [-0.41434098  0.2426858  -0.5447944  -0.45461537  0.13333036 -0.35968612\n",
      "  -0.23800926 -0.0337376   0.40336725]\n",
      " [ 0.35772882  0.04337438  0.11019095  0.0816425  -0.36227004  0.35603038\n",
      "   0.23063065 -0.20672906 -0.35040234]\n",
      " [ 0.24006946 -0.22027001  0.36437535  0.09790561 -0.25488467  0.30020117\n",
      "   0.42393977  0.50070758 -0.03994971]\n",
      " [-0.6796382   0.34766556 -0.59069403 -0.59201265 -0.00424224 -0.03175771\n",
      "  -0.66200728 -0.41687876  0.43444698]\n",
      " [ 0.73672211 -0.31852102  0.59049023  0.13210567 -0.102948   -0.09886101\n",
      "   0.36230782  0.54574451 -0.2216864 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.51218245  1.03921434 -0.61814399  0.12898783 -0.02803717 -0.84507473\n",
      "   0.4599413 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:22 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.59121989]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 22 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.59211924 -0.36265977  0.40598162  0.47274745 -0.20801572  0.16216767\n",
      "   0.99509558 -0.11354671 -0.29171513]\n",
      " [-0.42401547  0.2426858  -0.55446889 -0.45461537  0.13333036 -0.35968612\n",
      "  -0.23800926 -0.0434121   0.40336725]\n",
      " [ 0.36089993  0.04337438  0.11336206  0.0816425  -0.36227004  0.35603038\n",
      "   0.23063065 -0.20355796 -0.35040234]\n",
      " [ 0.25027176 -0.22027001  0.37457766  0.09790561 -0.25488467  0.30020117\n",
      "   0.42393977  0.51090988 -0.03994971]\n",
      " [-0.69061964  0.34766556 -0.60167547 -0.59201265 -0.00424224 -0.03175771\n",
      "  -0.66200728 -0.4278602   0.43444698]\n",
      " [ 0.74740403 -0.31852102  0.60117215  0.13210567 -0.102948   -0.09886101\n",
      "   0.36230782  0.55642643 -0.2216864 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.46278568  1.07390253 -0.60478982  0.15689349  0.00907088 -0.83736177\n",
      "   0.50275843]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:22 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.55170434]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 22 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.60138037 -0.36265977  0.40598162  0.48200858 -0.20801572  0.16216767\n",
      "   0.99509558 -0.11354671 -0.28245401]\n",
      " [-0.43024397  0.2426858  -0.55446889 -0.46084387  0.13333036 -0.35968612\n",
      "  -0.23800926 -0.0434121   0.39713876]\n",
      " [ 0.36217423  0.04337438  0.11336206  0.08291681 -0.36227004  0.35603038\n",
      "   0.23063065 -0.20355796 -0.34912804]\n",
      " [ 0.25444375 -0.22027001  0.37457766  0.1020776  -0.25488467  0.30020117\n",
      "   0.42393977  0.51090988 -0.03577772]\n",
      " [-0.70049064  0.34766556 -0.60167547 -0.60188366 -0.00424224 -0.03175771\n",
      "  -0.66200728 -0.4278602   0.42457598]\n",
      " [ 0.75560179 -0.31852102  0.60117215  0.14030342 -0.102948   -0.09886101\n",
      "   0.36230782  0.55642643 -0.21348865]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.40734795  1.11183316 -0.58353659  0.18588847  0.0410281  -0.82074078\n",
      "   0.53927921]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:22 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.5136319]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 22 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 5.97081095e-01 -3.62659769e-01  4.05981617e-01  4.82008577e-01\n",
      "  -2.12314987e-01  1.57868403e-01  9.95095583e-01 -1.13546708e-01\n",
      "  -2.86753279e-01]\n",
      " [-4.26151720e-01  2.42685799e-01 -5.54468887e-01 -4.60843868e-01\n",
      "   1.37422613e-01 -3.55593865e-01 -2.38009262e-01 -4.34120962e-02\n",
      "   4.01231007e-01]\n",
      " [ 3.62065059e-01  4.33743849e-02  1.13362057e-01  8.29168069e-02\n",
      "  -3.62379212e-01  3.55921210e-01  2.30630647e-01 -2.03557958e-01\n",
      "  -3.49237210e-01]\n",
      " [ 2.50282630e-01 -2.20270010e-01  3.74577658e-01  1.02077598e-01\n",
      "  -2.59045791e-01  2.96040050e-01  4.23939768e-01  5.10909884e-01\n",
      "  -3.99388401e-02]\n",
      " [-6.95607558e-01  3.47665560e-01 -6.01675474e-01 -6.01883657e-01\n",
      "   6.40846131e-04 -2.68746303e-02 -6.62007277e-01 -4.27860197e-01\n",
      "   4.29459061e-01]\n",
      " [ 7.50298648e-01 -3.18521024e-01  6.01172152e-01  1.40303424e-01\n",
      "  -1.08251142e-01 -1.04164143e-01  3.62307821e-01  5.56426430e-01\n",
      "  -2.18791787e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.47150421  1.07540212 -0.6114764   0.15370116  0.00474036 -0.84785626\n",
      "   0.50179499]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:22 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.40813761]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 22 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 5.98393267e-01 -3.61347597e-01  4.05981617e-01  4.82008577e-01\n",
      "  -2.11002816e-01  1.59180575e-01  9.95095583e-01 -1.13546708e-01\n",
      "  -2.85441108e-01]\n",
      " [-4.26146715e-01  2.42690805e-01 -5.54468887e-01 -4.60843868e-01\n",
      "   1.37427619e-01 -3.55588859e-01 -2.38009262e-01 -4.34120962e-02\n",
      "   4.01236013e-01]\n",
      " [ 3.61452401e-01  4.27617269e-02  1.13362057e-01  8.29168069e-02\n",
      "  -3.62991870e-01  3.55308552e-01  2.30630647e-01 -2.03557958e-01\n",
      "  -3.49849868e-01]\n",
      " [ 2.49949111e-01 -2.20603530e-01  3.74577658e-01  1.02077598e-01\n",
      "  -2.59379310e-01  2.95706531e-01  4.23939768e-01  5.10909884e-01\n",
      "  -4.02723596e-02]\n",
      " [-6.96288336e-01  3.46984781e-01 -6.01675474e-01 -6.01883657e-01\n",
      "  -3.99326430e-05 -2.75554091e-02 -6.62007277e-01 -4.27860197e-01\n",
      "   4.28778282e-01]\n",
      " [ 7.50291616e-01 -3.18528056e-01  6.01172152e-01  1.40303424e-01\n",
      "  -1.08258174e-01 -1.04171175e-01  3.62307821e-01  5.56426430e-01\n",
      "  -2.18798818e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.52079934  1.05206922 -0.63611896  0.12844069 -0.02024077 -0.87318495\n",
      "   0.4771404 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:22 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.55314454]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 22 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.58388764 -0.37585322  0.40598162  0.48200858 -0.22550844  0.14467495\n",
      "   0.98058996 -0.11354671 -0.28544111]\n",
      " [-0.41626123  0.25257629 -0.55446889 -0.46084387  0.14731311 -0.34570337\n",
      "  -0.22812378 -0.0434121   0.40123601]\n",
      " [ 0.35172246  0.03303178  0.11336206  0.08291681 -0.37272181  0.34557861\n",
      "   0.2209007  -0.20355796 -0.34984987]\n",
      " [ 0.2420635  -0.22848914  0.37457766  0.1020776  -0.26726492  0.28782092\n",
      "   0.41605416  0.51090988 -0.04027236]\n",
      " [-0.68257751  0.36069561 -0.60167547 -0.60188366  0.0136709  -0.01384458\n",
      "  -0.64829645 -0.4278602   0.42877828]\n",
      " [ 0.74114658 -0.32767309  0.60117215  0.14030342 -0.11740321 -0.11331621\n",
      "   0.35316279  0.55642643 -0.21879882]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.58916127  0.99977199 -0.65972645  0.08387927 -0.0626262  -0.8910521\n",
      "   0.43328997]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:22 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.59088445]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 22 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.59414093 -0.37585322  0.40598162  0.49226186 -0.22550844  0.14467495\n",
      "   0.99084325 -0.11354671 -0.28544111]\n",
      " [-0.4264748   0.25257629 -0.55446889 -0.47105744  0.14731311 -0.34570337\n",
      "  -0.23833735 -0.0434121   0.40123601]\n",
      " [ 0.35901465  0.03303178  0.11336206  0.090209   -0.37272181  0.34557861\n",
      "   0.2281929  -0.20355796 -0.34984987]\n",
      " [ 0.25022444 -0.22848914  0.37457766  0.11023854 -0.26726492  0.28782092\n",
      "   0.4242151   0.51090988 -0.04027236]\n",
      " [-0.69313439  0.36069561 -0.60167547 -0.61244054  0.0136709  -0.01384458\n",
      "  -0.65885333 -0.4278602   0.42877828]\n",
      " [ 0.751805   -0.32767309  0.60117215  0.15096184 -0.11740321 -0.11331621\n",
      "   0.36382121  0.55642643 -0.21879882]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.53971147  1.04356434 -0.64742525  0.11642999 -0.0289313  -0.88479938\n",
      "   0.47159501]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:22 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.58147443]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 22 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.60553563 -0.37585322  0.40598162  0.49226186 -0.21411374  0.15606965\n",
      "   1.00223794 -0.11354671 -0.28544111]\n",
      " [-0.43564736  0.25257629 -0.55446889 -0.47105744  0.13814054 -0.35487593\n",
      "  -0.24750991 -0.0434121   0.40123601]\n",
      " [ 0.36561402  0.03303178  0.11336206  0.090209   -0.36612245  0.35217797\n",
      "   0.23479226 -0.20355796 -0.34984987]\n",
      " [ 0.25808487 -0.22848914  0.37457766  0.11023854 -0.2594045   0.29568135\n",
      "   0.43207553  0.51090988 -0.04027236]\n",
      " [-0.704378    0.36069561 -0.60167547 -0.61244054  0.00242729 -0.02508819\n",
      "  -0.67009694 -0.4278602   0.42877828]\n",
      " [ 0.76112465 -0.32767309  0.60117215  0.15096184 -0.10808355 -0.10399655\n",
      "   0.37314086  0.55642643 -0.21879882]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.48878488  1.08523211 -0.63231682  0.1488431   0.00504067 -0.87433309\n",
      "   0.50764279]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:22 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.56965233]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 22 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.59204796 -0.37585322  0.40598162  0.4787742  -0.21411374  0.14258198\n",
      "   1.00223794 -0.11354671 -0.29892877]\n",
      " [-0.42309808  0.25257629 -0.55446889 -0.45850815  0.13814054 -0.34232665\n",
      "  -0.24750991 -0.0434121   0.4137853 ]\n",
      " [ 0.35802187  0.03303178  0.11336206  0.08261685 -0.36612245  0.34458582\n",
      "   0.23479226 -0.20355796 -0.35744202]\n",
      " [ 0.24819079 -0.22848914  0.37457766  0.10034447 -0.2594045   0.28578727\n",
      "   0.43207553  0.51090988 -0.05016644]\n",
      " [-0.69134835  0.36069561 -0.60167547 -0.59941089  0.00242729 -0.01205854\n",
      "  -0.67009694 -0.4278602   0.44180793]\n",
      " [ 0.75168168 -0.32767309  0.60117215  0.14151887 -0.10808355 -0.11343952\n",
      "   0.37314086  0.55642643 -0.22824179]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.5586096   1.03462276 -0.65307343  0.10607019 -0.04041989 -0.89432811\n",
      "   0.46273136]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:22 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.60076939]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 22 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.60246701 -0.36543417  0.40598162  0.48919325 -0.21411374  0.15300103\n",
      "   1.012657   -0.11354671 -0.29892877]\n",
      " [-0.43337389  0.24230048 -0.55446889 -0.46878397  0.13814054 -0.35260246\n",
      "  -0.25778572 -0.0434121   0.4137853 ]\n",
      " [ 0.36768903  0.04269894  0.11336206  0.09228401 -0.36612245  0.35425298\n",
      "   0.24445942 -0.20355796 -0.35744202]\n",
      " [ 0.25664678 -0.22003315  0.37457766  0.10880046 -0.2594045   0.29424326\n",
      "   0.44053152  0.51090988 -0.05016644]\n",
      " [-0.70204903  0.34999493 -0.60167547 -0.61011157  0.00242729 -0.02275922\n",
      "  -0.68079762 -0.4278602   0.44180793]\n",
      " [ 0.76005135 -0.31930342  0.60117215  0.14988854 -0.10808355 -0.10506985\n",
      "   0.38151053  0.55642643 -0.22824179]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.51073276  1.07593682 -0.642151    0.14156416 -0.00700068 -0.88636711\n",
      "   0.49602228]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:22 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.62922835]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 22 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.61202726 -0.35587392  0.40598162  0.4987535  -0.21411374  0.15300103\n",
      "   1.02221724 -0.11354671 -0.29892877]\n",
      " [-0.4414689   0.23420547 -0.55446889 -0.47687897  0.13814054 -0.35260246\n",
      "  -0.26588073 -0.0434121   0.4137853 ]\n",
      " [ 0.37473728  0.04974719  0.11336206  0.09933226 -0.36612245  0.35425298\n",
      "   0.25150767 -0.20355796 -0.35744202]\n",
      " [ 0.26246826 -0.21421167  0.37457766  0.11462193 -0.2594045   0.29424326\n",
      "   0.446353    0.51090988 -0.05016644]\n",
      " [-0.71169871  0.34034526 -0.60167547 -0.61976125  0.00242729 -0.02275922\n",
      "  -0.6904473  -0.4278602   0.44180793]\n",
      " [ 0.76842388 -0.31093089  0.60117215  0.15826107 -0.10808355 -0.10506985\n",
      "   0.38988306  0.55642643 -0.22824179]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.46748224  1.11272334 -0.62980564  0.17091194  0.02078494 -0.87935783\n",
      "   0.52740258]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:22 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.71382216]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 22 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.6178155  -0.35587392  0.41176985  0.4987535  -0.21411374  0.15878927\n",
      "   1.02800548 -0.11354671 -0.29892877]\n",
      " [-0.44800129  0.23420547 -0.56100128 -0.47687897  0.13814054 -0.35913485\n",
      "  -0.27241312 -0.0434121   0.4137853 ]\n",
      " [ 0.38074659  0.04974719  0.11937137  0.09933226 -0.36612245  0.3602623\n",
      "   0.25751699 -0.20355796 -0.35744202]\n",
      " [ 0.26894472 -0.21421167  0.38105412  0.11462193 -0.2594045   0.30071972\n",
      "   0.45282946  0.51090988 -0.05016644]\n",
      " [-0.71779317  0.34034526 -0.60776993 -0.61976125  0.00242729 -0.02885368\n",
      "  -0.69654176 -0.4278602   0.44180793]\n",
      " [ 0.77494018 -0.31093089  0.60768845  0.15826107 -0.10808355 -0.09855355\n",
      "   0.39639936  0.55642643 -0.22824179]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.43825203  1.13902    -0.62495413  0.19280853  0.04412854 -0.87595426\n",
      "   0.55194094]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:22 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.53228127]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 23 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.61056027 -0.36312916  0.41176985  0.49149827 -0.21411374  0.15878927\n",
      "   1.02800548 -0.11354671 -0.306184  ]\n",
      " [-0.44350156  0.2387052  -0.56100128 -0.47237925  0.13814054 -0.35913485\n",
      "  -0.27241312 -0.0434121   0.41828502]\n",
      " [ 0.37791226  0.04691285  0.11937137  0.09649792 -0.36612245  0.3602623\n",
      "   0.25751699 -0.20355796 -0.36027635]\n",
      " [ 0.26697743 -0.21617897  0.38105412  0.11265464 -0.2594045   0.30071972\n",
      "   0.45282946  0.51090988 -0.05213373]\n",
      " [-0.70926778  0.34887064 -0.60776993 -0.61123586  0.00242729 -0.02885368\n",
      "  -0.69654176 -0.4278602   0.45033332]\n",
      " [ 0.76866024 -0.31721083  0.60768845  0.15198112 -0.10808355 -0.09855355\n",
      "   0.39639936  0.55642643 -0.23452173]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.50450984  1.09837526 -0.6535256   0.15683122  0.00902767 -0.90011267\n",
      "   0.51236832]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:23 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.68384957]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 23 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.6176388  -0.36312916  0.41884838  0.49149827 -0.21411374  0.15878927\n",
      "   1.03508401 -0.11354671 -0.306184  ]\n",
      " [-0.45094292  0.2387052  -0.56844264 -0.47237925  0.13814054 -0.35913485\n",
      "  -0.27985448 -0.0434121   0.41828502]\n",
      " [ 0.38352336  0.04691285  0.12498248  0.09649792 -0.36612245  0.3602623\n",
      "   0.26312809 -0.20355796 -0.36027635]\n",
      " [ 0.27402376 -0.21617897  0.38810045  0.11265464 -0.2594045   0.30071972\n",
      "   0.45987579  0.51090988 -0.05213373]\n",
      " [-0.7164185   0.34887064 -0.61492065 -0.61123586  0.00242729 -0.02885368\n",
      "  -0.70369248 -0.4278602   0.45033332]\n",
      " [ 0.7761799  -0.31721083  0.61520812  0.15198112 -0.10808355 -0.09855355\n",
      "   0.40391902  0.55642643 -0.23452173]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.47033409  1.12865435 -0.6460727   0.18007838  0.03467389 -0.89608729\n",
      "   0.54158172]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:23 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.68344795]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 23 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.60123853 -0.37952942  0.40244812  0.49149827 -0.21411374  0.15878927\n",
      "   1.01868374 -0.11354671 -0.306184  ]\n",
      " [-0.43596326  0.25368485 -0.55346298 -0.47237925  0.13814054 -0.35913485\n",
      "  -0.26487482 -0.0434121   0.41828502]\n",
      " [ 0.37067026  0.03405976  0.11212938  0.09649792 -0.36612245  0.3602623\n",
      "   0.250275   -0.20355796 -0.36027635]\n",
      " [ 0.26029563 -0.2299071   0.37437233  0.11265464 -0.2594045   0.30071972\n",
      "   0.44614766  0.51090988 -0.05213373]\n",
      " [-0.69998122  0.36530792 -0.59848337 -0.61123586  0.00242729 -0.02885368\n",
      "  -0.6872552  -0.4278602   0.45033332]\n",
      " [ 0.75965449 -0.33373625  0.5986827   0.15198112 -0.10808355 -0.09855355\n",
      "   0.38739361  0.55642643 -0.23452173]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.54426499  1.06606192 -0.66508827  0.12877568 -0.01797558 -0.90764132\n",
      "   0.4813809 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:23 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.59363244]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 23 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.61024168 -0.37952942  0.41145127  0.49149827 -0.21411374  0.15878927\n",
      "   1.01868374 -0.10454356 -0.306184  ]\n",
      " [-0.44576461  0.25368485 -0.56326433 -0.47237925  0.13814054 -0.35913485\n",
      "  -0.26487482 -0.05321344  0.41828502]\n",
      " [ 0.37402615  0.03405976  0.11548527  0.09649792 -0.36612245  0.3602623\n",
      "   0.250275   -0.20020207 -0.36027635]\n",
      " [ 0.2705752  -0.2299071   0.3846519   0.11265464 -0.2594045   0.30071972\n",
      "   0.44614766  0.52118945 -0.05213373]\n",
      " [-0.71083225  0.36530792 -0.60933439 -0.61123586  0.00242729 -0.02885368\n",
      "  -0.6872552  -0.43871122  0.45033332]\n",
      " [ 0.77016098 -0.33373625  0.60918919  0.15198112 -0.10808355 -0.09855355\n",
      "   0.38739361  0.56693292 -0.23452173]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.49525037  1.10080932 -0.65222025  0.15668266  0.01921193 -0.90023725\n",
      "   0.52409977]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:23 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.54837077]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 23 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.61977493 -0.37952942  0.41145127  0.50103152 -0.21411374  0.15878927\n",
      "   1.01868374 -0.10454356 -0.29665075]\n",
      " [-0.45233431  0.25368485 -0.56326433 -0.47894895  0.13814054 -0.35913485\n",
      "  -0.26487482 -0.05321344  0.41171533]\n",
      " [ 0.37556289  0.03405976  0.11548527  0.09803466 -0.36612245  0.3602623\n",
      "   0.250275   -0.20020207 -0.35873962]\n",
      " [ 0.27507978 -0.2299071   0.3846519   0.11715921 -0.2594045   0.30071972\n",
      "   0.44614766  0.52118945 -0.04762916]\n",
      " [-0.72096932  0.36530792 -0.60933439 -0.62137294  0.00242729 -0.02885368\n",
      "  -0.6872552  -0.43871122  0.44019624]\n",
      " [ 0.77872232 -0.33373625  0.60918919  0.16054246 -0.10808355 -0.09855355\n",
      "   0.38739361  0.56693292 -0.22596039]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.43932506  1.13934315 -0.63110432  0.18618516  0.05176191 -0.88374524\n",
      "   0.56131456]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:23 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.50220589]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 23 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.61564664 -0.37952942  0.41145127  0.50103152 -0.21824203  0.15466098\n",
      "   1.01868374 -0.10454356 -0.30077904]\n",
      " [-0.44829811  0.25368485 -0.56326433 -0.47894895  0.14217675 -0.35509864\n",
      "  -0.26487482 -0.05321344  0.41575153]\n",
      " [ 0.37539084  0.03405976  0.11548527  0.09803466 -0.3662945   0.36009025\n",
      "   0.250275   -0.20020207 -0.35891167]\n",
      " [ 0.27093713 -0.2299071   0.3846519   0.11715921 -0.26354714  0.29657707\n",
      "   0.44614766  0.52118945 -0.05177181]\n",
      " [-0.71626022  0.36530792 -0.60933439 -0.62137294  0.00713638 -0.02414458\n",
      "  -0.6872552  -0.43871122  0.44490534]\n",
      " [ 0.77344986 -0.33373625  0.60918919  0.16054246 -0.11335601 -0.10382601\n",
      "   0.38739361  0.56693292 -0.23123285]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.50209957  1.10377808 -0.65840917  0.15462586  0.01618195 -0.91034899\n",
      "   0.52454893]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:23 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.39163529]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 23 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.6171363  -0.37803976  0.41145127  0.50103152 -0.21675237  0.15615064\n",
      "   1.01868374 -0.10454356 -0.29928938]\n",
      " [-0.44839394  0.25358902 -0.56326433 -0.47894895  0.14208092 -0.35519448\n",
      "  -0.26487482 -0.05321344  0.4156557 ]\n",
      " [ 0.37487399  0.0335429   0.11548527  0.09803466 -0.36681135  0.3595734\n",
      "   0.250275   -0.20020207 -0.35942852]\n",
      " [ 0.2706772  -0.23016703  0.3846519   0.11715921 -0.26380707  0.29631714\n",
      "   0.44614766  0.52118945 -0.05203174]\n",
      " [-0.71715636  0.36441178 -0.60933439 -0.62137294  0.00624025 -0.02504072\n",
      "  -0.6872552  -0.43871122  0.4440092 ]\n",
      " [ 0.77355134 -0.33363476  0.60918919  0.16054246 -0.11325452 -0.10372452\n",
      "   0.38739361  0.56693292 -0.23113136]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.54875452  1.08194436 -0.68183247  0.13078136 -0.00740547 -0.93457348\n",
      "   0.50132295]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:23 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.55412025]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 23 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.60253557 -0.39264049  0.41145127  0.50103152 -0.2313531   0.14154991\n",
      "   1.00408302 -0.10454356 -0.29928938]\n",
      " [-0.43809076  0.2638922  -0.56326433 -0.47894895  0.1523841  -0.34489129\n",
      "  -0.25457164 -0.05321344  0.4156557 ]\n",
      " [ 0.36482935  0.02349827  0.11548527  0.09803466 -0.37685599  0.34952876\n",
      "   0.24023036 -0.20020207 -0.35942852]\n",
      " [ 0.26236526 -0.23847896  0.3846519   0.11715921 -0.27211901  0.28800521\n",
      "   0.43783573  0.52118945 -0.05203174]\n",
      " [-0.70329754  0.3782706  -0.60933439 -0.62137294  0.02009907 -0.0111819\n",
      "  -0.67339638 -0.43871122  0.4440092 ]\n",
      " [ 0.76402185 -0.34316426  0.60918919  0.16054246 -0.12278402 -0.11325402\n",
      "   0.37786412  0.56693292 -0.23113136]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.61720804  1.02937051 -0.70496098  0.08578426 -0.05032262 -0.95220292\n",
      "   0.45696395]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:23 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.59914733]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 23 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.61233757 -0.39264049  0.41145127  0.51083351 -0.2313531   0.14154991\n",
      "   1.01388501 -0.10454356 -0.29928938]\n",
      " [-0.44827615  0.2638922  -0.56326433 -0.48913434  0.1523841  -0.34489129\n",
      "  -0.26475704 -0.05321344  0.4156557 ]\n",
      " [ 0.37232527  0.02349827  0.11548527  0.10553058 -0.37685599  0.34952876\n",
      "   0.24772628 -0.20020207 -0.35942852]\n",
      " [ 0.27072562 -0.23847896  0.3846519   0.12551957 -0.27211901  0.28800521\n",
      "   0.44619608  0.52118945 -0.05203174]\n",
      " [-0.71341066  0.3782706  -0.60933439 -0.63148606  0.02009907 -0.0111819\n",
      "  -0.6835095  -0.43871122  0.4440092 ]\n",
      " [ 0.77455853 -0.34316426  0.60918919  0.17107914 -0.12278402 -0.11325402\n",
      "   0.38840079  0.56693292 -0.23113136]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.56907168  1.07229103 -0.69357366  0.11798139 -0.0169316  -0.94645515\n",
      "   0.49481084]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:23 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.5850871]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 23 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.62361203 -0.39264049  0.41145127  0.51083351 -0.22007864  0.15282438\n",
      "   1.02515947 -0.10454356 -0.29928938]\n",
      " [-0.45762605  0.2638922  -0.56326433 -0.48913434  0.1430342  -0.35424119\n",
      "  -0.27410693 -0.05321344  0.4156557 ]\n",
      " [ 0.37916909  0.02349827  0.11548527  0.10553058 -0.37001217  0.35637258\n",
      "   0.2545701  -0.20020207 -0.35942852]\n",
      " [ 0.27881638 -0.23847896  0.3846519   0.12551957 -0.26402825  0.29609597\n",
      "   0.45428684  0.52118945 -0.05203174]\n",
      " [-0.72458365  0.3782706  -0.60933439 -0.63148606  0.00892608 -0.02235489\n",
      "  -0.69468249 -0.43871122  0.4440092 ]\n",
      " [ 0.78404189 -0.34316426  0.60918919  0.17107914 -0.11330065 -0.10377065\n",
      "   0.39788416  0.56693292 -0.23113136]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.51870952  1.11373625 -0.67907372  0.15041412  0.01708407 -0.93639647\n",
      "   0.53089283]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:23 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.56950835]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 23 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.60997584 -0.39264049  0.41145127  0.49719732 -0.22007864  0.13918818\n",
      "   1.02515947 -0.10454356 -0.31292557]\n",
      " [-0.44484618  0.2638922  -0.56326433 -0.47635448  0.1430342  -0.34146133\n",
      "  -0.27410693 -0.05321344  0.42843556]\n",
      " [ 0.37123214  0.02349827  0.11548527  0.09759363 -0.37001217  0.34843563\n",
      "   0.2545701  -0.20020207 -0.36736547]\n",
      " [ 0.26861053 -0.23847896  0.3846519   0.11531372 -0.26402825  0.28589012\n",
      "   0.45428684  0.52118945 -0.06223759]\n",
      " [-0.71137446  0.3782706  -0.60933439 -0.61827687  0.00892608 -0.00914569\n",
      "  -0.69468249 -0.43871122  0.4572184 ]\n",
      " [ 0.77419481 -0.34316426  0.60918919  0.16123206 -0.11330065 -0.11361773\n",
      "   0.39788416  0.56693292 -0.24097845]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.5885223   1.06286446 -0.69946402  0.10726033 -0.02875848 -0.95608576\n",
      "   0.48549579]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:23 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.60585978]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 23 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.62014012 -0.38247621  0.41145127  0.5073616  -0.22007864  0.14935247\n",
      "   1.03532376 -0.10454356 -0.31292557]\n",
      " [-0.45508359  0.2536548  -0.56326433 -0.48659188  0.1430342  -0.35169873\n",
      "  -0.28434434 -0.05321344  0.42843556]\n",
      " [ 0.38091272  0.03317885  0.11548527  0.10727421 -0.37001217  0.35811622\n",
      "   0.26425068 -0.20020207 -0.36736547]\n",
      " [ 0.27722683 -0.22986267  0.3846519   0.12393001 -0.26402825  0.29450642\n",
      "   0.46290314  0.52118945 -0.06223759]\n",
      " [-0.72186473  0.36778033 -0.60933439 -0.62876714  0.00892608 -0.01963597\n",
      "  -0.70517276 -0.43871122  0.4572184 ]\n",
      " [ 0.78275481 -0.33460426  0.60918919  0.16979205 -0.11330065 -0.10505773\n",
      "   0.40644416  0.56693292 -0.24097845]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.5414632   1.1036874  -0.68916913  0.14252567  0.0045587  -0.94853719\n",
      "   0.51872433]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:23 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.6337771]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 23 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.62948106 -0.37313527  0.41145127  0.51670254 -0.22007864  0.14935247\n",
      "   1.04466469 -0.10454356 -0.31292557]\n",
      " [-0.46331203  0.24542636 -0.56326433 -0.49482032  0.1430342  -0.35169873\n",
      "  -0.29257277 -0.05321344  0.42843556]\n",
      " [ 0.38809385  0.04035998  0.11548527  0.11445534 -0.37001217  0.35811622\n",
      "   0.27143181 -0.20020207 -0.36736547]\n",
      " [ 0.2833307  -0.22375879  0.3846519   0.13003389 -0.26402825  0.29450642\n",
      "   0.46900702  0.52118945 -0.06223759]\n",
      " [-0.7313124   0.35833266 -0.60933439 -0.63821481  0.00892608 -0.01963597\n",
      "  -0.71462043 -0.43871122  0.4572184 ]\n",
      " [ 0.79121785 -0.32614122  0.60918919  0.1782551  -0.11330065 -0.10505773\n",
      "   0.4149072   0.56693292 -0.24097845]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.49896235  1.14005707 -0.6775065   0.17171905  0.03233047 -0.94190553\n",
      "   0.54999757]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:23 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.71916591]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 23 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.63501516 -0.37313527  0.41698537  0.51670254 -0.22007864  0.15488657\n",
      "   1.05019879 -0.10454356 -0.31292557]\n",
      " [-0.46962594  0.24542636 -0.56957825 -0.49482032  0.1430342  -0.35801264\n",
      "  -0.29888669 -0.05321344  0.42843556]\n",
      " [ 0.39401475  0.04035998  0.12140616  0.11445534 -0.37001217  0.36403712\n",
      "   0.27735271 -0.20020207 -0.36736547]\n",
      " [ 0.2896503  -0.22375879  0.3909715   0.13003389 -0.26402825  0.30082602\n",
      "   0.47532662  0.52118945 -0.06223759]\n",
      " [-0.73714461  0.35833266 -0.61516661 -0.63821481  0.00892608 -0.02546818\n",
      "  -0.72045265 -0.43871122  0.4572184 ]\n",
      " [ 0.79750767 -0.32614122  0.61547901  0.1782551  -0.11330065 -0.09876792\n",
      "   0.42119701  0.56693292 -0.24097845]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.47060284  1.16567548 -0.67301687  0.19317062  0.05522041 -0.93874174\n",
      "   0.57401437]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:23 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.52395259]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 24 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.62780612 -0.3803443   0.41698537  0.50949351 -0.22007864  0.15488657\n",
      "   1.05019879 -0.10454356 -0.32013461]\n",
      " [-0.46497779  0.25007452 -0.56957825 -0.49017217  0.1430342  -0.35801264\n",
      "  -0.29888669 -0.05321344  0.43308372]\n",
      " [ 0.39107462  0.03741986  0.12140616  0.11151522 -0.37001217  0.36403712\n",
      "   0.27735271 -0.20020207 -0.37030559]\n",
      " [ 0.28747612 -0.22593298  0.3909715   0.1278597  -0.26402825  0.30082602\n",
      "   0.47532662  0.52118945 -0.06441177]\n",
      " [-0.72868027  0.36679701 -0.61516661 -0.62975046  0.00892608 -0.02546818\n",
      "  -0.72045265 -0.43871122  0.46568274]\n",
      " [ 0.79110321 -0.33254568  0.61547901  0.17185064 -0.11330065 -0.09876792\n",
      "   0.42119701  0.56693292 -0.24738291]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.53594661  1.12553119 -0.70097491  0.15754245  0.02036785 -0.9625002\n",
      "   0.53475829]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:24 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.68915414]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 24 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.63461227 -0.3803443   0.42379151  0.50949351 -0.22007864  0.15488657\n",
      "   1.05700494 -0.10454356 -0.32013461]\n",
      " [-0.47230682  0.25007452 -0.57690728 -0.49017217  0.1430342  -0.35801264\n",
      "  -0.30621572 -0.05321344  0.43308372]\n",
      " [ 0.39672153  0.03741986  0.12705307  0.11151522 -0.37001217  0.36403712\n",
      "   0.28299962 -0.20020207 -0.37030559]\n",
      " [ 0.29447897 -0.22593298  0.39797435  0.1278597  -0.26402825  0.30082602\n",
      "   0.48232947  0.52118945 -0.06441177]\n",
      " [-0.73554872  0.36679701 -0.62203507 -0.62975046  0.00892608 -0.02546818\n",
      "  -0.7273211  -0.43871122  0.46568274]\n",
      " [ 0.79836449 -0.33254568  0.62274029  0.17185064 -0.11330065 -0.09876792\n",
      "   0.4284583   0.56693292 -0.24738291]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.5026518   1.1551774  -0.69402993  0.18044247  0.04567856 -0.95875069\n",
      "   0.56344167]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:24 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.68612051]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 24 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.61827352 -0.39668305  0.40745277  0.50949351 -0.22007864  0.15488657\n",
      "   1.04066619 -0.10454356 -0.32013461]\n",
      " [-0.45704631  0.26533502 -0.56164677 -0.49017217  0.1430342  -0.35801264\n",
      "  -0.29095521 -0.05321344  0.43308372]\n",
      " [ 0.38360758  0.02430591  0.11393912  0.11151522 -0.37001217  0.36403712\n",
      "   0.26988566 -0.20020207 -0.37030559]\n",
      " [ 0.28037369 -0.24003825  0.38386907  0.1278597  -0.26402825  0.30082602\n",
      "   0.4682242   0.52118945 -0.06441177]\n",
      " [-0.71917704  0.38316869 -0.60566339 -0.62975046  0.00892608 -0.02546818\n",
      "  -0.71094942 -0.43871122  0.46568274]\n",
      " [ 0.78182872 -0.34908145  0.60620452  0.17185064 -0.11330065 -0.09876792\n",
      "   0.41192253  0.56693292 -0.24738291]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.57653297  1.09237404 -0.71240697  0.12877384 -0.00758118 -0.96998913\n",
      "   0.50285181]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:24 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.59622287]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 24 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.62739151 -0.39668305  0.41657076  0.50949351 -0.22007864  0.15488657\n",
      "   1.04066619 -0.09542557 -0.32013461]\n",
      " [-0.46694462  0.26533502 -0.57154508 -0.49017217  0.1430342  -0.35801264\n",
      "  -0.29095521 -0.06311175  0.43308372]\n",
      " [ 0.38714183  0.02430591  0.11747337  0.11151522 -0.37001217  0.36403712\n",
      "   0.26988566 -0.19666782 -0.37030559]\n",
      " [ 0.29070327 -0.24003825  0.39419865  0.1278597  -0.26402825  0.30082602\n",
      "   0.4682242   0.53151903 -0.06441177]\n",
      " [-0.72988516  0.38316869 -0.6163715  -0.62975046  0.00892608 -0.02546818\n",
      "  -0.71094942 -0.44941934  0.46568274]\n",
      " [ 0.79215077 -0.34908145  0.61652657  0.17185064 -0.11330065 -0.09876792\n",
      "   0.41192253  0.57725496 -0.24738291]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.52793008  1.12713886 -0.70001126  0.15666185  0.0296449  -0.96287628\n",
      "   0.54542731]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:24 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.54516572]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 24 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.63718043 -0.39668305  0.41657076  0.51928243 -0.22007864  0.15488657\n",
      "   1.04066619 -0.09542557 -0.31034569]\n",
      " [-0.47384739  0.26533502 -0.57154508 -0.49707493  0.1430342  -0.35801264\n",
      "  -0.29095521 -0.06311175  0.42618095]\n",
      " [ 0.38894385  0.02430591  0.11747337  0.11331723 -0.37001217  0.36403712\n",
      "   0.26988566 -0.19666782 -0.36850358]\n",
      " [ 0.29554261 -0.24003825  0.39419865  0.13269905 -0.26402825  0.30082602\n",
      "   0.4682242   0.53151903 -0.05957243]\n",
      " [-0.74027088  0.38316869 -0.6163715  -0.64013618  0.00892608 -0.02546818\n",
      "  -0.71094942 -0.44941934  0.45529702]\n",
      " [ 0.80105978 -0.34908145  0.61652657  0.18075965 -0.11330065 -0.09876792\n",
      "   0.41192253  0.57725496 -0.23847389]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.47153971  1.16624818 -0.67903914  0.186664    0.06278122 -0.94650646\n",
      "   0.58331385]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:24 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.49042998]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 24 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.63323981 -0.39668305  0.41657076  0.51928243 -0.22401926  0.15094594\n",
      "   1.04066619 -0.09542557 -0.31428631]\n",
      " [-0.46989218  0.26533502 -0.57154508 -0.49707493  0.14698941 -0.35405744\n",
      "  -0.29095521 -0.06311175  0.43013615]\n",
      " [ 0.38872225  0.02430591  0.11747337  0.11331723 -0.37023377  0.36381552\n",
      "   0.26988566 -0.19666782 -0.36872518]\n",
      " [ 0.2914405  -0.24003825  0.39419865  0.13269905 -0.26813036  0.2967239\n",
      "   0.4682242   0.53151903 -0.06367455]\n",
      " [-0.73575498  0.38316869 -0.6163715  -0.64013618  0.01344198 -0.02095228\n",
      "  -0.71094942 -0.44941934  0.45981292]\n",
      " [ 0.79585137 -0.34908145  0.61652657  0.18075965 -0.11850906 -0.10397633\n",
      "   0.41192253  0.57725496 -0.24368231]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.532821    1.1316218  -0.70567895  0.15580175  0.0279874  -0.97256246\n",
      "   0.54735748]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:24 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.37479625]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 24 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.63488594 -0.39503691  0.41657076  0.51928243 -0.22237313  0.15259208\n",
      "   1.04066619 -0.09542557 -0.31264018]\n",
      " [-0.47009538  0.26513183 -0.57154508 -0.49707493  0.14678621 -0.35426064\n",
      "  -0.29095521 -0.06311175  0.42993296]\n",
      " [ 0.3883065   0.02389016  0.11747337  0.11331723 -0.37064952  0.36339977\n",
      "   0.26988566 -0.19666782 -0.36914093]\n",
      " [ 0.29126134 -0.24021742  0.39419865  0.13269905 -0.26830953  0.29654474\n",
      "   0.4682242   0.53151903 -0.06385371]\n",
      " [-0.73684695  0.38207672 -0.6163715  -0.64013618  0.01235002 -0.02204424\n",
      "  -0.71094942 -0.44941934  0.45872096]\n",
      " [ 0.79606429 -0.34886852  0.61652657  0.18075965 -0.11829614 -0.1037634\n",
      "   0.41192253  0.57725496 -0.24346938]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.57673289  1.11131824 -0.7278381   0.13342996  0.00585229 -0.99561218\n",
      "   0.52561448]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:24 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.55506482]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 24 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.62020842 -0.40971444  0.41657076  0.51928243 -0.23705066  0.13791455\n",
      "   1.02598867 -0.09542557 -0.31264018]\n",
      " [-0.45941845  0.27580876 -0.57154508 -0.49707493  0.15746314 -0.34358371\n",
      "  -0.28027828 -0.06311175  0.42993296]\n",
      " [ 0.37796564  0.0135493   0.11747337  0.11331723 -0.38099037  0.35305891\n",
      "   0.2595448  -0.19666782 -0.36914093]\n",
      " [ 0.28254902 -0.24892973  0.39419865  0.13269905 -0.27702184  0.28783243\n",
      "   0.45951188  0.53151903 -0.06385371]\n",
      " [-0.72286639  0.39605728 -0.6163715  -0.64013618  0.02633057 -0.00806368\n",
      "  -0.69696886 -0.44941934  0.45872096]\n",
      " [ 0.78618479 -0.35874803  0.61652657  0.18075965 -0.12817564 -0.11364291\n",
      "   0.40204302  0.57725496 -0.24346938]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.64527447  1.05851182 -0.7505295   0.08801537 -0.03757265 -1.01304743\n",
      "   0.48078223]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:24 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.60766243]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 24 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.62955684 -0.40971444  0.41657076  0.52863085 -0.23705066  0.13791455\n",
      "   1.03533709 -0.09542557 -0.31264018]\n",
      " [-0.46950456  0.27580876 -0.57154508 -0.50716104  0.15746314 -0.34358371\n",
      "  -0.2903644  -0.06311175  0.42993296]\n",
      " [ 0.38561475  0.0135493   0.11747337  0.12096634 -0.38099037  0.35305891\n",
      "   0.26719391 -0.19666782 -0.36914093]\n",
      " [ 0.29104515 -0.24892973  0.39419865  0.14119518 -0.27702184  0.28783243\n",
      "   0.46800801  0.53151903 -0.06385371]\n",
      " [-0.73252642  0.39605728 -0.6163715  -0.64979622  0.02633057 -0.00806368\n",
      "  -0.7066289  -0.44941934  0.45872096]\n",
      " [ 0.79653527 -0.35874803  0.61652657  0.19111013 -0.12817564 -0.11364291\n",
      "   0.4123935   0.57725496 -0.24346938]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.59850611  1.10046802 -0.74000669  0.11978788 -0.00456657 -1.00776033\n",
      "   0.51806675]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:24 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.58880621]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 24 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.64069871 -0.40971444  0.41657076  0.52863085 -0.22590879  0.14905642\n",
      "   1.04647896 -0.09542557 -0.31264018]\n",
      " [-0.47899137  0.27580876 -0.57154508 -0.50716104  0.14797633 -0.35307052\n",
      "  -0.29985121 -0.06311175  0.42993296]\n",
      " [ 0.3926787   0.0135493   0.11747337  0.12096634 -0.37392642  0.36012286\n",
      "   0.27425786 -0.19666782 -0.36914093]\n",
      " [ 0.29933541 -0.24892973  0.39419865  0.14119518 -0.26873158  0.29612268\n",
      "   0.47629827  0.53151903 -0.06385371]\n",
      " [-0.74360843  0.39605728 -0.6163715  -0.64979622  0.01524857 -0.01914569\n",
      "  -0.7177109  -0.44941934  0.45872096]\n",
      " [ 0.80614319 -0.35874803  0.61652657  0.19111013 -0.11856772 -0.10403499\n",
      "   0.42200142  0.57725496 -0.24346938]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.54872833  1.1416431  -0.72608424  0.15220949  0.02945611 -0.99807744\n",
      "   0.55413289]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:24 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.56943555]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 24 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.62693171 -0.40971444  0.41657076  0.51486385 -0.22590879  0.13528942\n",
      "   1.04647896 -0.09542557 -0.32640719]\n",
      " [-0.46599858  0.27580876 -0.57154508 -0.49416826  0.14797633 -0.34007773\n",
      "  -0.29985121 -0.06311175  0.44292574]\n",
      " [ 0.38440979  0.0135493   0.11747337  0.11269743 -0.37392642  0.35185395\n",
      "   0.27425786 -0.19666782 -0.37740984]\n",
      " [ 0.28882847 -0.24892973  0.39419865  0.13068824 -0.26873158  0.28561575\n",
      "   0.47629827  0.53151903 -0.07436064]\n",
      " [-0.7302406   0.39605728 -0.6163715  -0.63642839  0.01524857 -0.00577787\n",
      "  -0.7177109  -0.44941934  0.47208878]\n",
      " [ 0.79592132 -0.35874803  0.61652657  0.18088826 -0.11856772 -0.11425686\n",
      "   0.42200142  0.57725496 -0.25369125]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.61853507  1.09053154 -0.74612891  0.10868177 -0.01676501 -1.01749177\n",
      "   0.50827309]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:24 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.61096712]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 24 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.63684413 -0.39980201  0.41657076  0.52477627 -0.22590879  0.14520184\n",
      "   1.05639139 -0.09542557 -0.32640719]\n",
      " [-0.47615989  0.26564745 -0.57154508 -0.50432957  0.14797633 -0.35023904\n",
      "  -0.31001252 -0.06311175  0.44292574]\n",
      " [ 0.39407543  0.02321494  0.11747337  0.12236307 -0.37392642  0.36151959\n",
      "   0.2839235  -0.19666782 -0.37740984]\n",
      " [ 0.29756569 -0.24019251  0.39419865  0.13942546 -0.26873158  0.29435297\n",
      "   0.48503549  0.53151903 -0.07436064]\n",
      " [-0.74051203  0.38578586 -0.6163715  -0.64669982  0.01524857 -0.01604929\n",
      "  -0.72798233 -0.44941934  0.47208878]\n",
      " [ 0.80462104 -0.35004831  0.61652657  0.18958798 -0.11856772 -0.10555714\n",
      "   0.43070114  0.57725496 -0.25369125]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.57230117  1.13082211 -0.73641489  0.1436848   0.01641162 -1.01031454\n",
      "   0.5413869 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:24 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.63836759]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 24 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.64596282 -0.39068332  0.41657076  0.53389496 -0.22590879  0.14520184\n",
      "   1.06551008 -0.09542557 -0.32640719]\n",
      " [-0.48447383  0.25733351 -0.57154508 -0.51264351  0.14797633 -0.35023904\n",
      "  -0.31832646 -0.06311175  0.44292574]\n",
      " [ 0.40136275  0.03050226  0.11747337  0.1296504  -0.37392642  0.36151959\n",
      "   0.29121082 -0.19666782 -0.37740984]\n",
      " [ 0.30391413 -0.23384407  0.39419865  0.1457739  -0.26873158  0.29435297\n",
      "   0.49138393  0.53151903 -0.07436064]\n",
      " [-0.74974965  0.37654824 -0.6163715  -0.65593744  0.01524857 -0.01604929\n",
      "  -0.73721995 -0.44941934  0.47208878]\n",
      " [ 0.81313322 -0.34153613  0.61652657  0.19810016 -0.11856772 -0.10555714\n",
      "   0.43921332  0.57725496 -0.25369125]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.53055896  1.16673507 -0.72539149  0.17269543  0.04413463 -1.00402552\n",
      "   0.57250657]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:24 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.72439583]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 24 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.65125669 -0.39068332  0.42186462  0.53389496 -0.22590879  0.1504957\n",
      "   1.07080394 -0.09542557 -0.32640719]\n",
      " [-0.49056572  0.25733351 -0.57763697 -0.51264351  0.14797633 -0.35633093\n",
      "  -0.32441834 -0.06311175  0.44292574]\n",
      " [ 0.407184    0.03050226  0.12329462  0.1296504  -0.37392642  0.36734083\n",
      "   0.29703207 -0.19666782 -0.37740984]\n",
      " [ 0.31006531 -0.23384407  0.40034983  0.1457739  -0.26873158  0.30050414\n",
      "   0.49753511  0.53151903 -0.07436064]\n",
      " [-0.75533142  0.37654824 -0.62195327 -0.65593744  0.01524857 -0.02163105\n",
      "  -0.74280172 -0.44941934  0.47208878]\n",
      " [ 0.81919476 -0.34153613  0.62258811  0.19810016 -0.11856772 -0.0994956\n",
      "   0.44527486  0.57725496 -0.25369125]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.50304725  1.1916782  -0.72122947  0.19369864  0.06656089 -1.00107554\n",
      "   0.59599124]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:24 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.51542725]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 25 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.64412267 -0.39781734  0.42186462  0.52676095 -0.22590879  0.1504957\n",
      "   1.07080394 -0.09542557 -0.3335412 ]\n",
      " [-0.48580089  0.26209834 -0.57763697 -0.50787867  0.14797633 -0.35633093\n",
      "  -0.32441834 -0.06311175  0.44769058]\n",
      " [ 0.40415513  0.0274734   0.12329462  0.12662153 -0.37392642  0.36734083\n",
      "   0.29703207 -0.19666782 -0.3804387 ]\n",
      " [ 0.30770251 -0.23620687  0.40034983  0.1434111  -0.26873158  0.30050414\n",
      "   0.49753511  0.53151903 -0.07672345]\n",
      " [-0.746958    0.38492165 -0.62195327 -0.64756403  0.01524857 -0.02163105\n",
      "  -0.74280172 -0.44941934  0.4804622 ]\n",
      " [ 0.81269666 -0.34803422  0.62258811  0.19160206 -0.11856772 -0.0994956\n",
      "   0.44527486  0.57725496 -0.26018935]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.56741432  1.1520973  -0.74857495  0.158468    0.03200596 -1.02443684\n",
      "   0.55711495]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:25 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.69447251]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 25 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.65066273 -0.39781734  0.42840468  0.52676095 -0.22590879  0.1504957\n",
      "   1.077344   -0.09542557 -0.3335412 ]\n",
      " [-0.49299178  0.26209834 -0.58482786 -0.50787867  0.14797633 -0.35633093\n",
      "  -0.33160924 -0.06311175  0.44769058]\n",
      " [ 0.40981807  0.0274734   0.12895756  0.12662153 -0.37392642  0.36734083\n",
      "   0.30269501 -0.19666782 -0.3804387 ]\n",
      " [ 0.31463328 -0.23620687  0.40728059  0.1434111  -0.26873158  0.30050414\n",
      "   0.50446587  0.53151903 -0.07672345]\n",
      " [-0.75355005  0.38492165 -0.62854532 -0.64756403  0.01524857 -0.02163105\n",
      "  -0.74939377 -0.44941934  0.4804622 ]\n",
      " [ 0.81969525 -0.34803422  0.6295867   0.19160206 -0.11856772 -0.0994956\n",
      "   0.45227345  0.57725496 -0.26018935]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.53500084  1.18108882 -0.74210035  0.18100143  0.05694869 -1.02093737\n",
      "   0.58523933]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:25 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.68873263]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 25 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.63438646 -0.41409361  0.41212841  0.52676095 -0.22590879  0.1504957\n",
      "   1.06106773 -0.09542557 -0.3335412 ]\n",
      " [-0.47749919  0.27759093 -0.56933527 -0.50787867  0.14797633 -0.35633093\n",
      "  -0.31611665 -0.06311175  0.44769058]\n",
      " [ 0.39646402  0.01411935  0.1156035   0.12662153 -0.37392642  0.36734083\n",
      "   0.28934096 -0.19666782 -0.3804387 ]\n",
      " [ 0.30019585 -0.2506443   0.39284317  0.1434111  -0.26873158  0.30050414\n",
      "   0.49002845  0.53151903 -0.07672345]\n",
      " [-0.73724753  0.40122417 -0.6122428  -0.64756403  0.01524857 -0.02163105\n",
      "  -0.73309125 -0.44941934  0.4804622 ]\n",
      " [ 0.80316847 -0.364561    0.61305992  0.19160206 -0.11856772 -0.0994956\n",
      "   0.43574667  0.57725496 -0.26018935]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.60882609  1.11811341 -0.75989054  0.12898963  0.00312107 -1.03189899\n",
      "   0.52430236]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:25 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.59899292]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 25 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.64359387 -0.41409361  0.42133581  0.52676095 -0.22590879  0.1504957\n",
      "   1.06106773 -0.08621816 -0.3335412 ]\n",
      " [-0.48746534  0.27759093 -0.57930142 -0.50787867  0.14797633 -0.35633093\n",
      "  -0.31611665 -0.0730779   0.44769058]\n",
      " [ 0.40016862  0.01411935  0.1193081   0.12662153 -0.37392642  0.36734083\n",
      "   0.28934096 -0.19296322 -0.3804387 ]\n",
      " [ 0.31054878 -0.2506443   0.4031961   0.1434111  -0.26873158  0.30050414\n",
      "   0.49002845  0.54187196 -0.07672345]\n",
      " [-0.74780208  0.40122417 -0.62279735 -0.64756403  0.01524857 -0.02163105\n",
      "  -0.73309125 -0.45997388  0.4804622 ]\n",
      " [ 0.81329883 -0.364561    0.62319027  0.19160206 -0.11856772 -0.0994956\n",
      "   0.43574667  0.58738532 -0.26018935]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.56066506  1.15285361 -0.74795234  0.15683647  0.04034303 -1.0250609\n",
      "   0.56668942]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:25 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.54211883]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 25 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.65362221 -0.41409361  0.42133581  0.5367893  -0.22590879  0.1504957\n",
      "   1.06106773 -0.08621816 -0.32351285]\n",
      " [-0.49469056  0.27759093 -0.57930142 -0.51510389  0.14797633 -0.35633093\n",
      "  -0.31611665 -0.0730779   0.44046536]\n",
      " [ 0.40223678  0.01411935  0.1193081   0.12868969 -0.37392642  0.36734083\n",
      "   0.28934096 -0.19296322 -0.37837054]\n",
      " [ 0.31572203 -0.2506443   0.4031961   0.14858435 -0.26873158  0.30050414\n",
      "   0.49002845  0.54187196 -0.07155019]\n",
      " [-0.75841922  0.40122417 -0.62279735 -0.65818116  0.01524857 -0.02163105\n",
      "  -0.73309125 -0.45997388  0.46984506]\n",
      " [ 0.82253784 -0.364561    0.62319027  0.20084108 -0.11856772 -0.0994956\n",
      "   0.43574667  0.58738532 -0.25095033]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.50383605  1.19250853 -0.72712968  0.18732652  0.07405436 -1.00880795\n",
      "   0.60522093]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:25 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.47836701]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 25 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.64988287 -0.41409361  0.42133581  0.5367893  -0.22964813  0.14675636\n",
      "   1.06106773 -0.08621816 -0.3272522 ]\n",
      " [-0.49083938  0.27759093 -0.57930142 -0.51510389  0.15182752 -0.35247974\n",
      "  -0.31611665 -0.0730779   0.44431654]\n",
      " [ 0.40197895  0.01411935  0.1193081   0.12868969 -0.37418425  0.36708301\n",
      "   0.28934096 -0.19296322 -0.37862836]\n",
      " [ 0.31168206 -0.2506443   0.4031961   0.14858435 -0.27277155  0.29646418\n",
      "   0.49002845  0.54187196 -0.07559016]\n",
      " [-0.75411252  0.40122417 -0.62279735 -0.65818116  0.01955526 -0.01732436\n",
      "  -0.73309125 -0.45997388  0.47415175]\n",
      " [ 0.81742436 -0.364561    0.62319027  0.20084108 -0.12368121 -0.10460909\n",
      "   0.43574667  0.58738532 -0.25606382]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.56352     1.15888665 -0.75307606  0.15722671  0.04012096 -1.03428051\n",
      "   0.57015829]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:25 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.35773486]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 25 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.65166067 -0.4123158   0.42133581  0.5367893  -0.22787033  0.14853416\n",
      "   1.06106773 -0.08621816 -0.32547439]\n",
      " [-0.4911518   0.27727851 -0.57930142 -0.51510389  0.15151509 -0.35279217\n",
      "  -0.31611665 -0.0730779   0.44400412]\n",
      " [ 0.40166701  0.01380741  0.1193081   0.12868969 -0.37449619  0.36677107\n",
      "   0.28934096 -0.19296322 -0.37894031]\n",
      " [ 0.31158816 -0.2507382   0.4031961   0.14858435 -0.27286546  0.29637027\n",
      "   0.49002845  0.54187196 -0.07568407]\n",
      " [-0.7553765   0.39996019 -0.62279735 -0.65818116  0.01829128 -0.01858834\n",
      "  -0.73309125 -0.45997388  0.47288778]\n",
      " [ 0.81774782 -0.36423754  0.62319027  0.20084108 -0.12335775 -0.10428562\n",
      "   0.43574667  0.58738532 -0.25574035]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.60461669  1.14012513 -0.77393688  0.13636637  0.0194787  -1.05609605\n",
      "   0.54993346]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:25 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.5559283]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 25 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.63692344 -0.42705303  0.42133581  0.5367893  -0.24260756  0.13379693\n",
      "   1.0463305  -0.08621816 -0.32547439]\n",
      " [-0.48014487  0.28828543 -0.57930142 -0.51510389  0.16252202 -0.34178524\n",
      "  -0.30510972 -0.0730779   0.44400412]\n",
      " [ 0.39105091  0.00319131  0.1193081   0.12868969 -0.38511229  0.35615497\n",
      "   0.27872486 -0.19296322 -0.37894031]\n",
      " [ 0.30250443 -0.25982193  0.4031961   0.14858435 -0.28194918  0.28728654\n",
      "   0.48094472  0.54187196 -0.07568407]\n",
      " [-0.74129873  0.41403796 -0.62279735 -0.65818116  0.03236905 -0.00451057\n",
      "  -0.71901348 -0.45997388  0.47288778]\n",
      " [ 0.80755437 -0.37443099  0.62319027  0.20084108 -0.1335512  -0.11447908\n",
      "   0.42555322  0.58738532 -0.25574035]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.67323826  1.08713192 -0.79623318  0.09055857 -0.02442359 -1.07337794\n",
      "   0.5046691 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:25 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.61637426]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 25 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.6458213  -0.42705303  0.42133581  0.54568716 -0.24260756  0.13379693\n",
      "   1.05522836 -0.08621816 -0.32547439]\n",
      " [-0.49006881  0.28828543 -0.57930142 -0.52502783  0.16252202 -0.34178524\n",
      "  -0.31503366 -0.0730779   0.44400412]\n",
      " [ 0.39880213  0.00319131  0.1193081   0.1364409  -0.38511229  0.35615497\n",
      "   0.28647607 -0.19296322 -0.37894031]\n",
      " [ 0.3110731  -0.25982193  0.4031961   0.15715302 -0.28194918  0.28728654\n",
      "   0.48951339  0.54187196 -0.07568407]\n",
      " [-0.75050336  0.41403796 -0.62279735 -0.66738579  0.03236905 -0.00451057\n",
      "  -0.72821811 -0.45997388  0.47288778]\n",
      " [ 0.8176634  -0.37443099  0.62319027  0.21095011 -0.1335512  -0.11447908\n",
      "   0.43566226  0.58738532 -0.25574035]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.62788276  1.12804417 -0.78652235  0.12183782  0.00811842 -1.06851074\n",
      "   0.54129439]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:25 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.59260068]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 25 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.65682175 -0.42705303  0.42133581  0.54568716 -0.23160712  0.14479737\n",
      "   1.0662288  -0.08621816 -0.32547439]\n",
      " [-0.49965522  0.28828543 -0.57930142 -0.52502783  0.15293561 -0.35137165\n",
      "  -0.32462006 -0.0730779   0.44400412]\n",
      " [ 0.40606112  0.00319131  0.1193081   0.1364409  -0.3778533   0.36341396\n",
      "   0.29373506 -0.19296322 -0.37894031]\n",
      " [ 0.31953205 -0.25982193  0.4031961   0.15715302 -0.27349023  0.29574549\n",
      "   0.49797234  0.54187196 -0.07568407]\n",
      " [-0.76147845  0.41403796 -0.62279735 -0.66738579  0.02139396 -0.01548566\n",
      "  -0.7391932  -0.45997388  0.47288778]\n",
      " [ 0.82735982 -0.37443099  0.62319027  0.21095011 -0.12385479 -0.10478266\n",
      "   0.44535867  0.58738532 -0.25574035]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.57870455  1.16890787 -0.77314458  0.1542184   0.04211238 -1.05917333\n",
      "   0.57729768]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:25 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.56943273]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 25 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.64293939 -0.42705303  0.42133581  0.53180481 -0.23160712  0.13091502\n",
      "   1.0662288  -0.08621816 -0.33935675]\n",
      " [-0.48646706  0.28828543 -0.57930142 -0.51183968  0.15293561 -0.3381835\n",
      "  -0.32462006 -0.0730779   0.45719227]\n",
      " [ 0.39747458  0.00319131  0.1193081   0.12785437 -0.3778533   0.35482743\n",
      "   0.29373506 -0.19296322 -0.38752684]\n",
      " [ 0.30873672 -0.25982193  0.4031961   0.14635769 -0.27349023  0.28495017\n",
      "   0.49797234  0.54187196 -0.08647939]\n",
      " [-0.74797051  0.41403796 -0.62279735 -0.65387785  0.02139396 -0.00197772\n",
      "  -0.7391932  -0.45997388  0.48639572]\n",
      " [ 0.81679219 -0.37443099  0.62319027  0.20038249 -0.12385479 -0.11535029\n",
      "   0.44535867  0.58738532 -0.26630798]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.64851105  1.1175771  -0.79286554  0.11032564 -0.00448109 -1.07834144\n",
      "   0.5309989 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:25 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.61605357]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 25 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.65260615 -0.41738628  0.42133581  0.54147156 -0.23160712  0.14058178\n",
      "   1.07589556 -0.08621816 -0.33935675]\n",
      " [-0.4965216   0.2782309  -0.57930142 -0.52189421  0.15293561 -0.34823803\n",
      "  -0.3346746  -0.0730779   0.45719227]\n",
      " [ 0.40709965  0.01281637  0.1193081   0.13747943 -0.3778533   0.36445249\n",
      "   0.30336012 -0.19296322 -0.38752684]\n",
      " [ 0.31755737 -0.25100128  0.4031961   0.15517834 -0.27349023  0.29377081\n",
      "   0.50679299  0.54187196 -0.08647939]\n",
      " [-0.75801983  0.40398863 -0.62279735 -0.66392717  0.02139396 -0.01202704\n",
      "  -0.74924252 -0.45997388  0.48639572]\n",
      " [ 0.82558539 -0.3656378   0.62319027  0.20917568 -0.12385479 -0.10655709\n",
      "   0.45415186  0.58738532 -0.26630798]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.60310332  1.15730352 -0.78368574  0.14503635  0.02851897 -1.07149768\n",
      "   0.56394982]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:25 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.64296927]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 25 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.66150311 -0.40848931  0.42133581  0.55036853 -0.23160712  0.14058178\n",
      "   1.08479253 -0.08621816 -0.33935675]\n",
      " [-0.50487833  0.26987417 -0.57930142 -0.53025094  0.15293561 -0.34823803\n",
      "  -0.34303133 -0.0730779   0.45719227]\n",
      " [ 0.41446708  0.02018381  0.1193081   0.14484687 -0.3778533   0.36445249\n",
      "   0.31072756 -0.19296322 -0.38752684]\n",
      " [ 0.32411222 -0.24444643  0.4031961   0.16173319 -0.27349023  0.29377081\n",
      "   0.51334783  0.54187196 -0.08647939]\n",
      " [-0.76704382  0.39496464 -0.62279735 -0.67295116  0.02139396 -0.01202704\n",
      "  -0.75826651 -0.45997388  0.48639572]\n",
      " [ 0.83410959 -0.35711359  0.62319027  0.21769989 -0.12385479 -0.10655709\n",
      "   0.46267607  0.58738532 -0.26630798]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.56212337  1.19272784 -0.77325716  0.17383781  0.05615921 -1.06551909\n",
      "   0.59487311]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:25 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.72949835]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 25 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.66657096 -0.40848931  0.42640366  0.55036853 -0.23160712  0.14564962\n",
      "   1.08986037 -0.08621816 -0.33935675]\n",
      " [-0.51074839  0.26987417 -0.58517149 -0.53025094  0.15293561 -0.3541081\n",
      "  -0.34890139 -0.0730779   0.45719227]\n",
      " [ 0.42017939  0.02018381  0.12502041  0.14484687 -0.3778533   0.3701648\n",
      "   0.31643987 -0.19296322 -0.38752684]\n",
      " [ 0.33008698 -0.24444643  0.40917086  0.16173319 -0.27349023  0.29974558\n",
      "   0.5193226   0.54187196 -0.08647939]\n",
      " [-0.77238811  0.39496464 -0.62814163 -0.67295116  0.02139396 -0.01737133\n",
      "  -0.7636108  -0.45997388  0.48639572]\n",
      " [ 0.83994463 -0.35711359  0.62902531  0.21769989 -0.12385479 -0.10072205\n",
      "   0.46851111  0.58738532 -0.26630798]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.53543426  1.21700306 -0.76939081  0.19439147  0.0781151  -1.06275975\n",
      "   0.61781973]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:25 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.50673669]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 26 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.65953802 -0.41552226  0.42640366  0.54333558 -0.23160712  0.14564962\n",
      "   1.08986037 -0.08621816 -0.34638969]\n",
      " [-0.50589848  0.27472408 -0.58517149 -0.52540103  0.15293561 -0.3541081\n",
      "  -0.34890139 -0.0730779   0.46204218]\n",
      " [ 0.41707992  0.01708434  0.12502041  0.1417474  -0.3778533   0.3701648\n",
      "   0.31643987 -0.19296322 -0.39062631]\n",
      " [ 0.32755601 -0.2469774   0.40917086  0.15920222 -0.27349023  0.29974558\n",
      "   0.5193226   0.54187196 -0.08901036]\n",
      " [-0.76413285  0.40321989 -0.62814163 -0.66469591  0.02139396 -0.01737133\n",
      "  -0.7636108  -0.45997388  0.49465097]\n",
      " [ 0.83338388 -0.36367434  0.62902531  0.21113914 -0.12385479 -0.10072205\n",
      "   0.46851111  0.58738532 -0.27286873]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.59876485  1.17804403 -0.79612614  0.15960648  0.0439079  -1.08572515\n",
      "   0.57938556]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:26 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.69978829]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 26 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.66581974 -0.41552226  0.43268538  0.54333558 -0.23160712  0.14564962\n",
      "   1.0961421  -0.08621816 -0.34638969]\n",
      " [-0.51293051  0.27472408 -0.59220352 -0.52540103  0.15293561 -0.3541081\n",
      "  -0.35593342 -0.0730779   0.46204218]\n",
      " [ 0.4227398   0.01708434  0.13068029  0.1417474  -0.3778533   0.3701648\n",
      "   0.32209975 -0.19296322 -0.39062631]\n",
      " [ 0.33438959 -0.2469774   0.41600443  0.15920222 -0.27349023  0.29974558\n",
      "   0.52615617  0.54187196 -0.08901036]\n",
      " [-0.77045632  0.40321989 -0.6344651  -0.66469591  0.02139396 -0.01737133\n",
      "  -0.76993426 -0.45997388  0.49465097]\n",
      " [ 0.84011901 -0.36367434  0.63576044  0.21113914 -0.12385479 -0.10072205\n",
      "   0.47524624  0.58738532 -0.27286873]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.56722991  1.20636415 -0.79008563  0.18175522  0.06845297 -1.08245238\n",
      "   0.60692664]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:26 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.69127209]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 26 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.64960476 -0.43173724  0.41647041  0.54333558 -0.23160712  0.14564962\n",
      "   1.07992712 -0.08621816 -0.34638969]\n",
      " [-0.49724902  0.29040558 -0.57652202 -0.52540103  0.15293561 -0.3541081\n",
      "  -0.34025192 -0.0730779   0.46204218]\n",
      " [ 0.40916713  0.00351166  0.11710761  0.1417474  -0.3778533   0.3701648\n",
      "   0.30852707 -0.19296322 -0.39062631]\n",
      " [ 0.31966302 -0.26170397  0.40127787  0.15920222 -0.27349023  0.29974558\n",
      "   0.51142961  0.54187196 -0.08901036]\n",
      " [-0.75422348  0.41945273 -0.61823226 -0.66469591  0.02139396 -0.01737133\n",
      "  -0.75370142 -0.45997388  0.49465097]\n",
      " [ 0.82361656 -0.38017679  0.619258    0.21113914 -0.12385479 -0.10072205\n",
      "   0.45874379  0.58738532 -0.27286873]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.64099383  1.14325257 -0.80734001  0.12942509  0.01410128 -1.09317315\n",
      "   0.54568362]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:26 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.60194102]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 26 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.65887731 -0.43173724  0.42574296  0.54333558 -0.23160712  0.14564962\n",
      "   1.07992712 -0.07694561 -0.34638969]\n",
      " [-0.50725509  0.29040558 -0.5865281  -0.52540103  0.15293561 -0.3541081\n",
      "  -0.34025192 -0.08308398  0.46204218]\n",
      " [ 0.4130326   0.00351166  0.12097309  0.1417474  -0.3778533   0.3701648\n",
      "   0.30852707 -0.18909775 -0.39062631]\n",
      " [ 0.33001369 -0.26170397  0.41162854  0.15920222 -0.27349023  0.29974558\n",
      "   0.51142961  0.55222263 -0.08901036]\n",
      " [-0.76461545  0.41945273 -0.62862423 -0.66469591  0.02139396 -0.01737133\n",
      "  -0.75370142 -0.47036585  0.49465097]\n",
      " [ 0.83354963 -0.38017679  0.62919106  0.21113914 -0.12385479 -0.10072205\n",
      "   0.45874379  0.59731838 -0.27286873]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.59330476  1.17792633 -0.79584384  0.15720706  0.05127538 -1.08659449\n",
      "   0.58783778]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:26 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.53925587]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 26 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.66912911 -0.43173724  0.42574296  0.55358739 -0.23160712  0.14564962\n",
      "   1.07992712 -0.07694561 -0.33613789]\n",
      " [-0.51479012  0.29040558 -0.5865281  -0.53293605  0.15293561 -0.3541081\n",
      "  -0.34025192 -0.08308398  0.45450716]\n",
      " [ 0.4153658   0.00351166  0.12097309  0.1440806  -0.3778533   0.3701648\n",
      "   0.30852707 -0.18909775 -0.3882931 ]\n",
      " [ 0.33551711 -0.26170397  0.41162854  0.16470564 -0.27349023  0.29974558\n",
      "   0.51142961  0.55222263 -0.08350694]\n",
      " [-0.77544707  0.41945273 -0.62862423 -0.67552753  0.02139396 -0.01737133\n",
      "  -0.75370142 -0.47036585  0.48381935]\n",
      " [ 0.84309971 -0.38017679  0.62919106  0.22068922 -0.12385479 -0.10072205\n",
      "   0.45874379  0.59731838 -0.26331865]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.53606675  1.21809524 -0.77517547  0.18816976  0.08554589 -1.07045451\n",
      "   0.62698368]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:26 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.46607947]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 26 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.66560172 -0.43173724  0.42574296  0.55358739 -0.23513451  0.14212223\n",
      "   1.07992712 -0.07694561 -0.33966528]\n",
      " [-0.5110636   0.29040558 -0.5865281  -0.53293605  0.15666213 -0.35038158\n",
      "  -0.34025192 -0.08308398  0.45823367]\n",
      " [ 0.4150848   0.00351166  0.12097309  0.1440806  -0.3781343   0.3698838\n",
      "   0.30852707 -0.18909775 -0.38857411]\n",
      " [ 0.33155993 -0.26170397  0.41162854  0.16470564 -0.27744741  0.2957884\n",
      "   0.51142961  0.55222263 -0.08746412]\n",
      " [-0.77136244  0.41945273 -0.62862423 -0.67552753  0.02547859 -0.01328669\n",
      "  -0.75370142 -0.47036585  0.48790398]\n",
      " [ 0.83810905 -0.38017679  0.62919106  0.22068922 -0.12884544 -0.10571271\n",
      "   0.45874379  0.59731838 -0.2683093 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.59405855  1.18553596 -0.80040225  0.15889284  0.05254155 -1.09530923\n",
      "   0.59289152]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:26 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.34056498]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 26 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.66748382 -0.42985514  0.42574296  0.55358739 -0.23325241  0.14400433\n",
      "   1.07992712 -0.07694561 -0.33778318]\n",
      " [-0.51148269  0.28998649 -0.5865281  -0.53293605  0.15624304 -0.35080067\n",
      "  -0.34025192 -0.08308398  0.45781459]\n",
      " [ 0.41487667  0.00330353  0.12097309  0.1440806  -0.37834243  0.36967567\n",
      "   0.30852707 -0.18909775 -0.38878223]\n",
      " [ 0.33155293 -0.26171097  0.41162854  0.16470564 -0.27745442  0.29578139\n",
      "   0.51142961  0.55222263 -0.08747113]\n",
      " [-0.77277143  0.41804374 -0.62862423 -0.67552753  0.0240696  -0.01469568\n",
      "  -0.75370142 -0.47036585  0.486495  ]\n",
      " [ 0.83853844 -0.3797474   0.62919106  0.22068922 -0.12841605 -0.10528332\n",
      "   0.45874379  0.59731838 -0.26787992]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.63230067  1.16830942 -0.81994253  0.13956364  0.03341348 -1.11584444\n",
      "   0.57419999]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:26 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.55665718]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 26 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 6.52703261e-01 -4.44635699e-01  4.25742956e-01  5.53587385e-01\n",
      "  -2.48032969e-01  1.29223772e-01  1.06514656e+00 -7.69456122e-02\n",
      "  -3.37783185e-01]\n",
      " [-5.00188765e-01  3.01280415e-01 -5.86528098e-01 -5.32936053e-01\n",
      "   1.67536964e-01 -3.39506743e-01 -3.28958001e-01 -8.30839817e-02\n",
      "   4.57814588e-01]\n",
      " [ 4.04008450e-01 -7.56468765e-03  1.20973085e-01  1.44080603e-01\n",
      "  -3.89210647e-01  3.58807450e-01  2.97658851e-01 -1.89097749e-01\n",
      "  -3.88782232e-01]\n",
      " [ 3.22129199e-01 -2.71134701e-01  4.11628539e-01  1.64705645e-01\n",
      "  -2.86878149e-01  2.86357662e-01  5.02005879e-01  5.52222630e-01\n",
      "  -8.74711280e-02]\n",
      " [-7.58619685e-01  4.32195487e-01 -6.28624227e-01 -6.75527531e-01\n",
      "   3.82213452e-02 -5.43942125e-04 -7.39549683e-01 -4.70365852e-01\n",
      "   4.86494996e-01]\n",
      " [ 8.28068021e-01 -3.90217820e-01  6.29191060e-01  2.20689219e-01\n",
      "  -1.38886472e-01 -1.15753738e-01  4.48273376e-01  5.97318379e-01\n",
      "  -2.67879917e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.70098938  1.11517718 -0.84188506  0.09339278 -0.01092951 -1.13301091\n",
      "   0.52854988]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:26 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.62522568]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 26 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 6.61157890e-01 -4.44635699e-01  4.25742956e-01  5.62042014e-01\n",
      "  -2.48032969e-01  1.29223772e-01  1.07360119e+00 -7.69456122e-02\n",
      "  -3.37783185e-01]\n",
      " [-5.09896949e-01  3.01280415e-01 -5.86528098e-01 -5.42644237e-01\n",
      "   1.67536964e-01 -3.39506743e-01 -3.38666185e-01 -8.30839817e-02\n",
      "   4.57814588e-01]\n",
      " [ 4.11811703e-01 -7.56468765e-03  1.20973085e-01  1.51883857e-01\n",
      "  -3.89210647e-01  3.58807450e-01  3.05462104e-01 -1.89097749e-01\n",
      "  -3.88782232e-01]\n",
      " [ 3.30709680e-01 -2.71134701e-01  4.11628539e-01  1.73286126e-01\n",
      "  -2.86878149e-01  2.86357662e-01  5.10586360e-01  5.52222630e-01\n",
      "  -8.74711280e-02]\n",
      " [-7.67372354e-01  4.32195487e-01 -6.28624227e-01 -6.84280200e-01\n",
      "   3.82213452e-02 -5.43942125e-04 -7.48302353e-01 -4.70365852e-01\n",
      "   4.86494996e-01]\n",
      " [ 8.37890217e-01 -3.90217820e-01  6.29191060e-01  2.30511415e-01\n",
      "  -1.38886472e-01 -1.15753738e-01  4.58095571e-01  5.97318379e-01\n",
      "  -2.67879917e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.65708109  1.15497932 -0.83293208  0.12411428  0.02107344 -1.12852623\n",
      "   0.56442871]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:26 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.59643822]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 26 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.67201122 -0.4446357   0.42574296  0.56204201 -0.23717964  0.1400771\n",
      "   1.08445452 -0.07694561 -0.33778318]\n",
      " [-0.51954936  0.30128042 -0.5865281  -0.54264424  0.15788456 -0.34915915\n",
      "  -0.34831859 -0.08308398  0.45781459]\n",
      " [ 0.41924057 -0.00756469  0.12097309  0.15188386 -0.38178178  0.36623632\n",
      "   0.31289097 -0.18909775 -0.38878223]\n",
      " [ 0.33930741 -0.2711347   0.41162854  0.17328613 -0.27828041  0.2949554\n",
      "   0.5191841   0.55222263 -0.08747113]\n",
      " [-0.7782286   0.43219549 -0.62862423 -0.6842802   0.02736509 -0.01140019\n",
      "  -0.7591586  -0.47036585  0.486495  ]\n",
      " [ 0.84764275 -0.39021782  0.62919106  0.23051141 -0.12913394 -0.10600121\n",
      "   0.4678481   0.59731838 -0.26787992]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.6085125   1.19549698 -0.82006511  0.15642553  0.05500484 -1.11950558\n",
      "   0.60032622]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:26 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.56949637]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 26 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.65802704 -0.4446357   0.42574296  0.54805783 -0.23717964  0.12609292\n",
      "   1.08445452 -0.07694561 -0.35176737]\n",
      " [-0.50618292  0.30128042 -0.5865281  -0.5292778   0.15788456 -0.33579271\n",
      "  -0.34831859 -0.08308398  0.47118103]\n",
      " [ 0.41035193 -0.00756469  0.12097309  0.14299522 -0.38178178  0.35734768\n",
      "   0.31289097 -0.18909775 -0.39767087]\n",
      " [ 0.32823793 -0.2711347   0.41162854  0.16221664 -0.27828041  0.28388592\n",
      "   0.5191841   0.55222263 -0.09854061]\n",
      " [-0.7645969   0.43219549 -0.62862423 -0.67064849  0.02736509  0.00223152\n",
      "  -0.7591586  -0.47036585  0.5001267 ]\n",
      " [ 0.83675759 -0.39021782  0.62919106  0.21962626 -0.12913394 -0.11688636\n",
      "   0.4678481   0.59731838 -0.27876508]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.67832429  1.14396579 -0.83948492  0.11217842  0.00804775 -1.13845412\n",
      "   0.55361275]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:26 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.62108341]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 26 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.66745684 -0.4352059   0.42574296  0.55748763 -0.23717964  0.13552272\n",
      "   1.09388431 -0.07694561 -0.35176737]\n",
      " [-0.51610686  0.29135647 -0.5865281  -0.53920174  0.15788456 -0.34571666\n",
      "  -0.35824254 -0.08308398  0.47118103]\n",
      " [ 0.41991405  0.00199743  0.12097309  0.15255734 -0.38178178  0.3669098\n",
      "   0.32245309 -0.18909775 -0.39767087]\n",
      " [ 0.3371074  -0.26226524  0.41162854  0.17108611 -0.27828041  0.29275538\n",
      "   0.52805356  0.55222263 -0.09854061]\n",
      " [-0.77442511  0.42236728 -0.62862423 -0.6804767   0.02736509 -0.00759669\n",
      "  -0.76898681 -0.47036585  0.5001267 ]\n",
      " [ 0.84560325 -0.38137216  0.62919106  0.22847191 -0.12913394 -0.10804071\n",
      "   0.47669376  0.59731838 -0.27876508]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.6337374   1.1831053  -0.83079362  0.14657108  0.04083863 -1.13190906\n",
      "   0.58635792]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:26 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.64755366]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 26 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.67613534 -0.4265274   0.42574296  0.56616614 -0.23717964  0.13552272\n",
      "   1.10256282 -0.07694561 -0.35176737]\n",
      " [-0.52446935  0.28299399 -0.5865281  -0.54756423  0.15788456 -0.34571666\n",
      "  -0.36660502 -0.08308398  0.47118103]\n",
      " [ 0.42733676  0.00942013  0.12097309  0.15998004 -0.38178178  0.3669098\n",
      "   0.3298758  -0.18909775 -0.39767087]\n",
      " [ 0.34383133 -0.25554131  0.41162854  0.17781004 -0.27828041  0.29275538\n",
      "   0.53477749  0.55222263 -0.09854061]\n",
      " [-0.78323546  0.41355693 -0.62862423 -0.68928705  0.02736509 -0.00759669\n",
      "  -0.77779716 -0.47036585  0.5001267 ]\n",
      " [ 0.85410721 -0.3728682   0.62919106  0.23697587 -0.12913394 -0.10804071\n",
      "   0.48519772  0.59731838 -0.27876508]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.59351835  1.2180165  -0.82091564  0.17513934  0.06836381 -1.12621139\n",
      "   0.6170465 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:26 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.73446281]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 26 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.68099139 -0.4265274   0.430599    0.56616614 -0.23717964  0.14037876\n",
      "   1.10741886 -0.07694561 -0.35176737]\n",
      " [-0.53012087  0.28299399 -0.59217962 -0.54756423  0.15788456 -0.35136818\n",
      "  -0.37225655 -0.08308398  0.47118103]\n",
      " [ 0.43293281  0.00942013  0.12656914  0.15998004 -0.38178178  0.37250585\n",
      "   0.33547185 -0.18909775 -0.39767087]\n",
      " [ 0.34962496 -0.25554131  0.41742217  0.17781004 -0.27828041  0.29854902\n",
      "   0.54057113  0.55222263 -0.09854061]\n",
      " [-0.78835585  0.41355693 -0.63374462 -0.68928705  0.02736509 -0.01271709\n",
      "  -0.78291755 -0.47036585  0.5001267 ]\n",
      " [ 0.85972037 -0.3728682   0.63480423  0.23697587 -0.12913394 -0.10242754\n",
      "   0.49081089  0.59731838 -0.27876508]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.56762486  1.24163481 -0.81731551  0.19524433  0.08984577 -1.12362211\n",
      "   0.63945343]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:26 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.49791107]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 27 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.67408283 -0.43343596  0.430599    0.55925758 -0.23717964  0.14037876\n",
      "   1.10741886 -0.07694561 -0.35867592]\n",
      " [-0.52521667  0.28789819 -0.59217962 -0.54266003  0.15788456 -0.35136818\n",
      "  -0.37225655 -0.08308398  0.47608523]\n",
      " [ 0.42978149  0.00626882  0.12656914  0.15682872 -0.38178178  0.37250585\n",
      "   0.33547185 -0.18909775 -0.40082219]\n",
      " [ 0.34694785 -0.25821842  0.41742217  0.17513292 -0.27828041  0.29854902\n",
      "   0.54057113  0.55222263 -0.10121772]\n",
      " [-0.78024328  0.42166949 -0.63374462 -0.68117448  0.02736509 -0.01271709\n",
      "  -0.78291755 -0.47036585  0.50823927]\n",
      " [ 0.85312735 -0.37946123  0.63480423  0.23038285 -0.12913394 -0.10242754\n",
      "   0.49081089  0.59731838 -0.2853581 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.62986266  1.20335144 -0.84344422  0.16095207  0.05603633 -1.14619152\n",
      "   0.60152169]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:27 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.70508605]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 27 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.68011499 -0.43343596  0.43663117  0.55925758 -0.23717964  0.14037876\n",
      "   1.11345103 -0.07694561 -0.35867592]\n",
      " [-0.53207389  0.28789819 -0.59903684 -0.54266003  0.15788456 -0.35136818\n",
      "  -0.37911376 -0.08308398  0.47608523]\n",
      " [ 0.43542029  0.00626882  0.13220793  0.15682872 -0.38178178  0.37250585\n",
      "   0.34111064 -0.18909775 -0.40082219]\n",
      " [ 0.35366283 -0.25821842  0.42413716  0.17513292 -0.27828041  0.29854902\n",
      "   0.54728611  0.55222263 -0.10121772]\n",
      " [-0.7863074   0.42166949 -0.63980873 -0.68117448  0.02736509 -0.01271709\n",
      "  -0.78898167 -0.47036585  0.50823927]\n",
      " [ 0.85960124 -0.37946123  0.64127811  0.23038285 -0.12913394 -0.10242754\n",
      "   0.49728477  0.59731838 -0.2853581 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.5992005   1.23098819 -0.83780318  0.1826997   0.08015719 -1.14312442\n",
      "   0.62845984]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:27 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.69372736]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 27 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.66395845 -0.4495925   0.42047462  0.55925758 -0.23717964  0.14037876\n",
      "   1.09729448 -0.07694561 -0.35867592]\n",
      " [-0.51624102  0.30373106 -0.58320397 -0.54266003  0.15788456 -0.35136818\n",
      "  -0.36328089 -0.08308398  0.47608523]\n",
      " [ 0.42165063 -0.00750084  0.11843827  0.15682872 -0.38178178  0.37250585\n",
      "   0.32734098 -0.18909775 -0.40082219]\n",
      " [ 0.33868743 -0.27319382  0.40916176  0.17513292 -0.27828041  0.29854902\n",
      "   0.53231071  0.55222263 -0.10121772]\n",
      " [-0.77014234  0.43783455 -0.62364368 -0.68117448  0.02736509 -0.01271709\n",
      "  -0.77281661 -0.47036585  0.50823927]\n",
      " [ 0.84313468 -0.39592778  0.62481155  0.23038285 -0.12913394 -0.10242754\n",
      "   0.48081821  0.59731838 -0.2853581 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.67289852  1.1677734  -0.8545715   0.13007755  0.0253258  -1.15363766\n",
      "   0.56695028]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:27 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.60506233]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 27 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.67327316 -0.4495925   0.42978933  0.55925758 -0.23717964  0.14037876\n",
      "   1.09729448 -0.0676309  -0.35867592]\n",
      " [-0.52626067  0.30373106 -0.59322362 -0.54266003  0.15788456 -0.35136818\n",
      "  -0.36328089 -0.09310363  0.47608523]\n",
      " [ 0.42566628 -0.00750084  0.12245392  0.15682872 -0.38178178  0.37250585\n",
      "   0.32734098 -0.1850821  -0.40082219]\n",
      " [ 0.34901164 -0.27319382  0.41948596  0.17513292 -0.27828041  0.29854902\n",
      "   0.53231071  0.56254684 -0.10121772]\n",
      " [-0.78036427  0.43783455 -0.63386561 -0.68117448  0.02736509 -0.01271709\n",
      "  -0.77281661 -0.48058778  0.50823927]\n",
      " [ 0.85286636 -0.39592778  0.63454324  0.23038285 -0.12913394 -0.10242754\n",
      "   0.48081821  0.60705006 -0.2853581 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.625711    1.20233959 -0.84350141  0.15776993  0.06240798 -1.14730416\n",
      "   0.60882831]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:27 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.53659841]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 27 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.68373277 -0.4495925   0.42978933  0.56971719 -0.23717964  0.14037876\n",
      "   1.09729448 -0.0676309  -0.34821631]\n",
      " [-0.53409129  0.30373106 -0.59322362 -0.55049064  0.15788456 -0.35136818\n",
      "  -0.36328089 -0.09310363  0.46825461]\n",
      " [ 0.42826157 -0.00750084  0.12245392  0.15942401 -0.38178178  0.37250585\n",
      "   0.32734098 -0.1850821  -0.3982269 ]\n",
      " [ 0.35483887 -0.27319382  0.41948596  0.18096016 -0.27828041  0.29854902\n",
      "   0.53231071  0.56254684 -0.09539049]\n",
      " [-0.79139384  0.43783455 -0.63386561 -0.69220406  0.02736509 -0.01271709\n",
      "  -0.77281661 -0.48058778  0.4972097 ]\n",
      " [ 0.86270775 -0.39592778  0.63454324  0.24022423 -0.12913394 -0.10242754\n",
      "   0.48081821  0.60705006 -0.27551671]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.56809615  1.24298959 -0.82299141  0.18918694  0.09721792 -1.13127445\n",
      "   0.64855508]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:27 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.45362811]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 27 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.68042521 -0.4495925   0.42978933  0.56971719 -0.24048719  0.13707121\n",
      "   1.09729448 -0.0676309  -0.35152387]\n",
      " [-0.5305074   0.30373106 -0.59322362 -0.55049064  0.16146844 -0.3477843\n",
      "  -0.36328089 -0.09310363  0.47183849]\n",
      " [ 0.42796985 -0.00750084  0.12245392  0.15942401 -0.38207349  0.37221414\n",
      "   0.32734098 -0.1850821  -0.39851861]\n",
      " [ 0.35098365 -0.27319382  0.41948596  0.18096016 -0.28213564  0.29469379\n",
      "   0.53231071  0.56254684 -0.09924572]\n",
      " [-0.78754101  0.43783455 -0.63386561 -0.69220406  0.03121792 -0.00886426\n",
      "  -0.77281661 -0.48058778  0.50106253]\n",
      " [ 0.8578645  -0.39592778  0.63454324  0.24022423 -0.13397718 -0.10727078\n",
      "   0.48081821  0.60705006 -0.28035996]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.62431193  1.21154264 -0.84747512  0.16078732  0.06520433 -1.15547914\n",
      "   0.61550121]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:27 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.32339737]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 27 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.68238284 -0.44763487  0.42978933  0.56971719 -0.23852956  0.13902884\n",
      "   1.09729448 -0.0676309  -0.34956623]\n",
      " [-0.53102659  0.30321187 -0.59322362 -0.55049064  0.16094925 -0.34830348\n",
      "  -0.36328089 -0.09310363  0.47131931]\n",
      " [ 0.42786291 -0.00760779  0.12245392  0.15942401 -0.38218043  0.3721072\n",
      "   0.32734098 -0.1850821  -0.39862555]\n",
      " [ 0.35106235 -0.27311512  0.41948596  0.18096016 -0.28205694  0.29477249\n",
      "   0.53231071  0.56254684 -0.09916701]\n",
      " [-0.78906601  0.43630955 -0.63386561 -0.69220406  0.02969293 -0.01038925\n",
      "  -0.77281661 -0.48058778  0.49953753]\n",
      " [ 0.85839185 -0.39540044  0.63454324  0.24022423 -0.13344984 -0.10674344\n",
      "   0.48081821  0.60705006 -0.27983261]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.65969346  1.19582593 -0.86568537  0.1429896   0.04759227 -1.17470259\n",
      "   0.5983381 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:27 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.55719648]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 27 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.66757505 -0.46244267  0.42978933  0.56971719 -0.25333736  0.12422104\n",
      "   1.08248668 -0.0676309  -0.34956623]\n",
      " [-0.5194875   0.31475097 -0.59322362 -0.55049064  0.17248835 -0.33676439\n",
      "  -0.3517418  -0.09310363  0.47131931]\n",
      " [ 0.41676739 -0.01870331  0.12245392  0.15942401 -0.39327595  0.36101168\n",
      "   0.31624546 -0.1850821  -0.39862555]\n",
      " [ 0.34133186 -0.28284561  0.41948596  0.18096016 -0.29178742  0.285042\n",
      "   0.52258022  0.56254684 -0.09916701]\n",
      " [-0.7748626   0.45051296 -0.63386561 -0.69220406  0.04389633  0.00381415\n",
      "  -0.75861321 -0.48058778  0.49953753]\n",
      " [ 0.84768171 -0.40611058  0.63454324  0.24022423 -0.14415998 -0.11745358\n",
      "   0.47010808  0.60705006 -0.27983261]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.72843161  1.14260452 -0.88731411  0.09649142  0.00285099 -1.19178862\n",
      "   0.55235304]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:27 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.63415986]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 27 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.67559723 -0.46244267  0.42978933  0.57773937 -0.25333736  0.12422104\n",
      "   1.09050887 -0.0676309  -0.34956623]\n",
      " [-0.52893611  0.31475097 -0.59322362 -0.55993925  0.17248835 -0.33676439\n",
      "  -0.36119041 -0.09310363  0.47131931]\n",
      " [ 0.42457507 -0.01870331  0.12245392  0.1672317  -0.39327595  0.36101168\n",
      "   0.32405314 -0.1850821  -0.39862555]\n",
      " [ 0.34986774 -0.28284561  0.41948596  0.18949603 -0.29178742  0.285042\n",
      "   0.5311161   0.56254684 -0.09916701]\n",
      " [-0.78317142  0.45051296 -0.63386561 -0.70051287  0.04389633  0.00381415\n",
      "  -0.76692202 -0.48058778  0.49953753]\n",
      " [ 0.85718158 -0.40611058  0.63454324  0.2497241  -0.14415998 -0.11745358\n",
      "   0.47960795  0.60705006 -0.27983261]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.68599394  1.18124409 -0.87906456  0.12659639  0.0342459  -1.18765231\n",
      "   0.58740944]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:27 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.60028653]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 27 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.68630051 -0.46244267  0.42978933  0.57773937 -0.24263408  0.13492433\n",
      "   1.10121215 -0.0676309  -0.34956623]\n",
      " [-0.53862499  0.31475097 -0.59322362 -0.55993925  0.16279947 -0.34645327\n",
      "  -0.37087929 -0.09310363  0.47131931]\n",
      " [ 0.43214923 -0.01870331  0.12245392  0.1672317  -0.38570179  0.36858584\n",
      "   0.3316273  -0.1850821  -0.39862555]\n",
      " [ 0.35857596 -0.28284561  0.41948596  0.18949603 -0.28307921  0.29375022\n",
      "   0.53982432  0.56254684 -0.09916701]\n",
      " [-0.79390048  0.45051296 -0.63386561 -0.70051287  0.03316726 -0.00691492\n",
      "  -0.77765109 -0.48058778  0.49953753]\n",
      " [ 0.86696186 -0.40611058  0.63454324  0.2497241  -0.1343797  -0.1076733\n",
      "   0.48938823  0.60705006 -0.27983261]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.6380398   1.22138771 -0.86667414  0.15881226  0.06808364 -1.17892128\n",
      "   0.62316309]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:27 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.56962117]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 27 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.67222635 -0.46244267  0.42978933  0.56366521 -0.24263408  0.12085017\n",
      "   1.10121215 -0.0676309  -0.3636404 ]\n",
      " [-0.52509658  0.31475097 -0.59322362 -0.54641084  0.16279947 -0.33292486\n",
      "  -0.37087929 -0.09310363  0.48484771]\n",
      " [ 0.42297485 -0.01870331  0.12245392  0.15805731 -0.38570179  0.35941145\n",
      "   0.3316273  -0.1850821  -0.40779994]\n",
      " [ 0.34724763 -0.28284561  0.41948596  0.17816771 -0.28307921  0.2824219\n",
      "   0.53982432  0.56254684 -0.11049534]\n",
      " [-0.78015942  0.45051296 -0.63386561 -0.6867718   0.03316726  0.00682615\n",
      "  -0.77765109 -0.48058778  0.5132786 ]\n",
      " [ 0.85578617 -0.40611058  0.63454324  0.23854842 -0.1343797  -0.11884899\n",
      "   0.48938823  0.60705006 -0.2910083 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.70786194  1.16967329 -0.88581546  0.11422307  0.02077382 -1.19767477\n",
      "   0.57605931]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:27 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.62602395]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 27 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.68142977 -0.45323925  0.42978933  0.57286863 -0.24263408  0.13005359\n",
      "   1.11041557 -0.0676309  -0.3636404 ]\n",
      " [-0.53487252  0.30497503 -0.59322362 -0.55618678  0.16279947 -0.3427008\n",
      "  -0.38065523 -0.09310363  0.48484771]\n",
      " [ 0.43245519 -0.00922296  0.12245392  0.16753765 -0.38570179  0.3688918\n",
      "   0.34110765 -0.1850821  -0.40779994]\n",
      " [ 0.35613493 -0.27395831  0.41948596  0.18705501 -0.28307921  0.2913092\n",
      "   0.54871161  0.56254684 -0.11049534]\n",
      " [-0.78977083  0.44090155 -0.63386561 -0.69638322  0.03316726 -0.00278527\n",
      "  -0.7872625  -0.48058778  0.5132786 ]\n",
      " [ 0.86464898 -0.39724776  0.63454324  0.24741123 -0.1343797  -0.10998617\n",
      "   0.49825104  0.60705006 -0.2910083 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.66408468  1.2082115  -0.87756863  0.14827662  0.05332701 -1.19139663\n",
      "   0.60856191]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:27 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.65209505]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 27 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.68989519 -0.44477383  0.42978933  0.58133404 -0.24263408  0.13005359\n",
      "   1.11888099 -0.0676309  -0.3636404 ]\n",
      " [-0.54320952  0.29663802 -0.59322362 -0.56452379  0.16279947 -0.3427008\n",
      "  -0.38899223 -0.09310363  0.48484771]\n",
      " [ 0.4399101  -0.00176805  0.12245392  0.17499256 -0.38570179  0.3688918\n",
      "   0.34856256 -0.1850821  -0.40779994]\n",
      " [ 0.36299242 -0.26710082  0.41948596  0.1939125  -0.28307921  0.2913092\n",
      "   0.5555691   0.56254684 -0.11049534]\n",
      " [-0.79837036  0.43230202 -0.63386561 -0.70498274  0.03316726 -0.00278527\n",
      "  -0.79586203 -0.48058778  0.5132786 ]\n",
      " [ 0.87310543 -0.38879132  0.63454324  0.25586768 -0.1343797  -0.10998617\n",
      "   0.50670749  0.60705006 -0.2910083 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.62462059  1.24259202 -0.86819809  0.17659037  0.08070718 -1.18595299\n",
      "   0.63898244]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:27 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.73928114]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 27 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.69455337 -0.44477383  0.43444751  0.58133404 -0.24263408  0.13471177\n",
      "   1.12353917 -0.0676309  -0.3636404 ]\n",
      " [-0.54864818  0.29663802 -0.59866228 -0.56452379  0.16279947 -0.34813946\n",
      "  -0.39443089 -0.09310363  0.48484771]\n",
      " [ 0.44538446 -0.00176805  0.12792828  0.17499256 -0.38570179  0.37436616\n",
      "   0.35403692 -0.1850821  -0.40779994]\n",
      " [ 0.36860312 -0.26710082  0.42509667  0.1939125  -0.28307921  0.2969199\n",
      "   0.5611798   0.56254684 -0.11049534]\n",
      " [-0.80328064  0.43230202 -0.63877588 -0.70498274  0.03316726 -0.00769554\n",
      "  -0.8007723  -0.48058778  0.5132786 ]\n",
      " [ 0.87850354 -0.38879132  0.63994135  0.25586768 -0.1343797  -0.10458807\n",
      "   0.5121056   0.60705006 -0.2910083 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.59949452  1.26556736 -0.86483726  0.19624948  0.10171462 -1.18351548\n",
      "   0.6608518 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:27 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.48897894]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 28 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.68778986 -0.45153734  0.43444751  0.57457054 -0.24263408  0.13471177\n",
      "   1.12353917 -0.0676309  -0.37040391]\n",
      " [-0.54371909  0.3015671  -0.59866228 -0.5595947   0.16279947 -0.34813946\n",
      "  -0.39443089 -0.09310363  0.4897768 ]\n",
      " [ 0.44220025 -0.00495227  0.12792828  0.17180835 -0.38570179  0.37436616\n",
      "   0.35403692 -0.1850821  -0.41098415]\n",
      " [ 0.36580282 -0.26990112  0.42509667  0.1911122  -0.28307921  0.2969199\n",
      "   0.5611798   0.56254684 -0.11329563]\n",
      " [-0.79533257  0.44025008 -0.63877588 -0.69703468  0.03316726 -0.00769554\n",
      "  -0.8007723  -0.48058778  0.52122666]\n",
      " [ 0.87190745 -0.39538741  0.63994135  0.24927158 -0.1343797  -0.10458807\n",
      "   0.5121056   0.60705006 -0.2976044 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.66058719  1.22800848 -0.89036363  0.16249529  0.068352   -1.20568766\n",
      "   0.62347985]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:28 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.71035139]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 28 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.69358196 -0.45153734  0.44023961  0.57457054 -0.24263408  0.13471177\n",
      "   1.12933127 -0.0676309  -0.37040391]\n",
      " [-0.5503899   0.3015671  -0.60533309 -0.5595947   0.16279947 -0.34813946\n",
      "  -0.4011017  -0.09310363  0.4897768 ]\n",
      " [ 0.44780128 -0.00495227  0.13352932  0.17180835 -0.38570179  0.37436616\n",
      "   0.35963796 -0.1850821  -0.41098415]\n",
      " [ 0.37238155 -0.26990112  0.4316754   0.1911122  -0.28307921  0.2969199\n",
      "   0.56775853  0.56254684 -0.11329563]\n",
      " [-0.80114751  0.44025008 -0.64459082 -0.69703468  0.03316726 -0.00769554\n",
      "  -0.80658724 -0.48058778  0.52122666]\n",
      " [ 0.87812473 -0.39538741  0.64615863  0.24927158 -0.1343797  -0.10458807\n",
      "   0.51832288  0.60705006 -0.2976044 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.63078926  1.25495432 -0.88508942  0.18382734  0.09202555 -1.20280734\n",
      "   0.64980004]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:28 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.69608769]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 28 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.67747969 -0.46763961  0.42413734  0.57457054 -0.24263408  0.13471177\n",
      "   1.113229   -0.0676309  -0.37040391]\n",
      " [-0.53443773  0.31751927 -0.58938092 -0.5595947   0.16279947 -0.34813946\n",
      "  -0.38514953 -0.09310363  0.4897768 ]\n",
      " [ 0.43385599 -0.01889756  0.11958403  0.17180835 -0.38570179  0.37436616\n",
      "   0.34569266 -0.1850821  -0.41098415]\n",
      " [ 0.35719436 -0.28508831  0.41648821  0.1911122  -0.28307921  0.2969199\n",
      "   0.55257135  0.56254684 -0.11329563]\n",
      " [-0.78504649  0.45635111 -0.62848979 -0.69703468  0.03316726 -0.00769554\n",
      "  -0.79048621 -0.48058778  0.52122666]\n",
      " [ 0.86170217 -0.41180997  0.62973607  0.24927158 -0.1343797  -0.10458807\n",
      "   0.50190032  0.60705006 -0.2976044 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.7044178   1.19166639 -0.90141959  0.13094038  0.03675859 -1.21314387\n",
      "   0.58806142]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:28 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.60834898]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 28 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.68681496 -0.46763961  0.43347261  0.57457054 -0.24263408  0.13471177\n",
      "   1.113229   -0.05829563 -0.37040391]\n",
      " [-0.54444643  0.31751927 -0.59938962 -0.5595947   0.16279947 -0.34813946\n",
      "  -0.38514953 -0.10311233  0.4897768 ]\n",
      " [ 0.43801014 -0.01889756  0.12373817  0.17180835 -0.38570179  0.37436616\n",
      "   0.34569266 -0.18092795 -0.41098415]\n",
      " [ 0.36746961 -0.28508831  0.42676345  0.1911122  -0.28307921  0.2969199\n",
      "   0.55257135  0.57282208 -0.11329563]\n",
      " [-0.79509233  0.45635111 -0.63853564 -0.69703468  0.03316726 -0.00769554\n",
      "  -0.79048621 -0.49063363  0.52122666]\n",
      " [ 0.87122977 -0.41180997  0.63926367  0.24927158 -0.1343797  -0.10458807\n",
      "   0.50190032  0.61657767 -0.2976044 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.65776032  1.22608505 -0.89075936  0.15851785  0.07370522 -1.20704224\n",
      "   0.62962186]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:28 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.53416381]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 28 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.69746711 -0.46763961  0.43347261  0.58522269 -0.24263408  0.13471177\n",
      "   1.113229   -0.05829563 -0.35975176]\n",
      " [-0.5525573   0.31751927 -0.59938962 -0.56770557  0.16279947 -0.34813946\n",
      "  -0.38514953 -0.10311233  0.48166593]\n",
      " [ 0.44086284 -0.01889756  0.12373817  0.17466105 -0.38570179  0.37436616\n",
      "   0.34569266 -0.18092795 -0.40813145]\n",
      " [ 0.373612   -0.28508831  0.42676345  0.19725459 -0.28307921  0.2969199\n",
      "   0.55257135  0.57282208 -0.10715324]\n",
      " [-0.80630381  0.45635111 -0.63853564 -0.70824616  0.03316726 -0.00769554\n",
      "  -0.79048621 -0.49063363  0.51001518]\n",
      " [ 0.8813423  -0.41180997  0.63926367  0.25938412 -0.1343797  -0.10458807\n",
      "   0.50190032  0.61657767 -0.28749186]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.59980265  1.26718241 -0.87041117  0.19036822  0.10903145 -1.1911212\n",
      "   0.6698938 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:28 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.44107078]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 28 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.69438464 -0.46763961  0.43347261  0.58522269 -0.24571655  0.1316293\n",
      "   1.113229   -0.05829563 -0.36283423]\n",
      " [-0.54913115  0.31751927 -0.59938962 -0.56770557  0.16622562 -0.3447133\n",
      "  -0.38514953 -0.10311233  0.48509208]\n",
      " [ 0.44057206 -0.01889756  0.12373817  0.17466105 -0.38599257  0.37407538\n",
      "   0.34569266 -0.18092795 -0.40842223]\n",
      " [ 0.36987604 -0.28508831  0.42676345  0.19725459 -0.28681517  0.29318393\n",
      "   0.55257135  0.57282208 -0.11088921]\n",
      " [-0.80268954  0.45635111 -0.63853564 -0.70824616  0.03678153 -0.00408127\n",
      "  -0.79048621 -0.49063363  0.51362944]\n",
      " [ 0.87666748 -0.41180997  0.63926367  0.25938412 -0.13905452 -0.10926289\n",
      "   0.50190032  0.61657767 -0.29216669]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.65417065  1.23688873 -0.89413141  0.16289342  0.07806237 -1.2146466\n",
      "   0.63793623]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:28 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.30633749]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 28 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.69638877 -0.46563547  0.43347261  0.58522269 -0.24371242  0.13363343\n",
      "   1.113229   -0.05829563 -0.36083009]\n",
      " [-0.5497405   0.31690992 -0.59938962 -0.56770557  0.16561627 -0.34532265\n",
      "  -0.38514953 -0.10311233  0.48448273]\n",
      " [ 0.4405612  -0.01890842  0.12373817  0.17466105 -0.38600343  0.37406452\n",
      "   0.34569266 -0.18092795 -0.40843309]\n",
      " [ 0.37003658 -0.28492776  0.42676345  0.19725459 -0.28665463  0.29334448\n",
      "   0.55257135  0.57282208 -0.11072866]\n",
      " [-0.80430069  0.45473996 -0.63853564 -0.70824616  0.03517039 -0.00569242\n",
      "  -0.79048621 -0.49063363  0.5120183 ]\n",
      " [ 0.87728197 -0.41119549  0.63926367  0.25938412 -0.13844004 -0.1086484\n",
      "   0.50190032  0.61657767 -0.2915522 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.68671822  1.22264006 -0.91101511  0.14660877  0.06194915 -1.23254228\n",
      "   0.62227751]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:28 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.55749163]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 28 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.68156987 -0.48045437  0.43347261  0.58522269 -0.25853131  0.11881453\n",
      "   1.0984101  -0.05829563 -0.36083009]\n",
      " [-0.5379966   0.32865382 -0.59938962 -0.56770557  0.17736017 -0.33357876\n",
      "  -0.37340563 -0.10311233  0.48448273]\n",
      " [ 0.42926449 -0.03020513  0.12373817  0.17466105 -0.39730014  0.36276781\n",
      "   0.33439595 -0.18092795 -0.40843309]\n",
      " [ 0.36003379 -0.29493056  0.42676345  0.19725459 -0.29665742  0.28334168\n",
      "   0.54256855  0.57282208 -0.11072866]\n",
      " [-0.79006734  0.46897331 -0.63853564 -0.70824616  0.04940374  0.00854093\n",
      "  -0.77625287 -0.49063363  0.5120183 ]\n",
      " [ 0.86636911 -0.42210834  0.63926367  0.25938412 -0.14935289 -0.11956126\n",
      "   0.49098746  0.61657767 -0.2915522 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.75548334  1.1693814  -0.9323681   0.09982404  0.01685706 -1.24957986\n",
      "   0.57601201]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:28 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.64312212]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 28 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.68917309 -0.48045437  0.43347261  0.5928259  -0.25853131  0.11881453\n",
      "   1.10601332 -0.05829563 -0.36083009]\n",
      " [-0.54715143  0.32865382 -0.59938962 -0.5768604   0.17736017 -0.33357876\n",
      "  -0.38256046 -0.10311233  0.48448273]\n",
      " [ 0.43703259 -0.03020513  0.12373817  0.18242916 -0.39730014  0.36276781\n",
      "   0.34216405 -0.18092795 -0.40843309]\n",
      " [ 0.36847432 -0.29493056  0.42676345  0.20569512 -0.29665742  0.28334168\n",
      "   0.55100908  0.57282208 -0.11072866]\n",
      " [-0.79794403  0.46897331 -0.63853564 -0.71612285  0.04940374  0.00854093\n",
      "  -0.78412955 -0.49063363  0.5120183 ]\n",
      " [ 0.87552063 -0.42210834  0.63926367  0.26853563 -0.14935289 -0.11956126\n",
      "   0.50013898  0.61657767 -0.2915522 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.71452874  1.20681944 -0.92476847  0.12926059  0.04758257 -1.24576079\n",
      "   0.61018228]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:28 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.60411457]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 28 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.69972578 -0.48045437  0.43347261  0.5928259  -0.24797863  0.12936722\n",
      "   1.116566   -0.05829563 -0.36083009]\n",
      " [-0.55685144  0.32865382 -0.59938962 -0.5768604   0.16766015 -0.34327877\n",
      "  -0.39226047 -0.10311233  0.48448273]\n",
      " [ 0.44472857 -0.03020513  0.12373817  0.18242916 -0.38960417  0.37046378\n",
      "   0.34986003 -0.18092795 -0.40843309]\n",
      " [ 0.37726689 -0.29493056  0.42676345  0.20569512 -0.28786485  0.29213426\n",
      "   0.55980165  0.57282208 -0.11072866]\n",
      " [-0.8085407   0.46897331 -0.63853564 -0.71612285  0.03880706 -0.00205575\n",
      "  -0.79472623 -0.49063363  0.5120183 ]\n",
      " [ 0.88530439 -0.42210834  0.63926367  0.26853563 -0.13956913 -0.10977749\n",
      "   0.50992274  0.61657767 -0.2915522 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.66718873  1.24656748 -0.91282055  0.16135783  0.08129885 -1.23729382\n",
      "   0.64575934]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:28 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.56980071]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 28 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.68557203 -0.48045437  0.43347261  0.57867216 -0.24797863  0.11521347\n",
      "   1.116566   -0.05829563 -0.37498384]\n",
      " [-0.54317641  0.32865382 -0.59938962 -0.56318537  0.16766015 -0.32960374\n",
      "  -0.39226047 -0.10311233  0.49815776]\n",
      " [ 0.4352853  -0.03020513  0.12373817  0.17298589 -0.38960417  0.36102052\n",
      "   0.34986003 -0.18092795 -0.41787635]\n",
      " [ 0.36569566 -0.29493056  0.42676345  0.19412389 -0.28786485  0.28056302\n",
      "   0.55980165  0.57282208 -0.12229989]\n",
      " [-0.79470298  0.46897331 -0.63853564 -0.70228512  0.03880706  0.01178198\n",
      "  -0.79472623 -0.49063363  0.52585603]\n",
      " [ 0.87386365 -0.42210834  0.63926367  0.25709489 -0.13956913 -0.12121823\n",
      "   0.50992274  0.61657767 -0.30299294]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.73702574  1.19468567 -0.93170566  0.11644013  0.03364897 -1.25587472\n",
      "   0.59828934]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:28 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.63084624]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 28 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.69456096 -0.47146544  0.43347261  0.58766108 -0.24797863  0.1242024\n",
      "   1.12555493 -0.05829563 -0.37498384]\n",
      " [-0.55279267  0.31903755 -0.59938962 -0.57280163  0.16766015 -0.33922001\n",
      "  -0.40187674 -0.10311233  0.49815776]\n",
      " [ 0.44466866 -0.02082178  0.12373817  0.18236925 -0.38960417  0.37040388\n",
      "   0.35924339 -0.18092795 -0.41787635]\n",
      " [ 0.37457387 -0.28605235  0.42676345  0.2030021  -0.28786485  0.28944124\n",
      "   0.56867987  0.57282208 -0.12229989]\n",
      " [-0.80410446  0.45957182 -0.63853564 -0.71168661  0.03880706  0.0023805\n",
      "  -0.80412772 -0.49063363  0.52585603]\n",
      " [ 0.8827141  -0.41325789  0.63926367  0.26594534 -0.13956913 -0.11236779\n",
      "   0.51877319  0.61657767 -0.30299294]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.69404161  1.23261581 -0.92386152  0.15013835  0.0659405  -1.24983449\n",
      "   0.63051891]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:28 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.65657103]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 28 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.70282022 -0.46320618  0.43347261  0.59592034 -0.24797863  0.1242024\n",
      "   1.13381419 -0.05829563 -0.37498384]\n",
      " [-0.56107855  0.31075168 -0.59938962 -0.5810875   0.16766015 -0.33922001\n",
      "  -0.41016261 -0.10311233  0.49815776]\n",
      " [ 0.45213485 -0.01335558  0.12373817  0.18983544 -0.38960417  0.37040388\n",
      "   0.36670958 -0.18092795 -0.41787635]\n",
      " [ 0.38153194 -0.27909427  0.42676345  0.20996018 -0.28786485  0.28944124\n",
      "   0.57563794  0.57282208 -0.12229989]\n",
      " [-0.81249813  0.45117815 -0.63853564 -0.72008028  0.03880706  0.0023805\n",
      "  -0.81252139 -0.49063363  0.52585603]\n",
      " [ 0.8911007  -0.40487129  0.63926367  0.27433194 -0.13956913 -0.11236779\n",
      "   0.52715979  0.61657767 -0.30299294]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.65532248  1.26645425 -0.91495696  0.17817927  0.09314855 -1.24462047\n",
      "   0.66064323]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:28 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.74394767]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 28 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.70729403 -0.46320618  0.43794642  0.59592034 -0.24797863  0.12867621\n",
      "   1.138288   -0.05829563 -0.37498384]\n",
      " [-0.56631178  0.31075168 -0.60462285 -0.5810875   0.16766015 -0.34445324\n",
      "  -0.41539584 -0.10311233  0.49815776]\n",
      " [ 0.45748387 -0.01335558  0.12908718  0.18983544 -0.38960417  0.37575289\n",
      "   0.37205859 -0.18092795 -0.41787635]\n",
      " [ 0.3869604  -0.27909427  0.4321919   0.20996018 -0.28786485  0.29486969\n",
      "   0.58106639  0.57282208 -0.12229989]\n",
      " [-0.81721193  0.45117815 -0.64324944 -0.72008028  0.03880706 -0.0023333\n",
      "  -0.81723518 -0.49063363  0.52585603]\n",
      " [ 0.89629214 -0.40487129  0.64445511  0.27433194 -0.13956913 -0.10717635\n",
      "   0.53235123  0.61657767 -0.30299294]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.63093484  1.2888029  -0.91181101  0.19739703  0.11368357 -1.24231853\n",
      "   0.68198041]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:28 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.47996705]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 29 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.70069364 -0.46980657  0.43794642  0.58931996 -0.24797863  0.12867621\n",
      "   1.138288   -0.05829563 -0.38158423]\n",
      " [-0.5613854   0.31567807 -0.60462285 -0.57616112  0.16766015 -0.34445324\n",
      "  -0.41539584 -0.10311233  0.50308415]\n",
      " [ 0.45428546 -0.01655399  0.12908718  0.18663704 -0.38960417  0.37575289\n",
      "   0.37205859 -0.18092795 -0.42107476]\n",
      " [ 0.38406022 -0.28199445  0.4321919   0.20706    -0.28786485  0.29486969\n",
      "   0.58106639  0.57282208 -0.12520007]\n",
      " [-0.80944752  0.45894256 -0.64324944 -0.71231587  0.03880706 -0.0023333\n",
      "  -0.81723518 -0.49063363  0.53362044]\n",
      " [ 0.8897205  -0.41144293  0.64445511  0.26776031 -0.13956913 -0.10717635\n",
      "   0.53235123  0.61657767 -0.30956457]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.69083441  1.25201226 -0.9367398   0.1642239   0.08081509 -1.26409129\n",
      "   0.64522192]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:29 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.71557109]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 29 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.70625559 -0.46980657  0.44350838  0.58931996 -0.24797863  0.12867621\n",
      "   1.14384995 -0.05829563 -0.38158423]\n",
      " [-0.56786203  0.31567807 -0.61109949 -0.57616112  0.16766015 -0.34445324\n",
      "  -0.42187248 -0.10311233  0.50308415]\n",
      " [ 0.45983368 -0.01655399  0.1346354   0.18663704 -0.38960417  0.37575289\n",
      "   0.37760681 -0.18092795 -0.42107476]\n",
      " [ 0.39048864 -0.28199445  0.43862033  0.20706    -0.28786485  0.29486969\n",
      "   0.58749482  0.57282208 -0.12520007]\n",
      " [-0.81502402  0.45894256 -0.64882594 -0.71231587  0.03880706 -0.0023333\n",
      "  -0.82281169 -0.49063363  0.53362044]\n",
      " [ 0.89568772 -0.41144293  0.65042232  0.26776031 -0.13956913 -0.10717635\n",
      "   0.53831844  0.61657767 -0.30956457]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.66188963  1.27826367 -0.93180191  0.18512799  0.1040218  -1.26138078\n",
      "   0.67091351]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:29 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.69834308]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 29 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.69020248 -0.48585968  0.42745526  0.58931996 -0.24797863  0.12867621\n",
      "   1.12779684 -0.05829563 -0.38158423]\n",
      " [-0.55181754  0.33172256 -0.595055   -0.57616112  0.16766015 -0.34445324\n",
      "  -0.40582799 -0.10311233  0.50308415]\n",
      " [ 0.44573339 -0.03065427  0.12053511  0.18663704 -0.38960417  0.37575289\n",
      "   0.36350652 -0.18092795 -0.42107476]\n",
      " [ 0.37512313 -0.29735997  0.42325482  0.20706    -0.28786485  0.29486969\n",
      "   0.5721293   0.57282208 -0.12520007]\n",
      " [-0.79898188  0.4749847  -0.6327838  -0.71231587  0.03880706 -0.0023333\n",
      "  -0.80676954 -0.49063363  0.53362044]\n",
      " [ 0.87931422 -0.42781643  0.63404882  0.26776031 -0.13956913 -0.10717635\n",
      "   0.52194494  0.61657767 -0.30956457]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.73544611  1.2149299  -0.94773968  0.13200371  0.04836245 -1.27156911\n",
      "   0.6089809 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:29 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.61179036]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 29 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.6995382  -0.48585968  0.43679098  0.58931996 -0.24797863  0.12867621\n",
      "   1.12779684 -0.04895991 -0.38158423]\n",
      " [-0.56179277  0.33172256 -0.60503022 -0.57616112  0.16766015 -0.34445324\n",
      "  -0.40582799 -0.11308756  0.50308415]\n",
      " [ 0.45001362 -0.03065427  0.12481534  0.18663704 -0.38960417  0.37575289\n",
      "   0.36350652 -0.17664772 -0.42107476]\n",
      " [ 0.38532884 -0.29735997  0.43346053  0.20706    -0.28786485  0.29486969\n",
      "   0.5721293   0.58302779 -0.12520007]\n",
      " [-0.80884691  0.4749847  -0.64264883 -0.71231587  0.03880706 -0.0023333\n",
      "  -0.80676954 -0.50049866  0.53362044]\n",
      " [ 0.88863628 -0.42781643  0.64337088  0.26776031 -0.13956913 -0.10717635\n",
      "   0.52194494  0.62589973 -0.30956457]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.68934565  1.24916265 -0.93747298  0.15944089  0.08513098 -1.26568695\n",
      "   0.65018454]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:29 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.53196526]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 29 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.71036806 -0.48585968  0.43679098  0.60014982 -0.24797863  0.12867621\n",
      "   1.12779684 -0.04895991 -0.37075436]\n",
      " [-0.57016784  0.33172256 -0.60503022 -0.58453618  0.16766015 -0.34445324\n",
      "  -0.40582799 -0.11308756  0.49470908]\n",
      " [ 0.45311757 -0.03065427  0.12481534  0.18974098 -0.38960417  0.37575289\n",
      "   0.36350652 -0.17664772 -0.41797081]\n",
      " [ 0.39177582 -0.29735997  0.43346053  0.21350698 -0.28786485  0.29486969\n",
      "   0.5721293   0.58302779 -0.11875309]\n",
      " [-0.82022481  0.4749847  -0.64264883 -0.72369377  0.03880706 -0.0023333\n",
      "  -0.80676954 -0.50049866  0.52224254]\n",
      " [ 0.89899979 -0.42781643  0.64337088  0.27812382 -0.13956913 -0.10717635\n",
      "   0.52194494  0.62589973 -0.29920106]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.63108042  1.29067325 -0.91728948  0.19170154  0.12094767 -1.2498739\n",
      "   0.6909645 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:29 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.42846161]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 29 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 7.07513384e-01 -4.85859681e-01  4.36790980e-01  6.00149818e-01\n",
      "  -2.50833303e-01  1.25821537e-01  1.12779684e+00 -4.89599130e-02\n",
      "  -3.73609040e-01]\n",
      " [-5.66911544e-01  3.31722558e-01 -6.05030225e-01 -5.84536184e-01\n",
      "   1.70916442e-01 -3.41196950e-01 -4.05827991e-01 -1.13087557e-01\n",
      "   4.97965373e-01]\n",
      " [ 4.52838300e-01 -3.06542746e-02  1.24815343e-01  1.89740980e-01\n",
      "  -3.89883431e-01  3.75473622e-01  3.63506525e-01 -1.76647725e-01\n",
      "  -4.18250080e-01]\n",
      " [ 3.88174238e-01 -2.97359965e-01  4.33460531e-01  2.13506979e-01\n",
      "  -2.91466436e-01  2.91268101e-01  5.72129303e-01  5.83027794e-01\n",
      "  -1.22354676e-01]\n",
      " [-8.16853015e-01  4.74984701e-01 -6.42648827e-01 -7.23693768e-01\n",
      "   4.21788529e-02  1.03849475e-03 -8.06769544e-01 -5.00498661e-01\n",
      "   5.25614336e-01]\n",
      " [ 8.94510724e-01 -4.27816428e-01  6.43370882e-01  2.78123817e-01\n",
      "  -1.44058190e-01 -1.11665411e-01  5.21944937e-01  6.25899727e-01\n",
      "  -3.03690125e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.68354175  1.26156476 -0.94022921  0.16519159  0.09106817 -1.2726942\n",
      "   0.66015093]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:29 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.28948375]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 29 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 7.09535743e-01 -4.83837322e-01  4.36790980e-01  6.00149818e-01\n",
      "  -2.48810944e-01  1.27843897e-01  1.12779684e+00 -4.89599130e-02\n",
      "  -3.71586681e-01]\n",
      " [-5.67598496e-01  3.31035606e-01 -6.05030225e-01 -5.84536184e-01\n",
      "   1.70229491e-01 -3.41883902e-01 -4.05827991e-01 -1.13087557e-01\n",
      "   4.97278422e-01]\n",
      " [ 4.52916267e-01 -3.05763077e-02  1.24815343e-01  1.89740980e-01\n",
      "  -3.89805464e-01  3.75551589e-01  3.63506525e-01 -1.76647725e-01\n",
      "  -4.18172113e-01]\n",
      " [ 3.88410402e-01 -2.97123802e-01  4.33460531e-01  2.13506979e-01\n",
      "  -2.91230273e-01  2.91504264e-01  5.72129303e-01  5.83027794e-01\n",
      "  -1.22118513e-01]\n",
      " [-8.18520674e-01  4.73317042e-01 -6.42648827e-01 -7.23693768e-01\n",
      "   4.05111939e-02 -6.29164278e-04 -8.06769544e-01 -5.00498661e-01\n",
      "   5.23946677e-01]\n",
      " [ 8.95199328e-01 -4.27127823e-01  6.43370882e-01  2.78123817e-01\n",
      "  -1.43369585e-01 -1.10976806e-01  5.21944937e-01  6.25899727e-01\n",
      "  -3.03001520e-01]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Theta two: \n",
      "[[-0.71331268  1.24872761 -0.95580261  0.1503841   0.07641891 -1.28926168\n",
      "   0.64595506]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:29 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.55749045]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 29 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.69472221 -0.49865086  0.43679098  0.60014982 -0.26362448  0.11303036\n",
      "   1.1129833  -0.04895991 -0.37158668]\n",
      " [-0.55568853  0.34294558 -0.60503022 -0.58453618  0.18213946 -0.32997393\n",
      "  -0.39391802 -0.11308756  0.49727842]\n",
      " [ 0.44144535 -0.04204723  0.12481534  0.18974098 -0.40127639  0.36408067\n",
      "   0.3520356  -0.17664772 -0.41817211]\n",
      " [ 0.37817039 -0.30736382  0.43346053  0.21350698 -0.30147029  0.28126425\n",
      "   0.56188929  0.58302779 -0.12211851]\n",
      " [-0.80427879  0.48755893 -0.64264883 -0.72369377  0.05475308  0.01361272\n",
      "  -0.79252766 -0.50049866  0.52394668]\n",
      " [ 0.88412012 -0.43820703  0.64337088  0.27812382 -0.15444879 -0.12205601\n",
      "   0.51086573  0.62589973 -0.30300152]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.78207769  1.19548556 -0.97691547  0.10335809  0.03102789 -1.30627973\n",
      "   0.59946659]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:29 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.65206154]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 29 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.70192194 -0.49865086  0.43679098  0.60734955 -0.26362448  0.11303036\n",
      "   1.12018303 -0.04895991 -0.37158668]\n",
      " [-0.56452439  0.34294558 -0.60503022 -0.59337205  0.18213946 -0.32997393\n",
      "  -0.40275389 -0.11308756  0.49727842]\n",
      " [ 0.44913431 -0.04204723  0.12481534  0.19742995 -0.40127639  0.36408067\n",
      "   0.35972457 -0.17664772 -0.41817211]\n",
      " [ 0.38647139 -0.30736382  0.43346053  0.22180798 -0.30147029  0.28126425\n",
      "   0.57019029  0.58302779 -0.12211851]\n",
      " [-0.81173782  0.48755893 -0.64264883 -0.7311528   0.05475308  0.01361272\n",
      "  -0.79998669 -0.50049866  0.52394668]\n",
      " [ 0.89290589 -0.43820703  0.64337088  0.28690958 -0.15444879 -0.12205601\n",
      "   0.51965149  0.62589973 -0.30300152]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.74260802  1.231696   -0.96991401  0.13208202  0.06103123 -1.30274959\n",
      "   0.63269966]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:29 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.60789352]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 29 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.71232548 -0.49865086  0.43679098  0.60734955 -0.25322094  0.1234339\n",
      "   1.13058657 -0.04895991 -0.37158668]\n",
      " [-0.57421432  0.34294558 -0.60503022 -0.59337205  0.17244953 -0.33966386\n",
      "  -0.41244382 -0.11308756  0.49727842]\n",
      " [ 0.45693019 -0.04204723  0.12481534  0.19742995 -0.39348051  0.37187654\n",
      "   0.36752045 -0.17664772 -0.41817211]\n",
      " [ 0.39532474 -0.30736382  0.43346053  0.22180798 -0.29261694  0.2901176\n",
      "   0.57904364  0.58302779 -0.12211851]\n",
      " [-0.82219957  0.48755893 -0.64264883 -0.7311528   0.04429132  0.00315097\n",
      "  -0.81044844 -0.50049866  0.52394668]\n",
      " [ 0.9026729  -0.43820703  0.64337088  0.28690958 -0.14468178 -0.112289\n",
      "   0.52941851  0.62589973 -0.30300152]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.69587697  1.27103313 -0.95837523  0.16404048  0.09460203 -1.29452271\n",
      "   0.66807292]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:29 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.5700281]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 29 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.69810128 -0.49865086  0.43679098  0.59312536 -0.25322094  0.10920971\n",
      "   1.13058657 -0.04895991 -0.38581087]\n",
      " [-0.56040692  0.34294558 -0.60503022 -0.57956465  0.17244953 -0.32585646\n",
      "  -0.41244382 -0.11308756  0.51108582]\n",
      " [ 0.4472351  -0.04204723  0.12481534  0.18773486 -0.39348051  0.36218146\n",
      "   0.36752045 -0.17664772 -0.4278672 ]\n",
      " [ 0.38352674 -0.30736382  0.43346053  0.21000998 -0.29261694  0.2783196\n",
      "   0.57904364  0.58302779 -0.13391652]\n",
      " [-0.80827635  0.48755893 -0.64264883 -0.71722957  0.04429132  0.01707419\n",
      "  -0.81044844 -0.50049866  0.5378699 ]\n",
      " [ 0.89099087 -0.43820703  0.64337088  0.27522755 -0.14468178 -0.12397103\n",
      "   0.52941851  0.62589973 -0.31468355]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.76573279  1.21899848 -0.97702571  0.1188089   0.04662609 -1.31295143\n",
      "   0.6202602 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:29 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.63552556]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 29 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.70688844 -0.4898637   0.43679098  0.60191251 -0.25322094  0.11799687\n",
      "   1.13937373 -0.04895991 -0.38581087]\n",
      " [-0.56985679  0.3334957  -0.60503022 -0.58901453  0.17244953 -0.33530633\n",
      "  -0.42189369 -0.11308756  0.51108582]\n",
      " [ 0.45650979 -0.03277254  0.12481534  0.19700955 -0.39348051  0.37145614\n",
      "   0.37679513 -0.17664772 -0.4278672 ]\n",
      " [ 0.39237321 -0.29851734  0.43346053  0.21885645 -0.29261694  0.28716607\n",
      "   0.58789011  0.58302779 -0.13391652]\n",
      " [-0.81747665  0.47835863 -0.64264883 -0.72642987  0.04429132  0.00787389\n",
      "  -0.81964874 -0.50049866  0.5378699 ]\n",
      " [ 0.89980499 -0.42939291  0.64337088  0.28404167 -0.14468178 -0.11515691\n",
      "   0.53823263  0.62589973 -0.31468355]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.72352067  1.25632051 -0.96954506  0.15214035  0.07863674 -1.30712274\n",
      "   0.65219269]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:29 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.66096273]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 29 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.71494956 -0.48180258  0.43679098  0.60997363 -0.25322094  0.11799687\n",
      "   1.14743484 -0.04895991 -0.38581087]\n",
      " [-0.57807105  0.32528144 -0.60503022 -0.59722878  0.17244953 -0.33530633\n",
      "  -0.43010795 -0.11308756  0.51108582]\n",
      " [ 0.46396871 -0.02531363  0.12481534  0.20446847 -0.39348051  0.37145614\n",
      "   0.38425405 -0.17664772 -0.4278672 ]\n",
      " [ 0.39940194 -0.29148861  0.43346053  0.22588519 -0.29261694  0.28716607\n",
      "   0.59491884  0.58302779 -0.13391652]\n",
      " [-0.825671    0.47016428 -0.64264883 -0.73462422  0.04429132  0.00787389\n",
      "  -0.82784309 -0.50049866  0.5378699 ]\n",
      " [ 0.90810401 -0.42109389  0.64337088  0.29234069 -0.14468178 -0.11515691\n",
      "   0.54653165  0.62589973 -0.31468355]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.68553307  1.28961095 -0.96106725  0.17989314  0.1056487  -1.30211624\n",
      "   0.68199782]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:29 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.7484588]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 29 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.7192519  -0.48180258  0.44109332  0.60997363 -0.25322094  0.12229921\n",
      "   1.15173719 -0.04895991 -0.38581087]\n",
      " [-0.58310756  0.32528144 -0.61006673 -0.59722878  0.17244953 -0.34034284\n",
      "  -0.43514445 -0.11308756  0.51108582]\n",
      " [ 0.46919032 -0.02531363  0.13003696  0.20446847 -0.39348051  0.37667776\n",
      "   0.38947566 -0.17664772 -0.4278672 ]\n",
      " [ 0.40465087 -0.29148861  0.43870946  0.22588519 -0.29261694  0.292415\n",
      "   0.60016777  0.58302779 -0.13391652]\n",
      " [-0.83020158  0.47016428 -0.64717941 -0.73462422  0.04429132  0.00334331\n",
      "  -0.83237368 -0.50049866  0.5378699 ]\n",
      " [ 0.91309823 -0.42109389  0.6483651   0.29234069 -0.14468178 -0.11016268\n",
      "   0.55152587  0.62589973 -0.31468355]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.66185446  1.31135093 -0.95811416  0.19867568  0.12571578 -1.29993549\n",
      "   0.70281095]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:29 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.47090032]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 30 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.71283021 -0.48822428  0.44109332  0.60355193 -0.25322094  0.12229921\n",
      "   1.15173719 -0.04895991 -0.39223257]\n",
      " [-0.5782093   0.3301797  -0.61006673 -0.59233052  0.17244953 -0.34034284\n",
      "  -0.43514445 -0.11308756  0.51598408]\n",
      " [ 0.46599582 -0.02850813  0.13003696  0.20127396 -0.39348051  0.37667776\n",
      "   0.38947566 -0.17664772 -0.4310617 ]\n",
      " [ 0.40167389 -0.29446559  0.43870946  0.22290821 -0.29261694  0.292415\n",
      "   0.60016777  0.58302779 -0.1368935 ]\n",
      " [-0.82263735  0.47772851 -0.64717941 -0.72706     0.04429132  0.00334331\n",
      "  -0.83237368 -0.50049866  0.54543413]\n",
      " [ 0.90657657 -0.42761554  0.6483651   0.28581903 -0.14468178 -0.11016268\n",
      "   0.55152587  0.62589973 -0.32120521]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.72051763  1.27536714 -0.98245032  0.16612365  0.09338629 -1.32130591\n",
      "   0.66671529]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:30 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.7207331]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 30 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.71817216 -0.48822428  0.44643528  0.60355193 -0.25322094  0.12229921\n",
      "   1.15707914 -0.04895991 -0.39223257]\n",
      " [-0.5844873   0.3301797  -0.61634474 -0.59233052  0.17244953 -0.34034284\n",
      "  -0.44142246 -0.11308756  0.51598408]\n",
      " [ 0.47147788 -0.02850813  0.13551902  0.20127396 -0.39348051  0.37667776\n",
      "   0.39495773 -0.17664772 -0.4310617 ]\n",
      " [ 0.40794135 -0.29446559  0.44497692  0.22290821 -0.29261694  0.292415\n",
      "   0.60643523  0.58302779 -0.1368935 ]\n",
      " [-0.82798644  0.47772851 -0.6525285  -0.72706     0.04429132  0.00334331\n",
      "  -0.83772276 -0.50049866  0.54543413]\n",
      " [ 0.91230167 -0.42761554  0.6540902   0.28581903 -0.14468178 -0.11016268\n",
      "   0.55725097  0.62589973 -0.32120521]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.69241264  1.30092427 -0.97782048  0.18658969  0.1161103  -1.31874999\n",
      "   0.69177174]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:30 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.70048436]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 30 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.70216238 -0.50423406  0.4304255   0.60355193 -0.25322094  0.12229921\n",
      "   1.14106936 -0.04895991 -0.39223257]\n",
      " [-0.56837282  0.34629419 -0.60023025 -0.59233052  0.17244953 -0.34034284\n",
      "  -0.42530798 -0.11308756  0.51598408]\n",
      " [ 0.45724224 -0.04274377  0.12128338  0.20127396 -0.39348051  0.37667776\n",
      "   0.38072209 -0.17664772 -0.4310617 ]\n",
      " [ 0.39242725 -0.30997969  0.42946282  0.22290821 -0.29261694  0.292415\n",
      "   0.59092114  0.58302779 -0.1368935 ]\n",
      " [-0.81199705  0.4937179  -0.6365391  -0.72706     0.04429132  0.00334331\n",
      "  -0.82173337 -0.50049866  0.54543413]\n",
      " [ 0.89597969 -0.44393753  0.63776822  0.28581903 -0.14468178 -0.11016268\n",
      "   0.54092899  0.62589973 -0.32120521]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.76589556  1.23756923 -0.99340914  0.13325536  0.06010017 -1.3288165\n",
      "   0.62967756]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:30 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.61537357]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 30 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.71147998 -0.50423406  0.4397431   0.60355193 -0.25322094  0.12229921\n",
      "   1.14106936 -0.03964231 -0.39223257]\n",
      " [-0.57829418  0.34629419 -0.61015161 -0.59233052  0.17244953 -0.34034284\n",
      "  -0.42530798 -0.12300892  0.51598408]\n",
      " [ 0.46163566 -0.04274377  0.1256768   0.20127396 -0.39348051  0.37667776\n",
      "   0.38072209 -0.17225431 -0.4310617 ]\n",
      " [ 0.40254494 -0.30997969  0.43958051  0.22290821 -0.29261694  0.292415\n",
      "   0.59092114  0.59314548 -0.1368935 ]\n",
      " [-0.82167774  0.4937179  -0.6462198  -0.72706     0.04429132  0.00334331\n",
      "  -0.82173337 -0.51017936  0.54543413]\n",
      " [ 0.90509586 -0.44393753  0.64688439  0.28581903 -0.14468178 -0.11016268\n",
      "   0.54092899  0.6350159  -0.32120521]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.72037714  1.27157967 -0.98351969  0.1605272   0.09664977 -1.32314223\n",
      "   0.67048786]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:30 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.53001198]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 30 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.72247321 -0.50423406  0.4397431   0.61454516 -0.25322094  0.12229921\n",
      "   1.14106936 -0.03964231 -0.38123934]\n",
      " [-0.58691702  0.34629419 -0.61015161 -0.60095336  0.17244953 -0.34034284\n",
      "  -0.42530798 -0.12300892  0.50736124]\n",
      " [ 0.46498338 -0.04274377  0.1256768   0.20462168 -0.39348051  0.37667776\n",
      "   0.38072209 -0.17225431 -0.42771398]\n",
      " [ 0.4092844  -0.30997969  0.43958051  0.22964767 -0.29261694  0.292415\n",
      "   0.59092114  0.59314548 -0.13015404]\n",
      " [-0.83320716  0.4937179  -0.6462198  -0.73858942  0.04429132  0.00334331\n",
      "  -0.82173337 -0.51017936  0.53390471]\n",
      " [ 0.91569044 -0.44393753  0.64688439  0.29641362 -0.14468178 -0.11016268\n",
      "   0.54092899  0.6350159  -0.31061062]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.66184031  1.31346935 -0.96350334  0.19317341  0.13292898 -1.30743724\n",
      "   0.7117379 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:30 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.41585052]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 30 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.7198467  -0.50423406  0.4397431   0.61454516 -0.25584745  0.1196727\n",
      "   1.14106936 -0.03964231 -0.38386585]\n",
      " [-0.58383977  0.34629419 -0.61015161 -0.60095336  0.17552679 -0.33726558\n",
      "  -0.42530798 -0.12300892  0.5104385 ]\n",
      " [ 0.46472497 -0.04274377  0.1256768   0.20462168 -0.39373892  0.37641935\n",
      "   0.38072209 -0.17225431 -0.42797239]\n",
      " [ 0.40582993 -0.30997969  0.43958051  0.22964767 -0.29607141  0.28896053\n",
      "   0.59092114  0.59314548 -0.13360851]\n",
      " [-0.83007909  0.4937179  -0.6462198  -0.73858942  0.0474194   0.00647139\n",
      "  -0.82173337 -0.51017936  0.53703278]\n",
      " [ 0.91140083 -0.44393753  0.64688439  0.29641362 -0.14897139 -0.11445229\n",
      "   0.54092899  0.6350159  -0.31490023]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.71234927  1.28556895 -0.98564907  0.1676605   0.10417506 -1.32953053\n",
      "   0.68210557]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:30 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.2729262]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 30 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.72186061 -0.50222015  0.4397431   0.61454516 -0.25383354  0.12168661\n",
      "   1.14106936 -0.03964231 -0.38185194]\n",
      " [-0.58458995  0.34554401 -0.61015161 -0.60095336  0.17477661 -0.33801576\n",
      "  -0.42530798 -0.12300892  0.50968832]\n",
      " [ 0.46488276 -0.04258598  0.1256768   0.20462168 -0.39358113  0.37657714\n",
      "   0.38072209 -0.17225431 -0.4278146 ]\n",
      " [ 0.40613353 -0.30967608  0.43958051  0.22964767 -0.2957678   0.28926413\n",
      "   0.59092114  0.59314548 -0.1333049 ]\n",
      " [-0.83177482  0.49202217 -0.6462198  -0.73858942  0.04572367  0.00477566\n",
      "  -0.82173337 -0.51017936  0.53533706]\n",
      " [ 0.91214904 -0.44318932  0.64688439  0.29641362 -0.14822318 -0.11370409\n",
      "   0.54092899  0.6350159  -0.31415203]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.73942867  1.27407441 -0.99994049  0.15427861  0.09093906 -1.34478433\n",
      "   0.66931562]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:30 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.55714473]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 30 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.70706942 -0.51701133  0.4397431   0.61454516 -0.26862473  0.10689543\n",
      "   1.12627818 -0.03964231 -0.38185194]\n",
      " [-0.57255091  0.35758304 -0.61015161 -0.60095336  0.18681564 -0.32597673\n",
      "  -0.41326894 -0.12300892  0.50968832]\n",
      " [ 0.4532651  -0.05420364  0.1256768   0.20462168 -0.40519879  0.36495948\n",
      "   0.36910443 -0.17225431 -0.4278146 ]\n",
      " [ 0.39569151 -0.32011811  0.43958051  0.22964767 -0.30620983  0.2788221\n",
      "   0.58047911  0.59314548 -0.1333049 ]\n",
      " [-0.81754568  0.50625131 -0.6462198  -0.73858942  0.05995281  0.01900479\n",
      "  -0.80750423 -0.51017936  0.53533706]\n",
      " [ 0.9009389  -0.45439946  0.64688439  0.29641362 -0.15943332 -0.12491423\n",
      "   0.52971884  0.6350159  -0.31415203]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.80816208  1.22090452 -1.02084605  0.10706047  0.04530462 -1.3618087\n",
      "   0.62266389]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:30 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.66093219]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 30 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.71388255 -0.51701133  0.4397431   0.62135828 -0.26862473  0.10689543\n",
      "   1.1330913  -0.03964231 -0.38185194]\n",
      " [-0.58105081  0.35758304 -0.61015161 -0.60945325  0.18681564 -0.32597673\n",
      "  -0.42176883 -0.12300892  0.50968832]\n",
      " [ 0.46084036 -0.05420364  0.1256768   0.21219695 -0.40519879  0.36495948\n",
      "   0.37667969 -0.17225431 -0.4278146 ]\n",
      " [ 0.40381578 -0.32011811  0.43958051  0.23777194 -0.30620983  0.2788221\n",
      "   0.58860339  0.59314548 -0.1333049 ]\n",
      " [-0.82460349  0.50625131 -0.6462198  -0.74564722  0.05995281  0.01900479\n",
      "  -0.81456204 -0.51017936  0.53533706]\n",
      " [ 0.9093491  -0.45439946  0.64688439  0.30482382 -0.15943332 -0.12491423\n",
      "   0.53812904  0.6350159  -0.31415203]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.77016939  1.25587326 -1.01439343  0.13503571  0.07454221 -1.35854179\n",
      "   0.65492111]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:30 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.61159771]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 30 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.72414003 -0.51701133  0.4397431   0.62135828 -0.25836725  0.11715291\n",
      "   1.14334878 -0.03964231 -0.38185194]\n",
      " [-0.59071332  0.35758304 -0.61015161 -0.60945325  0.17715313 -0.33563924\n",
      "  -0.43143135 -0.12300892  0.50968832]\n",
      " [ 0.46871606 -0.05420364  0.1256768   0.21219695 -0.39732309  0.37283518\n",
      "   0.38455539 -0.17225431 -0.4278146 ]\n",
      " [ 0.4127091  -0.32011811  0.43958051  0.23777194 -0.29731651  0.28771542\n",
      "   0.5974967   0.59314548 -0.1333049 ]\n",
      " [-0.83493003  0.50625131 -0.6462198  -0.74564722  0.04962626  0.00867825\n",
      "  -0.82488859 -0.51017936  0.53533706]\n",
      " [ 0.91908293 -0.45439946  0.64688439  0.30482382 -0.14969949 -0.1151804\n",
      "   0.54786288  0.6350159  -0.31415203]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.72403769  1.29478986 -1.00323155  0.16683856  0.10794746 -1.35053255\n",
      "   0.6900689 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:30 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57029655]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 30 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.70985343 -0.51701133  0.4397431   0.60707169 -0.25836725  0.10286631\n",
      "   1.14334878 -0.03964231 -0.39613854]\n",
      " [-0.57678664  0.35758304 -0.61015161 -0.59552657  0.17715313 -0.32171256\n",
      "  -0.43143135 -0.12300892  0.523615  ]\n",
      " [ 0.45878611 -0.05420364  0.1256768   0.202267   -0.39732309  0.36290523\n",
      "   0.38455539 -0.17225431 -0.43774455]\n",
      " [ 0.40070032 -0.32011811  0.43958051  0.22576316 -0.29731651  0.27570664\n",
      "   0.5974967   0.59314548 -0.14531368]\n",
      " [-0.82093113  0.50625131 -0.6462198  -0.73164832  0.04962626  0.02267715\n",
      "  -0.82488859 -0.51017936  0.54933596]\n",
      " [ 0.90718153 -0.45439946  0.64688439  0.29292242 -0.14969949 -0.12708179\n",
      "   0.54786288  0.6350159  -0.32605342]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.79391567  1.2426158  -1.02166798  0.12130838  0.05966039 -1.36882758\n",
      "   0.64193606]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:30 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.6400417]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 30 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.71845198 -0.50841279  0.4397431   0.61567024 -0.25836725  0.11146486\n",
      "   1.15194733 -0.03964231 -0.39613854]\n",
      " [-0.58606754  0.34830214 -0.61015161 -0.60480747  0.17715313 -0.33099346\n",
      "  -0.44071225 -0.12300892  0.523615  ]\n",
      " [ 0.46794375 -0.045046    0.1256768   0.21142464 -0.39732309  0.37206287\n",
      "   0.39371303 -0.17225431 -0.43774455]\n",
      " [ 0.40949659 -0.31132184  0.43958051  0.23455943 -0.29731651  0.28450291\n",
      "   0.60629297  0.59314548 -0.14531368]\n",
      " [-0.82994027  0.49724216 -0.6462198  -0.74065746  0.04962626  0.01366801\n",
      "  -0.83389774 -0.51017936  0.54933596]\n",
      " [ 0.91594052 -0.44564048  0.64688439  0.30168141 -0.14969949 -0.1183228\n",
      "   0.55662187  0.6350159  -0.32605342]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.75245058  1.27933546 -1.01451444  0.15426622  0.09137566 -1.36318648\n",
      "   0.67355357]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:30 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.66525492]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 30 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.72632366 -0.50054111  0.4397431   0.62354192 -0.25836725  0.11146486\n",
      "   1.15981901 -0.03964231 -0.39613854]\n",
      " [-0.59419433  0.34017535 -0.61015161 -0.61293426  0.17715313 -0.33099346\n",
      "  -0.44883904 -0.12300892  0.523615  ]\n",
      " [ 0.47537927 -0.03761049  0.1256768   0.21886015 -0.39732309  0.37206287\n",
      "   0.40114855 -0.17225431 -0.43774455]\n",
      " [ 0.41656938 -0.30424905  0.43958051  0.24163223 -0.29731651  0.28450291\n",
      "   0.61336577  0.59314548 -0.14531368]\n",
      " [-0.8379429   0.48923953 -0.6462198  -0.74866009  0.04962626  0.01366801\n",
      "  -0.84190036 -0.51017936  0.54933596]\n",
      " [ 0.92413839 -0.43744261  0.64688439  0.30987928 -0.14969949 -0.1183228\n",
      "   0.56481973  0.6350159  -0.32605342]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.71517825  1.31207663 -1.00642666  0.18171859  0.11817085 -1.35836755\n",
      "   0.70302143]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:30 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.75281272]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 30 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.73046677 -0.50054111  0.44388621  0.62354192 -0.25836725  0.11560797\n",
      "   1.16396212 -0.03964231 -0.39613854]\n",
      " [-0.59904357  0.34017535 -0.61500086 -0.61293426  0.17715313 -0.33584271\n",
      "  -0.45368828 -0.12300892  0.523615  ]\n",
      " [ 0.48047286 -0.03761049  0.13077039  0.21886015 -0.39732309  0.37715646\n",
      "   0.40624214 -0.17225431 -0.43774455]\n",
      " [ 0.42164314 -0.30424905  0.44465426  0.24163223 -0.29731651  0.28957667\n",
      "   0.61843952  0.59314548 -0.14531368]\n",
      " [-0.842303    0.48923953 -0.6505799  -0.74866009  0.04962626  0.00930791\n",
      "  -0.84626046 -0.51017936  0.54933596]\n",
      " [ 0.92894547 -0.43744261  0.65169146  0.30987928 -0.14969949 -0.11351573\n",
      "   0.56962681  0.6350159  -0.32605342]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.69217924  1.33322724 -1.00364667  0.20007338  0.13777656 -1.35629531\n",
      "   0.72332092]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:30 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.46180183]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 31 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.7242369  -0.50677098  0.44388621  0.61731204 -0.25836725  0.11560797\n",
      "   1.16396212 -0.03964231 -0.40236841]\n",
      " [-0.5941965   0.34502243 -0.61500086 -0.60808719  0.17715313 -0.33584271\n",
      "  -0.45368828 -0.12300892  0.52846207]\n",
      " [ 0.4772994  -0.04078395  0.13077039  0.21568669 -0.39732309  0.37715646\n",
      "   0.40624214 -0.17225431 -0.44091802]\n",
      " [ 0.41861174 -0.30728045  0.44465426  0.23860082 -0.29731651  0.28957667\n",
      "   0.61843952  0.59314548 -0.14834509]\n",
      " [-0.83495294  0.49658959 -0.6505799  -0.74131003  0.04962626  0.00930791\n",
      "  -0.84626046 -0.51017936  0.55668603]\n",
      " [ 0.92249703 -0.44389104  0.65169146  0.30343084 -0.14969949 -0.11351573\n",
      "   0.56962681  0.6350159  -0.33250186]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.74956756  1.29808376 -1.02739522  0.16817916  0.10602787 -1.37725987\n",
      "   0.68793274]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:31 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.7258266]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 31 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.72936905 -0.50677098  0.44901836  0.61731204 -0.25836725  0.11560797\n",
      "   1.16909427 -0.03964231 -0.40236841]\n",
      " [-0.60027419  0.34502243 -0.62107855 -0.60808719  0.17715313 -0.33584271\n",
      "  -0.45976597 -0.12300892  0.52846207]\n",
      " [ 0.48270377 -0.04078395  0.13617476  0.21568669 -0.39732309  0.37715646\n",
      "   0.41164651 -0.17225431 -0.44091802]\n",
      " [ 0.42471066 -0.30728045  0.45075318  0.23860082 -0.29731651  0.28957667\n",
      "   0.62453844  0.59314548 -0.14834509]\n",
      " [-0.84008568  0.49658959 -0.65571264 -0.74131003  0.04962626  0.00930791\n",
      "  -0.8513932  -0.51017936  0.55668603]\n",
      " [ 0.92798898 -0.44389104  0.65718341  0.30343084 -0.14969949 -0.11351573\n",
      "   0.57511876  0.6350159  -0.33250186]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.72228699  1.32295    -1.02304738  0.18819937  0.1282569  -1.37484489\n",
      "   0.71235129]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:31 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.70250319]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 31 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.71339634 -0.52274369  0.43304566  0.61731204 -0.25836725  0.11560797\n",
      "   1.15312157 -0.03964231 -0.40236841]\n",
      " [-0.5841079   0.36118871 -0.60491226 -0.60808719  0.17715313 -0.33584271\n",
      "  -0.44359968 -0.12300892  0.52846207]\n",
      " [ 0.46835118 -0.05513654  0.12182217  0.21568669 -0.39732309  0.37715646\n",
      "   0.39729392 -0.17225431 -0.44091802]\n",
      " [ 0.409074   -0.32291711  0.43511653  0.23860082 -0.29731651  0.28957667\n",
      "   0.60890179  0.59314548 -0.14834509]\n",
      " [-0.82414224  0.51253303 -0.6397692  -0.74131003  0.04962626  0.00930791\n",
      "  -0.83544977 -0.51017936  0.55668603]\n",
      " [ 0.9117188  -0.46016123  0.64091323  0.30343084 -0.14969949 -0.11351573\n",
      "   0.55884858  0.6350159  -0.33250186]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.79569592  1.25959568 -1.03832765  0.13468156  0.07193555 -1.38481399\n",
      "   0.65012514]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:31 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.6190839]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 31 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.72267885 -0.52274369  0.44232816  0.61731204 -0.25836725  0.11560797\n",
      "   1.15312157 -0.03035981 -0.40236841]\n",
      " [-0.59395718  0.36118871 -0.61476154 -0.60808719  0.17715313 -0.33584271\n",
      "  -0.44359968 -0.1328582   0.52846207]\n",
      " [ 0.47284467 -0.05513654  0.12631566  0.21568669 -0.39732309  0.37715646\n",
      "   0.39729392 -0.16776082 -0.44091802]\n",
      " [ 0.41908729 -0.32291711  0.44512982  0.23860082 -0.29731651  0.28957667\n",
      "   0.60890179  0.60315877 -0.14834509]\n",
      " [-0.83363619  0.51253303 -0.64926315 -0.74131003  0.04962626  0.00930791\n",
      "  -0.83544977 -0.5196733   0.55668603]\n",
      " [ 0.9206297  -0.46016123  0.64982414  0.30343084 -0.14969949 -0.11351573\n",
      "   0.55884858  0.6439268  -0.33250186]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.75078229  1.29334972 -1.02879934  0.16176381  0.10822764 -1.37933676\n",
      "   0.69050855]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:31 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.52830958]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 31 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.73382163 -0.52274369  0.44232816  0.62845483 -0.25836725  0.11560797\n",
      "   1.15312157 -0.03035981 -0.39122562]\n",
      " [-0.60281131  0.36118871 -0.61476154 -0.61694132  0.17715313 -0.33584271\n",
      "  -0.44359968 -0.1328582   0.51960794]\n",
      " [ 0.47642767 -0.05513654  0.12631566  0.21926969 -0.39732309  0.37715646\n",
      "   0.39729392 -0.16776082 -0.43733502]\n",
      " [ 0.42610597 -0.32291711  0.44512982  0.24561949 -0.29731651  0.28957667\n",
      "   0.60890179  0.60315877 -0.14132641]\n",
      " [-0.84530289  0.51253303 -0.64926315 -0.75297673  0.04962626  0.00930791\n",
      "  -0.83544977 -0.5196733   0.54501932]\n",
      " [ 0.93143596 -0.46016123  0.64982414  0.3142371  -0.14969949 -0.11351573\n",
      "   0.55884858  0.6439268  -0.3216956 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.69201     1.33558461 -1.00895222  0.19476968  0.14493989 -1.36374057\n",
      "   0.73219055]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:31 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.40328298]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 31 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.73142145 -0.52274369  0.44232816  0.62845483 -0.26076743  0.11320779\n",
      "   1.15312157 -0.03035981 -0.39362581]\n",
      " [-0.59991941  0.36118871 -0.61476154 -0.61694132  0.18004503 -0.3329508\n",
      "  -0.44359968 -0.1328582   0.52249985]\n",
      " [ 0.47619809 -0.05513654  0.12631566  0.21926969 -0.39755266  0.37692689\n",
      "   0.39729392 -0.16776082 -0.43756459]\n",
      " [ 0.42280886 -0.32291711  0.44512982  0.24561949 -0.30061362  0.28627956\n",
      "   0.60890179  0.60315877 -0.14462352]\n",
      " [-0.8424173   0.51253303 -0.64926315 -0.75297673  0.05251186  0.0121935\n",
      "  -0.83544977 -0.5196733   0.54790492]\n",
      " [ 0.92735596 -0.46016123  0.64982414  0.3142371  -0.15377949 -0.11759573\n",
      "   0.55884858  0.6439268  -0.3257756 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.74053418  1.30890633 -1.03029412  0.170278    0.11733835 -1.38508897\n",
      "   0.70376631]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:31 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.25674567]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 31 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.73340255 -0.52076259  0.44232816  0.62845483 -0.25878633  0.11518889\n",
      "   1.15312157 -0.03035981 -0.39164471]\n",
      " [-0.60071743  0.36039069 -0.61476154 -0.61694132  0.17924701 -0.33374882\n",
      "  -0.44359968 -0.1328582   0.52170183]\n",
      " [ 0.4764254  -0.05490923  0.12631566  0.21926969 -0.39732535  0.3771542\n",
      "   0.39729392 -0.16776082 -0.43733728]\n",
      " [ 0.42317027 -0.32255569  0.44512982  0.24561949 -0.3002522   0.28664098\n",
      "   0.60890179  0.60315877 -0.1442621 ]\n",
      " [-0.84411464  0.51083569 -0.64926315 -0.75297673  0.05081451  0.01049616\n",
      "  -0.83544977 -0.5196733   0.54620758]\n",
      " [ 0.9281485  -0.45936869  0.64982414  0.3142371  -0.15298696 -0.11680319\n",
      "   0.55884858  0.6439268  -0.32498306]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.76503123  1.29867561 -1.04334294  0.15825684  0.10545146 -1.39905754\n",
      "   0.69231255]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:31 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.55641163]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 31 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.71865136 -0.53551378  0.44232816  0.62845483 -0.27353753  0.10043769\n",
      "   1.13837037 -0.03035981 -0.39164471]\n",
      " [-0.58858462  0.3725235  -0.61476154 -0.61694132  0.19137982 -0.32161601\n",
      "  -0.43146688 -0.1328582   0.52170183]\n",
      " [ 0.46468865 -0.06664598  0.12631566  0.21926969 -0.40906211  0.36541745\n",
      "   0.38555717 -0.16776082 -0.43733728]\n",
      " [ 0.41256114 -0.33316483  0.44512982  0.24561949 -0.31086134  0.27603184\n",
      "   0.59829265  0.60315877 -0.1442621 ]\n",
      " [-0.82991959  0.52503074 -0.64926315 -0.75297673  0.06500957  0.02469121\n",
      "  -0.82125471 -0.5196733   0.54620758]\n",
      " [ 0.91684167 -0.47067552  0.64982414  0.3142371  -0.16429378 -0.12811002\n",
      "   0.54754175  0.6439268  -0.32498306]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.83369735  1.24563486 -1.06407091  0.11089896  0.05963197 -1.41611095\n",
      "   0.64555887]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:31 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.66969402]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 31 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.72509563 -0.53551378  0.44232816  0.63489911 -0.27353753  0.10043769\n",
      "   1.14481465 -0.03035981 -0.39164471]\n",
      " [-0.59673861  0.3725235  -0.61476154 -0.62509531  0.19137982 -0.32161601\n",
      "  -0.43962086 -0.1328582   0.52170183]\n",
      " [ 0.47212084 -0.06664598  0.12631566  0.22670187 -0.40906211  0.36541745\n",
      "   0.39298935 -0.16776082 -0.43733728]\n",
      " [ 0.42047848 -0.33316483  0.44512982  0.25353683 -0.31086134  0.27603184\n",
      "   0.60620999  0.60315877 -0.1442621 ]\n",
      " [-0.83659389  0.52503074 -0.64926315 -0.75965103  0.06500957  0.02469121\n",
      "  -0.82792902 -0.5196733   0.54620758]\n",
      " [ 0.92487295 -0.47067552  0.64982414  0.32226837 -0.16429378 -0.12811002\n",
      "   0.55557303  0.6439268  -0.32498306]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.79716486  1.27935864 -1.05812069  0.13809758  0.08806954 -1.41308398\n",
      "   0.67681339]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:31 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.61520531]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 31 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.73521144 -0.53551378  0.44232816  0.63489911 -0.26342172  0.1105535\n",
      "   1.15493046 -0.03035981 -0.39164471]\n",
      " [-0.60635994  0.3725235  -0.61476154 -0.62509531  0.18175849 -0.33123735\n",
      "  -0.4492422  -0.1328582   0.52170183]\n",
      " [ 0.48005833 -0.06664598  0.12631566  0.22670187 -0.40112461  0.37335494\n",
      "   0.40092685 -0.16776082 -0.43733728]\n",
      " [ 0.42939378 -0.33316483  0.44512982  0.25353683 -0.30194604  0.28494715\n",
      "   0.6151253   0.60315877 -0.1442621 ]\n",
      " [-0.84678677  0.52503074 -0.64926315 -0.75965103  0.05481669  0.01449833\n",
      "  -0.83812189 -0.5196733   0.54620758]\n",
      " [ 0.93456064 -0.47067552  0.64982414  0.32226837 -0.15460609 -0.11842232\n",
      "   0.56526072  0.6439268  -0.32498306]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.75161907  1.31785019 -1.04730489  0.16973128  0.12129314 -1.40527151\n",
      "   0.71171926]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:31 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57059981]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 31 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.7208695  -0.53551378  0.44232816  0.62055716 -0.26342172  0.09621156\n",
      "   1.15493046 -0.03035981 -0.40598665]\n",
      " [-0.59232589  0.3725235  -0.61476154 -0.61106126  0.18175849 -0.3172033\n",
      "  -0.4492422  -0.1328582   0.53573588]\n",
      " [ 0.46991012 -0.06664598  0.12631566  0.21655367 -0.40112461  0.36320673\n",
      "   0.40092685 -0.16776082 -0.44748549]\n",
      " [ 0.41718979 -0.33316483  0.44512982  0.24133284 -0.30194604  0.27274315\n",
      "   0.6151253   0.60315877 -0.1564661 ]\n",
      " [-0.83272078  0.52503074 -0.64926315 -0.74558505  0.05481669  0.02856432\n",
      "  -0.83812189 -0.5196733   0.56027356]\n",
      " [ 0.92245992 -0.47067552  0.64982414  0.31016766 -0.15460609 -0.13052304\n",
      "   0.56526072  0.6439268  -0.33708378]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.82152202  1.26554902 -1.06554669  0.12391821  0.07271036 -1.42344946\n",
      "   0.6632878 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:31 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.64437898]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 31 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.7292927  -0.52709058  0.44232816  0.62898037 -0.26342172  0.10463476\n",
      "   1.16335366 -0.03035981 -0.40598665]\n",
      " [-0.60143856  0.36341084 -0.61476154 -0.62017392  0.18175849 -0.32631596\n",
      "  -0.45835487 -0.1328582   0.53573588]\n",
      " [ 0.47894531 -0.0576108   0.12631566  0.22558885 -0.40112461  0.37224192\n",
      "   0.40996203 -0.16776082 -0.44748549]\n",
      " [ 0.42592135 -0.32443326  0.44512982  0.2500644  -0.30194604  0.28147471\n",
      "   0.62385686  0.60315877 -0.1564661 ]\n",
      " [-0.84154962  0.5162019  -0.64926315 -0.75441388  0.05481669  0.01973549\n",
      "  -0.84695073 -0.5196733   0.56027356]\n",
      " [ 0.93114956 -0.46198588  0.64982414  0.31885729 -0.15460609 -0.12183341\n",
      "   0.57395035  0.6439268  -0.33708378]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.7807759   1.30167691 -1.05868675  0.15649982  0.10412029 -1.41797427\n",
      "   0.69457818]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:31 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.66943591]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 31 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.73698402 -0.51939926  0.44232816  0.63667169 -0.26342172  0.10463476\n",
      "   1.17104498 -0.03035981 -0.40598665]\n",
      " [-0.60946604  0.35538336 -0.61476154 -0.6282014   0.18175849 -0.32631596\n",
      "  -0.46638235 -0.1328582   0.53573588]\n",
      " [ 0.48634371 -0.0502124   0.12631566  0.23298725 -0.40112461  0.37224192\n",
      "   0.41736043 -0.16776082 -0.44748549]\n",
      " [ 0.43301501 -0.3173396   0.44512982  0.25715806 -0.30194604  0.28147471\n",
      "   0.63095052  0.60315877 -0.1564661 ]\n",
      " [-0.84936879  0.50838273 -0.64926315 -0.76223306  0.05481669  0.01973549\n",
      "  -0.8547699  -0.5196733   0.56027356]\n",
      " [ 0.93923633 -0.45389911  0.64982414  0.32694406 -0.15460609 -0.12183341\n",
      "   0.58203712  0.6439268  -0.33708378]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.7442004   1.33387148 -1.05095499  0.18364232  0.13068129 -1.41332498\n",
      "   0.72369521]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:31 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.75700921]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 31 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.74097942 -0.51939926  0.44632357  0.63667169 -0.26342172  0.10863017\n",
      "   1.17504038 -0.03035981 -0.40598665]\n",
      " [-0.61413792  0.35538336 -0.61943343 -0.6282014   0.18175849 -0.33098785\n",
      "  -0.47105423 -0.1328582   0.53573588]\n",
      " [ 0.49130987 -0.0502124   0.13128183  0.23298725 -0.40112461  0.37720809\n",
      "   0.4223266  -0.16776082 -0.44748549]\n",
      " [ 0.43791919 -0.3173396   0.450034    0.25715806 -0.30194604  0.28637889\n",
      "   0.6358547   0.60315877 -0.1564661 ]\n",
      " [-0.85357049  0.50838273 -0.65346486 -0.76223306  0.05481669  0.01553378\n",
      "  -0.85897161 -0.5196733   0.56027356]\n",
      " [ 0.94386661 -0.45389911  0.65445442  0.32694406 -0.15460609 -0.11720312\n",
      "   0.58666741  0.6439268  -0.33708378]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.72185177  1.3544529  -1.04833043  0.20157804  0.149834   -1.41134997\n",
      "   0.74349326]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:31 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.45269291]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 32 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.73495216 -0.52542652  0.44632357  0.63064442 -0.26342172  0.10863017\n",
      "   1.17504038 -0.03035981 -0.41201392]\n",
      " [-0.6093626   0.36015868 -0.61943343 -0.62342608  0.18175849 -0.33098785\n",
      "  -0.47105423 -0.1328582   0.5405112 ]\n",
      " [ 0.48817339 -0.05334889  0.13128183  0.22985076 -0.40112461  0.37720809\n",
      "   0.4223266  -0.16776082 -0.45062198]\n",
      " [ 0.43485462 -0.32040417  0.450034    0.2540935  -0.30194604  0.28637889\n",
      "   0.6358547   0.60315877 -0.15953067]\n",
      " [-0.84644612  0.5155071  -0.65346486 -0.75510869  0.05481669  0.01553378\n",
      "  -0.85897161 -0.5196733   0.56739793]\n",
      " [ 0.93751224 -0.46025349  0.65445442  0.32058969 -0.15460609 -0.11720312\n",
      "   0.58666741  0.6439268  -0.34343815]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.77793183  1.32017802 -1.07149635  0.17037461  0.11870433 -1.4319048\n",
      "   0.7088522 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:32 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.73084201]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 32 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.7398846  -0.52542652  0.451256    0.63064442 -0.26342172  0.10863017\n",
      "   1.17997282 -0.03035981 -0.41201392]\n",
      " [-0.61524055  0.36015868 -0.62531138 -0.62342608  0.18175849 -0.33098785\n",
      "  -0.47693218 -0.1328582   0.5405112 ]\n",
      " [ 0.49349032 -0.05334889  0.13659876  0.22985076 -0.40112461  0.37720809\n",
      "   0.42764353 -0.16776082 -0.45062198]\n",
      " [ 0.44078016 -0.32040417  0.45595953  0.2540935  -0.30194604  0.28637889\n",
      "   0.64178023  0.60315877 -0.15953067]\n",
      " [-0.85137346  0.5155071  -0.6583922  -0.75510869  0.05481669  0.01553378\n",
      "  -0.86389895 -0.5196733   0.56739793]\n",
      " [ 0.94278066 -0.46025349  0.65972284  0.32058969 -0.15460609 -0.11720312\n",
      "   0.59193582  0.6439268  -0.34343815]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.75145853  1.34435966 -1.06740662  0.1899435   0.14042959 -1.4296185\n",
      "   0.73263354]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:32 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.70439211]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 32 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.72394246 -0.54136866  0.43531386  0.63064442 -0.26342172  0.10863017\n",
      "   1.16403068 -0.03035981 -0.41201392]\n",
      " [-0.59903705  0.37636219 -0.60910788 -0.62342608  0.18175849 -0.33098785\n",
      "  -0.46072868 -0.1328582   0.5405112 ]\n",
      " [ 0.47903781 -0.06780139  0.12214626  0.22985076 -0.40112461  0.37720809\n",
      "   0.41319103 -0.16776082 -0.45062198]\n",
      " [ 0.42504339 -0.33614093  0.44022277  0.2540935  -0.30194604  0.28637889\n",
      "   0.62604347  0.60315877 -0.15953067]\n",
      " [-0.8354688   0.53141177 -0.64248753 -0.75510869  0.05481669  0.01553378\n",
      "  -0.84799428 -0.5196733   0.56739793]\n",
      " [ 0.9265608  -0.47647334  0.64350298  0.32058969 -0.15460609 -0.11720312\n",
      "   0.57571597  0.6439268  -0.34343815]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.82479416  1.28102556 -1.08241662  0.13626772  0.08383416 -1.4395128\n",
      "   0.67030215]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:32 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.62290531]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 32 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.7331745  -0.54136866  0.44454591  0.63064442 -0.26342172  0.10863017\n",
      "   1.16403068 -0.02112777 -0.41201392]\n",
      " [-0.60879819  0.37636219 -0.61886902 -0.62342608  0.18175849 -0.33098785\n",
      "  -0.46072868 -0.14261934  0.5405112 ]\n",
      " [ 0.48361825 -0.06780139  0.12672669  0.22985076 -0.40112461  0.37720809\n",
      "   0.41319103 -0.16318038 -0.45062198]\n",
      " [ 0.43493808 -0.33614093  0.45011746  0.2540935  -0.30194604  0.28637889\n",
      "   0.62604347  0.61305347 -0.15953067]\n",
      " [-0.84477461  0.53141177 -0.65179334 -0.75510869  0.05481669  0.01553378\n",
      "  -0.84799428 -0.52897912  0.56739793]\n",
      " [ 0.93526793 -0.47647334  0.65221012  0.32058969 -0.15460609 -0.11720312\n",
      "   0.57571597  0.65263393 -0.34343815]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.78050546  1.31449165 -1.07323357  0.16313727  0.1198328  -1.43422242\n",
      "   0.71022835]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:32 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.52686032]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 32 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.74445361 -0.54136866  0.44454591  0.64192353 -0.26342172  0.10863017\n",
      "   1.16403068 -0.02112777 -0.40073481]\n",
      " [-0.61786731  0.37636219 -0.61886902 -0.6324952   0.18175849 -0.33098785\n",
      "  -0.46072868 -0.14261934  0.53144208]\n",
      " [ 0.48742722 -0.06780139  0.12672669  0.23365973 -0.40112461  0.37720809\n",
      "   0.41319103 -0.16318038 -0.44681301]\n",
      " [ 0.4422219  -0.33614093  0.45011746  0.26137732 -0.30194604  0.28637889\n",
      "   0.62604347  0.61305347 -0.15224685]\n",
      " [-0.85656502  0.53141177 -0.65179334 -0.76689909  0.05481669  0.01553378\n",
      "  -0.84799428 -0.52897912  0.55560753]\n",
      " [ 0.94626715 -0.47647334  0.65221012  0.33158891 -0.15460609 -0.11720312\n",
      "   0.57571597  0.65263393 -0.33243893]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.72153368  1.3570385  -1.05355751  0.19647614  0.15694768 -1.41873628\n",
      "   0.75230442]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:32 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.39080002]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 32 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.74227588 -0.54136866  0.44454591  0.64192353 -0.26559945  0.10645244\n",
      "   1.16403068 -0.02112777 -0.40291254]\n",
      " [-0.61516435  0.37636219 -0.61886902 -0.6324952   0.18446145 -0.32828489\n",
      "  -0.46072868 -0.14261934  0.53414504]\n",
      " [ 0.48723304 -0.06780139  0.12672669  0.23365973 -0.40131879  0.37701391\n",
      "   0.41319103 -0.16318038 -0.44700719]\n",
      " [ 0.43908988 -0.33614093  0.45011746  0.26137732 -0.30507806  0.28324686\n",
      "   0.62604347  0.61305347 -0.15537888]\n",
      " [-0.8539184   0.53141177 -0.65179334 -0.76689909  0.0574633   0.01818039\n",
      "  -0.84799428 -0.52897912  0.55825414]\n",
      " [ 0.94240357 -0.47647334  0.65221012  0.33158891 -0.15846967 -0.1210667\n",
      "   0.57571597  0.65263393 -0.33630251]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.76805361  1.33158782 -1.07408943  0.17302198  0.13051623 -1.43932611\n",
      "   0.72510511]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:32 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.24101325]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 32 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.74420265 -0.53944189  0.44454591  0.64192353 -0.26367267  0.10837921\n",
      "   1.16403068 -0.02112777 -0.40098576]\n",
      " [-0.61599459  0.37553195 -0.61886902 -0.6324952   0.18363121 -0.32911513\n",
      "  -0.46072868 -0.14261934  0.5333148 ]\n",
      " [ 0.48751876 -0.06751567  0.12672669  0.23365973 -0.40103308  0.37729963\n",
      "   0.41319103 -0.16318038 -0.44672147]\n",
      " [ 0.43949856 -0.33573224  0.45011746  0.26137732 -0.30466938  0.28365555\n",
      "   0.62604347  0.61305347 -0.15497019]\n",
      " [-0.85559353  0.52973664 -0.65179334 -0.76689909  0.05578818  0.01650527\n",
      "  -0.84799428 -0.52897912  0.55657902]\n",
      " [ 0.94322509 -0.47565183  0.65221012  0.33158891 -0.15764815 -0.12024519\n",
      "   0.57571597  0.65263393 -0.335481  ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.79009739  1.32253487 -1.08594474  0.16228594  0.1199034  -1.45205033\n",
      "   0.71490782]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:32 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.55525471]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 32 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.72950984 -0.5541347   0.44454591  0.64192353 -0.27836549  0.0936864\n",
      "   1.14933787 -0.02112777 -0.40098576]\n",
      " [-0.6038016   0.38772493 -0.61886902 -0.6324952   0.19582419 -0.31692214\n",
      "  -0.44853569 -0.14261934  0.5333148 ]\n",
      " [ 0.47569045 -0.07934398  0.12672669  0.23365973 -0.41286139  0.36547132\n",
      "   0.40136272 -0.16318038 -0.44672147]\n",
      " [ 0.42875655 -0.34647426  0.45011746  0.26137732 -0.31541139  0.27291354\n",
      "   0.61530146  0.61305347 -0.15497019]\n",
      " [-0.84145403  0.54387615 -0.65179334 -0.76689909  0.06992768  0.03064477\n",
      "  -0.83385478 -0.52897912  0.55657902]\n",
      " [ 0.93185455 -0.48702236  0.65221012  0.33158891 -0.16901869 -0.13161572\n",
      "   0.56434544  0.65263393 -0.335481  ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.85865661  1.26968138 -1.10652147  0.11484333  0.07395946 -1.4691523\n",
      "   0.6681145 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:32 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.67831325]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 32 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.73560347 -0.5541347   0.44454591  0.64801716 -0.27836549  0.0936864\n",
      "   1.15543149 -0.02112777 -0.40098576]\n",
      " [-0.61160574  0.38772493 -0.61886902 -0.64029933  0.19582419 -0.31692214\n",
      "  -0.45633982 -0.14261934  0.5333148 ]\n",
      " [ 0.48295536 -0.07934398  0.12672669  0.24092464 -0.41286139  0.36547132\n",
      "   0.40862763 -0.16318038 -0.44672147]\n",
      " [ 0.43644343 -0.34647426  0.45011746  0.2690642  -0.31541139  0.27291354\n",
      "   0.62298834  0.61305347 -0.15497019]\n",
      " [-0.84776328  0.54387615 -0.65179334 -0.77320834  0.06992768  0.03064477\n",
      "  -0.84016403 -0.52897912  0.55657902]\n",
      " [ 0.93950881 -0.48702236  0.65221012  0.33924316 -0.16901869 -0.13161572\n",
      "   0.57199969  0.65263393 -0.335481  ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.82355988  1.3021664  -1.10103036  0.14124527  0.10157176 -1.4663442\n",
      "   0.69835026]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:32 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.6186987]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 32 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.74558294 -0.5541347   0.44454591  0.64801716 -0.26838601  0.10366588\n",
      "   1.16541097 -0.02112777 -0.40098576]\n",
      " [-0.6211753   0.38772493 -0.61886902 -0.64029933  0.18625463 -0.32649171\n",
      "  -0.46590939 -0.14261934  0.5333148 ]\n",
      " [ 0.49093871 -0.07934398  0.12672669  0.24092464 -0.40487803  0.37345467\n",
      "   0.41661098 -0.16318038 -0.44672147]\n",
      " [ 0.4453655  -0.34647426  0.45011746  0.2690642  -0.30648932  0.2818356\n",
      "   0.63191041  0.61305347 -0.15497019]\n",
      " [-0.85782544  0.54387615 -0.65179334 -0.77320834  0.05986552  0.0205826\n",
      "  -0.8502262  -0.52897912  0.55657902]\n",
      " [ 0.94914048 -0.48702236  0.65221012  0.33924316 -0.15938701 -0.12198404\n",
      "   0.58163136  0.65263393 -0.335481  ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.77858337  1.34023285 -1.09053149  0.17269948  0.13460138 -1.45870915\n",
      "   0.73300258]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:32 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57093262]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 32 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.73119184 -0.5541347   0.44454591  0.63362606 -0.26838601  0.08927477\n",
      "   1.16541097 -0.02112777 -0.41537686]\n",
      " [-0.60704462  0.38772493 -0.61886902 -0.62616866  0.18625463 -0.31236103\n",
      "  -0.46590939 -0.14261934  0.54744548]\n",
      " [ 0.48058828 -0.07934398  0.12672669  0.23057422 -0.40487803  0.36310424\n",
      "   0.41661098 -0.16318038 -0.4570719 ]\n",
      " [ 0.43298117 -0.34647426  0.45011746  0.25667987 -0.30648932  0.26945128\n",
      "   0.63191041  0.61305347 -0.16735452]\n",
      " [-0.84369989  0.54387615 -0.65179334 -0.75908279  0.05986552  0.03470816\n",
      "  -0.8502262  -0.52897912  0.57070457]\n",
      " [ 0.93685863 -0.48702236  0.65221012  0.32696131 -0.15938701 -0.1342659\n",
      "   0.58163136  0.65263393 -0.34776285]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.84851364  1.28781575 -1.10859678  0.12661928  0.08573845 -1.47678488\n",
      "   0.68429271]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:32 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.64852616]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 32 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.73945281 -0.54587373  0.44454591  0.64188703 -0.26838601  0.09753575\n",
      "   1.17367194 -0.02112777 -0.41537686]\n",
      " [-0.61599237  0.37877718 -0.61886902 -0.63511641  0.18625463 -0.32130878\n",
      "  -0.47485714 -0.14261934  0.54744548]\n",
      " [ 0.48949821 -0.07043406  0.12672669  0.23948414 -0.40487803  0.37201416\n",
      "   0.4255209  -0.16318038 -0.4570719 ]\n",
      " [ 0.44163713 -0.3378183   0.45011746  0.26533583 -0.30648932  0.27810724\n",
      "   0.64056637  0.61305347 -0.16735452]\n",
      " [-0.85235967  0.53521637 -0.65179334 -0.76774257  0.05986552  0.02604838\n",
      "  -0.85888598 -0.52897912  0.57070457]\n",
      " [ 0.94546865 -0.47841233  0.65221012  0.33557134 -0.15938701 -0.12565587\n",
      "   0.59024139  0.65263393 -0.34776285]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.80845617  1.3233664  -1.10199983  0.15882579  0.11683726 -1.47145601\n",
      "   0.71524894]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:32 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.67349743]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 32 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.74697295 -0.5383536   0.44454591  0.64940716 -0.26838601  0.09753575\n",
      "   1.18119207 -0.02112777 -0.41537686]\n",
      " [-0.62391211  0.37085745 -0.61886902 -0.64303614  0.18625463 -0.32130878\n",
      "  -0.48277687 -0.14261934  0.54744548]\n",
      " [ 0.49684805 -0.06308422  0.12672669  0.24683398 -0.40487803  0.37201416\n",
      "   0.43287075 -0.16318038 -0.4570719 ]\n",
      " [ 0.44873179 -0.33072364  0.45011746  0.27243049 -0.30648932  0.27810724\n",
      "   0.64766102  0.61305347 -0.16735452]\n",
      " [-0.86000399  0.52757205 -0.65179334 -0.77538689  0.05986552  0.02604838\n",
      "  -0.8665303  -0.52897912  0.57070457]\n",
      " [ 0.95343746 -0.47044353  0.65221012  0.34354014 -0.15938701 -0.12565587\n",
      "   0.59821019  0.65263393 -0.34776285]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.77255743  1.35502023 -1.09459283  0.1856516   0.14314979 -1.46696027\n",
      "   0.74400561]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:32 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.76104932]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 32 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.75083141 -0.5383536   0.44840437  0.64940716 -0.26838601  0.10139421\n",
      "   1.18505054 -0.02112777 -0.41537686]\n",
      " [-0.62841665  0.37085745 -0.62337357 -0.64303614  0.18625463 -0.32581333\n",
      "  -0.48728142 -0.14261934  0.54744548]\n",
      " [ 0.50168841 -0.06308422  0.13156705  0.24683398 -0.40487803  0.37685452\n",
      "   0.43771111 -0.16318038 -0.4570719 ]\n",
      " [ 0.45347286 -0.33072364  0.45485853  0.27243049 -0.30648932  0.28284831\n",
      "   0.65240209  0.61305347 -0.16735452]\n",
      " [-0.86405868  0.52757205 -0.65584803 -0.77538689  0.05986552  0.02199369\n",
      "  -0.87058498 -0.52897912  0.57070457]\n",
      " [ 0.95790132 -0.47044353  0.65667398  0.34354014 -0.15938701 -0.12119201\n",
      "   0.60267406  0.65263393 -0.34776285]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.75083045  1.37505314 -1.09210793  0.20317791  0.16185931 -1.46507253\n",
      "   0.76331582]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:32 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.44359327]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 33 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.7450153  -0.54416971  0.44840437  0.64359105 -0.26838601  0.10139421\n",
      "   1.18505054 -0.02112777 -0.42119297]\n",
      " [-0.62373111  0.375543   -0.62337357 -0.63835059  0.18625463 -0.32581333\n",
      "  -0.48728142 -0.14261934  0.55213102]\n",
      " [ 0.49860341 -0.06616921  0.13156705  0.24374899 -0.40487803  0.37685452\n",
      "   0.43771111 -0.16318038 -0.46015689]\n",
      " [ 0.45039499 -0.33380151  0.45485853  0.26935262 -0.30648932  0.28284831\n",
      "   0.65240209  0.61305347 -0.17043239]\n",
      " [-0.85716917  0.53446156 -0.65584803 -0.76849738  0.05986552  0.02199369\n",
      "  -0.87058498 -0.52897912  0.57759408]\n",
      " [ 0.95165938 -0.47668547  0.65667398  0.3372982  -0.15938701 -0.12119201\n",
      "   0.60267406  0.65263393 -0.35400479]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.80557392  1.3416701  -1.11469611  0.1726943   0.13138301 -1.48521351\n",
      "   0.72945636]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:33 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.73577094]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 33 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.74975793 -0.54416971  0.453147    0.64359105 -0.26838601  0.10139421\n",
      "   1.18979317 -0.02112777 -0.42119297]\n",
      " [-0.62941172  0.375543   -0.62905418 -0.63835059  0.18625463 -0.32581333\n",
      "  -0.49296203 -0.14261934  0.55213102]\n",
      " [ 0.5038249  -0.06616921  0.13678853  0.24374899 -0.40487803  0.37685452\n",
      "   0.44293259 -0.16318038 -0.46015689]\n",
      " [ 0.45614464 -0.33380151  0.46060818  0.26935262 -0.30648932  0.28284831\n",
      "   0.65815175  0.61305347 -0.17043239]\n",
      " [-0.86190181  0.53446156 -0.66058068 -0.76849738  0.05986552  0.02199369\n",
      "  -0.87531763 -0.52897912  0.57759408]\n",
      " [ 0.95671425 -0.47668547  0.66172885  0.3372982  -0.15938701 -0.12119201\n",
      "   0.60772893  0.65263393 -0.35400479]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.77988926  1.36517595 -1.11084271  0.19180867  0.15259897 -1.48304491\n",
      "   0.75260424]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:33 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.70614449]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 33 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.73383977 -0.56008787  0.43722884  0.64359105 -0.26838601  0.10139421\n",
      "   1.17387501 -0.02112777 -0.42119297]\n",
      " [-0.61318249  0.39177223 -0.61282495 -0.63835059  0.18625463 -0.32581333\n",
      "  -0.4767328  -0.14261934  0.55213102]\n",
      " [ 0.48928807 -0.08070603  0.12225171  0.24374899 -0.40487803  0.37685452\n",
      "   0.42839577 -0.16318038 -0.46015689]\n",
      " [ 0.44032689 -0.34961927  0.44479043  0.26935262 -0.30648932  0.28284831\n",
      "   0.64233399  0.61305347 -0.17043239]\n",
      " [-0.84602857  0.5503348  -0.64470744 -0.76849738  0.05986552  0.02199369\n",
      "  -0.85944439 -0.52897912  0.57759408]\n",
      " [ 0.94054186 -0.49285787  0.64555646  0.3372982  -0.15938701 -0.12119201\n",
      "   0.59155654  0.65263393 -0.35400479]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.85315332  1.30187916 -1.12561792  0.13799905  0.09576394 -1.49288532\n",
      "   0.69019146]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:33 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.62682092]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 33 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.74300759 -0.56008787  0.44639667  0.64359105 -0.26838601  0.10139421\n",
      "   1.17387501 -0.01195994 -0.42119297]\n",
      " [-0.62284155  0.39177223 -0.62248401 -0.63835059  0.18625463 -0.32581333\n",
      "  -0.4767328  -0.15227841  0.55213102]\n",
      " [ 0.49394253 -0.08070603  0.12690616  0.24374899 -0.40487803  0.37685452\n",
      "   0.42839577 -0.15852593 -0.46015689]\n",
      " [ 0.45009086 -0.34961927  0.4545544   0.26935262 -0.30648932  0.28284831\n",
      "   0.64233399  0.62281744 -0.17043239]\n",
      " [-0.85514579  0.5503348  -0.65382466 -0.76849738  0.05986552  0.02199369\n",
      "  -0.85944439 -0.53809634  0.57759408]\n",
      " [ 0.94904746 -0.49285787  0.65406206  0.3372982  -0.15938701 -0.12119201\n",
      "   0.59155654  0.66113953 -0.35400479]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.80950696  1.33502858 -1.11676461  0.1646342   0.13143618 -1.48777222\n",
      "   0.72963355]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:33 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.5256636]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 33 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.75441037 -0.56008787  0.44639667  0.65499383 -0.26838601  0.10139421\n",
      "   1.17387501 -0.01195994 -0.40979019]\n",
      " [-0.63210972  0.39177223 -0.62248401 -0.64761876  0.18625463 -0.32581333\n",
      "  -0.4767328  -0.15227841  0.54286286]\n",
      " [ 0.49796758 -0.08070603  0.12690616  0.24777404 -0.40487803  0.37685452\n",
      "   0.42839577 -0.15852593 -0.45613184]\n",
      " [ 0.45762527 -0.34961927  0.4545544   0.27688703 -0.30648932  0.28284831\n",
      "   0.64233399  0.62281744 -0.16289798]\n",
      " [-0.867047    0.5503348  -0.65382466 -0.78039859  0.05986552  0.02199369\n",
      "  -0.85944439 -0.53809634  0.56569287]\n",
      " [ 0.96022173 -0.49285787  0.65406206  0.34847247 -0.15938701 -0.12119201\n",
      "   0.59155654  0.66113953 -0.34283052]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.75037111  1.37785492 -1.09726124  0.19827905  0.16892275 -1.47239782\n",
      "   0.77206648]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:33 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.37843848]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 33 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.75244938 -0.56008787  0.44639667  0.65499383 -0.270347    0.09943322\n",
      "   1.17387501 -0.01195994 -0.41175119]\n",
      " [-0.62959679  0.39177223 -0.62248401 -0.64761876  0.18876756 -0.3233004\n",
      "  -0.4767328  -0.15227841  0.54537578]\n",
      " [ 0.4978139  -0.08070603  0.12690616  0.24777404 -0.40503172  0.37670084\n",
      "   0.42839577 -0.15852593 -0.45628552]\n",
      " [ 0.45466359 -0.34961927  0.4545544   0.27688703 -0.309451    0.27988663\n",
      "   0.64233399  0.62281744 -0.16585966]\n",
      " [-0.86463383  0.5503348  -0.65382466 -0.78039859  0.06227869  0.02440686\n",
      "  -0.85944439 -0.53809634  0.56810605]\n",
      " [ 0.95657828 -0.49285787  0.65406206  0.34847247 -0.16303047 -0.12483546\n",
      "   0.59155654  0.66113953 -0.34647397]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.79487979  1.35362926 -1.11698067  0.17587102  0.14367033 -1.49221956\n",
      "   0.7460994 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:33 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.22579007]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 33 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.75430353 -0.55823372  0.44639667  0.65499383 -0.26849285  0.10128737\n",
      "   1.17387501 -0.01195994 -0.40989704]\n",
      " [-0.63044407  0.39092496 -0.62248401 -0.64761876  0.18792028 -0.32414768\n",
      "  -0.4767328  -0.15227841  0.54452851]\n",
      " [ 0.49814659 -0.08037334  0.12690616  0.24777404 -0.40469902  0.37703353\n",
      "   0.42839577 -0.15852593 -0.45595283]\n",
      " [ 0.4551086  -0.34917427  0.4545544   0.27688703 -0.309006    0.28033163\n",
      "   0.64233399  0.62281744 -0.16541465]\n",
      " [-0.86626597  0.54870265 -0.65382466 -0.78039859  0.06064654  0.02277472\n",
      "  -0.85944439 -0.53809634  0.5664739 ]\n",
      " [ 0.95741398 -0.49202217  0.65406206  0.34847247 -0.16219477 -0.12399976\n",
      "   0.59155654  0.66113953 -0.34563827]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.81461485  1.34566332 -1.12769971  0.16633644  0.13424841 -1.50375095\n",
      "   0.73707163]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:33 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.55364467]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 33 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.73968824 -0.57284901  0.44639667  0.65499383 -0.28310814  0.08667208\n",
      "   1.15925972 -0.01195994 -0.40989704]\n",
      " [-0.61822287  0.40314615 -0.62248401 -0.64761876  0.20014147 -0.31192649\n",
      "  -0.46451161 -0.15227841  0.54452851]\n",
      " [ 0.48625393 -0.092266    0.12690616  0.24777404 -0.41659169  0.36514087\n",
      "   0.41650311 -0.15852593 -0.45595283]\n",
      " [ 0.44426703 -0.36001583  0.4545544   0.27688703 -0.31984757  0.26949007\n",
      "   0.63149242  0.62281744 -0.16541465]\n",
      " [-0.85220366  0.56276496 -0.65382466 -0.78039859  0.07470885  0.03683703\n",
      "  -0.84538208 -0.53809634  0.5664739 ]\n",
      " [ 0.94601134 -0.5034248   0.65406206  0.34847247 -0.1735974  -0.1354024\n",
      "   0.5801539   0.66113953 -0.34563827]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.8830238   1.29305603 -1.14814803  0.11886619  0.08824218 -1.52091774\n",
      "   0.69030153]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:33 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.68676254]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 33 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.74544949 -0.57284901  0.44639667  0.66075507 -0.28310814  0.08667208\n",
      "   1.16502097 -0.01195994 -0.40989704]\n",
      " [-0.62567807  0.40314615 -0.62248401 -0.65507396  0.20014147 -0.31192649\n",
      "  -0.47196681 -0.15227841  0.54452851]\n",
      " [ 0.49333228 -0.092266    0.12690616  0.25485239 -0.41659169  0.36514087\n",
      "   0.42358146 -0.15852593 -0.45595283]\n",
      " [ 0.45170607 -0.36001583  0.4545544   0.28432607 -0.31984757  0.26949007\n",
      "   0.63893146  0.62281744 -0.16541465]\n",
      " [-0.85816659  0.56276496 -0.65382466 -0.78636151  0.07470885  0.03683703\n",
      "  -0.85134501 -0.53809634  0.5664739 ]\n",
      " [ 0.95329465 -0.5034248   0.65406206  0.35575578 -0.1735974  -0.1354024\n",
      "   0.58743721  0.66113953 -0.34563827]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.84933202  1.32431654 -1.14307599  0.14445873  0.11501239 -1.51830945\n",
      "   0.7195121 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:33 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.62206475]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 33 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.75529864 -0.57284901  0.44639667  0.66075507 -0.27325898  0.09652124\n",
      "   1.17487012 -0.01195994 -0.40989704]\n",
      " [-0.635188    0.40314615 -0.62248401 -0.65507396  0.19063154 -0.32143642\n",
      "  -0.48147673 -0.15227841  0.54452851]\n",
      " [ 0.50134763 -0.092266    0.12690616  0.25485239 -0.40857633  0.37315622\n",
      "   0.43159681 -0.15852593 -0.45595283]\n",
      " [ 0.46062225 -0.36001583  0.4545544   0.28432607 -0.31093139  0.27840625\n",
      "   0.64784765  0.62281744 -0.16541465]\n",
      " [-0.86810207  0.56276496 -0.65382466 -0.78636151  0.06477337  0.02690155\n",
      "  -0.86128049 -0.53809634  0.5664739 ]\n",
      " [ 0.96286308 -0.5034248   0.65406206  0.35575578 -0.16402898 -0.12583397\n",
      "   0.59700564  0.66113953 -0.34563827]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.8049057   1.36196158 -1.13286674  0.17572602  0.14783918 -1.51083401\n",
      "   0.75390348]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:33 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57129088]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 33 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.74086377 -0.57284901  0.44639667  0.64632019 -0.27325898  0.08208636\n",
      "   1.17487012 -0.01195994 -0.42433191]\n",
      " [-0.62097032  0.40314615 -0.62248401 -0.64085627  0.19063154 -0.30721873\n",
      "  -0.48147673 -0.15227841  0.55874619]\n",
      " [ 0.4908103  -0.092266    0.12690616  0.24431506 -0.40857633  0.36261889\n",
      "   0.43159681 -0.15852593 -0.46649016]\n",
      " [ 0.44807165 -0.36001583  0.4545544   0.27177547 -0.31093139  0.26585565\n",
      "   0.64784765  0.62281744 -0.17796525]\n",
      " [-0.85392349  0.56276496 -0.65382466 -0.77218293  0.06477337  0.04108013\n",
      "  -0.86128049 -0.53809634  0.58065249]\n",
      " [ 0.95041646 -0.5034248   0.65406206  0.34330916 -0.16402898 -0.13828059\n",
      "   0.59700564  0.66113953 -0.3580849 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.87486529  1.3094386  -1.15077231  0.12939424  0.09871145 -1.52882073\n",
      "   0.70493398]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:33 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.65247623]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 33 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.74897525 -0.56473753  0.44639667  0.65443167 -0.27325898  0.09019784\n",
      "   1.1829816  -0.01195994 -0.42433191]\n",
      " [-0.62975839  0.39435807 -0.62248401 -0.64964435  0.19063154 -0.31600681\n",
      "  -0.49026481 -0.15227841  0.55874619]\n",
      " [ 0.49959432 -0.08348198  0.12690616  0.25309909 -0.40857633  0.37140292\n",
      "   0.44038083 -0.15852593 -0.46649016]\n",
      " [ 0.45664427 -0.35144321  0.4545544   0.28034809 -0.31093139  0.27442827\n",
      "   0.65642027  0.62281744 -0.17796525]\n",
      " [-0.86242554  0.5542629  -0.65382466 -0.78068498  0.06477337  0.03257807\n",
      "  -0.86978255 -0.53809634  0.58065249]\n",
      " [ 0.95893997 -0.49490129  0.65406206  0.35183267 -0.16402898 -0.12975708\n",
      "   0.60552915  0.66113953 -0.3580849 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.83546461  1.34442962 -1.14441052  0.16123004  0.12949706 -1.52362053\n",
      "   0.73555348]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:33 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.67743428]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 33 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.75633325 -0.55737953  0.44639667  0.66178968 -0.27325898  0.09019784\n",
      "   1.19033961 -0.01195994 -0.42433191]\n",
      " [-0.63756474  0.38655172 -0.62248401 -0.6574507   0.19063154 -0.31600681\n",
      "  -0.49807116 -0.15227841  0.55874619]\n",
      " [ 0.50688628 -0.07619002  0.12690616  0.26039105 -0.40857633  0.37140292\n",
      "   0.44767279 -0.15852593 -0.46649016]\n",
      " [ 0.46372318 -0.34436431  0.4545544   0.28742699 -0.31093139  0.27442827\n",
      "   0.66349917  0.62281744 -0.17796525]\n",
      " [-0.86990368  0.54678477 -0.65382466 -0.78816312  0.06477337  0.03257807\n",
      "  -0.87726068 -0.53809634  0.58065249]\n",
      " [ 0.96678649 -0.48705477  0.65406206  0.3596792  -0.16402898 -0.12975708\n",
      "   0.61337568  0.66113953 -0.3580849 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.80022155  1.37555106 -1.13729977  0.18773469  0.15554978 -1.51926394\n",
      "   0.76394381]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:33 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.7649352]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 33 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.76006481 -0.55737953  0.45012823  0.66178968 -0.27325898  0.0939294\n",
      "   1.19407116 -0.01195994 -0.42433191]\n",
      " [-0.64191189  0.38655172 -0.62683116 -0.6574507   0.19063154 -0.32035395\n",
      "  -0.5024183  -0.15227841  0.55874619]\n",
      " [ 0.51160327 -0.07619002  0.13162315  0.26039105 -0.40857633  0.3761199\n",
      "   0.45238978 -0.15852593 -0.46649016]\n",
      " [ 0.4683082  -0.34436431  0.45913943  0.28742699 -0.31093139  0.27901329\n",
      "   0.66808419  0.62281744 -0.17796525]\n",
      " [-0.87382199  0.54678477 -0.65774297 -0.78816312  0.06477337  0.02865976\n",
      "  -0.88117899 -0.53809634  0.58065249]\n",
      " [ 0.97109411 -0.48705477  0.65836968  0.3596792  -0.16402898 -0.12544946\n",
      "   0.61768329  0.66113953 -0.3580849 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.77908813  1.39505637 -1.13494049  0.20486205  0.1738271  -1.51745464\n",
      "   0.7827808 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:33 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.43452118]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 34 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.75446625 -0.56297809  0.45012823  0.65619112 -0.27325898  0.0939294\n",
      "   1.19407116 -0.01195994 -0.42993047]\n",
      " [-0.63733163  0.39113199 -0.62683116 -0.65287044  0.19063154 -0.32035395\n",
      "  -0.5024183  -0.15227841  0.56332645]\n",
      " [ 0.50858275 -0.07921054  0.13162315  0.25737052 -0.40857633  0.3761199\n",
      "   0.45238978 -0.15852593 -0.46951068]\n",
      " [ 0.46523523 -0.34743728  0.45913943  0.28435402 -0.31093139  0.27901329\n",
      "   0.66808419  0.62281744 -0.18103822]\n",
      " [-0.86717426  0.5534325  -0.65774297 -0.78151539  0.06477337  0.02865976\n",
      "  -0.88117899 -0.53809634  0.58730022]\n",
      " [ 0.96498049 -0.49316839  0.65836968  0.35356557 -0.16402898 -0.12544946\n",
      "   0.61768329  0.66113953 -0.36419852]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.83247178  1.36258347 -1.15695575  0.17512316  0.14403433 -1.53717759\n",
      "   0.7497322 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:34 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.74060616]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 34 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.7590287  -0.56297809  0.45469067  0.65619112 -0.27325898  0.0939294\n",
      "   1.19863361 -0.01195994 -0.42993047]\n",
      " [-0.64281872  0.39113199 -0.63231825 -0.65287044  0.19063154 -0.32035395\n",
      "  -0.50790539 -0.15227841  0.56332645]\n",
      " [ 0.5137024  -0.07921054  0.13674281  0.25737052 -0.40857633  0.3761199\n",
      "   0.45750943 -0.15852593 -0.46951068]\n",
      " [ 0.47080851 -0.34743728  0.46471271  0.28435402 -0.31093139  0.27901329\n",
      "   0.67365747  0.62281744 -0.18103822]\n",
      " [-0.87172256  0.5534325  -0.66229127 -0.78151539  0.06477337  0.02865976\n",
      "  -0.8857273  -0.53809634  0.58730022]\n",
      " [ 0.96983195 -0.49316839  0.66322114  0.35356557 -0.16402898 -0.12544946\n",
      "   0.62253475  0.66113953 -0.36419852]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.80755587  1.38542451 -1.15331883  0.19378194  0.1647385  -1.53511682\n",
      "   0.77225304]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:34 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.70775463]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 34 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.743128   -0.57887878  0.43878997  0.65619112 -0.27325898  0.0939294\n",
      "   1.18273291 -0.01195994 -0.42993047]\n",
      " [-0.62657263  0.40737807 -0.61607216 -0.65287044  0.19063154 -0.32035395\n",
      "  -0.49165931 -0.15227841  0.56332645]\n",
      " [ 0.49909539 -0.09381755  0.1221358   0.25737052 -0.40857633  0.3761199\n",
      "   0.44290243 -0.15852593 -0.46951068]\n",
      " [ 0.45492582 -0.36331997  0.44883002  0.28435402 -0.31093139  0.27901329\n",
      "   0.65777478  0.62281744 -0.18103822]\n",
      " [-0.85587343  0.56928163 -0.64644214 -0.78151539  0.06477337  0.02865976\n",
      "  -0.86987817 -0.53809634  0.58730022]\n",
      " [ 0.95370311 -0.50929723  0.6470923   0.35356557 -0.16402898 -0.12544946\n",
      "   0.60640592  0.66113953 -0.36419852]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.88075115  1.32217986 -1.16789222  0.139861    0.10769553 -1.54492275\n",
      "   0.70977996]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:34 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.63081343]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 34 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.75221943 -0.57887878  0.4478814   0.65619112 -0.27325898  0.0939294\n",
      "   1.18273291 -0.00286851 -0.42993047]\n",
      " [-0.6361177   0.40737807 -0.62561724 -0.65287044  0.19063154 -0.32035395\n",
      "  -0.49165931 -0.16182348  0.56332645]\n",
      " [ 0.50381131 -0.09381755  0.12685172  0.25737052 -0.40857633  0.3761199\n",
      "   0.44290243 -0.15381001 -0.46951068]\n",
      " [ 0.46454898 -0.36331997  0.45845318  0.28435402 -0.31093139  0.27901329\n",
      "   0.65777478  0.6324406  -0.18103822]\n",
      " [-0.86480244  0.56928163 -0.65537115 -0.78151539  0.06477337  0.02865976\n",
      "  -0.86987817 -0.54702535  0.58730022]\n",
      " [ 0.96201007 -0.50929723  0.65539926  0.35356557 -0.16402898 -0.12544946\n",
      "   0.60640592  0.66944648 -0.36419852]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.83776162  1.35498678 -1.15935352  0.16624177  0.14301162 -1.53997789\n",
      "   0.74871451]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:34 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.52471628]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 34 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.76373383 -0.57887878  0.4478814   0.66770553 -0.27325898  0.0939294\n",
      "   1.18273291 -0.00286851 -0.41841606]\n",
      " [-0.64556948  0.40737807 -0.62561724 -0.66232221  0.19063154 -0.32035395\n",
      "  -0.49165931 -0.16182348  0.55387468]\n",
      " [ 0.50804219 -0.09381755  0.12685172  0.26160139 -0.40857633  0.3761199\n",
      "   0.44290243 -0.15381001 -0.46527981]\n",
      " [ 0.47231922 -0.36331997  0.45845318  0.29212426 -0.31093139  0.27901329\n",
      "   0.65777478  0.6324406  -0.17326799]\n",
      " [-0.87680225  0.56928163 -0.65537115 -0.7935152   0.06477337  0.02865976\n",
      "  -0.86987817 -0.54702535  0.57530041]\n",
      " [ 0.97334236 -0.50929723  0.65539926  0.36489787 -0.16402898 -0.12544946\n",
      "   0.60640592  0.66944648 -0.35286622]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.77849633  1.39806118 -1.14002429  0.20016553  0.1808389  -1.52471724\n",
      "   0.79146802]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:34 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.3662313]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 34 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.76198221 -0.57887878  0.4478814   0.66770553 -0.27501061  0.09217778\n",
      "   1.18273291 -0.00286851 -0.42016769]\n",
      " [-0.6432454   0.40737807 -0.62561724 -0.66232221  0.19295562 -0.31802987\n",
      "  -0.49165931 -0.16182348  0.55619876]\n",
      " [ 0.50793268 -0.09381755  0.12685172  0.26160139 -0.40868584  0.3760104\n",
      "   0.44290243 -0.15381001 -0.46538931]\n",
      " [ 0.46953082 -0.36331997  0.45845318  0.29212426 -0.31371978  0.2762249\n",
      "   0.65777478  0.6324406  -0.17605638]\n",
      " [-0.87461516  0.56928163 -0.65537115 -0.7935152   0.06696046  0.03084685\n",
      "  -0.86987817 -0.54702535  0.5774875 ]\n",
      " [ 0.96991993 -0.50929723  0.65539926  0.36489787 -0.16745141 -0.1288719\n",
      "   0.60640592  0.66944648 -0.35628866]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.82099856  1.37505039 -1.15893229  0.17880491  0.15676611 -1.54376545\n",
      "   0.76673163]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:34 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.21112732]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 34 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.76374884 -0.57711216  0.4478814   0.66770553 -0.27324398  0.0939444\n",
      "   1.18273291 -0.00286851 -0.41840106]\n",
      " [-0.64409552  0.40652795 -0.62561724 -0.66232221  0.19210551 -0.31887999\n",
      "  -0.49165931 -0.16182348  0.55534864]\n",
      " [ 0.50830103 -0.0934492   0.12685172  0.26160139 -0.40831749  0.37637875\n",
      "   0.44290243 -0.15381001 -0.46502096]\n",
      " [ 0.47000128 -0.36284952  0.45845318  0.29212426 -0.31324933  0.27669535\n",
      "   0.65777478  0.6324406  -0.17558593]\n",
      " [-0.87618691  0.56770987 -0.65537115 -0.7935152   0.0653887   0.0292751\n",
      "  -0.86987817 -0.54702535  0.57591574]\n",
      " [ 0.97075608 -0.50846107  0.65539926  0.36489787 -0.16661526 -0.12803574\n",
      "   0.60640592  0.66944648 -0.3554525 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.83858046  1.36807844 -1.16857877  0.17038275  0.14844652 -1.55416426\n",
      "   0.75878198]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:34 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.55155978]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 34 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.74923096 -0.59163003  0.4478814   0.66770553 -0.28776186  0.07942653\n",
      "   1.16821504 -0.00286851 -0.41840106]\n",
      " [-0.63187655  0.41874692 -0.62561724 -0.66232221  0.20432448 -0.30666102\n",
      "  -0.47944034 -0.16182348  0.55534864]\n",
      " [ 0.49637072 -0.10537951  0.12685172  0.26160139 -0.4202478   0.36444844\n",
      "   0.43097212 -0.15381001 -0.46502096]\n",
      " [ 0.45909235 -0.37375845  0.45845318  0.29212426 -0.32415826  0.26578642\n",
      "   0.64686585  0.6324406  -0.17558593]\n",
      " [-0.8622236   0.58167319 -0.65537115 -0.7935152   0.07935202  0.04323841\n",
      "  -0.85591485 -0.54702535  0.57591574]\n",
      " [ 0.95935152 -0.51986563  0.65539926  0.36489787 -0.17801981 -0.1394403\n",
      "   0.59500136  0.66944648 -0.3554525 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.90679229  1.31577677 -1.18891787  0.1229435   0.10244118 -1.57140876\n",
      "   0.71209805]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:34 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.69502069]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 34 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.75467787 -0.59163003  0.4478814   0.67315243 -0.28776186  0.07942653\n",
      "   1.17366194 -0.00286851 -0.41840106]\n",
      " [-0.63898758  0.41874692 -0.62561724 -0.66943324  0.20432448 -0.30666102\n",
      "  -0.48655137 -0.16182348  0.55534864]\n",
      " [ 0.50324776 -0.10537951  0.12685172  0.26847843 -0.4202478   0.36444844\n",
      "   0.43784916 -0.15381001 -0.46502096]\n",
      " [ 0.46627161 -0.37375845  0.45845318  0.29930352 -0.32415826  0.26578642\n",
      "   0.65404512  0.6324406  -0.17558593]\n",
      " [-0.86785883  0.58167319 -0.65537115 -0.79915043  0.07935202  0.04323841\n",
      "  -0.86155008 -0.54702535  0.57591574]\n",
      " [ 0.96627315 -0.51986563  0.65539926  0.37181949 -0.17801981 -0.1394403\n",
      "   0.60192298  0.66944648 -0.3554525 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.87446953  1.3458337  -1.18422815  0.14772059  0.12836014 -1.56898309\n",
      "   0.74028527]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:34 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.62529483]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 34 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.76440309 -0.59163003  0.4478814   0.67315243 -0.27803663  0.08915175\n",
      "   1.18338716 -0.00286851 -0.41840106]\n",
      " [-0.64843231  0.41874692 -0.62561724 -0.66943324  0.19487975 -0.31610575\n",
      "  -0.4959961  -0.16182348  0.55534864]\n",
      " [ 0.51128322 -0.10537951  0.12685172  0.26847843 -0.41221234  0.37248389\n",
      "   0.44588461 -0.15381001 -0.46502096]\n",
      " [ 0.4751716  -0.37375845  0.45845318  0.29930352 -0.31525827  0.27468641\n",
      "   0.66294511  0.6324406  -0.17558593]\n",
      " [-0.87767239  0.58167319 -0.65537115 -0.79915043  0.06953846  0.03342485\n",
      "  -0.87136364 -0.54702535  0.57591574]\n",
      " [ 0.97577332 -0.51986563  0.65539926  0.37181949 -0.16851964 -0.12994012\n",
      "   0.61142316  0.66944648 -0.3554525 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.83057259  1.38306398 -1.17428314  0.17879605  0.16097828 -1.56165095\n",
      "   0.77441195]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:34 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57167181]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 34 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.74992909 -0.59163003  0.4478814   0.65867843 -0.27803663  0.07467775\n",
      "   1.18338716 -0.00286851 -0.43287506]\n",
      " [-0.63413616  0.41874692 -0.62561724 -0.65513709  0.19487975 -0.3018096\n",
      "  -0.4959961  -0.16182348  0.56964479]\n",
      " [ 0.50057344 -0.10537951  0.12685172  0.25776866 -0.41221234  0.36177412\n",
      "   0.44588461 -0.15381001 -0.47573074]\n",
      " [ 0.46246784 -0.37375845  0.45845318  0.28659976 -0.31525827  0.26198265\n",
      "   0.66294511  0.6324406  -0.18828969]\n",
      " [-0.8634464   0.58167319 -0.65537115 -0.78492443  0.06953846  0.04765085\n",
      "  -0.87136364 -0.54702535  0.59014174]\n",
      " [ 0.96317657 -0.51986563  0.65539926  0.35922274 -0.16851964 -0.14253688\n",
      "   0.61142316  0.66944648 -0.36804926]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.90056327  1.33044406 -1.19204445  0.13222781  0.11160062 -1.57956032\n",
      "   0.72520009]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:34 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.65622602]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 34 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.75790328 -0.58365584  0.4478814   0.66665262 -0.27803663  0.08265194\n",
      "   1.19136135 -0.00286851 -0.43287506]\n",
      " [-0.64277114  0.41011194 -0.62561724 -0.66377207  0.19487975 -0.31044458\n",
      "  -0.50463108 -0.16182348  0.56964479]\n",
      " [ 0.50923271 -0.09672025  0.12685172  0.26642792 -0.41221234  0.37043338\n",
      "   0.45454388 -0.15381001 -0.47573074]\n",
      " [ 0.47095208 -0.3652742   0.45845318  0.295084   -0.31525827  0.27046689\n",
      "   0.67142935  0.6324406  -0.18828969]\n",
      " [-0.8718019   0.57331768 -0.65537115 -0.79327994  0.06953846  0.03929534\n",
      "  -0.87971915 -0.54702535  0.59014174]\n",
      " [ 0.97160941 -0.51143278  0.65539926  0.36765558 -0.16851964 -0.13410403\n",
      "   0.619856    0.66944648 -0.36804926]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.8617867   1.36489525 -1.18589267  0.16369995  0.14207415 -1.57447292\n",
      "   0.75548404]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:34 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.68124402]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 34 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.76510795 -0.57645117  0.4478814   0.67385729 -0.27803663  0.08265194\n",
      "   1.19856602 -0.00286851 -0.43287506]\n",
      " [-0.65046074  0.40242235 -0.62561724 -0.67146167  0.19487975 -0.31044458\n",
      "  -0.51232067 -0.16182348  0.56964479]\n",
      " [ 0.51645933 -0.08949363  0.12685172  0.27365455 -0.41221234  0.37043338\n",
      "   0.4617705  -0.15381001 -0.47573074]\n",
      " [ 0.47800133 -0.35822495  0.45845318  0.30213326 -0.31525827  0.27046689\n",
      "   0.6784786   0.6324406  -0.18828969]\n",
      " [-0.87912239  0.56599719 -0.65537115 -0.80060043  0.06953846  0.03929534\n",
      "  -0.88703964 -0.54702535  0.59014174]\n",
      " [ 0.97933138 -0.50371082  0.65539926  0.37537755 -0.16851964 -0.13410403\n",
      "   0.62757797  0.66944648 -0.36804926]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.82717767  1.39549458 -1.17905228  0.18988103  0.16785834 -1.57024267\n",
      "   0.78350498]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:34 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.76866988]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 34 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.76872189 -0.57645117  0.45149534  0.67385729 -0.27803663  0.08626588\n",
      "   1.20217996 -0.00286851 -0.43287506]\n",
      " [-0.65466016  0.40242235 -0.62981666 -0.67146167  0.19487975 -0.314644\n",
      "  -0.5165201  -0.16182348  0.56964479]\n",
      " [ 0.52105602 -0.08949363  0.13144841  0.27365455 -0.41221234  0.37503008\n",
      "   0.46636719 -0.15381001 -0.47573074]\n",
      " [ 0.48243771 -0.35822495  0.46288956  0.30213326 -0.31525827  0.27490327\n",
      "   0.68291498  0.6324406  -0.18828969]\n",
      " [-0.88291421  0.56599719 -0.65916297 -0.80060043  0.06953846  0.03550352\n",
      "  -0.89083146 -0.54702535  0.59014174]\n",
      " [ 0.9834926  -0.50371082  0.65956047  0.37537755 -0.16851964 -0.12994282\n",
      "   0.63173918  0.66944648 -0.36804926]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.80661052  1.41449318 -1.17680616  0.20662052  0.18571537 -1.56850398\n",
      "   0.80188409]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:34 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.42549359]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 35 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.76334526 -0.5818278   0.45149534  0.66848066 -0.27803663  0.08626588\n",
      "   1.20217996 -0.00286851 -0.43825169]\n",
      " [-0.65019824  0.40688427 -0.62981666 -0.66699975  0.19487975 -0.314644\n",
      "  -0.5165201  -0.16182348  0.5741067 ]\n",
      " [ 0.51811132 -0.09243833  0.13144841  0.27070984 -0.41221234  0.37503008\n",
      "   0.46636719 -0.15381001 -0.47867545]\n",
      " [ 0.47938608 -0.36127658  0.46288956  0.29908162 -0.31525827  0.27490327\n",
      "   0.68291498  0.6324406  -0.19134132]\n",
      " [-0.87651307  0.57239834 -0.65916297 -0.79419928  0.06953846  0.03550352\n",
      "  -0.89083146 -0.54702535  0.59654288]\n",
      " [ 0.97752077 -0.50968264  0.65956047  0.36940572 -0.16851964 -0.12994282\n",
      "   0.63173918  0.66944648 -0.37402108]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.85861621  1.38294391 -1.19825328  0.17764705  0.15663198 -1.58780481\n",
      "   0.76967044]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:35 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.74534154]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 35 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.76773682 -0.5818278   0.4558869   0.66848066 -0.27803663  0.08626588\n",
      "   1.20657152 -0.00286851 -0.43825169]\n",
      " [-0.6554967   0.40688427 -0.63511512 -0.66699975  0.19487975 -0.314644\n",
      "  -0.52181856 -0.16182348  0.5741067 ]\n",
      " [ 0.52312428 -0.09243833  0.13646138  0.27070984 -0.41221234  0.37503008\n",
      "   0.47138016 -0.15381001 -0.47867545]\n",
      " [ 0.48478414 -0.36127658  0.46828762  0.29908162 -0.31525827  0.27490327\n",
      "   0.68831304  0.6324406  -0.19134132]\n",
      " [-0.88088699  0.57239834 -0.6635369  -0.79419928  0.06953846  0.03550352\n",
      "  -0.89520538 -0.54702535  0.59654288]\n",
      " [ 0.98217891 -0.50968264  0.66421861  0.36940572 -0.16851964 -0.12994282\n",
      "   0.63639733  0.66944648 -0.37402108]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.83444817  1.40513298 -1.19481481  0.19585121  0.17682462 -1.58584301\n",
      "   0.79157301]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:35 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.70921766]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 35 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.75184725 -0.59771737  0.43999733  0.66848066 -0.27803663  0.08626588\n",
      "   1.19068195 -0.00286851 -0.43825169]\n",
      " [-0.63924047  0.4231405  -0.61885888 -0.66699975  0.19487975 -0.314644\n",
      "  -0.50556232 -0.16182348  0.5741067 ]\n",
      " [ 0.5084598  -0.10710281  0.12179689  0.27070984 -0.41221234  0.37503008\n",
      "   0.45671567 -0.15381001 -0.47867545]\n",
      " [ 0.46884983 -0.37721089  0.45235331  0.29908162 -0.31525827  0.27490327\n",
      "   0.67237873  0.6324406  -0.19134132]\n",
      " [-0.86505481  0.58823052 -0.64770471 -0.79419928  0.06953846  0.03550352\n",
      "  -0.8793732  -0.54702535  0.59654288]\n",
      " [ 0.96608897 -0.52577258  0.64812867  0.36940572 -0.16851964 -0.12994282\n",
      "   0.62030738  0.66944648 -0.37402108]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.90757843  1.34195317 -1.2092169   0.1418397   0.11960252 -1.5956324\n",
      "   0.72905803]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:35 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.63486553]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 35 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.76085165 -0.59771737  0.44900173  0.66848066 -0.27803663  0.08626588\n",
      "   1.19068195  0.00613589 -0.43825169]\n",
      " [-0.64866156  0.4231405  -0.62827998 -0.66699975  0.19487975 -0.314644\n",
      "  -0.50556232 -0.17124458  0.5741067 ]\n",
      " [ 0.51322515 -0.10710281  0.12656225  0.27070984 -0.41221234  0.37503008\n",
      "   0.45671567 -0.14904466 -0.47867545]\n",
      " [ 0.47832398 -0.37721089  0.46182746  0.29908162 -0.31525827  0.27490327\n",
      "   0.67237873  0.64191476 -0.19134132]\n",
      " [-0.87379674  0.58823052 -0.65644664 -0.79419928  0.06953846  0.03550352\n",
      "  -0.8793732  -0.55576728  0.59654288]\n",
      " [ 0.97420072 -0.52577258  0.65624042  0.36940572 -0.16851964 -0.12994282\n",
      "   0.62030738  0.67755823 -0.37402108]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.86525728  1.37439478 -1.20097809  0.16794797  0.15453606 -1.59084726\n",
      "   0.7674651 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:35 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.5240131]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 35 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.77246625 -0.59771737  0.44900173  0.68009526 -0.27803663  0.08626588\n",
      "   1.19068195  0.00613589 -0.42663709]\n",
      " [-0.65828212  0.4231405  -0.62827998 -0.67662031  0.19487975 -0.314644\n",
      "  -0.50556232 -0.17124458  0.56448615]\n",
      " [ 0.51765137 -0.10710281  0.12656225  0.27513606 -0.41221234  0.37503008\n",
      "   0.45671567 -0.14904466 -0.47424923]\n",
      " [ 0.48631528 -0.37721089  0.46182746  0.30707292 -0.31525827  0.27490327\n",
      "   0.67237873  0.64191476 -0.18335002]\n",
      " [-0.88588361  0.58823052 -0.65644664 -0.80628615  0.06953846  0.03550352\n",
      "  -0.8793732  -0.55576728  0.58445601]\n",
      " [ 0.98567493 -0.52577258  0.65624042  0.38087993 -0.16851964 -0.12994282\n",
      "   0.62030738  0.67755823 -0.36254687]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.80589615  1.41768693 -1.18182437  0.20212382  0.1926733  -1.57570264\n",
      "   0.81050411]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:35 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.3542079]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 35 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.77091518 -0.59771737  0.44900173  0.68009526 -0.2795877   0.08471481\n",
      "   1.19068195  0.00613589 -0.42818815]\n",
      " [-0.65614368  0.4231405  -0.62827998 -0.67662031  0.19701819 -0.31250556\n",
      "  -0.50556232 -0.17124458  0.56662459]\n",
      " [ 0.51758838 -0.10710281  0.12656225  0.27513606 -0.41227534  0.37496708\n",
      "   0.45671567 -0.14904466 -0.47431222]\n",
      " [ 0.48370092 -0.37721089  0.46182746  0.30707292 -0.31787262  0.27228892\n",
      "   0.67237873  0.64191476 -0.18596438]\n",
      " [-0.88391369  0.58823052 -0.65644664 -0.80628615  0.07150838  0.03747344\n",
      "  -0.8793732  -0.55576728  0.58642593]\n",
      " [ 0.9824719  -0.52577258  0.65624042  0.38087993 -0.17172267 -0.13314584\n",
      "   0.62030738  0.67755823 -0.3657499 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.84640773  1.39587393 -1.19992543  0.18180504  0.169773   -1.59397583\n",
      "   0.78698872]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:35 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.19706642]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 35 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.77258283 -0.59604971  0.44900173  0.68009526 -0.27792005  0.08638246\n",
      "   1.19068195  0.00613589 -0.4265205 ]\n",
      " [-0.65698388  0.4223003  -0.62827998 -0.67662031  0.19617799 -0.31334576\n",
      "  -0.50556232 -0.17124458  0.56578439]\n",
      " [ 0.51798157 -0.10670962  0.12656225  0.27513606 -0.41188214  0.37536028\n",
      "   0.45671567 -0.14904466 -0.47391903]\n",
      " [ 0.48418647 -0.37672535  0.46182746  0.30707292 -0.31738708  0.27277446\n",
      "   0.67237873  0.64191476 -0.18547883]\n",
      " [-0.88541111  0.58673311 -0.65644664 -0.80628615  0.07001096  0.03597602\n",
      "  -0.8793732  -0.55576728  0.58492851]\n",
      " [ 0.98329624 -0.52494824  0.65624042  0.38087993 -0.17089833 -0.13232151\n",
      "   0.62030738  0.67755823 -0.36492556]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.86199877  1.38980288 -1.20856783  0.17440338  0.1624643  -1.60330896\n",
      "   0.78002384]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:35 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.54898607]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 35 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.75818291 -0.61044964  0.44900173  0.68009526 -0.29231997  0.07198254\n",
      "   1.17628202  0.00613589 -0.4265205 ]\n",
      " [-0.64479607  0.43448811 -0.62827998 -0.67662031  0.2083658  -0.30115795\n",
      "  -0.49337451 -0.17124458  0.56578439]\n",
      " [ 0.50603966 -0.11865153  0.12656225  0.27513606 -0.42382405  0.36341837\n",
      "   0.44477377 -0.14904466 -0.47391903]\n",
      " [ 0.47324111 -0.3876707   0.46182746  0.30707292 -0.32833243  0.26182911\n",
      "   0.66143337  0.64191476 -0.18547883]\n",
      " [-0.87156869  0.60057552 -0.65644664 -0.80628615  0.08385338  0.04981844\n",
      "  -0.86553078 -0.55576728  0.58492851]\n",
      " [ 0.9719185  -0.53632598  0.65624042  0.38087993 -0.18227607 -0.14369924\n",
      "   0.60892964  0.67755823 -0.36492556]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.92996334  1.33786641 -1.22881311  0.12705486  0.11652356 -1.62064065\n",
      "   0.7334888 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:35 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.70307223]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 35 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.76333306 -0.61044964  0.44900173  0.68524541 -0.29231997  0.07198254\n",
      "   1.18143217  0.00613589 -0.4265205 ]\n",
      " [-0.65157062  0.43448811 -0.62827998 -0.68339486  0.2083658  -0.30115795\n",
      "  -0.50014906 -0.17124458  0.56578439]\n",
      " [ 0.5127047  -0.11865153  0.12656225  0.28180109 -0.42382405  0.36341837\n",
      "   0.4514388  -0.14904466 -0.47391903]\n",
      " [ 0.48015336 -0.3876707   0.46182746  0.31398517 -0.32833243  0.26182911\n",
      "   0.66834563  0.64191476 -0.18547883]\n",
      " [-0.87689449  0.60057552 -0.65644664 -0.81161195  0.08385338  0.04981844\n",
      "  -0.87085658 -0.55576728  0.58492851]\n",
      " [ 0.97849    -0.53632598  0.65624042  0.38745143 -0.18227607 -0.14369924\n",
      "   0.61550114  0.67755823 -0.36492556]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.89896977  1.366746   -1.22447215  0.15101626  0.14158883 -1.61838209\n",
      "   0.76066148]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:35 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.62838467]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 35 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.77294088 -0.61044964  0.44900173  0.68524541 -0.28271215  0.08159036\n",
      "   1.19104     0.00613589 -0.4265205 ]\n",
      " [-0.66094647  0.43448811 -0.62827998 -0.68339486  0.19898995 -0.3105338\n",
      "  -0.50952491 -0.17124458  0.56578439]\n",
      " [ 0.52075017 -0.11865153  0.12656225  0.28180109 -0.41577858  0.37146384\n",
      "   0.45948428 -0.14904466 -0.47391903]\n",
      " [ 0.48902892 -0.3876707   0.46182746  0.31398517 -0.31945688  0.27070466\n",
      "   0.67722118  0.64191476 -0.18547883]\n",
      " [-0.88659133  0.60057552 -0.65644664 -0.81161195  0.07415654  0.0401216\n",
      "  -0.88055342 -0.55576728  0.58492851]\n",
      " [ 0.98791873 -0.53632598  0.65624042  0.38745143 -0.17284734 -0.13427052\n",
      "   0.62492987  0.67755823 -0.36492556]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.85558045  1.40357044 -1.21476806  0.18189718  0.17399508 -1.61117846\n",
      "   0.79452263]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:35 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57207398]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 35 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.75843173 -0.61044964  0.44900173  0.67073626 -0.28271215  0.06708121\n",
      "   1.19104     0.00613589 -0.44102966]\n",
      " [-0.64657938  0.43448811 -0.62827998 -0.66902778  0.19898995 -0.29616672\n",
      "  -0.50952491 -0.17124458  0.58015147]\n",
      " [ 0.50988147 -0.11865153  0.12656225  0.27093239 -0.41577858  0.36059514\n",
      "   0.45948428 -0.14904466 -0.48478773]\n",
      " [ 0.47618409 -0.3876707   0.46182746  0.30114035 -0.31945688  0.25785983\n",
      "   0.67722118  0.64191476 -0.19832366]\n",
      " [-0.87232272  0.60057552 -0.65644664 -0.79734335  0.07415654  0.0543902\n",
      "  -0.88055342 -0.55576728  0.59919712]\n",
      " [ 0.97518485 -0.53632598  0.65624042  0.37471755 -0.17284734 -0.1470044\n",
      "   0.62492987  0.67755823 -0.37765944]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.92560384  1.35086134 -1.23239922  0.13510692  0.12438163 -1.62902069\n",
      "   0.74508411]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:35 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.65977583]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 35 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.76628016 -0.6026012   0.44900173  0.6785847  -0.28271215  0.07492964\n",
      "   1.19888844  0.00613589 -0.44102966]\n",
      " [-0.65506873  0.42599877 -0.62827998 -0.67751712  0.19898995 -0.30465606\n",
      "  -0.51801425 -0.17124458  0.58015147]\n",
      " [ 0.5184185  -0.1101145   0.12656225  0.27946942 -0.41577858  0.36913217\n",
      "   0.4680213  -0.14904466 -0.48478773]\n",
      " [ 0.48457714 -0.37927765  0.46182746  0.3095334  -0.31945688  0.26625288\n",
      "   0.68561423  0.64191476 -0.19832366]\n",
      " [-0.88054247  0.59235577 -0.65644664 -0.8055631   0.07415654  0.04617045\n",
      "  -0.88877317 -0.55576728  0.59919712]\n",
      " [ 0.98352504 -0.52798579  0.65624042  0.38305775 -0.17284734 -0.1386642\n",
      "   0.63327006  0.67755823 -0.37765944]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.88741849  1.38479405 -1.22643483  0.16622457  0.15454685 -1.62403191\n",
      "   0.77503674]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:35 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.68492661]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 35 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.77333989 -0.59554147  0.44900173  0.68564442 -0.28271215  0.07492964\n",
      "   1.20594816  0.00613589 -0.44102966]\n",
      " [-0.66263998  0.41842752 -0.62827998 -0.68508837  0.19898995 -0.30465606\n",
      "  -0.52558551 -0.17124458  0.58015147]\n",
      " [ 0.52557398 -0.10295902  0.12656225  0.2866249  -0.41577858  0.36913217\n",
      "   0.47517678 -0.14904466 -0.48478773]\n",
      " [ 0.49158539 -0.3722694   0.46182746  0.31654165 -0.31945688  0.26625288\n",
      "   0.69262248  0.64191476 -0.19832366]\n",
      " [-0.88771356  0.58518468 -0.65644664 -0.81273419  0.07415654  0.04617045\n",
      "  -0.89594426 -0.55576728  0.59919712]\n",
      " [ 0.99112179 -0.52038904  0.65624042  0.3906545  -0.17284734 -0.1386642\n",
      "   0.64086681  0.67755823 -0.37765944]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.85342173  1.4148829  -1.21984145  0.19208141  0.18005611 -1.61991659\n",
      "   0.8026877 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:35 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.7722571]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 35 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.77684479 -0.59554147  0.45250662  0.68564442 -0.28271215  0.07843454\n",
      "   1.20945306  0.00613589 -0.44102966]\n",
      " [-0.66670099  0.41842752 -0.63234099 -0.68508837  0.19898995 -0.30871707\n",
      "  -0.52964652 -0.17124458  0.58015147]\n",
      " [ 0.53005395 -0.10295902  0.13104222  0.2866249  -0.41577858  0.37361214\n",
      "   0.47965675 -0.14904466 -0.48478773]\n",
      " [ 0.49588066 -0.3722694   0.46612273  0.31654165 -0.31945688  0.27054815\n",
      "   0.69691774  0.64191476 -0.19832366]\n",
      " [-0.89138805  0.58518468 -0.66012113 -0.81273419  0.07415654  0.04249596\n",
      "  -0.89961875 -0.55576728  0.59919712]\n",
      " [ 0.99514603 -0.52038904  0.66026466  0.3906545  -0.17284734 -0.13463997\n",
      "   0.64489105  0.67755823 -0.37765944]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.83339447  1.43339544 -1.2176974   0.20844456  0.19750538 -1.61824158\n",
      "   0.82062468]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:35 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.41652623]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 36 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.77169258 -0.60069368  0.45250662  0.68049222 -0.28271215  0.07843454\n",
      "   1.20945306  0.00613589 -0.44618187]\n",
      " [-0.66236814  0.42276037 -0.63234099 -0.68075552  0.19898995 -0.30871707\n",
      "  -0.52964652 -0.17124458  0.58448433]\n",
      " [ 0.52719475 -0.10581822  0.13104222  0.2837657  -0.41577858  0.37361214\n",
      "   0.47965675 -0.14904466 -0.48764693]\n",
      " [ 0.49286494 -0.37528512  0.46612273  0.31352593 -0.31945688  0.27054815\n",
      "   0.69691774  0.64191476 -0.20133938]\n",
      " [-0.88523631  0.59133642 -0.66012113 -0.80658245  0.07415654  0.04249596\n",
      "  -0.89961875 -0.55576728  0.60534886]\n",
      " [ 0.98932717 -0.52620789  0.66026466  0.38483564 -0.17284734 -0.13463997\n",
      "   0.64489105  0.67755823 -0.38347829]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.8840091   1.40277873 -1.2385812   0.180253    0.16915287 -1.63711643\n",
      "   0.78926503]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:36 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.74997199]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 36 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.77592217 -0.60069368  0.45673621  0.68049222 -0.28271215  0.07843454\n",
      "   1.21368265  0.00613589 -0.44618187]\n",
      " [-0.66748365  0.42276037 -0.6374565  -0.68075552  0.19898995 -0.30871707\n",
      "  -0.53476203 -0.17124458  0.58448433]\n",
      " [ 0.53209751 -0.10581822  0.13594498  0.2837657  -0.41577858  0.37361214\n",
      "   0.48455951 -0.14904466 -0.48764693]\n",
      " [ 0.49809026 -0.37528512  0.47134805  0.31352593 -0.31945688  0.27054815\n",
      "   0.70214306  0.64191476 -0.20133938]\n",
      " [-0.88944538  0.59133642 -0.6643302  -0.80658245  0.07415654  0.04249596\n",
      "  -0.90382781 -0.55576728  0.60534886]\n",
      " [ 0.99380192 -0.52620789  0.6647394   0.38483564 -0.17284734 -0.13463997\n",
      "   0.64936579  0.67755823 -0.38347829]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.86056722  1.4243302  -1.23532484  0.19800537  0.18883672 -1.63524561\n",
      "   0.81056008]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:36 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.71052959]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 36 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.76003768 -0.61657817  0.44085172  0.68049222 -0.28271215  0.07843454\n",
      "   1.19779816  0.00613589 -0.44618187]\n",
      " [-0.65122219  0.43902184 -0.62119504 -0.68075552  0.19898995 -0.30871707\n",
      "  -0.51850057 -0.17124458  0.58448433]\n",
      " [ 0.5173869  -0.12052883  0.12123437  0.2837657  -0.41577858  0.37361214\n",
      "   0.46984891 -0.14904466 -0.48764693]\n",
      " [ 0.48211524 -0.39126013  0.45537303  0.31352593 -0.31945688  0.27054815\n",
      "   0.68616805  0.64191476 -0.20133938]\n",
      " [-0.87362327  0.60715853 -0.64850809 -0.80658245  0.07415654  0.04249596\n",
      "  -0.8880057  -0.55576728  0.60534886]\n",
      " [ 0.97774569 -0.54226412  0.64868318  0.38483564 -0.17284734 -0.13463997\n",
      "   0.63330957  0.67755823 -0.38347829]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.93363712  1.36122592 -1.24958381  0.14392218  0.13146143 -1.6450351\n",
      "   0.74801907]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:36 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.63896021]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 36 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.76894588 -0.61657817  0.44975993  0.68049222 -0.28271215  0.07843454\n",
      "   1.19779816  0.01504409 -0.44618187]\n",
      " [-0.6605111   0.43902184 -0.63048395 -0.68075552  0.19898995 -0.30871707\n",
      "  -0.51850057 -0.18053349  0.58448433]\n",
      " [ 0.52219028 -0.12052883  0.12603775  0.2837657  -0.41577858  0.37361214\n",
      "   0.46984891 -0.14424128 -0.48764693]\n",
      " [ 0.49143395 -0.39126013  0.46469174  0.31352593 -0.31945688  0.27054815\n",
      "   0.68616805  0.65123346 -0.20133938]\n",
      " [-0.88217993  0.60715853 -0.65706475 -0.80658245  0.07415654  0.04249596\n",
      "  -0.8880057  -0.56432394  0.60534886]\n",
      " [ 0.98566615 -0.54226412  0.65660364  0.38483564 -0.17284734 -0.13463997\n",
      "   0.63330957  0.68547869 -0.38347829]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.89199298  1.39328242 -1.24163065  0.16974183  0.16598942 -1.64040162\n",
      "   0.78588221]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:36 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.52354698]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 36 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.78064985 -0.61657817  0.44975993  0.69219618 -0.28271215  0.07843454\n",
      "   1.19779816  0.01504409 -0.4344779 ]\n",
      " [-0.67028626  0.43902184 -0.63048395 -0.69053069  0.19898995 -0.30871707\n",
      "  -0.51850057 -0.18053349  0.57470916]\n",
      " [ 0.52680134 -0.12052883  0.12603775  0.28837675 -0.41577858  0.37361214\n",
      "   0.46984891 -0.14424128 -0.48303587]\n",
      " [ 0.49963174 -0.39126013  0.46469174  0.32172371 -0.31945688  0.27054815\n",
      "   0.68616805  0.65123346 -0.19314159]\n",
      " [-0.89434299  0.60715853 -0.65706475 -0.81874551  0.07415654  0.04249596\n",
      "  -0.8880057  -0.56432394  0.59318579]\n",
      " [ 0.99726709 -0.54226412  0.65660364  0.39643658 -0.17284734 -0.13463997\n",
      "   0.63330957  0.68547869 -0.37187735]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.83256844  1.43676326 -1.22265371  0.20414342  0.20440645 -1.6253755\n",
      "   0.82917296]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:36 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.34239451]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 36 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.77928932 -0.61657817  0.44975993  0.69219618 -0.28407268  0.07707401\n",
      "   1.19779816  0.01504409 -0.43583843]\n",
      " [-0.66832851  0.43902184 -0.63048395 -0.69053069  0.20094771 -0.30675931\n",
      "  -0.51850057 -0.18053349  0.57666692]\n",
      " [ 0.52678593 -0.12052883  0.12603775  0.28837675 -0.41579399  0.37359673\n",
      "   0.46984891 -0.14424128 -0.48305128]\n",
      " [ 0.49719022 -0.39126013  0.46469174  0.32172371 -0.3218984   0.26810664\n",
      "   0.68616805  0.65123346 -0.19558311]\n",
      " [-0.89258001  0.60715853 -0.65706475 -0.81874551  0.07591951  0.04425894\n",
      "  -0.8880057  -0.56432394  0.59494877]\n",
      " [ 0.99427967 -0.54226412  0.65660364  0.39643658 -0.17583476 -0.13762739\n",
      "   0.63330957  0.68547869 -0.37486477]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.8711153   1.41612472 -1.2399556   0.18485458  0.18266441 -1.64287594\n",
      "   0.80686154]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:36 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.18363934]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 36 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.7808499  -0.61501759  0.44975993  0.69219618 -0.2825121   0.07863459\n",
      "   1.19779816  0.01504409 -0.43427785]\n",
      " [-0.66914774  0.4382026  -0.63048395 -0.69053069  0.20012847 -0.30757855\n",
      "  -0.51850057 -0.18053349  0.57584768]\n",
      " [ 0.52719397 -0.12012079  0.12603775  0.28837675 -0.41538595  0.37400477\n",
      "   0.46984891 -0.14424128 -0.48264324]\n",
      " [ 0.49768133 -0.39076903  0.46469174  0.32172371 -0.32140729  0.26859774\n",
      "   0.68616805  0.65123346 -0.195092  ]\n",
      " [-0.89399255  0.605746   -0.65706475 -0.81874551  0.07450698  0.0428464\n",
      "  -0.8880057  -0.56432394  0.59353623]\n",
      " [ 0.99508165 -0.54146215  0.65660364  0.39643658 -0.17503279 -0.13682541\n",
      "   0.63330957  0.68547869 -0.3740628 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.88488053  1.41086334 -1.24766544  0.17838096  0.17627459 -1.65121496\n",
      "   0.80078838]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:36 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.54591728]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 36 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.76658898 -0.62927851  0.44975993  0.69219618 -0.29677302  0.06437367\n",
      "   1.18353723  0.01504409 -0.43427785]\n",
      " [-0.65701859  0.45033175 -0.63048395 -0.69053069  0.21225762 -0.2954494\n",
      "  -0.50637142 -0.18053349  0.57584768]\n",
      " [ 0.51526575 -0.132049    0.12603775  0.28837675 -0.42731416  0.36207656\n",
      "   0.45792069 -0.14424128 -0.48264324]\n",
      " [ 0.48672912 -0.40172124  0.46469174  0.32172371 -0.3323595   0.25764553\n",
      "   0.67521584  0.65123346 -0.195092  ]\n",
      " [-0.88029292  0.61944563 -0.65706475 -0.81874551  0.08820661  0.05654603\n",
      "  -0.87430607 -0.56432394  0.59353623]\n",
      " [ 0.983758   -0.55278579  0.65660364  0.39643658 -0.18635643 -0.14814906\n",
      "   0.62198592  0.68547869 -0.3740628 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.95254468  1.35935145 -1.26782857  0.13118351  0.13046223 -1.66863983\n",
      "   0.75446437]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:36 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.71090676]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 36 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.77145933 -0.62927851  0.44975993  0.69706654 -0.29677302  0.06437367\n",
      "   1.18840759  0.01504409 -0.43427785]\n",
      " [-0.66346651  0.45033175 -0.63048395 -0.6969786   0.21225762 -0.2954494\n",
      "  -0.51281934 -0.18053349  0.57584768]\n",
      " [ 0.52171162 -0.132049    0.12603775  0.29482262 -0.42731416  0.36207656\n",
      "   0.46436656 -0.14424128 -0.48264324]\n",
      " [ 0.49337107 -0.40172124  0.46469174  0.32836567 -0.3323595   0.25764553\n",
      "   0.6818578   0.65123346 -0.195092  ]\n",
      " [-0.88532696  0.61944563 -0.65706475 -0.82377955  0.08820661  0.05654603\n",
      "  -0.87934011 -0.56432394  0.59353623]\n",
      " [ 0.98999252 -0.55278579  0.65660364  0.4026711  -0.18635643 -0.14814906\n",
      "   0.62822044  0.68547869 -0.3740628 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.9228377   1.38708399 -1.26380585  0.15433401  0.15467717 -1.66653439\n",
      "   0.78063698]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:36 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.63133403]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 36 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.78095622 -0.62927851  0.44975993  0.69706654 -0.28727614  0.07387055\n",
      "   1.19790447  0.01504409 -0.43427785]\n",
      " [-0.67277127  0.45033175 -0.63048395 -0.6969786   0.20295285 -0.30475416\n",
      "  -0.5221241  -0.18053349  0.57584768]\n",
      " [ 0.52975864 -0.132049    0.12603775  0.29482262 -0.41926714  0.37012358\n",
      "   0.47241358 -0.14424128 -0.48264324]\n",
      " [ 0.50221569 -0.40172124  0.46469174  0.32836567 -0.32351488  0.26649015\n",
      "   0.69070241  0.65123346 -0.195092  ]\n",
      " [-0.89491247  0.61944563 -0.65706475 -0.82377955  0.0786211   0.04696053\n",
      "  -0.88892562 -0.56432394  0.59353623]\n",
      " [ 0.99934802 -0.55278579  0.65660364  0.4026711  -0.17700094 -0.13879356\n",
      "   0.63757594  0.68547869 -0.3740628 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.87993395  1.42351306 -1.25432138  0.18501935  0.18687028 -1.65944597\n",
      "   0.81423406]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:36 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57249717]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 36 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.76641524 -0.62927851  0.44975993  0.68252556 -0.28727614  0.05932958\n",
      "   1.19790447  0.01504409 -0.44881883]\n",
      " [-0.65833985  0.45033175 -0.63048395 -0.68254717  0.20295285 -0.29032274\n",
      "  -0.5221241  -0.18053349  0.59027911]\n",
      " [ 0.51874355 -0.132049    0.12603775  0.28380753 -0.41926714  0.35910848\n",
      "   0.47241358 -0.14424128 -0.49365834]\n",
      " [ 0.48924086 -0.40172124  0.46469174  0.31539084 -0.32351488  0.25351532\n",
      "   0.69070241  0.65123346 -0.20806684]\n",
      " [-0.88060529  0.61944563 -0.65706475 -0.80947238  0.0786211   0.0612677\n",
      "  -0.88892562 -0.56432394  0.6078434 ]\n",
      " [ 0.9864885  -0.55278579  0.65660364  0.38981159 -0.17700094 -0.15165308\n",
      "   0.63757594  0.68547869 -0.38692231]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.94999162  1.37072137 -1.27183522  0.1380207   0.13703433 -1.67722995\n",
      "   0.76458298]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:36 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.66312894]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 36 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.77414872 -0.62154504  0.44975993  0.69025904 -0.28727614  0.06706305\n",
      "   1.20563795  0.01504409 -0.44881883]\n",
      " [-0.66669148  0.44198012 -0.63048395 -0.69089881  0.20295285 -0.29867437\n",
      "  -0.53047573 -0.18053349  0.59027911]\n",
      " [ 0.52716189 -0.12363066  0.12603775  0.29222587 -0.41926714  0.36752683\n",
      "   0.48083192 -0.14424128 -0.49365834]\n",
      " [ 0.49754171 -0.39342039  0.46469174  0.32369169 -0.32351488  0.26181617\n",
      "   0.69900326  0.65123346 -0.20806684]\n",
      " [-0.88869957  0.61135135 -0.65706475 -0.81756665  0.0786211   0.05317342\n",
      "  -0.89701989 -0.56432394  0.6078434 ]\n",
      " [ 0.99473578 -0.54453852  0.65660364  0.39805886 -0.17700094 -0.1434058\n",
      "   0.64582321  0.68547869 -0.38692231]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.91236498  1.4041578  -1.26603795  0.16879463  0.16689707 -1.67232712\n",
      "   0.7942109 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:36 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.68848402]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 36 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.78107142 -0.61462234  0.44975993  0.69718174 -0.28727614  0.06706305\n",
      "   1.21256065  0.01504409 -0.44881883]\n",
      " [-0.67414416  0.43452743 -0.63048395 -0.69835149  0.20295285 -0.29867437\n",
      "  -0.53792842 -0.18053349  0.59027911]\n",
      " [ 0.53424183 -0.11655072  0.12603775  0.29930581 -0.41926714  0.36752683\n",
      "   0.48791186 -0.14424128 -0.49365834]\n",
      " [ 0.5044998  -0.3864623   0.46469174  0.33064978 -0.32351488  0.26181617\n",
      "   0.70596135  0.65123346 -0.20806684]\n",
      " [-0.89572911  0.60432181 -0.65706475 -0.82459619  0.0786211   0.05317342\n",
      "  -0.90404943 -0.56432394  0.6078434 ]\n",
      " [ 1.00220787 -0.53706643  0.65660364  0.40553095 -0.17700094 -0.1434058\n",
      "   0.6532953   0.68547869 -0.38692231]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.87895898  1.43374871 -1.25967061  0.19432798  0.19212699 -1.66831665\n",
      "   0.82149325]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:36 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.77570109]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 36 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.78447516 -0.61462234  0.45316367  0.69718174 -0.28727614  0.0704668\n",
      "   1.2159644   0.01504409 -0.44881883]\n",
      " [-0.67807563  0.43452743 -0.63441541 -0.69835149  0.20295285 -0.30260583\n",
      "  -0.54185988 -0.18053349  0.59027911]\n",
      " [ 0.53860898 -0.11655072  0.1304049   0.29930581 -0.41926714  0.37189398\n",
      "   0.49227901 -0.14424128 -0.49365834]\n",
      " [ 0.50866147 -0.3864623   0.46885341  0.33064978 -0.32351488  0.26597784\n",
      "   0.71012303  0.65123346 -0.20806684]\n",
      " [-0.89929471  0.60432181 -0.66063035 -0.82459619  0.0786211   0.04960782\n",
      "  -0.90761503 -0.56432394  0.6078434 ]\n",
      " [ 1.00610405 -0.53706643  0.66049982  0.40553095 -0.17700094 -0.13950962\n",
      "   0.65719148  0.68547869 -0.38692231]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.85944622  1.45179551 -1.25761877  0.21032663  0.20918151 -1.66669917\n",
      "   0.83900406]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:36 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.4076337]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 37 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.77954811 -0.6195494   0.45316367  0.69225468 -0.28727614  0.0704668\n",
      "   1.2159644   0.01504409 -0.45374588]\n",
      " [-0.67388036  0.4387227  -0.63441541 -0.69415623  0.20295285 -0.30260583\n",
      "  -0.54185988 -0.18053349  0.59447437]\n",
      " [ 0.53584333 -0.11931637  0.1304049   0.29654016 -0.41926714  0.37189398\n",
      "   0.49227901 -0.14424128 -0.49642398]\n",
      " [ 0.50569438 -0.38942939  0.46885341  0.32768269 -0.32351488  0.26597784\n",
      "   0.71012303  0.65123346 -0.21103393]\n",
      " [-0.89339336  0.61022316 -0.66063035 -0.81869484  0.0786211   0.04960782\n",
      "  -0.90761503 -0.56432394  0.61374475]\n",
      " [ 1.00044714 -0.54272333  0.66049982  0.39987405 -0.17700094 -0.13950962\n",
      "   0.65719148  0.68547869 -0.39257922]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.90866156  1.42211589 -1.27794419  0.18292935  0.18157703 -1.68514454\n",
      "   0.80851261]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:37 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.75449335]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 37 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.78362426 -0.6195494   0.45723983  0.69225468 -0.28727614  0.0704668\n",
      "   1.22004055  0.01504409 -0.45374588]\n",
      " [-0.67881916  0.4387227  -0.63935421 -0.69415623  0.20295285 -0.30260583\n",
      "  -0.54679868 -0.18053349  0.59447437]\n",
      " [ 0.5406336  -0.11931637  0.13519517  0.29654016 -0.41926714  0.37189398\n",
      "   0.49706928 -0.14424128 -0.49642398]\n",
      " [ 0.51075049 -0.38942939  0.47390952  0.32768269 -0.32351488  0.26597784\n",
      "   0.71517914  0.65123346 -0.21103393]\n",
      " [-0.89744662  0.61022316 -0.66468361 -0.81869484  0.0786211   0.04960782\n",
      "  -0.91166829 -0.56432394  0.61374475]\n",
      " [ 1.00474816 -0.54272333  0.66480083  0.39987405 -0.17700094 -0.13950962\n",
      "   0.6614925   0.68547869 -0.39257922]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.88592357  1.4430454  -1.27485513  0.20023443  0.20075702 -1.68335753\n",
      "   0.82921255]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:37 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.71168727]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 37 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.76773914 -0.63543452  0.44135471  0.69225468 -0.28727614  0.0704668\n",
      "   1.20415543  0.01504409 -0.45374588]\n",
      " [-0.66255595  0.45498591 -0.623091   -0.69415623  0.20295285 -0.30260583\n",
      "  -0.53053547 -0.18053349  0.59447437]\n",
      " [ 0.52588694 -0.13406303  0.12044851  0.29654016 -0.41926714  0.37189398\n",
      "   0.48232262 -0.14424128 -0.49642398]\n",
      " [ 0.4947436  -0.40543627  0.45790264  0.32768269 -0.32351488  0.26597784\n",
      "   0.69917225  0.65123346 -0.21103393]\n",
      " [-0.88162807  0.62604171 -0.64886506 -0.81869484  0.0786211   0.04960782\n",
      "  -0.89584974 -0.56432394  0.61374475]\n",
      " [ 0.98872017 -0.55875132  0.64877284  0.39987405 -0.17700094 -0.13950962\n",
      "   0.6454645   0.68547869 -0.39257922]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.9589386   1.38002548 -1.28899696  0.14609656  0.14325172 -1.69316249\n",
      "   0.76665905]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:37 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.64308104]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 37 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.7765434  -0.63543452  0.45015897  0.69225468 -0.28727614  0.0704668\n",
      "   1.20415543  0.02384836 -0.45374588]\n",
      " [-0.67170613  0.45498591 -0.63224118 -0.69415623  0.20295285 -0.30260583\n",
      "  -0.53053547 -0.18968366  0.59447437]\n",
      " [ 0.53071766 -0.13406303  0.12527923  0.29654016 -0.41926714  0.37189398\n",
      "   0.48232262 -0.13941056 -0.49642398]\n",
      " [ 0.50390205 -0.40543627  0.46706108  0.32768269 -0.32351488  0.26597784\n",
      "   0.69917225  0.6603919  -0.21103393]\n",
      " [-0.89000187  0.62604171 -0.65723885 -0.81869484  0.0786211   0.04960782\n",
      "  -0.89584974 -0.57269773  0.61374475]\n",
      " [ 0.99645364 -0.55875132  0.65650631  0.39987405 -0.17700094 -0.13950962\n",
      "   0.6454645   0.69321217 -0.39257922]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.91797718  1.41168009 -1.28131571  0.17161353  0.17735455 -1.68867305\n",
      "   0.80396514]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:37 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.5233094]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 37 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.78832651 -0.63543452  0.45015897  0.70403779 -0.28727614  0.0704668\n",
      "   1.20415543  0.02384836 -0.44196278]\n",
      " [-0.68162244  0.45498591 -0.63224118 -0.70407253  0.20295285 -0.30260583\n",
      "  -0.53053547 -0.18968366  0.58455807]\n",
      " [ 0.5355031  -0.13406303  0.12527923  0.3013256  -0.41926714  0.37189398\n",
      "   0.48232262 -0.13941056 -0.49163854]\n",
      " [ 0.51229207 -0.40543627  0.46706108  0.33607271 -0.32351488  0.26597784\n",
      "   0.69917225  0.6603919  -0.2026439 ]\n",
      " [-0.90223089  0.62604171 -0.65723885 -0.83092387  0.0786211   0.04960782\n",
      "  -0.89584974 -0.57269773  0.60151573]\n",
      " [ 1.00816704 -0.55875132  0.65650631  0.41158745 -0.17700094 -0.13950962\n",
      "   0.6454645   0.69321217 -0.38086582]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.85852036  1.45532186 -1.26251672  0.20621515  0.21602193 -1.673768\n",
      "   0.84747534]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:37 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.33081451]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 37 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.78714549 -0.63543452  0.45015897  0.70403779 -0.28845716  0.06928578\n",
      "   1.20415543  0.02384836 -0.4431438 ]\n",
      " [-0.67983894  0.45498591 -0.63224118 -0.70407253  0.20473636 -0.30082233\n",
      "  -0.53053547 -0.18968366  0.58634157]\n",
      " [ 0.53553522 -0.13406303  0.12527923  0.3013256  -0.41923502  0.3719261\n",
      "   0.48232262 -0.13941056 -0.49160642]\n",
      " [ 0.51002046 -0.40543627  0.46706108  0.33607271 -0.3257865   0.26370623\n",
      "   0.69917225  0.6603919  -0.20491552]\n",
      " [-0.90066358  0.62604171 -0.65723885 -0.83092387  0.08018841  0.05117513\n",
      "  -0.89584974 -0.57269773  0.60308303]\n",
      " [ 1.00538958 -0.55875132  0.65650631  0.41158745 -0.1797784  -0.14228709\n",
      "   0.6454645   0.69321217 -0.38364328]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.8951376   1.43582891 -1.27903031  0.18793865  0.19541755 -1.69050153\n",
      "   0.82634433]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:37 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.17086903]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 37 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.78859406 -0.63398595  0.45015897  0.70403779 -0.28700858  0.07073435\n",
      "   1.20415543  0.02384836 -0.44169523]\n",
      " [-0.68062805  0.45419679 -0.63224118 -0.70407253  0.20394724 -0.30161145\n",
      "  -0.53053547 -0.18968366  0.58555245]\n",
      " [ 0.53594916 -0.13364909  0.12527923  0.3013256  -0.41882109  0.37234004\n",
      "   0.48232262 -0.13941056 -0.49119249]\n",
      " [ 0.51050868 -0.40494805  0.46706108  0.33607271 -0.32529828  0.26419445\n",
      "   0.69917225  0.6603919  -0.2044273 ]\n",
      " [-0.90198395  0.62472134 -0.65723885 -0.83092387  0.07886804  0.04985476\n",
      "  -0.89584974 -0.57269773  0.60176267]\n",
      " [ 1.0061605  -0.5579804   0.65650631  0.41158745 -0.17900748 -0.14151616\n",
      "   0.6454645   0.69321217 -0.38287236]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.90724135  1.43128938 -1.2858806   0.18230202  0.18985605 -1.6979208\n",
      "   0.82107203]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:37 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.54235459]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 37 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.77449352 -0.64808649  0.45015897  0.70403779 -0.30110913  0.05663381\n",
      "   1.19005489  0.02384836 -0.44169523]\n",
      " [-0.66858366  0.46624119 -0.63224118 -0.70407253  0.21599163 -0.28956706\n",
      "  -0.51849108 -0.18968366  0.58555245]\n",
      " [ 0.52405906 -0.14553919  0.12527923  0.3013256  -0.43071118  0.36044994\n",
      "   0.47043253 -0.13941056 -0.49119249]\n",
      " [ 0.49957775 -0.41587898  0.46706108  0.33607271 -0.33622921  0.25326352\n",
      "   0.68824132  0.6603919  -0.2044273 ]\n",
      " [-0.88844883  0.63825647 -0.65723885 -0.83092387  0.09240317  0.06338989\n",
      "  -0.88231462 -0.57269773  0.60176267]\n",
      " [ 0.99491674 -0.56922416  0.65650631  0.41158745 -0.19025124 -0.15275993\n",
      "   0.63422074  0.69321217 -0.38287236]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.9745492   1.38026085 -1.30596945  0.13531618  0.14423553 -1.7154413\n",
      "   0.77502029]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:37 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.71851823]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 37 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.7791003  -0.64808649  0.45015897  0.70864456 -0.30110913  0.05663381\n",
      "   1.19466167  0.02384836 -0.44169523]\n",
      " [-0.67471626  0.46624119 -0.63224118 -0.71020513  0.21599163 -0.28956706\n",
      "  -0.52462368 -0.18968366  0.58555245]\n",
      " [ 0.53028162 -0.14553919  0.12527923  0.30754816 -0.43071118  0.36044994\n",
      "   0.47665509 -0.13941056 -0.49119249]\n",
      " [ 0.50594935 -0.41587898  0.46706108  0.34244431 -0.33622921  0.25326352\n",
      "   0.69461292  0.6603919  -0.2044273 ]\n",
      " [-0.89320805  0.63825647 -0.65723885 -0.8356831   0.09240317  0.06338989\n",
      "  -0.88707384 -0.57269773  0.60176267]\n",
      " [ 1.00082842 -0.56922416  0.65650631  0.41749913 -0.19025124 -0.15275993\n",
      "   0.64013242  0.69321217 -0.38287236]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.94608439  1.40687964 -1.30223733  0.15766476  0.1676083  -1.71347637\n",
      "   0.80021185]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:37 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.63414628]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 37 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.78849246 -0.64808649  0.45015897  0.70864456 -0.29171696  0.06602597\n",
      "   1.20405383  0.02384836 -0.44169523]\n",
      " [-0.68394887  0.46624119 -0.63224118 -0.71020513  0.20675903 -0.29879966\n",
      "  -0.53385629 -0.18968366  0.58555245]\n",
      " [ 0.53832311 -0.14553919  0.12527923  0.30754816 -0.42266969  0.36849143\n",
      "   0.48469658 -0.13941056 -0.49119249]\n",
      " [ 0.51475799 -0.41587898  0.46706108  0.34244431 -0.32742056  0.26207216\n",
      "   0.70342157  0.6603919  -0.2044273 ]\n",
      " [-0.9026876   0.63825647 -0.65723885 -0.8356831   0.08292363  0.05391035\n",
      "  -0.89655339 -0.57269773  0.60176267]\n",
      " [ 1.01010997 -0.56922416  0.65650631  0.41749913 -0.18096968 -0.14347837\n",
      "   0.64941398  0.69321217 -0.38287236]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.90364448  1.44292475 -1.29295323  0.18815476  0.19958858 -1.70649133\n",
      "   0.83354796]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:37 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57294224]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 37 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.77392241 -0.64808649  0.45015897  0.69407452 -0.29171696  0.05145592\n",
      "   1.20405383  0.02384836 -0.45626527]\n",
      " [-0.66945882  0.46624119 -0.63224118 -0.69571509  0.20675903 -0.28430961\n",
      "  -0.53385629 -0.18968366  0.6000425 ]\n",
      " [ 0.52717315 -0.14553919  0.12527923  0.2963982  -0.42266969  0.35734147\n",
      "   0.48469658 -0.13941056 -0.50234245]\n",
      " [ 0.50166318 -0.41587898  0.46706108  0.3293495  -0.32742056  0.24897735\n",
      "   0.70342157  0.6603919  -0.21752211]\n",
      " [-0.88834519  0.63825647 -0.65723885 -0.82134069  0.08292363  0.06825275\n",
      "  -0.89655339 -0.57269773  0.61610507]\n",
      " [ 0.99713491 -0.56922416  0.65650631  0.40452407 -0.18096968 -0.15645343\n",
      "   0.64941398  0.69321217 -0.39584742]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.97373807  1.3900559  -1.31036138  0.14096042  0.14954238 -1.72422472\n",
      "   0.78369683]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:37 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.66629118]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 37 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.7815509  -0.64045801  0.45015897  0.701703   -0.29171696  0.0590844\n",
      "   1.21168231  0.02384836 -0.45626527]\n",
      " [-0.67768084  0.45801916 -0.63224118 -0.70393711  0.20675903 -0.29253164\n",
      "  -0.54207831 -0.18968366  0.6000425 ]\n",
      " [ 0.53547708 -0.13723526  0.12527923  0.30470214 -0.42266969  0.3656454\n",
      "   0.49300051 -0.13941056 -0.50234245]\n",
      " [ 0.5098722  -0.40766996  0.46706108  0.33755852 -0.32742056  0.25718637\n",
      "   0.71163059  0.6603919  -0.21752211]\n",
      " [-0.89632365  0.63027801 -0.65723885 -0.82931915  0.08292363  0.06027429\n",
      "  -0.90453184 -0.57269773  0.61610507]\n",
      " [ 1.00529027 -0.5610688   0.65650631  0.41267942 -0.18096968 -0.14829807\n",
      "   0.65756933  0.69321217 -0.39584742]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.93663845  1.42301858 -1.30471313  0.17140246  0.17911008 -1.71939661\n",
      "   0.81300843]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:37 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.6919198]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 37 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.78834395 -0.63366495  0.45015897  0.70849605 -0.29171696  0.0590844\n",
      "   1.21847537  0.02384836 -0.45626527]\n",
      " [-0.68501576  0.45068424 -0.63224118 -0.71127203  0.20675903 -0.29253164\n",
      "  -0.54941323 -0.18968366  0.6000425 ]\n",
      " [ 0.54247826 -0.13023407  0.12527923  0.31170332 -0.42266969  0.3656454\n",
      "   0.50000169 -0.13941056 -0.50234245]\n",
      " [ 0.51677288 -0.40076928  0.46706108  0.3444592  -0.32742056  0.25718637\n",
      "   0.71853127  0.6603919  -0.21752211]\n",
      " [-0.90321902  0.62338264 -0.65723885 -0.83621452  0.08292363  0.06027429\n",
      "  -0.91142721 -0.57269773  0.61610507]\n",
      " [ 1.01263916 -0.55371991  0.65650631  0.42002832 -0.18096968 -0.14829807\n",
      "   0.66491823  0.69321217 -0.39584742]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.90380222  1.45212459 -1.29855304  0.19661421  0.2040579  -1.71548208\n",
      "   0.83992503]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:37 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.77900647]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 37 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.79165379 -0.63366495  0.45346881  0.70849605 -0.29171696  0.06239425\n",
      "   1.22178521  0.02384836 -0.45626527]\n",
      " [-0.68882606  0.45068424 -0.63605148 -0.71127203  0.20675903 -0.29634194\n",
      "  -0.55322353 -0.18968366  0.6000425 ]\n",
      " [ 0.54673674 -0.13023407  0.1295377   0.31170332 -0.42266969  0.36990387\n",
      "   0.50426016 -0.13941056 -0.50234245]\n",
      " [ 0.52080834 -0.40076928  0.47109654  0.3444592  -0.32742056  0.26122183\n",
      "   0.72256673  0.6603919  -0.21752211]\n",
      " [-0.9066835   0.62338264 -0.66070333 -0.83621452  0.08292363  0.05680982\n",
      "  -0.91489169 -0.57269773  0.61610507]\n",
      " [ 1.01641569 -0.55371991  0.66028284  0.42002832 -0.18096968 -0.14452155\n",
      "   0.66869475  0.69321217 -0.39584742]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.88477961  1.46972546 -1.29658468  0.21226038  0.22073093 -1.71391667\n",
      "   0.85702568]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:37 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.39882951]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 38 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.78695101 -0.63836773  0.45346881  0.70379327 -0.29171696  0.06239425\n",
      "   1.22178521  0.02384836 -0.46096805]\n",
      " [-0.68477486  0.45473544 -0.63605148 -0.70722083  0.20675903 -0.29634194\n",
      "  -0.55322353 -0.18968366  0.6040937 ]\n",
      " [ 0.5440711  -0.13289971  0.1295377   0.30903768 -0.42266969  0.36990387\n",
      "   0.50426016 -0.13941056 -0.50500809]\n",
      " [ 0.51790076 -0.40367686  0.47109654  0.34155162 -0.32742056  0.26122183\n",
      "   0.72256673  0.6603919  -0.22042969]\n",
      " [-0.90103185  0.62903429 -0.66070333 -0.83056287  0.08292363  0.05680982\n",
      "  -0.91489169 -0.57269773  0.62175672]\n",
      " [ 1.01092769 -0.55920791  0.66028284  0.41454032 -0.18096968 -0.14452155\n",
      "   0.66869475  0.69321217 -0.40133542]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.93259219  1.44098338 -1.31635685  0.18566573  0.19388741 -1.73192957\n",
      "   0.82741198]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:38 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.75890233]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 38 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.79088187 -0.63836773  0.45739967  0.70379327 -0.29171696  0.06239425\n",
      "   1.22571607  0.02384836 -0.46096805]\n",
      " [-0.68954354  0.45473544 -0.64082016 -0.70722083  0.20675903 -0.29634194\n",
      "  -0.55799221 -0.18968366  0.6040937 ]\n",
      " [ 0.54874764 -0.13289971  0.13421425  0.30903768 -0.42266969  0.36990387\n",
      "   0.50893671 -0.13941056 -0.50500809]\n",
      " [ 0.522792   -0.40367686  0.47598778  0.34155162 -0.32742056  0.26122183\n",
      "   0.72745797  0.6603919  -0.22042969]\n",
      " [-0.9049379   0.62903429 -0.66460938 -0.83056287  0.08292363  0.05680982\n",
      "  -0.91879774 -0.57269773  0.62175672]\n",
      " [ 1.01506429 -0.55920791  0.66441945  0.41454032 -0.18096968 -0.14452155\n",
      "   0.67283136  0.69321217 -0.40133542]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.91053542  1.46130758 -1.31342168  0.20252955  0.21257035 -1.73021988\n",
      "   0.84753059]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:38 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.71268831]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 38 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.77499082 -0.65425878  0.44150862  0.70379327 -0.29171696  0.06239425\n",
      "   1.20982502  0.02384836 -0.46096805]\n",
      " [-0.67328092  0.47099806 -0.62455754 -0.70722083  0.20675903 -0.29634194\n",
      "  -0.54172959 -0.18968366  0.6040937 ]\n",
      " [ 0.53397383 -0.14767353  0.11944043  0.30903768 -0.42266969  0.36990387\n",
      "   0.49416289 -0.13941056 -0.50500809]\n",
      " [ 0.50676029 -0.41970856  0.45995607  0.34155162 -0.32742056  0.26122183\n",
      "   0.71142626  0.6603919  -0.22042969]\n",
      " [-0.88911683  0.64485536 -0.64878831 -0.83056287  0.08292363  0.05680982\n",
      "  -0.90297667 -0.57269773  0.62175672]\n",
      " [ 0.99905892 -0.57521328  0.64841407  0.41454032 -0.18096968 -0.14452155\n",
      "   0.65682599  0.69321217 -0.40133542]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.98350176  1.39837914 -1.32747027  0.1483521   0.15495559 -1.74005453\n",
      "   0.78497597]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:38 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.64721239]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 38 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.78368471 -0.65425878  0.45020251  0.70379327 -0.29171696  0.06239425\n",
      "   1.20982502  0.03254224 -0.46096805]\n",
      " [-0.68228733  0.47099806 -0.63356395 -0.70722083  0.20675903 -0.29634194\n",
      "  -0.54172959 -0.19869007  0.6040937 ]\n",
      " [ 0.53882196 -0.14767353  0.12428857  0.30903768 -0.42266969  0.36990387\n",
      "   0.49416289 -0.13456242 -0.50500809]\n",
      " [ 0.51575512 -0.41970856  0.46895089  0.34155162 -0.32742056  0.26122183\n",
      "   0.71142626  0.66938673 -0.22042969]\n",
      " [-0.89731067  0.64485536 -0.65698215 -0.83056287  0.08292363  0.05680982\n",
      "  -0.90297667 -0.58089158  0.62175672]\n",
      " [ 1.00661004 -0.57521328  0.65596519  0.41454032 -0.18096968 -0.14452155\n",
      "   0.65682599  0.70076329 -0.40133542]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.94322603  1.42961799 -1.32004769  0.17355442  0.18861696 -1.73570191\n",
      "   0.82171513]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:38 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.5232906]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 38 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.7955373  -0.65425878  0.45020251  0.71564586 -0.29171696  0.06239425\n",
      "   1.20982502  0.03254224 -0.44911546]\n",
      " [-0.69233202  0.47099806 -0.63356395 -0.71726552  0.20675903 -0.29634194\n",
      "  -0.54172959 -0.19869007  0.59404901]\n",
      " [ 0.54377152 -0.14767353  0.12428857  0.31398723 -0.42266969  0.36990387\n",
      "   0.49416289 -0.13456242 -0.50005853]\n",
      " [ 0.52432356 -0.41970856  0.46895089  0.35012007 -0.32742056  0.26122183\n",
      "   0.71142626  0.66938673 -0.21186124]\n",
      " [-0.90959605  0.64485536 -0.65698215 -0.84284825  0.08292363  0.05680982\n",
      "  -0.90297667 -0.58089158  0.60947134]\n",
      " [ 1.01842253 -0.57521328  0.65596519  0.4263528  -0.18096968 -0.14452155\n",
      "   0.65682599  0.70076329 -0.38952293]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.88376665  1.47339428 -1.30142774  0.20833121  0.2275062  -1.72092057\n",
      "   0.86541401]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:38 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.31948866]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 38 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.79452403 -0.65425878  0.45020251  0.71564586 -0.29273023  0.06138098\n",
      "   1.20982502  0.03254224 -0.45012873]\n",
      " [-0.69071514  0.47099806 -0.63356395 -0.71726552  0.20837591 -0.29472506\n",
      "  -0.54172959 -0.19869007  0.59566589]\n",
      " [ 0.54385012 -0.14767353  0.12428857  0.31398723 -0.42259109  0.36998248\n",
      "   0.49416289 -0.13456242 -0.49997993]\n",
      " [ 0.52221741 -0.41970856  0.46895089  0.35012007 -0.32952671  0.25911569\n",
      "   0.71142626  0.66938673 -0.21396739]\n",
      " [-0.90821233  0.64485536 -0.65698215 -0.84284825  0.08430734  0.05819353\n",
      "  -0.90297667 -0.58089158  0.61085506]\n",
      " [ 1.01584783 -0.57521328  0.65596519  0.4263528  -0.18354438 -0.14709625\n",
      "   0.65682599  0.70076329 -0.39209763]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.91849757  1.45501325 -1.31716679  0.19104435  0.20801324 -1.73689638\n",
      "   0.84543416]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:38 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.15876987]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 38 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.7958585  -0.65292431  0.45020251  0.71564586 -0.29139576  0.06271545\n",
      "   1.20982502  0.03254224 -0.44879426]\n",
      " [-0.69146691  0.47024629 -0.63356395 -0.71726552  0.20762414 -0.29547683\n",
      "  -0.54172959 -0.19869007  0.59491412]\n",
      " [ 0.54426219 -0.14726145  0.12428857  0.31398723 -0.42217901  0.37039455\n",
      "   0.49416289 -0.13456242 -0.49956786]\n",
      " [ 0.52269553 -0.41923045  0.46895089  0.35012007 -0.32904859  0.2595938\n",
      "   0.71142626  0.66938673 -0.21348927]\n",
      " [-0.90943623  0.64363146 -0.65698215 -0.84284825  0.08308345  0.05696964\n",
      "  -0.90297667 -0.58089158  0.60963116]\n",
      " [ 1.01658092 -0.57448019  0.65596519  0.4263528  -0.18281129 -0.14636316\n",
      "   0.65682599  0.70076329 -0.39136454]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.92910038  1.45111241 -1.32323052  0.1861567   0.20319259 -1.74347125\n",
      "   0.84087561]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:38 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.53830626]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 38 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.78193981 -0.666843    0.45020251  0.71564586 -0.30531445  0.04879676\n",
      "   1.19590633  0.03254224 -0.44879426]\n",
      " [-0.67953195  0.48218126 -0.63356395 -0.71726552  0.2195591  -0.28354186\n",
      "  -0.52979463 -0.19869007  0.59491412]\n",
      " [ 0.53243368 -0.15908997  0.12428857  0.31398723 -0.43400753  0.35856604\n",
      "   0.48233438 -0.13456242 -0.49956786]\n",
      " [ 0.51181252 -0.43011346  0.46895089  0.35012007 -0.3399316   0.24871079\n",
      "   0.70054325  0.66938673 -0.21348927]\n",
      " [-0.89608696  0.65698073 -0.65698215 -0.84284825  0.09643272  0.07031891\n",
      "  -0.8896274  -0.58089158  0.60963116]\n",
      " [ 1.0054413  -0.58561981  0.65596519  0.4263528  -0.19395091 -0.15750277\n",
      "   0.64568637  0.70076329 -0.39136454]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.99599371  1.40062503 -1.34324932  0.13944278  0.15782664 -1.76108632\n",
      "   0.7951562 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:38 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.72590416]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 38 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.78629842 -0.666843    0.45020251  0.72000447 -0.30531445  0.04879676\n",
      "   1.20026494  0.03254224 -0.44879426]\n",
      " [-0.68536152  0.48218126 -0.63356395 -0.72309509  0.2195591  -0.28354186\n",
      "  -0.5356242  -0.19869007  0.59491412]\n",
      " [ 0.53843131 -0.15908997  0.12428857  0.31998486 -0.43400753  0.35856604\n",
      "   0.48833201 -0.13456242 -0.49956786]\n",
      " [ 0.51791628 -0.43011346  0.46895089  0.35622383 -0.3399316   0.24871079\n",
      "   0.70664701  0.66938673 -0.21348927]\n",
      " [-0.9005875   0.65698073 -0.65698215 -0.84734879  0.09643272  0.07031891\n",
      "  -0.89412794 -0.58089158  0.60963116]\n",
      " [ 1.01104479 -0.58561981  0.65596519  0.43195629 -0.19395091 -0.15750277\n",
      "   0.65128986  0.70076329 -0.39136454]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.96872566  1.42616543 -1.33978278  0.16100185  0.18036936 -1.75925056\n",
      "   0.81938919]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:38 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.6368279]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 38 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.79559169 -0.666843    0.45020251  0.72000447 -0.29602118  0.05809003\n",
      "   1.20955821  0.03254224 -0.44879426]\n",
      " [-0.69452171  0.48218126 -0.63356395 -0.72309509  0.21039891 -0.29270206\n",
      "  -0.54478439 -0.19869007  0.59491412]\n",
      " [ 0.54646139 -0.15908997  0.12428857  0.31998486 -0.42597745  0.36659612\n",
      "   0.49636209 -0.13456242 -0.49956786]\n",
      " [ 0.52668508 -0.43011346  0.46895089  0.35622383 -0.3311628   0.2574796\n",
      "   0.71541581  0.66938673 -0.21348927]\n",
      " [-0.90996625  0.65698073 -0.65698215 -0.84734879  0.08705397  0.06094016\n",
      "  -0.90350669 -0.58089158  0.60963116]\n",
      " [ 1.02025247 -0.58561981  0.65596519  0.43195629 -0.18474323 -0.14829509\n",
      "   0.66049754  0.70076329 -0.39136454]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.92672878  1.46183835 -1.33068182  0.1912976   0.21213812 -1.75235847\n",
      "   0.85246847]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:38 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57341083]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 38 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.78099477 -0.666843    0.45020251  0.70540756 -0.29602118  0.04349311\n",
      "   1.20955821  0.03254224 -0.46339118]\n",
      " [-0.67997797  0.48218126 -0.63356395 -0.70855135  0.21039891 -0.27815832\n",
      "  -0.54478439 -0.19869007  0.60945786]\n",
      " [ 0.53518709 -0.15908997  0.12428857  0.30871056 -0.42597745  0.35532181\n",
      "   0.49636209 -0.13456242 -0.51084217]\n",
      " [ 0.51347932 -0.43011346  0.46895089  0.34301806 -0.3311628   0.24427383\n",
      "   0.71541581  0.66938673 -0.22669504]\n",
      " [-0.89559132  0.65698073 -0.65698215 -0.83297386  0.08705397  0.07531509\n",
      "  -0.90350669 -0.58089158  0.62400609]\n",
      " [ 1.00717069 -0.58561981  0.65596519  0.41887451 -0.18474323 -0.16137688\n",
      "   0.66049754  0.70076329 -0.40444632]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.99686003  1.40889661 -1.34799474  0.14391922  0.16189288 -1.77004781\n",
      "   0.80242831]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:38 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.66927034]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 38 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.78852739 -0.65931038  0.45020251  0.71294017 -0.29602118  0.05102573\n",
      "   1.21709083  0.03254224 -0.46339118]\n",
      " [-0.68807842  0.47408081 -0.63356395 -0.7166518   0.21039891 -0.28625877\n",
      "  -0.55288484 -0.19869007  0.60945786]\n",
      " [ 0.54338134 -0.15089571  0.12428857  0.31690481 -0.42597745  0.36351607\n",
      "   0.50455635 -0.13456242 -0.51084217]\n",
      " [ 0.52159793 -0.42199485  0.46895089  0.35113667 -0.3311628   0.25239244\n",
      "   0.72353442  0.66938673 -0.22669504]\n",
      " [-0.90346292  0.64910914 -0.65698215 -0.84084546  0.08705397  0.0674435\n",
      "  -0.91137829 -0.58089158  0.62400609]\n",
      " [ 1.01523605 -0.57755445  0.65596519  0.42693987 -0.18474323 -0.15331152\n",
      "   0.6685629   0.70076329 -0.40444632]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.96025693  1.44140794 -1.34247945  0.17404189  0.19117407 -1.76528451\n",
      "   0.83143317]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:38 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.69523873]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 38 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.79519762 -0.65264016  0.45020251  0.7196104  -0.29602118  0.05102573\n",
      "   1.22376105  0.03254224 -0.46339118]\n",
      " [-0.69529711  0.46686212 -0.63356395 -0.72387049  0.21039891 -0.28625877\n",
      "  -0.56010353 -0.19869007  0.60945786]\n",
      " [ 0.55030153 -0.14397553  0.12428857  0.323825   -0.42597745  0.36351607\n",
      "   0.51147653 -0.13456242 -0.51084217]\n",
      " [ 0.52843554 -0.41515724  0.46895089  0.35797428 -0.3311628   0.25239244\n",
      "   0.73037203  0.66938673 -0.22669504]\n",
      " [-0.91023099  0.64234107 -0.65698215 -0.84761353  0.08705397  0.0674435\n",
      "  -0.91814635 -0.58089158  0.62400609]\n",
      " [ 1.02246385 -0.57032664  0.65596519  0.43416767 -0.18474323 -0.15331152\n",
      "   0.6757907   0.70076329 -0.40444632]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.92797024  1.47004232 -1.33650983  0.19893482  0.21583841 -1.76145809\n",
      "   0.85798806]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:38 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.78217807]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 38 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.79842019 -0.65264016  0.45342508  0.7196104  -0.29602118  0.05424831\n",
      "   1.22698363  0.03254224 -0.46339118]\n",
      " [-0.69899411  0.46686212 -0.63726095 -0.72387049  0.21039891 -0.28995577\n",
      "  -0.56380053 -0.19869007  0.60945786]\n",
      " [ 0.55445558 -0.14397553  0.12844262  0.323825   -0.42597745  0.36767012\n",
      "   0.51563059 -0.13456242 -0.51084217]\n",
      " [ 0.53235194 -0.41515724  0.47286729  0.35797428 -0.3311628   0.25630884\n",
      "   0.73428843  0.66938673 -0.22669504]\n",
      " [-0.91360146  0.64234107 -0.66035263 -0.84761353  0.08705397  0.06407302\n",
      "  -0.92151683 -0.58089158  0.62400609]\n",
      " [ 1.02612859 -0.57032664  0.65962993  0.43416767 -0.18474323 -0.14964678\n",
      "   0.67945544  0.70076329 -0.40444632]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.90941448  1.48721651 -1.33461718  0.21424061  0.23214331 -1.75993992\n",
      "   0.87469439]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:38 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.39012605]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 39 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.79393939 -0.65712096  0.45342508  0.71512959 -0.29602118  0.05424831\n",
      "   1.22698363  0.03254224 -0.46787199]\n",
      " [-0.6950916   0.47076464 -0.63726095 -0.71996797  0.21039891 -0.28995577\n",
      "  -0.56380053 -0.19869007  0.61336038]\n",
      " [ 0.5518949  -0.14653621  0.12844262  0.32126432 -0.42597745  0.36767012\n",
      "   0.51563059 -0.13456242 -0.51340284]\n",
      " [ 0.52951301 -0.41799617  0.47286729  0.35513535 -0.3311628   0.25630884\n",
      "   0.73428843  0.66938673 -0.22953397]\n",
      " [-0.90819732  0.64774521 -0.66035263 -0.84220938  0.08705397  0.06407302\n",
      "  -0.92151683 -0.58089158  0.62941024]\n",
      " [ 1.02081457 -0.57564066  0.65962993  0.42885365 -0.18474323 -0.14964678\n",
      "   0.67945544  0.70076329 -0.40976034]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.95582538  1.45940859 -1.35384154  0.18845311  0.20606963 -1.77751793\n",
      "   0.84596366]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:39 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.76319641]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 39 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.79773267 -0.65712096  0.45721836  0.71512959 -0.29602118  0.05424831\n",
      "   1.23077691  0.03254224 -0.46787199]\n",
      " [-0.69969694  0.47076464 -0.6418663  -0.71996797  0.21039891 -0.28995577\n",
      "  -0.56840588 -0.19869007  0.61336038]\n",
      " [ 0.55645741 -0.14653621  0.13300513  0.32126432 -0.42597745  0.36767012\n",
      "   0.5201931  -0.13456242 -0.51340284]\n",
      " [ 0.53424432 -0.41799617  0.4775986   0.35513535 -0.3311628   0.25630884\n",
      "   0.73901974  0.66938673 -0.22953397]\n",
      " [-0.91196428  0.64774521 -0.66411959 -0.84220938  0.08705397  0.06407302\n",
      "  -0.92528379 -0.58089158  0.62941024]\n",
      " [ 1.02479572 -0.57564066  0.66361109  0.42885365 -0.18474323 -0.14964678\n",
      "   0.68343659  0.70076329 -0.40976034]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.9344269   1.47914492 -1.35104808  0.20488301  0.22426397 -1.77587971\n",
      "   0.86551584]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:39 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.71353106]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 39 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.78183085 -0.67302278  0.44131654  0.71512959 -0.29602118  0.05424831\n",
      "   1.21487509  0.03254224 -0.46787199]\n",
      " [-0.68343636  0.48702523 -0.62560571 -0.71996797  0.21039891 -0.28995577\n",
      "  -0.55214529 -0.19869007  0.61336038]\n",
      " [ 0.54166426 -0.16132936  0.11821198  0.32126432 -0.42597745  0.36767012\n",
      "   0.50539995 -0.13456242 -0.51340284]\n",
      " [ 0.51819334 -0.43404715  0.46154762  0.35513535 -0.3311628   0.25630884\n",
      "   0.72296876  0.66938673 -0.22953397]\n",
      " [-0.89613509  0.6635744  -0.6482904  -0.84220938  0.08705397  0.06407302\n",
      "  -0.9094546  -0.58089158  0.62941024]\n",
      " [ 1.00880734 -0.59162904  0.64762271  0.42885365 -0.18474323 -0.14964678\n",
      "   0.66744822  0.70076329 -0.40976034]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.00735137  1.41631351 -1.36502541  0.15067925  0.16655779 -1.7857572\n",
      "   0.80296953]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:39 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.65133954]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 39 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.79040915 -0.67302278  0.44989485  0.71512959 -0.29602118  0.05424831\n",
      "   1.21487509  0.04112054 -0.46787199]\n",
      " [-0.69229533  0.48702523 -0.63446468 -0.71996797  0.21039891 -0.28995577\n",
      "  -0.55214529 -0.20754904  0.61336038]\n",
      " [ 0.54652071 -0.16132936  0.12306843  0.32126432 -0.42597745  0.36767012\n",
      "   0.50539995 -0.12970598 -0.51340284]\n",
      " [ 0.52702249 -0.43404715  0.47037678  0.35513535 -0.3311628   0.25630884\n",
      "   0.72296876  0.67821588 -0.22953397]\n",
      " [-0.90415235  0.6635744  -0.65630767 -0.84220938  0.08705397  0.06407302\n",
      "  -0.9094546  -0.58890884  0.62941024]\n",
      " [ 1.01618101 -0.59162904  0.65499637  0.42885365 -0.18474323 -0.14964678\n",
      "   0.66744822  0.70813695 -0.40976034]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.96776161  1.44712555 -1.35784877  0.17555704  0.19976461 -1.78153454\n",
      "   0.83913494]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:39 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: [1.] Net Result: [[0.52347989]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 39 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.80232213 -0.67302278  0.44989485  0.72704257 -0.29602118  0.05424831\n",
      "   1.21487509  0.04112054 -0.45595901]\n",
      " [-0.70245636  0.48702523 -0.63446468 -0.730129    0.21039891 -0.28995577\n",
      "  -0.55214529 -0.20754904  0.60319934]\n",
      " [ 0.55162435 -0.16132936  0.12306843  0.32636795 -0.42597745  0.36767012\n",
      "   0.50539995 -0.12970598 -0.50829921]\n",
      " [ 0.53575603 -0.43404715  0.47037678  0.36386888 -0.3311628   0.25630884\n",
      "   0.72296876  0.67821588 -0.22080043]\n",
      " [-0.91648506  0.6635744  -0.65630767 -0.85454209  0.08705397  0.06407302\n",
      "  -0.9094546  -0.58890884  0.61707753]\n",
      " [ 1.02808007 -0.59162904  0.65499637  0.44075271 -0.18474323 -0.14964678\n",
      "   0.66744822  0.70813695 -0.39786128]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.90832795  1.49101134 -1.33940887  0.210485    0.23884828 -1.76687956\n",
      "   0.88299325]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:39 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.30843526]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 39 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.80146435 -0.67302278  0.44989485  0.72704257 -0.29687896  0.05339053\n",
      "   1.21487509  0.04112054 -0.45681679]\n",
      " [-0.70099753  0.48702523 -0.63446468 -0.730129    0.21185774 -0.28849694\n",
      "  -0.55214529 -0.20754904  0.60465818]\n",
      " [ 0.55174755 -0.16132936  0.12306843  0.32636795 -0.42585424  0.36779332\n",
      "   0.50539995 -0.12970598 -0.508176  ]\n",
      " [ 0.53380967 -0.43404715  0.47037678  0.36386888 -0.33310915  0.25436249\n",
      "   0.72296876  0.67821588 -0.22274679]\n",
      " [-0.91527232  0.6635744  -0.65630767 -0.85454209  0.08826672  0.06528577\n",
      "  -0.9094546  -0.58890884  0.61829028]\n",
      " [ 1.0256997  -0.59162904  0.65499637  0.44075271 -0.1871236  -0.15202715\n",
      "   0.66744822  0.70813695 -0.40024165]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.94122303  1.47370446 -1.35438979  0.19416067  0.22043563 -1.7821099\n",
      "   0.86413047]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:39 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.14734828]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 39 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.80268511 -0.67180202  0.44989485  0.72704257 -0.29565819  0.05461129\n",
      "   1.21487509  0.04112054 -0.45559602]\n",
      " [-0.70170662  0.48631614 -0.63446468 -0.730129    0.21114865 -0.28920603\n",
      "  -0.55214529 -0.20754904  0.60394908]\n",
      " [ 0.55215127 -0.16092563  0.12306843  0.32636795 -0.42545052  0.36819705\n",
      "   0.50539995 -0.12970598 -0.50777228]\n",
      " [ 0.53427177 -0.43358505  0.47037678  0.36386888 -0.33264706  0.25482458\n",
      "   0.72296876  0.67821588 -0.22228469]\n",
      " [-0.91639808  0.66244864 -0.65630767 -0.85454209  0.08714095  0.06416\n",
      "  -0.9094546  -0.58890884  0.61716451]\n",
      " [ 1.02639002 -0.59093873  0.65499637  0.44075271 -0.18643328 -0.15133683\n",
      "   0.66744822  0.70813695 -0.39955133]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.95047921  1.47036476 -1.35973868  0.18993839  0.21627278 -1.7879152\n",
      "   0.86020347]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:39 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.53378703]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 39 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.78896957 -0.68551757  0.44989485  0.72704257 -0.30937374  0.04089575\n",
      "   1.20115955  0.04112054 -0.45559602]\n",
      " [-0.68990431  0.49811845 -0.63446468 -0.730129    0.22295096 -0.27740372\n",
      "  -0.54034298 -0.20754904  0.60394908]\n",
      " [ 0.54040675 -0.17267015  0.12306843  0.32636795 -0.43719504  0.35645252\n",
      "   0.49365543 -0.12970598 -0.50777228]\n",
      " [ 0.52346176 -0.44439506  0.47037678  0.36386888 -0.34345707  0.24401457\n",
      "   0.71215875  0.67821588 -0.22228469]\n",
      " [-0.90325541  0.67559131 -0.65630767 -0.85454209  0.10028362  0.07730267\n",
      "  -0.89631194 -0.58890884  0.61716451]\n",
      " [ 1.01537725 -0.60195149  0.65499637  0.44075271 -0.19744604 -0.1623496\n",
      "   0.65643545  0.70813695 -0.39955133]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.01689791  1.4204749  -1.37968808  0.14355602  0.17122296 -1.80562034\n",
      "   0.81487494]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:39 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.73306489]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 39 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.79309457 -0.68551757  0.44989485  0.73116757 -0.30937374  0.04089575\n",
      "   1.20528455  0.04112054 -0.45559602]\n",
      " [-0.69544367  0.49811845 -0.63446468 -0.73566836  0.22295096 -0.27740372\n",
      "  -0.54588234 -0.20754904  0.60394908]\n",
      " [ 0.5461799  -0.17267015  0.12306843  0.33214111 -0.43719504  0.35645252\n",
      "   0.49942858 -0.12970598 -0.50777228]\n",
      " [ 0.52930222 -0.44439506  0.47037678  0.36970935 -0.34345707  0.24401457\n",
      "   0.71799921  0.67821588 -0.22228469]\n",
      " [-0.90751254  0.67559131 -0.65630767 -0.85879922  0.10028362  0.07730267\n",
      "  -0.90056907 -0.58890884  0.61716451]\n",
      " [ 1.02068736 -0.60195149  0.65499637  0.44606282 -0.19744604 -0.1623496\n",
      "   0.66174556  0.70813695 -0.39955133]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.99078088  1.44497352 -1.37646455  0.16434073  0.19295095 -1.8039035\n",
      "   0.83817443]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:39 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.63938791]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 39 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.80229427 -0.68551757  0.44989485  0.73116757 -0.30017403  0.05009546\n",
      "   1.21448425  0.04112054 -0.45559602]\n",
      " [-0.70453175  0.49811845 -0.63446468 -0.73566836  0.21386287 -0.28649181\n",
      "  -0.55497043 -0.20754904  0.60394908]\n",
      " [ 0.55419368 -0.17267015  0.12306843  0.33214111 -0.42918127  0.3644663\n",
      "   0.50744235 -0.12970598 -0.50777228]\n",
      " [ 0.53802822 -0.44439506  0.47037678  0.36970935 -0.33473107  0.25274057\n",
      "   0.72672521  0.67821588 -0.22228469]\n",
      " [-0.91679533  0.67559131 -0.65630767 -0.85879922  0.09100083  0.06801988\n",
      "  -0.90985186 -0.58890884  0.61716451]\n",
      " [ 1.02982172 -0.60195149  0.65499637  0.44606282 -0.18831168 -0.15321524\n",
      "   0.67087992  0.70813695 -0.39955133]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.94920753  1.48028591 -1.36753142  0.19444378  0.22451009 -1.79709535\n",
      "   0.87100159]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:39 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57390513]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 39 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.78767219 -0.68551757  0.44989485  0.71654549 -0.30017403  0.03547338\n",
      "   1.21448425  0.04112054 -0.4702181 ]\n",
      " [-0.68993854  0.49811845 -0.63446468 -0.72107515  0.21386287 -0.27189859\n",
      "  -0.55497043 -0.20754904  0.6185423 ]\n",
      " [ 0.54280457 -0.17267015  0.12306843  0.320752   -0.42918127  0.35307719\n",
      "   0.50744235 -0.12970598 -0.51916139]\n",
      " [ 0.52471957 -0.44439506  0.47037678  0.3564007  -0.33473107  0.23943193\n",
      "   0.72672521  0.67821588 -0.23559333]\n",
      " [-0.90239     0.67559131 -0.65630767 -0.84439389  0.09100083  0.08242521\n",
      "  -0.90985186 -0.58890884  0.63156985]\n",
      " [ 1.01664089 -0.60195149  0.65499637  0.43288199 -0.18831168 -0.16639607\n",
      "   0.67087992  0.70813695 -0.41273216]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.01937835  1.4272745  -1.38475856  0.14689198  0.1740759  -1.81474619\n",
      "   0.82078195]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:39 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.67207576]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 39 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.79511722 -0.67807253  0.44989485  0.72399052 -0.30017403  0.04291841\n",
      "   1.22192928  0.04112054 -0.4702181 ]\n",
      " [-0.69792519  0.49013179 -0.63446468 -0.7290618   0.21386287 -0.27988525\n",
      "  -0.56295708 -0.20754904  0.6185423 ]\n",
      " [ 0.55089412 -0.1645806   0.12306843  0.32884155 -0.42918127  0.36116674\n",
      "   0.5155319  -0.12970598 -0.51916139]\n",
      " [ 0.53274993 -0.4363647   0.47037678  0.36443106 -0.33473107  0.24746229\n",
      "   0.73475557  0.67821588 -0.23559333]\n",
      " [-0.91016298  0.66781833 -0.65630767 -0.85216686  0.09100083  0.07465223\n",
      "  -0.91762484 -0.58890884  0.63156985]\n",
      " [ 1.02461881 -0.59397357  0.65499637  0.44085991 -0.18831168 -0.15841815\n",
      "   0.67885784  0.70813695 -0.41273216]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.98324275  1.45935638 -1.37936197  0.17670807  0.20307981 -1.81003899\n",
      "   0.84949043]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:39 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.69844646]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 39 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.80167087 -0.67151889  0.44989485  0.73054416 -0.30017403  0.04291841\n",
      "   1.22848293  0.04112054 -0.4702181 ]\n",
      " [-0.70502969  0.48302729 -0.63446468 -0.7361663   0.21386287 -0.27988525\n",
      "  -0.57006158 -0.20754904  0.6185423 ]\n",
      " [ 0.55773185 -0.15774287  0.12306843  0.33567928 -0.42918127  0.36116674\n",
      "   0.52236964 -0.12970598 -0.51916139]\n",
      " [ 0.53952013 -0.4295945   0.47037678  0.37120126 -0.33473107  0.24746229\n",
      "   0.74152577  0.67821588 -0.23559333]\n",
      " [-0.91681006  0.66117125 -0.65630767 -0.85881395  0.09100083  0.07465223\n",
      "  -0.92427192 -0.58890884  0.63156985]\n",
      " [ 1.03172806 -0.58686432  0.65499637  0.44796916 -0.18831168 -0.15841815\n",
      "   0.68596709  0.70813695 -0.41273216]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.95148629  1.48753233 -1.37356789  0.20128562  0.22746038 -1.80629381\n",
      "   0.87568843]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:39 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.78522079]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 39 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.80481226 -0.67151889  0.45303624  0.73054416 -0.30017403  0.0460598\n",
      "   1.23162432  0.04112054 -0.4702181 ]\n",
      " [-0.70862076  0.48302729 -0.63805575 -0.7361663   0.21386287 -0.28347632\n",
      "  -0.57365265 -0.20754904  0.6185423 ]\n",
      " [ 0.5617858  -0.15774287  0.12712238  0.33567928 -0.42918127  0.36522069\n",
      "   0.52642359 -0.12970598 -0.51916139]\n",
      " [ 0.54332435 -0.4295945   0.474181    0.37120126 -0.33473107  0.25126651\n",
      "   0.74532999  0.67821588 -0.23559333]\n",
      " [-0.92009307  0.66117125 -0.65959068 -0.85881395  0.09100083  0.07136923\n",
      "  -0.92755493 -0.58890884  0.63156985]\n",
      " [ 1.03528835 -0.58686432  0.65855666  0.44796916 -0.18831168 -0.15485786\n",
      "   0.68952738  0.70813695 -0.41273216]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.93337513  1.50429849 -1.37174405  0.21626309  0.24341052 -1.80481856\n",
      "   0.89201608]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:39 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.38153456]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 40 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.80054985 -0.6757813   0.45303624  0.72628175 -0.30017403  0.0460598\n",
      "   1.23162432  0.04112054 -0.47448051]\n",
      " [-0.70486987  0.48677819 -0.63805575 -0.7324154   0.21386287 -0.28347632\n",
      "  -0.57365265 -0.20754904  0.6222932 ]\n",
      " [ 0.55933363 -0.16019504  0.12712238  0.33322711 -0.42918127  0.36522069\n",
      "   0.52642359 -0.12970598 -0.52161356]\n",
      " [ 0.54056156 -0.4323573   0.474181    0.36843846 -0.33473107  0.25126651\n",
      "   0.74532999  0.67821588 -0.23835613]\n",
      " [-0.9149329   0.66633142 -0.65959068 -0.85365377  0.09100083  0.07136923\n",
      "  -0.92755493 -0.58890884  0.63673002]\n",
      " [ 1.0301517  -0.59200097  0.65855666  0.44283251 -0.18831168 -0.15485786\n",
      "   0.68952738  0.70813695 -0.41786881]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.97838971  1.47741786 -1.39042636  0.1912837   0.21811172 -1.82195998\n",
      "   0.86416952]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:40 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.76737374]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 40 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.80421288 -0.6757813   0.45669927  0.72628175 -0.30017403  0.0460598\n",
      "   1.23528735  0.04112054 -0.47448051]\n",
      " [-0.70931876  0.48677819 -0.64250465 -0.7324154   0.21386287 -0.28347632\n",
      "  -0.57810154 -0.20754904  0.6222932 ]\n",
      " [ 0.56378258 -0.16019504  0.13157133  0.33322711 -0.42918127  0.36522069\n",
      "   0.53087254 -0.12970598 -0.52161356]\n",
      " [ 0.54513829 -0.4323573   0.47875773  0.36843846 -0.33473107  0.25126651\n",
      "   0.74990672  0.67821588 -0.23835613]\n",
      " [-0.91856845  0.66633142 -0.66322623 -0.85365377  0.09100083  0.07136923\n",
      "  -0.93119048 -0.58890884  0.63673002]\n",
      " [ 1.03398596 -0.59200097  0.66239092  0.44283251 -0.18831168 -0.15485786\n",
      "   0.69336164  0.70813695 -0.41786881]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.95762651  1.49658435 -1.38776361  0.20728815  0.23582727 -1.82038795\n",
      "   0.88317102]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:40 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.71421453]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 40 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.78829591 -0.69169827  0.4407823   0.72628175 -0.30017403  0.0460598\n",
      "   1.21937038  0.04112054 -0.47448051]\n",
      " [-0.69306097  0.50303598 -0.62624685 -0.7324154   0.21386287 -0.28347632\n",
      "  -0.56184375 -0.20754904  0.6222932 ]\n",
      " [ 0.54897696 -0.17500066  0.11676571  0.33322711 -0.42918127  0.36522069\n",
      "   0.51606691 -0.12970598 -0.52161356]\n",
      " [ 0.52907236 -0.44842324  0.4626918   0.36843846 -0.33473107  0.25126651\n",
      "   0.73384079  0.67821588 -0.23835613]\n",
      " [-0.90272604  0.68217383 -0.64738382 -0.85365377  0.09100083  0.07136923\n",
      "  -0.91534807 -0.58890884  0.63673002]\n",
      " [ 1.01800906 -0.60797787  0.64641402  0.44283251 -0.18831168 -0.15485786\n",
      "   0.67738474  0.70813695 -0.41786881]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.03051643  1.43385409 -1.40168986  0.1530696   0.17804542 -1.83032039\n",
      "   0.8206407 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:40 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.65544883]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 40 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.79675457 -0.69169827  0.44924096  0.72628175 -0.30017403  0.0460598\n",
      "   1.21937038  0.0495792  -0.47448051]\n",
      " [-0.70177006  0.50303598 -0.63495594 -0.7324154   0.21386287 -0.28347632\n",
      "  -0.56184375 -0.21625813  0.6222932 ]\n",
      " [ 0.55383343 -0.17500066  0.12162217  0.33322711 -0.42918127  0.36522069\n",
      "   0.51606691 -0.12484951 -0.52161356]\n",
      " [ 0.53773495 -0.44842324  0.47135439  0.36843846 -0.33473107  0.25126651\n",
      "   0.73384079  0.68687848 -0.23835613]\n",
      " [-0.91057048  0.68217383 -0.65522826 -0.85365377  0.09100083  0.07136923\n",
      "  -0.91534807 -0.59675328  0.63673002]\n",
      " [ 1.02521037 -0.60797787  0.65361532  0.44283251 -0.18831168 -0.15485786\n",
      "   0.67738474  0.71533826 -0.41786881]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.99161046  1.46423097 -1.39474693  0.17761499  0.21078764 -1.82622118\n",
      "   0.85622841]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:40 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.5238658]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 40 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.80871937 -0.69169827  0.44924096  0.73824655 -0.30017403  0.0460598\n",
      "   1.21937038  0.0495792  -0.46251571]\n",
      " [-0.71203608  0.50303598 -0.63495594 -0.74268143  0.21386287 -0.28347632\n",
      "  -0.56184375 -0.21625813  0.61202717]\n",
      " [ 0.55908142 -0.17500066  0.12162217  0.3384751  -0.42918127  0.36522069\n",
      "   0.51606691 -0.12484951 -0.51636557]\n",
      " [ 0.5466208  -0.44842324  0.47135439  0.37732431 -0.33473107  0.25126651\n",
      "   0.73384079  0.68687848 -0.22947028]\n",
      " [-0.92294206  0.68217383 -0.65522826 -0.86602535  0.09100083  0.07136923\n",
      "  -0.91534807 -0.59675328  0.62435844]\n",
      " [ 1.0371843  -0.60797787  0.65361532  0.45480645 -0.18831168 -0.15485786\n",
      "   0.67738474  0.71533826 -0.40589487]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.93222929  1.50820258 -1.37648796  0.21267116  0.25003945 -1.81169513\n",
      "   0.90021848]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:40 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.29767022]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 40 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.80800453 -0.69169827  0.44924096  0.73824655 -0.30088887  0.04534495\n",
      "   1.21937038  0.0495792  -0.46323056]\n",
      " [-0.71072604  0.50303598 -0.63495594 -0.74268143  0.21517291 -0.28216628\n",
      "  -0.56184375 -0.21625813  0.61333721]\n",
      " [ 0.55924666 -0.17500066  0.12162217  0.3384751  -0.42901602  0.36538593\n",
      "   0.51606691 -0.12484951 -0.51620032]\n",
      " [ 0.54482756 -0.44842324  0.47135439  0.37732431 -0.33652431  0.24947327\n",
      "   0.73384079  0.68687848 -0.23126352]\n",
      " [-0.92188735  0.68217383 -0.65522826 -0.86602535  0.09205553  0.07242393\n",
      "  -0.91534807 -0.59675328  0.62541314]\n",
      " [ 1.03498891 -0.60797787  0.65361532  0.45480645 -0.19050708 -0.15705326\n",
      "   0.67738474  0.71533826 -0.40809027]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.96334515  1.49192879 -1.39072956  0.19727848  0.23267191 -1.8261951\n",
      "   0.88243465]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:40 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.13660336]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 40 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.80911409 -0.69058871  0.44924096  0.73824655 -0.29977932  0.04645451\n",
      "   1.21937038  0.0495792  -0.462121  ]\n",
      " [-0.71138888  0.50237314 -0.63495594 -0.74268143  0.21451007 -0.28282911\n",
      "  -0.56184375 -0.21625813  0.61267438]\n",
      " [ 0.55963681 -0.17461051  0.12162217  0.3384751  -0.42862587  0.36577608\n",
      "   0.51606691 -0.12484951 -0.51581018]\n",
      " [ 0.54526901 -0.44798178  0.47135439  0.37732431 -0.33608286  0.24991472\n",
      "   0.73384079  0.68687848 -0.23082207]\n",
      " [-0.92291558  0.6811456  -0.65522826 -0.86602535  0.0910273   0.0713957\n",
      "  -0.91534807 -0.59675328  0.62438492]\n",
      " [ 1.03563321 -0.60733357  0.65361532  0.45480645 -0.18986278 -0.15640895\n",
      "   0.67738474  0.71533826 -0.40744597]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.97140085  1.48907886 -1.39543299  0.19364328  0.22908915 -1.83130383\n",
      "   0.87906276]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:40 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.52881751]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 40 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.7956225  -0.70408029  0.44924096  0.73824655 -0.3132709   0.03296293\n",
      "   1.2058788   0.0495792  -0.462121  ]\n",
      " [-0.69974094  0.51402108 -0.63495594 -0.74268143  0.22615801 -0.27118118\n",
      "  -0.55019582 -0.21625813  0.61267438]\n",
      " [ 0.54799754 -0.18624978  0.12162217  0.3384751  -0.44026514  0.35413681\n",
      "   0.50442765 -0.12484951 -0.51581018]\n",
      " [ 0.53455547 -0.45869532  0.47135439  0.37732431 -0.34679639  0.23920118\n",
      "   0.72312725  0.68687848 -0.23082207]\n",
      " [-0.90999944  0.69406175 -0.65522826 -0.86602535  0.10394345  0.08431184\n",
      "  -0.90243193 -0.59675328  0.62438492]\n",
      " [ 1.02476839 -0.61819839  0.65361532  0.45480645 -0.2007276  -0.16727377\n",
      "   0.66651991  0.71533826 -0.40744597]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.03728346  1.43984096 -1.41531029  0.14765095  0.1844155  -1.8490912\n",
      "   0.8341818 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:40 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.74000286]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 40 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.7995276  -0.70408029  0.44924096  0.74215165 -0.3132709   0.03296293\n",
      "   1.20978389  0.0495792  -0.462121  ]\n",
      " [-0.70500313  0.51402108 -0.63495594 -0.74794362  0.22615801 -0.27118118\n",
      "  -0.55545801 -0.21625813  0.61267438]\n",
      " [ 0.55354833 -0.18624978  0.12162217  0.34402589 -0.44026514  0.35413681\n",
      "   0.50997844 -0.12484951 -0.51581018]\n",
      " [ 0.5401387  -0.45869532  0.47135439  0.38290754 -0.34679639  0.23920118\n",
      "   0.72871048  0.68687848 -0.23082207]\n",
      " [-0.91402753  0.69406175 -0.65522826 -0.87005345  0.10394345  0.08431184\n",
      "  -0.90646002 -0.59675328  0.62438492]\n",
      " [ 1.02979983 -0.61819839  0.65361532  0.45983789 -0.2007276  -0.16727377\n",
      "   0.67155136  0.71533826 -0.40744597]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.01227191  1.4633351  -1.4123094   0.16767859  0.20534652 -1.84748404\n",
      "   0.85657472]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:40 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.64183724]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 40 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.80863851 -0.70408029  0.44924096  0.74215165 -0.30415999  0.04207384\n",
      "   1.2188948   0.0495792  -0.462121  ]\n",
      " [-0.71401975  0.51402108 -0.63495594 -0.74794362  0.21714139 -0.28019779\n",
      "  -0.56447462 -0.21625813  0.61267438]\n",
      " [ 0.56154169 -0.18624978  0.12162217  0.34402589 -0.43227178  0.36213018\n",
      "   0.5179718  -0.12484951 -0.51581018]\n",
      " [ 0.54881962 -0.45869532  0.47135439  0.38290754 -0.33811548  0.2478821\n",
      "   0.73739139  0.68687848 -0.23082207]\n",
      " [-0.92321878  0.69406175 -0.65522826 -0.87005345  0.0947522   0.0751206\n",
      "  -0.91565127 -0.59675328  0.62438492]\n",
      " [ 1.03886173 -0.61819839  0.65361532  0.45983789 -0.1916657  -0.15821188\n",
      "   0.68061326  0.71533826 -0.40744597]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.97110429  1.49829814 -1.40353068  0.19759064  0.23669815 -1.84075211\n",
      "   0.88915454]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:40 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57442753]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 40 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.79399253 -0.70408029  0.44924096  0.72750567 -0.30415999  0.02742785\n",
      "   1.2188948   0.0495792  -0.47676698]\n",
      " [-0.69938063  0.51402108 -0.63495594 -0.7333045   0.21714139 -0.26555867\n",
      "  -0.56447462 -0.21625813  0.6273135 ]\n",
      " [ 0.5500464  -0.18624978  0.12162217  0.3325306  -0.43227178  0.35063488\n",
      "   0.5179718  -0.12484951 -0.52730547]\n",
      " [ 0.53541529 -0.45869532  0.47135439  0.36950321 -0.33811548  0.23447777\n",
      "   0.73739139  0.68687848 -0.2442264 ]\n",
      " [-0.90878466  0.69406175 -0.65522826 -0.85561933  0.0947522   0.08955472\n",
      "  -0.91565127 -0.59675328  0.63881903]\n",
      " [ 1.0255885  -0.61819839  0.65361532  0.44656467 -0.1916657  -0.1714851\n",
      "   0.68061326  0.71533826 -0.42071919]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.04131672  1.44521922 -1.42068047  0.14987499  0.18608403 -1.85836909\n",
      "   0.83876364]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:40 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.67471785]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 40 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.80135741 -0.69671541  0.44924096  0.73487055 -0.30415999  0.03479273\n",
      "   1.22625968  0.0495792  -0.47676698]\n",
      " [-0.7072609   0.50614081 -0.63495594 -0.74118477  0.21714139 -0.27343895\n",
      "  -0.57235489 -0.21625813  0.6273135 ]\n",
      " [ 0.55803629 -0.17825989  0.12162217  0.34052049 -0.43227178  0.35862477\n",
      "   0.52596169 -0.12484951 -0.52730547]\n",
      " [ 0.54336006 -0.45075055  0.47135439  0.37744798 -0.33811548  0.24242254\n",
      "   0.74533617  0.68687848 -0.2442264 ]\n",
      " [-0.91646651  0.6863799  -0.65522826 -0.86330118  0.0947522   0.08187287\n",
      "  -0.92333312 -0.59675328  0.63881903]\n",
      " [ 1.03348193 -0.61030497  0.65361532  0.4544581  -0.1916657  -0.16359167\n",
      "   0.68850668  0.71533826 -0.42071919]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.00562129  1.47689281 -1.41538999  0.17939726  0.21482026 -1.85371041\n",
      "   0.8671865 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:40 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.70154915]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 40 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.80780014 -0.69027268  0.44924096  0.74131329 -0.30415999  0.03479273\n",
      "   1.23270242  0.0495792  -0.47676698]\n",
      " [-0.71425359  0.49914812 -0.63495594 -0.74817746  0.21714139 -0.27343895\n",
      "  -0.57934758 -0.21625813  0.6273135 ]\n",
      " [ 0.56479076 -0.17150542  0.12162217  0.34727496 -0.43227178  0.35862477\n",
      "   0.53271616 -0.12484951 -0.52730547]\n",
      " [ 0.55005959 -0.44405102  0.47135439  0.38414751 -0.33811548  0.24242254\n",
      "   0.7520357   0.68687848 -0.2442264 ]\n",
      " [-0.9229984   0.67984801 -0.65522826 -0.86983307  0.0947522   0.08187287\n",
      "  -0.92986501 -0.59675328  0.63881903]\n",
      " [ 1.04047542 -0.60331148  0.65361532  0.46145158 -0.1916657  -0.16359167\n",
      "   0.69550017  0.71533826 -0.42071919]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.97437677  1.5046233  -1.4097582   0.20366336  0.23891765 -1.85004047\n",
      "   0.89303298]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:40 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.78813955]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 40 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.8108659  -0.69027268  0.45230672  0.74131329 -0.30415999  0.03785849\n",
      "   1.23576818  0.0495792  -0.47676698]\n",
      " [-0.71774558  0.49914812 -0.63844794 -0.74817746  0.21714139 -0.27693094\n",
      "  -0.58283957 -0.21625813  0.6273135 ]\n",
      " [ 0.5687489  -0.17150542  0.12558032  0.34727496 -0.43227178  0.36258292\n",
      "   0.53667431 -0.12484951 -0.52730547]\n",
      " [ 0.55375819 -0.44405102  0.47505299  0.38414751 -0.33811548  0.24612114\n",
      "   0.7557343   0.68687848 -0.2442264 ]\n",
      " [-0.92619992  0.67984801 -0.65842977 -0.86983307  0.0947522   0.07867135\n",
      "  -0.93306653 -0.59675328  0.63881903]\n",
      " [ 1.04393809 -0.60331148  0.65707799  0.46145158 -0.1916657  -0.160129\n",
      "   0.69896284  0.71533826 -0.42071919]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.95668901  1.52099941 -1.407997    0.2183245   0.25452627 -1.84860431\n",
      "   0.90899725]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:40 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.37306507]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 41 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.80681722 -0.69432135  0.45230672  0.73726461 -0.30415999  0.03785849\n",
      "   1.23576818  0.0495792  -0.48081566]\n",
      " [-0.71414775  0.50274595 -0.63844794 -0.74457963  0.21714139 -0.27693094\n",
      "  -0.58283957 -0.21625813  0.63091132]\n",
      " [ 0.56640751 -0.17384682  0.12558032  0.34493356 -0.43227178  0.36258292\n",
      "   0.53667431 -0.12484951 -0.52964687]\n",
      " [ 0.5510775  -0.44673171  0.47505299  0.38146682 -0.33811548  0.24612114\n",
      "   0.7557343   0.68687848 -0.2469071 ]\n",
      " [-0.92127904  0.68476889 -0.65842977 -0.86491219  0.0947522   0.07867135\n",
      "  -0.93306653 -0.59675328  0.64373992]\n",
      " [ 1.0389807  -0.60826887  0.65707799  0.4564942  -0.1916657  -0.160129\n",
      "   0.69896284  0.71533826 -0.42567658]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.00031664  1.49503608 -1.42614349  0.19415083  0.23000384 -1.86530821\n",
      "   0.88203238]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:41 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.77143309]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 41 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.81035694 -0.69432135  0.45584644  0.73726461 -0.30415999  0.03785849\n",
      "   1.2393079   0.0495792  -0.48081566]\n",
      " [-0.71844706  0.50274595 -0.64274724 -0.74457963  0.21714139 -0.27693094\n",
      "  -0.58713888 -0.21625813  0.63091132]\n",
      " [ 0.57074403 -0.17384682  0.12991684  0.34493356 -0.43227178  0.36258292\n",
      "   0.54101083 -0.12484951 -0.52964687]\n",
      " [ 0.55550531 -0.44673171  0.4794808   0.38146682 -0.33811548  0.24612114\n",
      "   0.76016211  0.68687848 -0.2469071 ]\n",
      " [-0.92479042  0.68476889 -0.66194116 -0.86491219  0.0947522   0.07867135\n",
      "  -0.93657791 -0.59675328  0.64373992]\n",
      " [ 1.04267621 -0.60826887  0.6607735   0.4564942  -0.1916657  -0.160129\n",
      "   0.70265835  0.71533826 -0.42567658]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.98016572  1.51365116 -1.42360145  0.20973927  0.24725156 -1.86379755\n",
      "   0.90049963]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:41 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.71473831]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 41 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.79442095 -0.71025735  0.43991044  0.73726461 -0.30415999  0.03785849\n",
      "   1.2233719   0.0495792  -0.48081566]\n",
      " [-0.70219231  0.51900069 -0.6264925  -0.74457963  0.21714139 -0.27693094\n",
      "  -0.57088414 -0.21625813  0.63091132]\n",
      " [ 0.55593193 -0.18865892  0.11510474  0.34493356 -0.43227178  0.36258292\n",
      "   0.52619873 -0.12484951 -0.52964687]\n",
      " [ 0.53942771 -0.46280931  0.4634032   0.38146682 -0.33811548  0.24612114\n",
      "   0.74408451  0.68687848 -0.2469071 ]\n",
      " [-0.90893021  0.7006291  -0.64608095 -0.86491219  0.0947522   0.07867135\n",
      "  -0.9207177  -0.59675328  0.64373992]\n",
      " [ 1.02670545 -0.62423963  0.64480274  0.4564942  -0.1916657  -0.160129\n",
      "   0.68668759  0.71533826 -0.42567658]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.05302881  1.45102492 -1.4374951   0.1555158   0.18940769 -1.87379611\n",
      "   0.83799143]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:41 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.65952769]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 41 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.80275693 -0.71025735  0.44824642  0.73726461 -0.30415999  0.03785849\n",
      "   1.2233719   0.05791519 -0.48081566]\n",
      " [-0.71075018  0.51900069 -0.63505036 -0.74457963  0.21714139 -0.27693094\n",
      "  -0.57088414 -0.22481599  0.63091132]\n",
      " [ 0.56078094 -0.18865892  0.11995375  0.34493356 -0.43227178  0.36258292\n",
      "   0.52619873 -0.1200005  -0.52964687]\n",
      " [ 0.54792388 -0.46280931  0.47189938  0.38146682 -0.33811548  0.24612114\n",
      "   0.74408451  0.69537465 -0.2469071 ]\n",
      " [-0.91660589  0.7006291  -0.65375662 -0.86491219  0.0947522   0.07867135\n",
      "  -0.9207177  -0.60442895  0.64373992]\n",
      " [ 1.03373967 -0.62423963  0.65183696  0.4564942  -0.1916657  -0.160129\n",
      "   0.68668759  0.72237248 -0.42567658]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.01480212  1.4809608  -1.43077417  0.17972286  0.22167812 -1.86981418\n",
      "   0.87300014]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:41 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.5244363]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 41 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.81476551 -0.71025735  0.44824642  0.74927318 -0.30415999  0.03785849\n",
      "   1.2233719   0.05791519 -0.46880708]\n",
      " [-0.72111051  0.51900069 -0.63505036 -0.75493997  0.21714139 -0.27693094\n",
      "  -0.57088414 -0.22481599  0.62055099]\n",
      " [ 0.56616389 -0.18865892  0.11995375  0.35031651 -0.43227178  0.36258292\n",
      "   0.52619873 -0.1200005  -0.52426391]\n",
      " [ 0.55694983 -0.46280931  0.47189938  0.39049276 -0.33811548  0.24612114\n",
      "   0.74408451  0.69537465 -0.23788115]\n",
      " [-0.92900841  0.7006291  -0.65375662 -0.87731472  0.0947522   0.07867135\n",
      "  -0.9207177  -0.60442895  0.63133739]\n",
      " [ 1.04577756 -0.62423963  0.65183696  0.46853209 -0.1916657  -0.160129\n",
      "   0.68668759  0.72237248 -0.41363869]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.95549865  1.52499593 -1.41269689  0.21488534  0.26107299 -1.85541953\n",
      "   0.91709586]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:41 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.28720712]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 41 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.81418099 -0.71025735  0.44824642  0.74927318 -0.30474451  0.03727398\n",
      "   1.2233719   0.05791519 -0.4693916 ]\n",
      " [-0.71993955  0.51900069 -0.63505036 -0.75493997  0.21831235 -0.27575998\n",
      "  -0.57088414 -0.22481599  0.62172195]\n",
      " [ 0.56636809 -0.18865892  0.11995375  0.35031651 -0.43206758  0.36278712\n",
      "   0.52619873 -0.1200005  -0.52405972]\n",
      " [ 0.55530225 -0.46280931  0.47189938  0.39049276 -0.33976305  0.24447357\n",
      "   0.74408451  0.69537465 -0.23952873]\n",
      " [-0.92809876  0.7006291  -0.65375662 -0.87731472  0.09566186  0.07958101\n",
      "  -0.9207177  -0.60442895  0.63224704]\n",
      " [ 1.0437571  -0.62423963  0.65183696  0.46853209 -0.19368616 -0.16214946\n",
      "   0.68668759  0.72237248 -0.41565914]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.98489705  1.50971159 -1.42622011  0.20039036  0.24471202 -1.86920674\n",
      "   0.90034963]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:41 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.1265276]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 41 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.81518353 -0.70925481  0.44824642  0.74927318 -0.30374197  0.03827652\n",
      "   1.2233719   0.05791519 -0.46838906]\n",
      " [-0.72055414  0.5183861  -0.63505036 -0.75493997  0.21769777 -0.27637457\n",
      "  -0.57088414 -0.22481599  0.62110736]\n",
      " [ 0.56674066 -0.18828636  0.11995375  0.35031651 -0.43169502  0.36315968\n",
      "   0.52619873 -0.1200005  -0.52368715]\n",
      " [ 0.55571967 -0.4623919   0.47189938  0.39049276 -0.33934564  0.24489098\n",
      "   0.74408451  0.69537465 -0.23911131]\n",
      " [-0.9290319   0.69969596 -0.65375662 -0.87731472  0.09472871  0.07864786\n",
      "  -0.9207177  -0.60442895  0.6313139 ]\n",
      " [ 1.04435367 -0.62364306  0.65183696  0.46853209 -0.19308959 -0.16155289\n",
      "   0.68668759  0.72237248 -0.41506257]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.99188886  1.50728652 -1.43034421  0.19726991  0.24163762 -1.87368899\n",
      "   0.89746269]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:41 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.52342338]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 41 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.80193593 -0.72250241  0.44824642  0.74927318 -0.31698957  0.02502891\n",
      "   1.2101243   0.05791519 -0.46838906]\n",
      " [-0.70908073  0.52985952 -0.63505036 -0.75493997  0.22917118 -0.26490115\n",
      "  -0.55941073 -0.22481599  0.62110736]\n",
      " [ 0.55522665 -0.19980036  0.11995375  0.35031651 -0.44320902  0.35164568\n",
      "   0.51468472 -0.1200005  -0.52368715]\n",
      " [ 0.54512441 -0.47298716  0.47189938  0.39049276 -0.3499409   0.23429572\n",
      "   0.73348925  0.69537465 -0.23911131]\n",
      " [-0.9163611   0.71236676 -0.65375662 -0.87731472  0.10739951  0.09131866\n",
      "  -0.90804691 -0.60442895  0.6313139 ]\n",
      " [ 1.03365621 -0.63434052  0.65183696  0.46853209 -0.20378705 -0.17225035\n",
      "   0.67599013  0.72237248 -0.41506257]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.0571732   1.45875266 -1.45014362  0.1517245   0.1973982  -1.89154763\n",
      "   0.85308378]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:41 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.74672203]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 41 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.80563399 -0.72250241  0.44824642  0.75297125 -0.31698957  0.02502891\n",
      "   1.21382237  0.05791519 -0.46838906]\n",
      " [-0.71407877  0.52985952 -0.63505036 -0.75993801  0.22917118 -0.26490115\n",
      "  -0.56440877 -0.22481599  0.62110736]\n",
      " [ 0.56055853 -0.19980036  0.11995375  0.35564839 -0.44320902  0.35164568\n",
      "   0.5200166  -0.1200005  -0.52368715]\n",
      " [ 0.5504576  -0.47298716  0.47189938  0.39582596 -0.3499409   0.23429572\n",
      "   0.73882245  0.69537465 -0.23911131]\n",
      " [-0.92017368  0.71236676 -0.65375662 -0.8811273   0.10739951  0.09131866\n",
      "  -0.91185949 -0.60442895  0.6313139 ]\n",
      " [ 1.03842341 -0.63434052  0.65183696  0.47329928 -0.20378705 -0.17225035\n",
      "   0.68075732  0.72237248 -0.41506257]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.03322219  1.48127979 -1.44734699  0.17101402  0.21755194 -1.89004178\n",
      "   0.87459832]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:41 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.64418818]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 41 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.81466026 -0.72250241  0.44824642  0.75297125 -0.30796331  0.03405518\n",
      "   1.22284863  0.05791519 -0.46838906]\n",
      " [-0.72302471  0.52985952 -0.63505036 -0.75993801  0.22022524 -0.2738471\n",
      "  -0.57335471 -0.22481599  0.62110736]\n",
      " [ 0.568528   -0.19980036  0.11995375  0.35564839 -0.43523955  0.35961515\n",
      "   0.52798607 -0.1200005  -0.52368715]\n",
      " [ 0.55909165 -0.47298716  0.47189938  0.39582596 -0.34130685  0.24292977\n",
      "   0.74745649  0.69537465 -0.23911131]\n",
      " [-0.92927729  0.71236676 -0.65375662 -0.8811273   0.0982959   0.08221505\n",
      "  -0.9209631  -0.60442895  0.6313139 ]\n",
      " [ 1.04741382 -0.63434052  0.65183696  0.47329928 -0.19479664 -0.16325994\n",
      "   0.68974773  0.72237248 -0.41506257]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.99244442  1.51590386 -1.43871099  0.20073664  0.24869806 -1.88337964\n",
      "   0.90693535]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:41 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57498034]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 41 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.79999124 -0.72250241  0.44824642  0.73830223 -0.30796331  0.01938616\n",
      "   1.22284863  0.05791519 -0.48305808]\n",
      " [-0.7083427   0.52985952 -0.63505036 -0.745256    0.22022524 -0.25916509\n",
      "  -0.57335471 -0.22481599  0.63578937]\n",
      " [ 0.55693427 -0.19980036  0.11995375  0.34405466 -0.43523955  0.34802142\n",
      "   0.52798607 -0.1200005  -0.53528088]\n",
      " [ 0.54559801 -0.47298716  0.47189938  0.38233232 -0.34130685  0.22943613\n",
      "   0.74745649  0.69537465 -0.25260495]\n",
      " [-0.91481555  0.71236676 -0.65375662 -0.86666556  0.0982959   0.09667679\n",
      "  -0.9209631  -0.60442895  0.64577564]\n",
      " [ 1.03405396 -0.63434052  0.65183696  0.45993943 -0.19479664 -0.1766198\n",
      "   0.68974773  0.72237248 -0.42842243]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.06270067  1.46275873 -1.45579101  0.15286568  0.19791201 -1.9009666\n",
      "   0.85638019]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:41 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.6772076]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 41 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.80728259 -0.71521106  0.44824642  0.74559359 -0.30796331  0.02667752\n",
      "   1.23013999  0.05791519 -0.48305808]\n",
      " [-0.71612356  0.52207866 -0.63505036 -0.75303685  0.22022524 -0.26694594\n",
      "  -0.58113556 -0.22481599  0.63578937]\n",
      " [ 0.56482947 -0.19190516  0.11995375  0.35194986 -0.43523955  0.35591663\n",
      "   0.53588128 -0.1200005  -0.53528088]\n",
      " [ 0.55346017 -0.465125    0.47189938  0.39019448 -0.34130685  0.23729828\n",
      "   0.75531865  0.69537465 -0.25260495]\n",
      " [-0.92241304  0.70476927 -0.65375662 -0.87426304  0.0982959   0.08907931\n",
      "  -0.92856058 -0.60442895  0.64577564]\n",
      " [ 1.04186607 -0.62652841  0.65183696  0.46775153 -0.19479664 -0.16880769\n",
      "   0.69755984  0.72237248 -0.42842243]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.02741987  1.49404419 -1.45059558  0.18210666  0.22639022 -1.89634985\n",
      "   0.88452824]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:41 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.7045532]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 41 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.81361957 -0.70887408  0.44824642  0.75193056 -0.30796331  0.02667752\n",
      "   1.23647696  0.05791519 -0.48305808]\n",
      " [-0.723007    0.51519522 -0.63505036 -0.75992029  0.22022524 -0.26694594\n",
      "  -0.58801901 -0.22481599  0.63578937]\n",
      " [ 0.57150035 -0.18523428  0.11995375  0.35862074 -0.43523955  0.35591663\n",
      "   0.54255216 -0.1200005  -0.53528088]\n",
      " [ 0.56008666 -0.4584985   0.47189938  0.39682097 -0.34130685  0.23729828\n",
      "   0.76194514  0.69537465 -0.25260495]\n",
      " [-0.92883501  0.6983473  -0.65375662 -0.88068501  0.0982959   0.08907931\n",
      "  -0.93498255 -0.60442895  0.64577564]\n",
      " [ 1.04874673 -0.61964775  0.65183696  0.4746322  -0.19479664 -0.16880769\n",
      "   0.7044405   0.72237248 -0.42842243]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.99667007  1.52134185 -1.44511428  0.20606556  0.25020568 -1.89274995\n",
      "   0.91002895]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:41 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.79093914]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 41 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.81661478 -0.70887408  0.45124164  0.75193056 -0.30796331  0.02967273\n",
      "   1.23947217  0.05791519 -0.48305808]\n",
      " [-0.72640629  0.51519522 -0.63844965 -0.75992029  0.22022524 -0.27034523\n",
      "  -0.5914183  -0.22481599  0.63578937]\n",
      " [ 0.57536695 -0.18523428  0.12382035  0.35862074 -0.43523955  0.35978323\n",
      "   0.54641876 -0.1200005  -0.53528088]\n",
      " [ 0.56368587 -0.4584985   0.47549858  0.39682097 -0.34130685  0.24089749\n",
      "   0.76554435  0.69537465 -0.25260495]\n",
      " [-0.93196051  0.6983473  -0.65688213 -0.88068501  0.0982959   0.0859538\n",
      "  -0.93810806 -0.60442895  0.64577564]\n",
      " [ 1.05211812 -0.61964775  0.65520835  0.4746322  -0.19479664 -0.1654363\n",
      "   0.70781189  0.72237248 -0.42842243]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.9793855   1.53734524 -1.44341025  0.22042221  0.26548588 -1.89134946\n",
      "   0.92564477]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:41 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.36472629]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 42 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.81277425 -0.71271461  0.45124164  0.74809003 -0.30796331  0.02967273\n",
      "   1.23947217  0.05791519 -0.4868986 ]\n",
      " [-0.72296168  0.51863983 -0.63844965 -0.75647568  0.22022524 -0.27034523\n",
      "  -0.5914183  -0.22481599  0.63923398]\n",
      " [ 0.57313745 -0.18746378  0.12382035  0.35639124 -0.43523955  0.35978323\n",
      "   0.54641876 -0.1200005  -0.53751039]\n",
      " [ 0.56109186 -0.46109251  0.47549858  0.39422696 -0.34130685  0.24089749\n",
      "   0.76554435  0.69537465 -0.25519896]\n",
      " [-0.92727328  0.70303454 -0.65688213 -0.87599778  0.0982959   0.0859538\n",
      "  -0.93810806 -0.60442895  0.65046288]\n",
      " [ 1.04734055 -0.62442531  0.65520835  0.46985463 -0.19479664 -0.1654363\n",
      "   0.70781189  0.72237248 -0.43319999]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.02163923  1.51228642 -1.46102758  0.19704887  0.24173803 -1.90761571\n",
      "   0.8995558 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:42 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.77537376]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 42 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.81619722 -0.71271461  0.45466461  0.74809003 -0.30796331  0.02967273\n",
      "   1.24289514  0.05791519 -0.4868986 ]\n",
      " [-0.72711819  0.51863983 -0.64260616 -0.75647568  0.22022524 -0.27034523\n",
      "  -0.5955748  -0.22481599  0.63923398]\n",
      " [ 0.57736322 -0.18746378  0.12804612  0.35639124 -0.43523955  0.35978323\n",
      "   0.55064453 -0.1200005  -0.53751039]\n",
      " [ 0.56537656 -0.46109251  0.47978328  0.39422696 -0.34130685  0.24089749\n",
      "   0.76982905  0.69537465 -0.25519896]\n",
      " [-0.93066731  0.70303454 -0.66027617 -0.87599778  0.0982959   0.0859538\n",
      "  -0.9415021  -0.60442895  0.65046288]\n",
      " [ 1.05090505 -0.62442531  0.65877284  0.46985463 -0.19479664 -0.1654363\n",
      "   0.71137638  0.72237248 -0.43319999]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.00207773  1.53036879 -1.45859715  0.21223155  0.25852979 -1.90616205\n",
      "   0.9175057 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:42 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.71510246]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 42 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.80023879 -0.72867304  0.43870618  0.74809003 -0.30796331  0.02967273\n",
      "   1.22693671  0.05791519 -0.4868986 ]\n",
      " [-0.71086638  0.53489164 -0.62635435 -0.75647568  0.22022524 -0.27034523\n",
      "  -0.579323   -0.22481599  0.63923398]\n",
      " [ 0.56254987 -0.20227713  0.11323277  0.35639124 -0.43523955  0.35978323\n",
      "   0.53583118 -0.1200005  -0.53751039]\n",
      " [ 0.54928977 -0.4771793   0.46369649  0.39422696 -0.34130685  0.24089749\n",
      "   0.75374226  0.69537465 -0.25519896]\n",
      " [-0.91478524  0.71891661 -0.6443941  -0.87599778  0.0982959   0.0859538\n",
      "  -0.92562003 -0.60442895  0.65046288]\n",
      " [ 1.03493534 -0.64039502  0.64280313  0.46985463 -0.19479664 -0.1654363\n",
      "   0.69540667  0.72237248 -0.43319999]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.07492198  1.46784834 -1.47247517  0.15801149  0.20063567 -1.91623702\n",
      "   0.85502441]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:42 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.66356473]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 42 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.80845002 -0.72867304  0.44691741  0.74809003 -0.30796331  0.02967273\n",
      "   1.22693671  0.06612642 -0.4868986 ]\n",
      " [-0.71927263  0.53489164 -0.6347606  -0.75647568  0.22022524 -0.27034523\n",
      "  -0.579323   -0.23322224  0.63923398]\n",
      " [ 0.56738476 -0.20227713  0.11806766  0.35639124 -0.43523955  0.35978323\n",
      "   0.53583118 -0.11516561 -0.53751039]\n",
      " [ 0.55762053 -0.4771793   0.47202726  0.39422696 -0.34130685  0.24089749\n",
      "   0.75374226  0.70370542 -0.25519896]\n",
      " [-0.9222965   0.71891661 -0.65190535 -0.87599778  0.0982959   0.0859538\n",
      "  -0.92562003 -0.61194021  0.65046288]\n",
      " [ 1.04180786 -0.64039502  0.64967565  0.46985463 -0.19479664 -0.1654363\n",
      "   0.69540667  0.72924499 -0.43319999]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.03736796  1.49733973 -1.46596501  0.18187616  0.23242978 -1.91236647\n",
      "   0.88945528]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:42 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.52517894]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 42 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.82049481 -0.72867304  0.44691741  0.76013482 -0.30796331  0.02967273\n",
      "   1.22693671  0.06612642 -0.47485382]\n",
      " [-0.72971725  0.53489164 -0.6347606  -0.7669203   0.22022524 -0.27034523\n",
      "  -0.579323   -0.23322224  0.62878936]\n",
      " [ 0.57289364 -0.20227713  0.11806766  0.36190012 -0.43523955  0.35978323\n",
      "   0.53583118 -0.11516561 -0.5320015 ]\n",
      " [ 0.56677494 -0.4771793   0.47202726  0.40338137 -0.34130685  0.24089749\n",
      "   0.75374226  0.70370542 -0.24604455]\n",
      " [-0.93472255  0.71891661 -0.65190535 -0.88842383  0.0982959   0.0859538\n",
      "  -0.92562003 -0.61194021  0.63803683]\n",
      " [ 1.05389951 -0.64039502  0.64967565  0.48194629 -0.19479664 -0.1654363\n",
      "   0.69540667  0.72924499 -0.42110834]]\n",
      "\n",
      "Theta two: \n",
      "[[-0.97816584  1.54141741 -1.44807002  0.21712412  0.27194389 -1.89810554\n",
      "   0.93363207]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:42 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.2770572]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 42 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.82002815 -0.72867304  0.44691741  0.76013482 -0.30842997  0.02920607\n",
      "   1.22693671  0.06612642 -0.47532048]\n",
      " [-0.72867543  0.53489164 -0.6347606  -0.7669203   0.22126706 -0.26930341\n",
      "  -0.579323   -0.23322224  0.62983118]\n",
      " [ 0.57313333 -0.20227713  0.11806766  0.36190012 -0.43499986  0.36002292\n",
      "   0.53583118 -0.11516561 -0.53176181]\n",
      " [ 0.56526504 -0.4771793   0.47202726  0.40338137 -0.34281676  0.23938759\n",
      "   0.75374226  0.70370542 -0.24755445]\n",
      " [-0.93394508  0.71891661 -0.65190535 -0.88842383  0.09907338  0.08673128\n",
      "  -0.92562003 -0.61194021  0.6388143 ]\n",
      " [ 1.05204354 -0.64039502  0.64967565  0.48194629 -0.19665261 -0.16729228\n",
      "   0.69540667  0.72924499 -0.42296431]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.00591264  1.527077   -1.46089763  0.20349046  0.25654835 -1.91119982\n",
      "   0.91787964]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:42 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.11710765]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 42 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.82092915 -0.72777204  0.44691741  0.76013482 -0.30752897  0.03010707\n",
      "   1.22693671  0.06612642 -0.47441948]\n",
      " [-0.72924113  0.53432594 -0.6347606  -0.7669203   0.22070135 -0.26986911\n",
      "  -0.579323   -0.23322224  0.62926548]\n",
      " [ 0.57348543 -0.20192503  0.11806766  0.36190012 -0.43464776  0.36037501\n",
      "   0.53583118 -0.11516561 -0.53140972]\n",
      " [ 0.56565615 -0.4767882   0.47202726  0.40338137 -0.34242565  0.23977869\n",
      "   0.75374226  0.70370542 -0.24716335]\n",
      " [-0.93478703  0.71807465 -0.65190535 -0.88842383  0.09823142  0.08588932\n",
      "  -0.92562003 -0.61194021  0.63797235]\n",
      " [ 1.05259197 -0.63984659  0.64967565  0.48194629 -0.19610418 -0.16674385\n",
      "   0.69540667  0.72924499 -0.42241588]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.01196672  1.52501847 -1.46450467  0.20081879  0.25391693 -1.91512194\n",
      "   0.91541399]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:42 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.51763458]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 42 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.80794445 -0.74075674  0.44691741  0.76013482 -0.32051367  0.01712237\n",
      "   1.21395202  0.06612642 -0.47441948]\n",
      " [-0.71796073  0.54560633 -0.6347606  -0.7669203   0.23198175 -0.25858872\n",
      "  -0.5680426  -0.23322224  0.62926548]\n",
      " [ 0.56211534 -0.21329512  0.11806766  0.36190012 -0.44601785  0.34900492\n",
      "   0.52446109 -0.11516561 -0.53140972]\n",
      " [ 0.55519925 -0.48724509  0.47202726  0.40338137 -0.35288254  0.2293218\n",
      "   0.74328537  0.70370542 -0.24716335]\n",
      " [-0.92237906  0.73048262 -0.65190535 -0.88842383  0.11063939  0.09829729\n",
      "  -0.91321206 -0.61194021  0.63797235]\n",
      " [ 1.04207956 -0.650359    0.64967565  0.48194629 -0.20661659 -0.17725625\n",
      "   0.68489427  0.72924499 -0.42241588]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Theta two: \n",
      "[[-1.07659056  1.47723785 -1.4842176   0.15577507  0.21016738 -1.93303802\n",
      "   0.87158904]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:42 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.75322733]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 42 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.81144756 -0.74075674  0.44691741  0.76363793 -0.32051367  0.01712237\n",
      "   1.21745513  0.06612642 -0.47441948]\n",
      " [-0.72270747  0.54560633 -0.6347606  -0.77166704  0.23198175 -0.25858872\n",
      "  -0.57278934 -0.23322224  0.62926548]\n",
      " [ 0.56723278 -0.21329512  0.11806766  0.36701756 -0.44601785  0.34900492\n",
      "   0.52957854 -0.11516561 -0.53140972]\n",
      " [ 0.56029042 -0.48724509  0.47202726  0.40847254 -0.35288254  0.2293218\n",
      "   0.74837654  0.70370542 -0.24716335]\n",
      " [-0.92598882  0.73048262 -0.65190535 -0.89203358  0.11063939  0.09829729\n",
      "  -0.91682181 -0.61194021  0.63797235]\n",
      " [ 1.0465965  -0.650359    0.64967565  0.48646323 -0.20661659 -0.17725625\n",
      "   0.6894112   0.72924499 -0.42241588]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.05365601  1.49883525 -1.48160863  0.17434664  0.22956493 -1.93162592\n",
      "   0.89225416]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:42 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.64645374]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 42 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.82039269 -0.74075674  0.44691741  0.76363793 -0.31156854  0.0260675\n",
      "   1.22640026  0.06612642 -0.47441948]\n",
      " [-0.73158357  0.54560633 -0.6347606  -0.77166704  0.22310565 -0.26746482\n",
      "  -0.58166543 -0.23322224  0.62926548]\n",
      " [ 0.57517538 -0.21329512  0.11806766  0.36701756 -0.43807525  0.35694752\n",
      "   0.53752114 -0.11516561 -0.53140972]\n",
      " [ 0.56887613 -0.48724509  0.47202726  0.40847254 -0.34429683  0.23790751\n",
      "   0.75696225  0.70370542 -0.24716335]\n",
      " [-0.93500817  0.73048262 -0.65190535 -0.89203358  0.10162004  0.08927794\n",
      "  -0.92584116 -0.61194021  0.63797235]\n",
      " [ 1.05551639 -0.650359    0.64967565  0.48646323 -0.1976967  -0.16833636\n",
      "   0.69833109  0.72924499 -0.42241588]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.01325428  1.53312975 -1.47310534  0.20388107  0.26050718 -1.9250283\n",
      "   0.92435246]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:42 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57556553]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 42 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.80570116 -0.74075674  0.44691741  0.7489464  -0.31156854  0.01137598\n",
      "   1.22640026  0.06612642 -0.48911101]\n",
      " [-0.71686119  0.54560633 -0.6347606  -0.75694466  0.22310565 -0.25274244\n",
      "  -0.58166543 -0.23322224  0.64398786]\n",
      " [ 0.56349015 -0.21329512  0.11806766  0.35533234 -0.43807525  0.34526229\n",
      "   0.53752114 -0.11516561 -0.54309495]\n",
      " [ 0.55529884 -0.48724509  0.47202726  0.39489525 -0.34429683  0.22433021\n",
      "   0.75696225  0.70370542 -0.26074065]\n",
      " [-0.92051959  0.73048262 -0.65190535 -0.87754501  0.10162004  0.10376651\n",
      "  -0.92584116 -0.61194021  0.65246092]\n",
      " [ 1.04207488 -0.650359    0.64967565  0.47302171 -0.1976967  -0.18177788\n",
      "   0.69833109  0.72924499 -0.43585739]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.08355669  1.47991889 -1.49012233  0.15586243  0.20955627 -1.94258836\n",
      "   0.87363898]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:42 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.67955626]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 42 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.81292486 -0.73353305  0.44691741  0.75617009 -0.31156854  0.01859967\n",
      "   1.23362395  0.06612642 -0.48911101]\n",
      " [-0.72454909  0.53791843 -0.6347606  -0.76463256  0.22310565 -0.26043034\n",
      "  -0.58935333 -0.23322224  0.64398786]\n",
      " [ 0.5712955  -0.20548978  0.11806766  0.36313768 -0.43807525  0.35306763\n",
      "   0.54532648 -0.11516561 -0.54309495]\n",
      " [ 0.5630815  -0.47946242  0.47202726  0.40267791 -0.34429683  0.23211288\n",
      "   0.76474491  0.70370542 -0.26074065]\n",
      " [-0.92803877  0.72296344 -0.65190535 -0.88506419  0.10162004  0.09624733\n",
      "  -0.93336034 -0.61194021  0.65246092]\n",
      " [ 1.04980891 -0.64262496  0.64967565  0.48075575 -0.1976967  -0.17404384\n",
      "   0.70606513  0.72924499 -0.43585739]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.04866685  1.51083533 -1.48501223  0.1848342   0.23778601 -1.9380079\n",
      "   0.90152288]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:42 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.70746501]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 42 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.8191607  -0.72729721  0.44691741  0.76240593 -0.31156854  0.01859967\n",
      "   1.23985979  0.06612642 -0.48911101]\n",
      " [-0.73132594  0.53114158 -0.6347606  -0.77140942  0.22310565 -0.26043034\n",
      "  -0.59613019 -0.23322224  0.64398786]\n",
      " [ 0.57788286 -0.19890242  0.11806766  0.36972504 -0.43807525  0.35306763\n",
      "   0.55191384 -0.11516561 -0.54309495]\n",
      " [ 0.5696333  -0.47291062  0.47202726  0.40922971 -0.34429683  0.23211288\n",
      "   0.77129671  0.70370542 -0.26074065]\n",
      " [-0.9343556   0.71664662 -0.65190535 -0.89138101  0.10162004  0.09624733\n",
      "  -0.93967717 -0.61194021  0.65246092]\n",
      " [ 1.05657974 -0.63585414  0.64967565  0.48752657 -0.1976967  -0.17404384\n",
      "   0.71283595  0.72924499 -0.43585739]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.01839558  1.53771234 -1.47967099  0.20849037  0.26132132 -1.9344735\n",
      "   0.92668371]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:42 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.79362415]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 42 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.82209002 -0.72729721  0.44984674  0.76240593 -0.31156854  0.02152899\n",
      "   1.24278911  0.06612642 -0.48911101]\n",
      " [-0.73463846  0.53114158 -0.63807312 -0.77140942  0.22310565 -0.26374285\n",
      "  -0.5994427  -0.23322224  0.64398786]\n",
      " [ 0.58166207 -0.19890242  0.12184687  0.36972504 -0.43807525  0.35684685\n",
      "   0.55569305 -0.11516561 -0.54309495]\n",
      " [ 0.573139   -0.47291062  0.47553296  0.40922971 -0.34429683  0.23561858\n",
      "   0.77480241  0.70370542 -0.26074065]\n",
      " [-0.93741012  0.71664662 -0.65495988 -0.89138101  0.10162004  0.09319281\n",
      "  -0.94273169 -0.61194021  0.65246092]\n",
      " [ 1.05986573 -0.63585414  0.65296165  0.48752657 -0.1976967  -0.17075785\n",
      "   0.71612195  0.72924499 -0.43585739]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.00149496  1.55335968 -1.47801921  0.2225542   0.27628595 -1.93310562\n",
      "   0.94196563]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:42 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.35652561]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 43 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.81845132 -0.73093591  0.44984674  0.75876723 -0.31156854  0.02152899\n",
      "   1.24278911  0.06612642 -0.49274971]\n",
      " [-0.73134609  0.53443394 -0.63807312 -0.76811705  0.22310565 -0.26374285\n",
      "  -0.5994427  -0.23322224  0.64728022]\n",
      " [ 0.57954455 -0.20101994  0.12184687  0.36760752 -0.43807525  0.35684685\n",
      "   0.55569305 -0.11516561 -0.54521247]\n",
      " [ 0.57063503 -0.4754146   0.47553296  0.40672574 -0.34429683  0.23561858\n",
      "   0.77480241  0.70370542 -0.26324462]\n",
      " [-0.93295009  0.72110665 -0.65495988 -0.88692098  0.10162004  0.09319281\n",
      "  -0.94273169 -0.61194021  0.65692096]\n",
      " [ 1.05526741 -0.64045246  0.65296165  0.48292825 -0.1976967  -0.17075785\n",
      "   0.71612195  0.72924499 -0.44045571]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.04239114  1.52919017 -1.49511451  0.19997308  0.25330799 -1.94893494\n",
      "   0.91674384]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:43 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.77919553]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 43 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.82176375 -0.73093591  0.45315917  0.75876723 -0.31156854  0.02152899\n",
      "   1.24610154  0.06612642 -0.49274971]\n",
      " [-0.73536645  0.53443394 -0.64209347 -0.76811705  0.22310565 -0.26374285\n",
      "  -0.60346306 -0.23322224  0.64728022]\n",
      " [ 0.58366169 -0.20101994  0.12596401  0.36760752 -0.43807525  0.35684685\n",
      "   0.55981019 -0.11516561 -0.54521247]\n",
      " [ 0.5747825  -0.4754146   0.47968043  0.40672574 -0.34429683  0.23561858\n",
      "   0.77894989  0.70370542 -0.26324462]\n",
      " [-0.93623321  0.72110665 -0.658243   -0.88692098  0.10162004  0.09319281\n",
      "  -0.94601481 -0.61194021  0.65692096]\n",
      " [ 1.05870822 -0.64045246  0.65640245  0.48292825 -0.1976967  -0.17075785\n",
      "   0.71956276  0.72924499 -0.44045571]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.02339645  1.54675865 -1.49278745  0.2147609   0.26965639 -1.94753431\n",
      "   0.9341936 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:43 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.71530745]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 43 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.80577997 -0.74691969  0.43717539  0.75876723 -0.31156854  0.02152899\n",
      "   1.23011776  0.06612642 -0.49274971]\n",
      " [-0.71911723  0.55068317 -0.62584425 -0.76811705  0.22310565 -0.26374285\n",
      "  -0.58721384 -0.23322224  0.64728022]\n",
      " [ 0.56885167 -0.21582996  0.11115399  0.36760752 -0.43807525  0.35684685\n",
      "   0.54500017 -0.11516561 -0.54521247]\n",
      " [ 0.55868835 -0.49150875  0.46358628  0.40672574 -0.34429683  0.23561858\n",
      "   0.76285573  0.70370542 -0.26324462]\n",
      " [-0.92032573  0.73701413 -0.64233552 -0.88692098  0.10162004  0.09319281\n",
      "  -0.93010733 -0.61194021  0.65692096]\n",
      " [ 1.04273477 -0.65642591  0.640429    0.48292825 -0.1976967  -0.17075785\n",
      "   0.7035893   0.72924499 -0.44045571]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.09623002  1.48434479 -1.50666534  0.1605512   0.21172208 -1.95769516\n",
      "   0.87174289]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:43 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.66754965]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 43 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.81386522 -0.74691969  0.44526063  0.75876723 -0.31156854  0.02152899\n",
      "   1.23011776  0.07421166 -0.49274971]\n",
      " [-0.72737232  0.55068317 -0.63409934 -0.76811705  0.22310565 -0.26374285\n",
      "  -0.58721384 -0.24147733  0.64728022]\n",
      " [ 0.57366653 -0.21582996  0.11596885  0.36760752 -0.43807525  0.35684685\n",
      "   0.54500017 -0.11035075 -0.54521247]\n",
      " [ 0.56685549 -0.49150875  0.47175342  0.40672574 -0.34429683  0.23561858\n",
      "   0.76285573  0.71187256 -0.26324462]\n",
      " [-0.92767711  0.73701413 -0.64968691 -0.88692098  0.10162004  0.09319281\n",
      "  -0.93010733 -0.61929159  0.65692096]\n",
      " [ 1.04945104 -0.65642591  0.64714528  0.48292825 -0.1976967  -0.17075785\n",
      "   0.7035893   0.73596127 -0.44045571]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.05934015  1.51339038 -1.50035522  0.18407114  0.2430378  -1.95393038\n",
      "   0.90559926]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:43 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.52608098]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 43 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.82593911 -0.74691969  0.44526063  0.77084112 -0.31156854  0.02152899\n",
      "   1.23011776  0.07421166 -0.48067582]\n",
      " [-0.73789181  0.55068317 -0.63409934 -0.77863654  0.22310565 -0.26374285\n",
      "  -0.58721384 -0.24147733  0.63676073]\n",
      " [ 0.5792927  -0.21582996  0.11596885  0.37323368 -0.43807525  0.35684685\n",
      "   0.54500017 -0.11035075 -0.5395863 ]\n",
      " [ 0.57612732 -0.49150875  0.47175342  0.41599757 -0.34429683  0.23561858\n",
      "   0.76285573  0.71187256 -0.25397279]\n",
      " [-0.94011976  0.73701413 -0.64968691 -0.89936362  0.10162004  0.09319281\n",
      "  -0.93010733 -0.61929159  0.64447832]\n",
      " [ 1.06158698 -0.65642591  0.64714528  0.49506419 -0.1976967  -0.17075785\n",
      "   0.7035893   0.73596127 -0.42831978]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.00026145  1.55749095 -1.48264292  0.2193849   0.28264862 -1.93980527\n",
      "   0.94983407]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:43 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.26722931]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 43 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.82557816 -0.74691969  0.44526063  0.77084112 -0.31192948  0.02116805\n",
      "   1.23011776  0.07421166 -0.48103676]\n",
      " [-0.73696914  0.55068317 -0.63409934 -0.77863654  0.22402832 -0.26282019\n",
      "  -0.58721384 -0.24147733  0.6376834 ]\n",
      " [ 0.57956418 -0.21582996  0.11596885  0.37323368 -0.43780377  0.35711833\n",
      "   0.54500017 -0.11035075 -0.53931482]\n",
      " [ 0.57474675 -0.49150875  0.47175342  0.41599757 -0.3456774   0.23423801\n",
      "   0.76285573  0.71187256 -0.25535335]\n",
      " [-0.93946191  0.73701413 -0.64968691 -0.89936362  0.10227789  0.09385066\n",
      "  -0.93010733 -0.61929159  0.64513617]\n",
      " [ 1.05988484 -0.65642591  0.64714528  0.49506419 -0.19939884 -0.17245998\n",
      "   0.7035893   0.73596127 -0.43002191]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.02642558  1.54404776 -1.49479922  0.20657439  0.26817548 -1.95222837\n",
      "   0.93502991]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:43 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.10832516]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 43 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.82638399 -0.74611385  0.44526063  0.77084112 -0.31112365  0.02197388\n",
      "   1.23011776  0.07421166 -0.48023093]\n",
      " [-0.73748646  0.55016585 -0.63409934 -0.77863654  0.223511   -0.2633375\n",
      "  -0.58721384 -0.24147733  0.63716608]\n",
      " [ 0.5798939  -0.21550023  0.11596885  0.37323368 -0.43747404  0.35744806\n",
      "   0.54500017 -0.11035075 -0.53898509]\n",
      " [ 0.57511026 -0.49114525  0.47175342  0.41599757 -0.34531389  0.23460152\n",
      "   0.76285573  0.71187256 -0.25498985]\n",
      " [-0.94021765  0.73625839 -0.64968691 -0.89936362  0.10152214  0.09309491\n",
      "  -0.93010733 -0.61929159  0.64438042]\n",
      " [ 1.0603858  -0.65592496  0.64714528  0.49506419 -0.19889788 -0.17195903\n",
      "   0.7035893   0.73596127 -0.42952096]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.03165719  1.54230388 -1.49794715  0.20429194  0.26592807 -1.95565239\n",
      "   0.93292841]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:43 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.51148447]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 43 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.81367973 -0.75881811  0.44526063  0.77084112 -0.32382791  0.00926962\n",
      "   1.2174135   0.07421166 -0.48023093]\n",
      " [-0.72641584  0.56123647 -0.63409934 -0.77863654  0.23458162 -0.25226688\n",
      "  -0.57614322 -0.24147733  0.63716608]\n",
      " [ 0.56868495 -0.22670918  0.11596885  0.37323368 -0.448683    0.3462391\n",
      "   0.53379122 -0.11035075 -0.53898509]\n",
      " [ 0.56481005 -0.50144545  0.47175342  0.41599757 -0.3556141   0.22430131\n",
      "   0.75255552  0.71187256 -0.25498985]\n",
      " [-0.92808844  0.7483876  -0.64968691 -0.89936362  0.11365136  0.10522413\n",
      "  -0.91797811 -0.61929159  0.64438042]\n",
      " [ 1.05007435 -0.6662364   0.64714528  0.49506419 -0.20920933 -0.18227047\n",
      "   0.69327786  0.73596127 -0.42952096]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.09555902  1.49532238 -1.51756258  0.15980212  0.22272121 -1.97360954\n",
      "   0.88970638]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:43 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.75952423]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 43 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.8169992  -0.75881811  0.44526063  0.77416059 -0.32382791  0.00926962\n",
      "   1.22073297  0.07421166 -0.48023093]\n",
      " [-0.7309238   0.56123647 -0.63409934 -0.7831445   0.23458162 -0.25226688\n",
      "  -0.58065117 -0.24147733  0.63716608]\n",
      " [ 0.57359324 -0.22670918  0.11596885  0.37814197 -0.448683    0.3462391\n",
      "   0.5386995  -0.11035075 -0.53898509]\n",
      " [ 0.56966775 -0.50144545  0.47175342  0.42085527 -0.3556141   0.22430131\n",
      "   0.75741322  0.71187256 -0.25498985]\n",
      " [-0.93150726  0.7483876  -0.64968691 -0.90278244  0.11365136  0.10522413\n",
      "  -0.92139693 -0.61929159  0.64438042]\n",
      " [ 1.05435452 -0.6662364   0.64714528  0.49934435 -0.20920933 -0.18227047\n",
      "   0.69755802  0.73596127 -0.42952096]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.07359791  1.51602688 -1.51512626  0.17767679  0.24138469 -1.97228429\n",
      "   0.90955146]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:43 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.64864716]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 43 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.82586609 -0.75881811  0.44526063  0.77416059 -0.31496102  0.01813651\n",
      "   1.22959986  0.07421166 -0.48023093]\n",
      " [-0.73973081  0.56123647 -0.63409934 -0.7831445   0.22577461 -0.2610739\n",
      "  -0.58945819 -0.24147733  0.63716608]\n",
      " [ 0.58150634 -0.22670918  0.11596885  0.37814197 -0.44076989  0.3541522\n",
      "   0.54661261 -0.11035075 -0.53898509]\n",
      " [ 0.57820387 -0.50144545  0.47175342  0.42085527 -0.34707798  0.23283743\n",
      "   0.76594934  0.71187256 -0.25498985]\n",
      " [-0.94044518  0.7483876  -0.64968691 -0.90278244  0.10471344  0.09628621\n",
      "  -0.93033486 -0.61929159  0.64438042]\n",
      " [ 1.06320475 -0.6662364   0.64714528  0.49934435 -0.20035909 -0.17342024\n",
      "   0.70640826  0.73596127 -0.42952096]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.03356055  1.55000008 -1.50674717  0.2070238   0.27212422 -1.96574702\n",
      "   0.9414144 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:43 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57618448]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 43 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.81115231 -0.75881811  0.44526063  0.75944681 -0.31496102  0.00342273\n",
      "   1.22959986  0.07421166 -0.49494471]\n",
      " [-0.72497018  0.56123647 -0.63409934 -0.76838386  0.22577461 -0.24631326\n",
      "  -0.58945819 -0.24147733  0.65192671]\n",
      " [ 0.56973583 -0.22670918  0.11596885  0.36637146 -0.44076989  0.34238169\n",
      "   0.54661261 -0.11035075 -0.5507556 ]\n",
      " [ 0.56454791 -0.50144545  0.47175342  0.40719932 -0.34707798  0.21918148\n",
      "   0.76594934  0.71187256 -0.2686458 ]\n",
      " [-0.92593024  0.7483876  -0.64968691 -0.8882675   0.10471344  0.11080114\n",
      "  -0.93033486 -0.61929159  0.65889535]\n",
      " [ 1.0496859  -0.6662364   0.64714528  0.4858255  -0.20035909 -0.18693909\n",
      "   0.70640826  0.73596127 -0.44303982]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.1039115   1.49672331 -1.52370717  0.15886424  0.22101468 -1.98328272\n",
      "   0.89054762]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:43 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.68177497]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 43 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.81831349 -0.75165694  0.44526063  0.76660799 -0.31496102  0.01058391\n",
      "   1.23676104  0.07421166 -0.49494471]\n",
      " [-0.73257106  0.55363558 -0.63409934 -0.77598475  0.22577461 -0.25391415\n",
      "  -0.59705907 -0.24147733  0.65192671]\n",
      " [ 0.5774559  -0.21898911  0.11596885  0.37409153 -0.44076989  0.35010176\n",
      "   0.55433268 -0.11035075 -0.5507556 ]\n",
      " [ 0.57225426 -0.49373911  0.47175342  0.41490566 -0.34707798  0.22688783\n",
      "   0.77365568  0.71187256 -0.2686458 ]\n",
      " [-0.93337652  0.74094133 -0.64968691 -0.89571378  0.10471344  0.10335487\n",
      "  -0.93778113 -0.61929159  0.65889535]\n",
      " [ 1.05734509 -0.65857721  0.64714528  0.49348469 -0.20035909 -0.1792799\n",
      "   0.71406745  0.73596127 -0.44303982]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.06939079  1.52728869 -1.51867388  0.18757834  0.24900521 -1.97873371\n",
      "   0.91817771]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:43 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.71029072]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 43 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.82445236 -0.74551807  0.44526063  0.77274686 -0.31496102  0.01058391\n",
      "   1.24289991  0.07421166 -0.49494471]\n",
      " [-0.73924403  0.54696262 -0.63409934 -0.78265771  0.22577461 -0.25391415\n",
      "  -0.60373204 -0.24147733  0.65192671]\n",
      " [ 0.58396011 -0.2124849   0.11596885  0.38059574 -0.44076989  0.35010176\n",
      "   0.56083689 -0.11035075 -0.5507556 ]\n",
      " [ 0.57873028 -0.48726309  0.47175342  0.42138168 -0.34707798  0.22688783\n",
      "   0.7801317   0.71187256 -0.2686458 ]\n",
      " [-0.93959252  0.73472532 -0.64968691 -0.90192978  0.10471344  0.10335487\n",
      "  -0.94399714 -0.61929159  0.65889535]\n",
      " [ 1.06400906 -0.65191324  0.64714528  0.50014866 -0.20035909 -0.1792799\n",
      "   0.72073142  0.73596127 -0.44303982]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.03958292  1.55375676 -1.51346345  0.2109364   0.27226256 -1.97526088\n",
      "   0.94300468]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:43 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.79619896]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 43 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.82732006 -0.74551807  0.44812833  0.77274686 -0.31496102  0.01345161\n",
      "   1.24576762  0.07421166 -0.49494471]\n",
      " [-0.74247527  0.54696262 -0.63733058 -0.78265771  0.22577461 -0.25714539\n",
      "  -0.60696328 -0.24147733  0.65192671]\n",
      " [ 0.58765599 -0.2124849   0.11966473  0.38059574 -0.44076989  0.35379764\n",
      "   0.56453277 -0.11035075 -0.5507556 ]\n",
      " [ 0.58214802 -0.48726309  0.47517116  0.42138168 -0.34707798  0.23030557\n",
      "   0.78354944  0.71187256 -0.2686458 ]\n",
      " [-0.94258067  0.73472532 -0.65267506 -0.90192978  0.10471344  0.10036672\n",
      "  -0.94698529 -0.61929159  0.65889535]\n",
      " [ 1.06721512 -0.65191324  0.65035135  0.50014866 -0.20035909 -0.17607383\n",
      "   0.72393748  0.73596127 -0.44303982]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.02304791  1.56906408 -1.51185952  0.22471886  0.28692421 -1.97392288\n",
      "   0.95796679]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:43 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.34846903]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 44 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.8238763  -0.74896183  0.44812833  0.7693031  -0.31496102  0.01345161\n",
      "   1.24576762  0.07421166 -0.49838847]\n",
      " [-0.73933324  0.55010465 -0.63733058 -0.77951568  0.22577461 -0.25714539\n",
      "  -0.60696328 -0.24147733  0.65506874]\n",
      " [ 0.58564969 -0.2144912   0.11966473  0.37858944 -0.44076989  0.35379764\n",
      "   0.56453277 -0.11035075 -0.5527619 ]\n",
      " [ 0.57973634 -0.48967477  0.47517116  0.41897    -0.34707798  0.23030557\n",
      "   0.78354944  0.71187256 -0.27105748]\n",
      " [-0.93834078  0.73896522 -0.65267506 -0.89768988  0.10471344  0.10036672\n",
      "  -0.94698529 -0.61929159  0.66313525]\n",
      " [ 1.06279449 -0.65633388  0.65035135  0.49572802 -0.20035909 -0.17607383\n",
      "   0.72393748  0.73596127 -0.44746045]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.06260583  1.54576665 -1.52844041  0.20291951  0.26470885 -1.98931681\n",
      "   0.93360093]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:44 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.78289857]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 44 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.82708406 -0.74896183  0.45133609  0.7693031  -0.31496102  0.01345161\n",
      "   1.24897537  0.07421166 -0.49838847]\n",
      " [-0.74322393  0.55010465 -0.64122127 -0.77951568  0.22577461 -0.25714539\n",
      "  -0.61085397 -0.24147733  0.65506874]\n",
      " [ 0.58966068 -0.2144912   0.12367572  0.37858944 -0.44076989  0.35379764\n",
      "   0.56854376 -0.11035075 -0.5527619 ]\n",
      " [ 0.58375248 -0.48967477  0.4791873   0.41897    -0.34707798  0.23030557\n",
      "   0.78756558  0.71187256 -0.27105748]\n",
      " [-0.94151904  0.73896522 -0.65585332 -0.89768988  0.10471344  0.10036672\n",
      "  -0.95016355 -0.61929159  0.66313525]\n",
      " [ 1.06611854 -0.65633388  0.6536754   0.49572802 -0.20035909 -0.17607383\n",
      "   0.72726154  0.73596127 -0.44746045]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.04415564  1.56284006 -1.52620918  0.21732388  0.28062705 -1.98796558\n",
      "   0.95056794]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:44 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.71535402]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 44 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.81107248 -0.76497341  0.43532451  0.7693031  -0.31496102  0.01345161\n",
      "   1.23296379  0.07421166 -0.49838847]\n",
      " [-0.72697678  0.5663518  -0.62497412 -0.77951568  0.22577461 -0.25714539\n",
      "  -0.59460682 -0.24147733  0.65506874]\n",
      " [ 0.57485797 -0.22929392  0.10887301  0.37858944 -0.44076989  0.35379764\n",
      "   0.55374104 -0.11035075 -0.5527619 ]\n",
      " [ 0.56765228 -0.50577497  0.46308711  0.41897    -0.34707798  0.23030557\n",
      "   0.77146539  0.71187256 -0.27105748]\n",
      " [-0.9255831   0.75490116 -0.63991738 -0.89768988  0.10471344  0.10036672\n",
      "  -0.93422761 -0.61929159  0.66313525]\n",
      " [ 1.05013687 -0.67231555  0.63769373  0.49572802 -0.20035909 -0.17607383\n",
      "   0.71127987  0.73596127 -0.44746045]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.11698678  1.50053278 -1.54010116  0.16313023  0.22266113 -1.99822101\n",
      "   0.88815051]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:44 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.67147333]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 44 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.81903126 -0.76497341  0.44328329  0.7693031  -0.31496102  0.01345161\n",
      "   1.23296379  0.08217044 -0.49838847]\n",
      " [-0.73508191  0.5663518  -0.63307925 -0.77951568  0.22577461 -0.25714539\n",
      "  -0.59460682 -0.24958246  0.65506874]\n",
      " [ 0.57964764 -0.22929392  0.11366268  0.37858944 -0.44076989  0.35379764\n",
      "   0.55374104 -0.10556108 -0.5527619 ]\n",
      " [ 0.57565822 -0.50577497  0.47109304  0.41897    -0.34707798  0.23030557\n",
      "   0.77146539  0.71987849 -0.27105748]\n",
      " [-0.93277935  0.75490116 -0.64711363 -0.89768988  0.10471344  0.10036672\n",
      "  -0.93422761 -0.62648784  0.66313525]\n",
      " [ 1.05670242 -0.67231555  0.64425928  0.49572802 -0.20035909 -0.17607383\n",
      "   0.71127987  0.74252682 -0.44746045]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.08075079  1.52913322 -1.53398082  0.18630469  0.25349862 -1.99455666\n",
      "   0.92143774]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:44 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.52712957]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 44 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.83112759 -0.76497341  0.44328329  0.78139943 -0.31496102  0.01345161\n",
      "   1.23296379  0.08217044 -0.48629214]\n",
      " [-0.74566745  0.5663518  -0.63307925 -0.79010123  0.22577461 -0.25714539\n",
      "  -0.59460682 -0.24958246  0.6444832 ]\n",
      " [ 0.58538281 -0.22929392  0.11366268  0.38432461 -0.44076989  0.35379764\n",
      "   0.55374104 -0.10556108 -0.54702673]\n",
      " [ 0.58503701 -0.50577497  0.47109304  0.42834879 -0.34707798  0.23030557\n",
      "   0.77146539  0.71987849 -0.26167869]\n",
      " [-0.9452321   0.75490116 -0.64711363 -0.91014263  0.10471344  0.10036672\n",
      "  -0.93422761 -0.62648784  0.6506825 ]\n",
      " [ 1.0688738  -0.67231555  0.64425928  0.50789939 -0.20035909 -0.17607383\n",
      "   0.71127987  0.74252682 -0.43528908]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.02181601  1.57323833 -1.51645139  0.2216657   0.29318488 -1.98056924\n",
      "   0.96570897]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:44 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.25772996]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 44 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.83086069 -0.76497341  0.44328329  0.78139943 -0.31522792  0.01318471\n",
      "   1.23296379  0.08217044 -0.48655904]\n",
      " [-0.74485407  0.5663518  -0.63307925 -0.79010123  0.22658798 -0.25633201\n",
      "  -0.59460682 -0.24958246  0.64529658]\n",
      " [ 0.58568226 -0.22929392  0.11366268  0.38432461 -0.44047044  0.35409709\n",
      "   0.55374104 -0.10556108 -0.54672728]\n",
      " [ 0.58377728 -0.50577497  0.47109304  0.42834879 -0.34833771  0.22904584\n",
      "   0.77146539  0.71987849 -0.26293842]\n",
      " [-0.94468179  0.75490116 -0.64711363 -0.91014263  0.10526375  0.10091703\n",
      "  -0.93422761 -0.62648784  0.65123281]\n",
      " [ 1.06731485 -0.67231555  0.64425928  0.50789939 -0.20191804 -0.17763278\n",
      "   0.71127987  0.74252682 -0.43684803]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.04646855  1.56064508 -1.5279619   0.209639    0.2795899  -1.99234447\n",
      "   0.95180651]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:44 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.10015758]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 44 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.83157831 -0.76425579  0.44328329  0.78139943 -0.3145103   0.01390233\n",
      "   1.23296379  0.08217044 -0.48584142]\n",
      " [-0.7453244   0.56588147 -0.63307925 -0.79010123  0.22611766 -0.25680234\n",
      "  -0.59460682 -0.24958246  0.64482625]\n",
      " [ 0.58598859 -0.22898759  0.11366268  0.38432461 -0.44016412  0.35440342\n",
      "   0.55374104 -0.10556108 -0.54642095]\n",
      " [ 0.58411273 -0.50543952  0.47109304  0.42834879 -0.34800226  0.22938129\n",
      "   0.77146539  0.71987849 -0.26260296]\n",
      " [-0.94535703  0.75422592 -0.64711363 -0.91014263  0.10458851  0.10024178\n",
      "  -0.93422761 -0.62648784  0.65055757]\n",
      " [ 1.06776984 -0.67186056  0.64425928  0.50789939 -0.20146305 -0.17717779\n",
      "   0.71127987  0.74252682 -0.43639304]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.05098196  1.55917012 -1.53070405  0.20769255  0.27767386 -1.99532771\n",
      "   0.95001838]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:44 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.50500886]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 44 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.81917036 -0.77666374  0.44328329  0.78139943 -0.32691825  0.00149438\n",
      "   1.22055584  0.08217044 -0.48584142]\n",
      " [-0.73447851  0.57672736 -0.63307925 -0.79010123  0.23696355 -0.24595645\n",
      "  -0.58376093 -0.24958246  0.64482625]\n",
      " [ 0.57495644 -0.24001974  0.11366268  0.38432461 -0.45119626  0.34337127\n",
      "   0.54270889 -0.10556108 -0.54642095]\n",
      " [ 0.57398572 -0.51556653  0.47109304  0.42834879 -0.35812928  0.21925428\n",
      "   0.76133837  0.71987849 -0.26260296]\n",
      " [-0.93352072  0.76606223 -0.64711363 -0.91014263  0.11642481  0.11207809\n",
      "  -0.9223913  -0.62648784  0.65055757]\n",
      " [ 1.05767345 -0.68195695  0.64425928  0.50789939 -0.21155944 -0.18727418\n",
      "   0.70118348  0.74252682 -0.43639304]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.11410173  1.51302986 -1.55020892  0.16380583  0.23505928 -2.01330739\n",
      "   0.90744494]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:44 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.76561849]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 44 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.82231679 -0.77666374  0.44328329  0.78454586 -0.32691825  0.00149438\n",
      "   1.22370227  0.08217044 -0.48584142]\n",
      " [-0.73875983  0.57672736 -0.63307925 -0.79438254  0.23696355 -0.24595645\n",
      "  -0.58804225 -0.24958246  0.64482625]\n",
      " [ 0.57966145 -0.24001974  0.11366268  0.38902962 -0.45119626  0.34337127\n",
      "   0.5474139  -0.10556108 -0.54642095]\n",
      " [ 0.57861883 -0.51556653  0.47109304  0.4329819  -0.35812928  0.21925428\n",
      "   0.76597149  0.71987849 -0.26260296]\n",
      " [-0.93675975  0.76606223 -0.64711363 -0.91338166  0.11642481  0.11207809\n",
      "  -0.92563033 -0.62648784  0.65055757]\n",
      " [ 1.06172978 -0.68195695  0.64425928  0.51195572 -0.21155944 -0.18727418\n",
      "   0.70523981  0.74252682 -0.43639304]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.09307222  1.53287765 -1.54793166  0.18100526  0.25301153 -2.01206273\n",
      "   0.92649949]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:44 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.65078147]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 44 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.83110773 -0.77666374  0.44328329  0.78454586 -0.3181273   0.01028533\n",
      "   1.23249322  0.08217044 -0.48584142]\n",
      " [-0.74749838  0.57672736 -0.63307925 -0.79438254  0.22822499 -0.254695\n",
      "  -0.5967808  -0.24958246  0.64482625]\n",
      " [ 0.5875427  -0.24001974  0.11366268  0.38902962 -0.44331501  0.35125253\n",
      "   0.55529515 -0.10556108 -0.54642095]\n",
      " [ 0.58710421 -0.51556653  0.47109304  0.4329819  -0.34964389  0.22773966\n",
      "   0.77445687  0.71987849 -0.26260296]\n",
      " [-0.94561855  0.76606223 -0.64711363 -0.91338166  0.10756602  0.1032193\n",
      "  -0.93448913 -0.62648784  0.65055757]\n",
      " [ 1.07051105 -0.68195695  0.64425928  0.51195572 -0.20277816 -0.1784929\n",
      "   0.71402109  0.74252682 -0.43639304]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.05338966  1.56653662 -1.53966965  0.21016506  0.28354884 -2.00558262\n",
      "   0.95812962]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:44 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57683782]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 44 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.81637172 -0.77666374  0.44328329  0.76980985 -0.3181273  -0.00445069\n",
      "   1.23249322  0.08217044 -0.50057744]\n",
      " [-0.73270126  0.57672736 -0.63307925 -0.77958543  0.22822499 -0.23989789\n",
      "  -0.5967808  -0.24958246  0.65962337]\n",
      " [ 0.57569247 -0.24001974  0.11366268  0.37717939 -0.44331501  0.3394023\n",
      "   0.55529515 -0.10556108 -0.55827118]\n",
      " [ 0.57337405 -0.51556653  0.47109304  0.41925174 -0.34964389  0.2140095\n",
      "   0.77445687  0.71987849 -0.27633313]\n",
      " [-0.93107747  0.76606223 -0.64711363 -0.89884059  0.10756602  0.11776037\n",
      "  -0.93448913 -0.62648784  0.66509864]\n",
      " [ 1.05691861 -0.68195695  0.64425928  0.49836328 -0.20277816 -0.19208535\n",
      "   0.71402109  0.74252682 -0.44998548]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.12379154  1.51319329 -1.55657805  0.16187059  0.23228618 -2.02309597\n",
      "   0.9071138 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:44 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.68387446]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 44 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.82347488 -0.76956058  0.44328329  0.77691301 -0.3181273   0.00265248\n",
      "   1.23959638  0.08217044 -0.50057744]\n",
      " [-0.74022057  0.56920805 -0.63307925 -0.78710474  0.22822499 -0.2474172\n",
      "  -0.60430011 -0.24958246  0.65962337]\n",
      " [ 0.58333161 -0.23238061  0.11366268  0.38481852 -0.44331501  0.34704143\n",
      "   0.56293429 -0.10556108 -0.55827118]\n",
      " [ 0.5810072  -0.50793338  0.47109304  0.42688489 -0.34964389  0.22164265\n",
      "   0.78209002  0.71987849 -0.27633313]\n",
      " [-0.93845564  0.75868406 -0.64711363 -0.90621875  0.10756602  0.11038221\n",
      "  -0.94186729 -0.62648784  0.66509864]\n",
      " [ 1.06450611 -0.67436944  0.64425928  0.50595078 -0.20277816 -0.18449784\n",
      "   0.72160859  0.74252682 -0.44998548]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.08961992  1.54342439 -1.55161414  0.19033797  0.2600464  -2.01857432\n",
      "   0.93449999]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:44 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.71303614]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 44 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.82952053 -0.76351493  0.44328329  0.78295866 -0.3181273   0.00265248\n",
      "   1.24564203  0.08217044 -0.50057744]\n",
      " [-0.7467923   0.56263632 -0.63307925 -0.79367647  0.22822499 -0.2474172\n",
      "  -0.61087184 -0.24958246  0.65962337]\n",
      " [ 0.58975327 -0.22595894  0.11366268  0.39124019 -0.44331501  0.34704143\n",
      "   0.56935595 -0.10556108 -0.55827118]\n",
      " [ 0.58740681 -0.50153377  0.47109304  0.4332845  -0.34964389  0.22164265\n",
      "   0.78848963  0.71987849 -0.27633313]\n",
      " [-0.94457473  0.75256497 -0.64711363 -0.91233784  0.10756602  0.11038221\n",
      "  -0.94798638 -0.62648784  0.66509864]\n",
      " [ 1.07106613 -0.66780942  0.64425928  0.5125108  -0.20277816 -0.18449784\n",
      "   0.72816861  0.74252682 -0.44998548]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.06026128  1.56949472 -1.54652628  0.21340261  0.28302829 -2.01515965\n",
      "   0.9589991 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:44 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.79866765]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 44 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.83233054 -0.76351493  0.4460933   0.78295866 -0.3181273   0.00546249\n",
      "   1.24845204  0.08217044 -0.50057744]\n",
      " [-0.74994738  0.56263632 -0.63623432 -0.79367647  0.22822499 -0.25057227\n",
      "  -0.61402692 -0.24958246  0.65962337]\n",
      " [ 0.59336974 -0.22595894  0.11727916  0.39124019 -0.44331501  0.35065791\n",
      "   0.57297242 -0.10556108 -0.55827118]\n",
      " [ 0.59074182 -0.50153377  0.47442805  0.4332845  -0.34964389  0.22497766\n",
      "   0.79182464  0.71987849 -0.27633313]\n",
      " [-0.94750075  0.75256497 -0.65003965 -0.91233784  0.10756602  0.10745618\n",
      "  -0.95091241 -0.62648784  0.66509864]\n",
      " [ 1.07419735 -0.66780942  0.64739049  0.5125108  -0.20277816 -0.18136663\n",
      "   0.73129983  0.74252682 -0.44998548]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.0440744   1.58447742 -1.54496626  0.22691493  0.29739928 -2.01384906\n",
      "   0.97365509]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:44 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.34056112]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 45 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.82907441 -0.76677107  0.4460933   0.77970252 -0.3181273   0.00546249\n",
      "   1.24845204  0.08217044 -0.50383357]\n",
      " [-0.746953    0.5656307  -0.63623432 -0.79068209  0.22822499 -0.25057227\n",
      "  -0.61402692 -0.24958246  0.66261775]\n",
      " [ 0.59147314 -0.22785554  0.11727916  0.38934358 -0.44331501  0.35065791\n",
      "   0.57297242 -0.10556108 -0.56016778]\n",
      " [ 0.58842374 -0.50385185  0.47442805  0.43096642 -0.34964389  0.22497766\n",
      "   0.79182464  0.71987849 -0.27865121]\n",
      " [-0.94347347  0.75659225 -0.65003965 -0.90831057  0.10756602  0.10745618\n",
      "  -0.95091241 -0.62648784  0.66912592]\n",
      " [ 1.06995204 -0.67205473  0.64739049  0.50826549 -0.20277816 -0.18136663\n",
      "   0.73129983  0.74252682 -0.4542308 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.08231588  1.5620332  -1.56104081  0.20588487  0.27593697 -2.02881\n",
      "   0.95013174]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:45 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.78648344]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 45 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.83218304 -0.76677107  0.44920193  0.77970252 -0.3181273   0.00546249\n",
      "   1.25156067  0.08217044 -0.50383357]\n",
      " [-0.7507203   0.5656307  -0.64000163 -0.79068209  0.22822499 -0.25057227\n",
      "  -0.61779423 -0.24958246  0.66261775]\n",
      " [ 0.59538074 -0.22785554  0.12118676  0.38934358 -0.44331501  0.35065791\n",
      "   0.57688002 -0.10556108 -0.56016778]\n",
      " [ 0.59231438 -0.50385185  0.47831869  0.43096642 -0.34964389  0.22497766\n",
      "   0.79571528  0.71987849 -0.27865121]\n",
      " [-0.9465526   0.75659225 -0.65311878 -0.90831057  0.10756602  0.10745618\n",
      "  -0.95399153 -0.62648784  0.66912592]\n",
      " [ 1.07316591 -0.67205473  0.65060436  0.50826549 -0.20277816 -0.18136663\n",
      "   0.7345137   0.74252682 -0.4542308 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.06438826  1.57863026 -1.55889854  0.21991759  0.29143855 -2.02750484\n",
      "   0.96663344]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:45 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.71524315]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 45 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.81614164 -0.78281246  0.43316054  0.77970252 -0.3181273   0.00546249\n",
      "   1.23551927  0.08217044 -0.50383357]\n",
      " [-0.73447465  0.58187635 -0.62375598 -0.79068209  0.22822499 -0.25057227\n",
      "  -0.60154858 -0.24958246  0.66261775]\n",
      " [ 0.58058881 -0.24264747  0.10639483  0.38934358 -0.44331501  0.35065791\n",
      "   0.56208809 -0.10556108 -0.56016778]\n",
      " [ 0.57620909 -0.51995714  0.4622134   0.43096642 -0.34964389  0.22497766\n",
      "   0.77960999  0.71987849 -0.27865121]\n",
      " [-0.93058562  0.77255923 -0.6371518  -0.90831057  0.10756602  0.10745618\n",
      "  -0.93802455 -0.62648784  0.66912592]\n",
      " [ 1.05717188 -0.68804876  0.63461033  0.50826549 -0.20277816 -0.18136663\n",
      "   0.71851967  0.74252682 -0.4542308 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.13722518  1.51642889 -1.5728176   0.16574457  0.23344833 -2.03786283\n",
      "   0.90425121]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:45 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.67532774]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 45 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.82397416 -0.78281246  0.44099305  0.77970252 -0.3181273   0.00546249\n",
      "   1.23551927  0.09000296 -0.50383357]\n",
      " [-0.74243163  0.58187635 -0.63171296 -0.79068209  0.22822499 -0.25057227\n",
      "  -0.60154858 -0.25753944  0.66261775]\n",
      " [ 0.58534883 -0.24264747  0.11115485  0.38934358 -0.44331501  0.35065791\n",
      "   0.56208809 -0.10080106 -0.56016778]\n",
      " [ 0.58405679 -0.51995714  0.4700611   0.43096642 -0.34964389  0.22497766\n",
      "   0.77960999  0.72772619 -0.27865121]\n",
      " [-0.93763158  0.77255923 -0.64419776 -0.90831057  0.10756602  0.10745618\n",
      "  -0.93802455 -0.63353381  0.66912592]\n",
      " [ 1.06359223 -0.68804876  0.64103068  0.50826549 -0.20277816 -0.18136663\n",
      "   0.71851967  0.74894717 -0.4542308 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.10163133  1.54458662 -1.56687725  0.18857427  0.26380976 -2.03429383\n",
      "   0.93697642]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:45 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.52831182]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 45 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.83608669 -0.78281246  0.44099305  0.79181506 -0.3181273   0.00546249\n",
      "   1.23551927  0.09000296 -0.49172104]\n",
      " [-0.75307496  0.58187635 -0.63171296 -0.80132542  0.22822499 -0.25057227\n",
      "  -0.60154858 -0.25753944  0.65197442]\n",
      " [ 0.59118512 -0.24264747  0.11115485  0.39517987 -0.44331501  0.35065791\n",
      "   0.56208809 -0.10080106 -0.5543315 ]\n",
      " [ 0.59353264 -0.51995714  0.4700611   0.44044227 -0.34964389  0.22497766\n",
      "   0.77960999  0.72772619 -0.26917536]\n",
      " [-0.9500884   0.77255923 -0.64419776 -0.92076738  0.10756602  0.10745618\n",
      "  -0.93802455 -0.63353381  0.6566691 ]\n",
      " [ 1.07579083 -0.68804876  0.64103068  0.52046409 -0.20277816 -0.18136663\n",
      "   0.71851967  0.74894717 -0.44203219]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.04285935  1.58867916 -1.54953064  0.22396513  0.30355149 -2.02044567\n",
      "   0.98126395]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:45 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.24856327]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 45 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.83590277 -0.78281246  0.44099305  0.79181506 -0.31831123  0.00527856\n",
      "   1.23551927  0.09000296 -0.49190496]\n",
      " [-0.75236128  0.58187635 -0.63171296 -0.80132542  0.22893868 -0.24985859\n",
      "  -0.60154858 -0.25753944  0.65268811]\n",
      " [ 0.59150871 -0.24264747  0.11115485  0.39517987 -0.44299141  0.3509815\n",
      "   0.56208809 -0.10080106 -0.55400791]\n",
      " [ 0.59238525 -0.51995714  0.4700611   0.44044227 -0.35079129  0.22383027\n",
      "   0.77960999  0.72772619 -0.27032275]\n",
      " [-0.94963413  0.77255923 -0.64419776 -0.92076738  0.10802028  0.10791045\n",
      "  -0.93802455 -0.63353381  0.65712337]\n",
      " [ 1.07436457 -0.68804876  0.64103068  0.52046409 -0.20420442 -0.18279289\n",
      "   0.71851967  0.74894717 -0.44345845]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.06607262  1.57688857 -1.56042177  0.21268226  0.29078982 -2.03159758\n",
      "   0.9682162 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:45 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.09257903]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 45 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.83653941 -0.78217582  0.44099305  0.79181506 -0.31767459  0.0059152\n",
      "   1.23551927  0.09000296 -0.49126832]\n",
      " [-0.75278669  0.58145094 -0.63171296 -0.80132542  0.22851326 -0.250284\n",
      "  -0.60154858 -0.25753944  0.65226269]\n",
      " [ 0.59179131 -0.24236487  0.11115485  0.39517987 -0.44270881  0.3512641\n",
      "   0.56208809 -0.10080106 -0.5537253 ]\n",
      " [ 0.59269288 -0.51964952  0.4700611   0.44044227 -0.35048366  0.2241379\n",
      "   0.77960999  0.72772619 -0.27001512]\n",
      " [-0.950235    0.77195836 -0.64419776 -0.92076738  0.10741942  0.10730958\n",
      "  -0.93802455 -0.63353381  0.6565225 ]\n",
      " [ 1.07477574 -0.68763759  0.64103068  0.52046409 -0.20379325 -0.18238172\n",
      "   0.71851967  0.74894717 -0.44304728]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.06996132  1.57564256 -1.56280678  0.21102469  0.28915854 -2.03419249\n",
      "   0.96669667]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:45 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.49824518]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 45 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.82444176 -0.79427347  0.44099305  0.79181506 -0.32977223 -0.00618244\n",
      "   1.22342163  0.09000296 -0.49126832]\n",
      " [-0.74217863  0.592059   -0.63171296 -0.80132542  0.23912133 -0.23967594\n",
      "  -0.59094051 -0.25753944  0.65226269]\n",
      " [ 0.58095005 -0.25320614  0.11115485  0.39517987 -0.45355008  0.34042283\n",
      "   0.55124683 -0.10080106 -0.5537253 ]\n",
      " [ 0.58275372 -0.52958868  0.4700611   0.44044227 -0.36042282  0.21419874\n",
      "   0.76967083  0.72772619 -0.27001512]\n",
      " [-0.93870385  0.78348951 -0.64419776 -0.92076738  0.11895056  0.11884073\n",
      "  -0.92649341 -0.63353381  0.6565225 ]\n",
      " [ 1.06490667 -0.69750667  0.64103068  0.52046409 -0.21366233 -0.19225079\n",
      "   0.70865059  0.74894717 -0.44304728]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.1322412   1.53038154 -1.58218641  0.16778681  0.24718225 -2.0521744\n",
      "   0.92481388]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:45 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.77151584]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 45 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.82742511 -0.79427347  0.44099305  0.7947984  -0.32977223 -0.00618244\n",
      "   1.22640497  0.09000296 -0.49126832]\n",
      " [-0.74624502  0.592059   -0.63171296 -0.80539181  0.23912133 -0.23967594\n",
      "  -0.59500691 -0.25753944  0.65226269]\n",
      " [ 0.58545807 -0.25320614  0.11115485  0.39968789 -0.45355008  0.34042283\n",
      "   0.55575485 -0.10080106 -0.5537253 ]\n",
      " [ 0.58717132 -0.52958868  0.4700611   0.44485987 -0.36042282  0.21419874\n",
      "   0.77408843  0.72772619 -0.27001512]\n",
      " [-0.94177354  0.78348951 -0.64419776 -0.92383707  0.11895056  0.11884073\n",
      "  -0.9295631  -0.63353381  0.6565225 ]\n",
      " [ 1.06875152 -0.69750667  0.64103068  0.52430895 -0.21366233 -0.19225079\n",
      "   0.71249544  0.74894717 -0.44304728]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.1121027   1.54940805 -1.58005589  0.18433304  0.26444655 -2.05100461\n",
      "   0.9431073 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:45 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.65286907]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 45 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.83614184 -0.79427347  0.44099305  0.7947984  -0.3210555   0.00253429\n",
      "   1.23512171  0.09000296 -0.49126832]\n",
      " [-0.75491557  0.592059   -0.63171296 -0.80539181  0.23045079 -0.24834648\n",
      "  -0.60367745 -0.25753944  0.65226269]\n",
      " [ 0.59330533 -0.25320614  0.11115485  0.39968789 -0.44570281  0.3482701\n",
      "   0.56360211 -0.10080106 -0.5537253 ]\n",
      " [ 0.59560486 -0.52958868  0.4700611   0.44485987 -0.35198928  0.22263227\n",
      "   0.78252197  0.72772619 -0.27001512]\n",
      " [-0.950555    0.78348951 -0.64419776 -0.92383707  0.1101691   0.11005927\n",
      "  -0.93834456 -0.63353381  0.6565225 ]\n",
      " [ 1.07746433 -0.69750667  0.64103068  0.52430895 -0.20494951 -0.18353798\n",
      "   0.72120826  0.74894717 -0.44304728]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.07276738  1.5827587  -1.57190509  0.21330526  0.2947815  -2.04457937\n",
      "   0.97450635]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:45 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57752533]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 45 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.82138346 -0.79427347  0.44099305  0.78004002 -0.3210555  -0.01222409\n",
      "   1.23512171  0.09000296 -0.5060267 ]\n",
      " [-0.74008345  0.592059   -0.63171296 -0.7905597   0.23045079 -0.23351437\n",
      "  -0.60367745 -0.25753944  0.66709481]\n",
      " [ 0.58138038 -0.25320614  0.11115485  0.38776294 -0.44570281  0.33634515\n",
      "   0.56360211 -0.10080106 -0.56565026]\n",
      " [ 0.58180445 -0.52958868  0.4700611   0.43105946 -0.35198928  0.20883186\n",
      "   0.78252197  0.72772619 -0.28381553]\n",
      " [-0.93598783  0.78348951 -0.64419776 -0.9092699   0.1101691   0.12462644\n",
      "  -0.93834456 -0.63353381  0.67108968]\n",
      " [ 1.06380159 -0.69750667  0.64103068  0.5106462  -0.20494951 -0.19720072\n",
      "   0.72120826  0.74894717 -0.45671002]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.14322253  1.52934774 -1.58876667  0.16488127  0.24337064 -2.06207193\n",
      "   0.92334518]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:45 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.68586488]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 45 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.82843252 -0.7872244   0.44099305  0.78708908 -0.3210555  -0.00517503\n",
      "   1.24217077  0.09000296 -0.5060267 ]\n",
      " [-0.74752614  0.58461632 -0.63171296 -0.79800238  0.23045079 -0.24095705\n",
      "  -0.61112013 -0.25753944  0.66709481]\n",
      " [ 0.58894262 -0.2456439   0.11115485  0.39532518 -0.44570281  0.34390738\n",
      "   0.57116435 -0.10080106 -0.56565026]\n",
      " [ 0.58936744 -0.52202568  0.4700611   0.43862245 -0.35198928  0.21639486\n",
      "   0.79008497  0.72772619 -0.28381553]\n",
      " [-0.94330212  0.77617522 -0.64419776 -0.91658419  0.1101691   0.11731215\n",
      "  -0.94565885 -0.63353381  0.67108968]\n",
      " [ 1.07132042 -0.68998784  0.64103068  0.51816503 -0.20494951 -0.18968189\n",
      "   0.72872709  0.74894717 -0.45671002]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.10938166  1.55926023 -1.58386565  0.19311223  0.27090899 -2.0575742\n",
      "   0.95049687]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:45 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.71570656]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 45 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.83438832 -0.78126861  0.44099305  0.79304488 -0.3210555  -0.00517503\n",
      "   1.24812656  0.09000296 -0.5060267 ]\n",
      " [-0.75399924  0.57814322 -0.63171296 -0.80447549  0.23045079 -0.24095705\n",
      "  -0.61759324 -0.25753944  0.66709481]\n",
      " [ 0.59528252 -0.239304    0.11115485  0.40166508 -0.44570281  0.34390738\n",
      "   0.57750425 -0.10080106 -0.56565026]\n",
      " [ 0.59569039 -0.51570273  0.4700611   0.4449454  -0.35198928  0.21639486\n",
      "   0.79640792  0.72772619 -0.28381553]\n",
      " [-0.94932785  0.77014949 -0.64419776 -0.92260992  0.1101691   0.11731215\n",
      "  -0.95168458 -0.63353381  0.67108968]\n",
      " [ 1.07777934 -0.68352892  0.64103068  0.52462396 -0.20494951 -0.18968189\n",
      "   0.73518601  0.74894717 -0.45671002]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.08045897  1.58494356 -1.57889307  0.2158882   0.2936182  -2.05421473\n",
      "   0.97467411]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:45 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.80103404]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 45 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.83714425 -0.78126861  0.44374899  0.79304488 -0.3210555  -0.0024191\n",
      "   1.2508825   0.09000296 -0.5060267 ]\n",
      " [-0.7570829   0.57814322 -0.63479662 -0.80447549  0.23045079 -0.24404071\n",
      "  -0.6206769  -0.25753944  0.66709481]\n",
      " [ 0.59882339 -0.239304    0.11469571  0.40166508 -0.44570281  0.34744825\n",
      "   0.58104512 -0.10080106 -0.56565026]\n",
      " [ 0.59894759 -0.51570273  0.47331831  0.4449454  -0.35198928  0.21965206\n",
      "   0.79966512  0.72772619 -0.28381553]\n",
      " [-0.95219566  0.77014949 -0.64706557 -0.92260992  0.1101691   0.11444433\n",
      "  -0.95455239 -0.63353381  0.67108968]\n",
      " [ 1.08084043 -0.68352892  0.64409177  0.52462396 -0.20494951 -0.1866208\n",
      "   0.7382471   0.74894717 -0.45671002]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.06460352  1.59961647 -1.57737339  0.22914137  0.30771054 -2.05292933\n",
      "   0.98903721]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:45 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.33280511]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 46 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.83406816 -0.7843447   0.44374899  0.78996879 -0.3210555  -0.0024191\n",
      "   1.2508825   0.09000296 -0.50910279]\n",
      " [-0.75423286  0.58099326 -0.63479662 -0.80162545  0.23045079 -0.24404071\n",
      "  -0.6206769  -0.25753944  0.66994485]\n",
      " [ 0.59703434 -0.24109304  0.11469571  0.39987604 -0.44570281  0.34744825\n",
      "   0.58104512 -0.10080106 -0.5674393 ]\n",
      " [ 0.59672361 -0.51792671  0.47331831  0.44272142 -0.35198928  0.21965206\n",
      "   0.79966512  0.72772619 -0.28603952]\n",
      " [-0.94837317  0.77397198 -0.64706557 -0.91878743  0.1101691   0.11444433\n",
      "  -0.95455239 -0.63353381  0.67491217]\n",
      " [ 1.07676742 -0.68760193  0.64409177  0.52055094 -0.20494951 -0.1866208\n",
      "   0.7382471   0.74894717 -0.46078304]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.10155252  1.57800524 -1.59295008  0.20886641  0.28698986 -2.06746045\n",
      "   0.96634118]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:46 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.789951]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 46 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.83708292 -0.7843447   0.44676374  0.78996879 -0.3210555  -0.0024191\n",
      "   1.25389725  0.09000296 -0.50910279]\n",
      " [-0.75788285  0.58099326 -0.63844662 -0.80162545  0.23045079 -0.24404071\n",
      "  -0.62432689 -0.25753944  0.66994485]\n",
      " [ 0.60084153 -0.24109304  0.1185029   0.39987604 -0.44570281  0.34744825\n",
      "   0.58485231 -0.10080106 -0.5674393 ]\n",
      " [ 0.60049449 -0.51792671  0.47708919  0.44272142 -0.35198928  0.21965206\n",
      "   0.803436    0.72772619 -0.28603952]\n",
      " [-0.95135855  0.77397198 -0.65005095 -0.91878743  0.1101691   0.11444433\n",
      "  -0.95753777 -0.63353381  0.67491217]\n",
      " [ 1.0798773  -0.68760193  0.64720165  0.52055094 -0.20494951 -0.1866208\n",
      "   0.74135698  0.74894717 -0.46078304]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.08412597  1.59414448 -1.59089048  0.22253956  0.30208865 -2.06619829\n",
      "   0.98239495]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:46 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.7149759]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 46 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.82101013 -0.80041748  0.43069095  0.78996879 -0.3210555  -0.0024191\n",
      "   1.23782446  0.09000296 -0.50910279]\n",
      " [-0.74163811  0.597238   -0.62220188 -0.80162545  0.23045079 -0.24404071\n",
      "  -0.60808215 -0.25753944  0.66994485]\n",
      " [ 0.58606343 -0.25587114  0.1037248   0.39987604 -0.44570281  0.34744825\n",
      "   0.57007421 -0.10080106 -0.5674393 ]\n",
      " [ 0.58438477 -0.53403643  0.46097947  0.44272142 -0.35198928  0.21965206\n",
      "   0.78732628  0.72772619 -0.28603952]\n",
      " [-0.93535841  0.78997213 -0.63405081 -0.91878743  0.1101691   0.11444433\n",
      "  -0.94153763 -0.63353381  0.67491217]\n",
      " [ 1.06386713 -0.7036121   0.63119149  0.52055094 -0.20494951 -0.1866208\n",
      "   0.72534681  0.74894717 -0.46078304]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.15697678  1.53204783 -1.60484852  0.16839078  0.24408032 -2.07666622\n",
      "   0.92004923]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:46 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.67910593]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 46 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.82871717 -0.80041748  0.43839799  0.78996879 -0.3210555  -0.0024191\n",
      "   1.23782446  0.09770999 -0.50910279]\n",
      " [-0.7494493   0.597238   -0.63001306 -0.80162545  0.23045079 -0.24404071\n",
      "  -0.60808215 -0.26535063  0.66994485]\n",
      " [ 0.59078998 -0.25587114  0.10845135  0.39987604 -0.44570281  0.34744825\n",
      "   0.57007421 -0.09607451 -0.5674393 ]\n",
      " [ 0.59207766 -0.53403643  0.46867236  0.44272142 -0.35198928  0.21965206\n",
      "   0.78732628  0.73541908 -0.28603952]\n",
      " [-0.94225904  0.78997213 -0.64095144 -0.91878743  0.1101691   0.11444433\n",
      "  -0.94153763 -0.64043443  0.67491217]\n",
      " [ 1.0701478  -0.7036121   0.63747215  0.52055094 -0.20494951 -0.1866208\n",
      "   0.72534681  0.75522784 -0.46078304]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.122012    1.55976687 -1.59907881  0.19087778  0.27396967 -2.07318769\n",
      "   0.95222111]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:46 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.52961493]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 46 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.84084006 -0.80041748  0.43839799  0.80209169 -0.3210555  -0.0024191\n",
      "   1.23782446  0.09770999 -0.49697989]\n",
      " [-0.76014269  0.597238   -0.63001306 -0.81231883  0.23045079 -0.24404071\n",
      "  -0.60808215 -0.26535063  0.65925146]\n",
      " [ 0.59671986 -0.25587114  0.10845135  0.40580592 -0.44570281  0.34744825\n",
      "   0.57007421 -0.09607451 -0.56150942]\n",
      " [ 0.60164123 -0.53403643  0.46867236  0.45228499 -0.35198928  0.21965206\n",
      "   0.78732628  0.73541908 -0.27647595]\n",
      " [-0.95471429  0.78997213 -0.64095144 -0.93124268  0.1101691   0.11444433\n",
      "  -0.94153763 -0.64043443  0.66245691]\n",
      " [ 1.08236601 -0.7036121   0.63747215  0.53276916 -0.20494951 -0.1866208\n",
      "   0.72534681  0.75522784 -0.44856482]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.06342014  1.60383096 -1.5819147   0.22628221  0.31374818 -2.05948007\n",
      "   0.9965062 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:46 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.23973106]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 46 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.84072874 -0.80041748  0.43839799  0.80209169 -0.32116683 -0.00253042\n",
      "   1.23782446  0.09770999 -0.49709122]\n",
      " [-0.75951947  0.597238   -0.63001306 -0.81231883  0.231074   -0.2434175\n",
      "  -0.60808215 -0.26535063  0.65987467]\n",
      " [ 0.59706384 -0.25587114  0.10845135  0.40580592 -0.44535883  0.34779223\n",
      "   0.57007421 -0.09607451 -0.56116543]\n",
      " [ 0.60059779 -0.53403643  0.46867236  0.45228499 -0.35303272  0.21860862\n",
      "   0.78732628  0.73541908 -0.27751938]\n",
      " [-0.95434526  0.78997213 -0.64095144 -0.93124268  0.11053813  0.11481336\n",
      "  -0.94153763 -0.64043443  0.66282594]\n",
      " [ 1.08106224 -0.7036121   0.63747215  0.53276916 -0.20625328 -0.18792457\n",
      "   0.72534681  0.75522784 -0.44986859]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.08526684  1.59279627 -1.59221347  0.21570307  0.30177491 -2.07003411\n",
      "   0.9842663 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:46 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.08556112]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 46 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.84129167 -0.79985455  0.43839799  0.80209169 -0.3206039  -0.00196749\n",
      "   1.23782446  0.09770999 -0.49652829]\n",
      " [-0.75990253  0.59685494 -0.63001306 -0.81231883  0.23069094 -0.24380055\n",
      "  -0.60808215 -0.26535063  0.65949161]\n",
      " [ 0.59732297 -0.25561201  0.10845135  0.40580592 -0.4450997   0.34805136\n",
      "   0.57007421 -0.09607451 -0.56090631]\n",
      " [ 0.60087836 -0.53375587  0.46867236  0.45228499 -0.35275215  0.21888919\n",
      "   0.78732628  0.73541908 -0.27723882]\n",
      " [-0.95487806  0.78943933 -0.64095144 -0.93124268  0.11000534  0.11428057\n",
      "  -0.94153763 -0.64043443  0.66229315]\n",
      " [ 1.08143219 -0.70324216  0.63747215  0.53276916 -0.20588334 -0.18755463\n",
      "   0.72534681  0.75522784 -0.44949865]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.08861401  1.59174454 -1.59428532  0.21429299  0.3003875  -2.07228824\n",
      "   0.98297622]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:46 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.49123166]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 46 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.82951629 -0.81162993  0.43839799  0.80209169 -0.33237928 -0.01374287\n",
      "   1.22604909  0.09770999 -0.49652829]\n",
      " [-0.74954349  0.60721398 -0.63001306 -0.81231883  0.24104998 -0.23344152\n",
      "  -0.59772311 -0.26535063  0.65949161]\n",
      " [ 0.586685   -0.26624998  0.10845135  0.40580592 -0.45573767  0.33741339\n",
      "   0.55943624 -0.09607451 -0.56090631]\n",
      " [ 0.59113987 -0.54349436  0.46867236  0.45228499 -0.36249064  0.2091507\n",
      "   0.77758779  0.73541908 -0.27723882]\n",
      " [-0.9436623   0.80065509 -0.64095144 -0.93124268  0.12122109  0.12549632\n",
      "  -0.93032187 -0.64043443  0.66229315]\n",
      " [ 1.07180083 -0.71287351  0.63747215  0.53276916 -0.21551469 -0.19718598\n",
      "   0.71571546  0.75522784 -0.44949865]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.14999908  1.54739636 -1.61352383  0.17174589  0.25909155 -2.09025078\n",
      "   0.94182226]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:46 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.77722196]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 46 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.83234588 -0.81162993  0.43839799  0.80492128 -0.33237928 -0.01374287\n",
      "   1.22887868  0.09770999 -0.49652829]\n",
      " [-0.75340622  0.60721398 -0.63001306 -0.81618156  0.24104998 -0.23344152\n",
      "  -0.60158584 -0.26535063  0.65949161]\n",
      " [ 0.59100265 -0.26624998  0.10845135  0.41012357 -0.45573767  0.33741339\n",
      "   0.56375389 -0.09607451 -0.56090631]\n",
      " [ 0.59535109 -0.54349436  0.46867236  0.45649621 -0.36249064  0.2091507\n",
      "   0.78179901  0.73541908 -0.27723882]\n",
      " [-0.94657247  0.80065509 -0.64095144 -0.93415285  0.12122109  0.12549632\n",
      "  -0.93323204 -0.64043443  0.66229315]\n",
      " [ 1.07544598 -0.71287351  0.63747215  0.5364143  -0.21551469 -0.19718598\n",
      "   0.7193606   0.75522784 -0.44949865]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.1307123   1.56563612 -1.61152883  0.18766119  0.27569145 -2.08915061\n",
      "   0.95938371]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:46 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.65492146]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 46 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.84098963 -0.81162993  0.43839799  0.80492128 -0.32373553 -0.00509913\n",
      "   1.23752243  0.09770999 -0.49652829]\n",
      " [-0.76200902  0.60721398 -0.63001306 -0.81618156  0.23244719 -0.24204431\n",
      "  -0.61018863 -0.26535063  0.65949161]\n",
      " [ 0.59881392 -0.26624998  0.10845135  0.41012357 -0.44792641  0.34522466\n",
      "   0.57156515 -0.09607451 -0.56090631]\n",
      " [ 0.60373168 -0.54349436  0.46867236  0.45649621 -0.35411005  0.21753129\n",
      "   0.7901796   0.73541908 -0.27723882]\n",
      " [-0.95527792  0.80065509 -0.64095144 -0.93415285  0.11251564  0.11679087\n",
      "  -0.94193749 -0.64043443  0.66229315]\n",
      " [ 1.08409061 -0.71287351  0.63747215  0.5364143  -0.20687006 -0.18854135\n",
      "   0.72800524  0.75522784 -0.44949865]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.09171853  1.59868322 -1.60348448  0.21644487  0.30582323 -2.08277872\n",
      "   0.99055255]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:46 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57824586]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 46 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.82620864 -0.81162993  0.43839799  0.79014029 -0.32373553 -0.01988011\n",
      "   1.23752243  0.09770999 -0.51130928]\n",
      " [-0.74714318  0.60721398 -0.63001306 -0.80131573  0.23244719 -0.22717848\n",
      "  -0.61018863 -0.26535063  0.67435745]\n",
      " [ 0.58681874 -0.26624998  0.10845135  0.39812839 -0.44792641  0.33322948\n",
      "   0.57156515 -0.09607451 -0.57290149]\n",
      " [ 0.58986458 -0.54349436  0.46867236  0.44262911 -0.35411005  0.20366419\n",
      "   0.7901796   0.73541908 -0.29110593]\n",
      " [-0.94068457  0.80065509 -0.64095144 -0.9195595   0.11251564  0.13138422\n",
      "  -0.94193749 -0.64043443  0.6768865 ]\n",
      " [ 1.0703605  -0.71287351  0.63747215  0.52268419 -0.20687006 -0.20227147\n",
      "   0.72800524  0.75522784 -0.46322876]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.16222914  1.54520338 -1.62030353  0.1678962   0.25426864 -2.10025169\n",
      "   0.93924925]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:46 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.6877556]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 46 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.833207   -0.80463157  0.43839799  0.79713865 -0.32373553 -0.01288175\n",
      "   1.24452078  0.09770999 -0.51130928]\n",
      " [-0.75451372  0.59984344 -0.63001306 -0.80868627  0.23244719 -0.23454901\n",
      "  -0.61755917 -0.26535063  0.67435745]\n",
      " [ 0.59430785 -0.25876087  0.10845135  0.4056175  -0.44792641  0.34071859\n",
      "   0.57905426 -0.09607451 -0.57290149]\n",
      " [ 0.59736033 -0.53599861  0.46867236  0.45012486 -0.35411005  0.21115993\n",
      "   0.79767535  0.73541908 -0.29110593]\n",
      " [-0.94793873  0.79340092 -0.64095144 -0.92681366  0.11251564  0.12413006\n",
      "  -0.94919165 -0.64043443  0.6768865 ]\n",
      " [ 1.07781352 -0.70542049  0.63747215  0.53013722 -0.20687006 -0.19481844\n",
      "   0.73545826  0.75522784 -0.46322876]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.12870223  1.57481182 -1.61545974  0.19590042  0.28159312 -2.095775\n",
      "   0.96617535]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:46 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.71830676]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 46 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.83907599 -0.79876258  0.43839799  0.80300765 -0.32373553 -0.01288175\n",
      "   1.25038978  0.09770999 -0.51130928]\n",
      " [-0.76089074  0.59346642 -0.63001306 -0.81506328  0.23244719 -0.23454901\n",
      "  -0.62393619 -0.26535063  0.67435745]\n",
      " [ 0.60056691 -0.25250181  0.10845135  0.41187656 -0.44792641  0.34071859\n",
      "   0.58531332 -0.09607451 -0.57290149]\n",
      " [ 0.60360666 -0.52975227  0.46867236  0.45637119 -0.35411005  0.21115993\n",
      "   0.80392169  0.73541908 -0.29110593]\n",
      " [-0.95387431  0.78746534 -0.64095144 -0.93274924  0.11251564  0.12413006\n",
      "  -0.95512723 -0.64043443  0.6768865 ]\n",
      " [ 1.08417409 -0.69905992  0.63747215  0.53649779 -0.20687006 -0.19481844\n",
      "   0.74181884  0.75522784 -0.46322876]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.10020302  1.60011845 -1.6105959   0.21839251  0.30403262 -2.09246815\n",
      "   0.99003663]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:46 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.80330163]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 46 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.8417812  -0.79876258  0.44110319  0.80300765 -0.32373553 -0.01017655\n",
      "   1.25309498  0.09770999 -0.51130928]\n",
      " [-0.76390741  0.59346642 -0.63302974 -0.81506328  0.23244719 -0.23756569\n",
      "  -0.62695286 -0.26535063  0.67435745]\n",
      " [ 0.60403582 -0.25250181  0.11192026  0.41187656 -0.44792641  0.3441875\n",
      "   0.58878224 -0.09607451 -0.57290149]\n",
      " [ 0.60679068 -0.52975227  0.47185637  0.45637119 -0.35411005  0.21434395\n",
      "   0.8071057   0.73541908 -0.29110593]\n",
      " [-0.95668755  0.78746534 -0.64376467 -0.93274924  0.11251564  0.12131682\n",
      "  -0.95794047 -0.64043443  0.6768865 ]\n",
      " [ 1.08716946 -0.69905992  0.64046751  0.53649779 -0.20687006 -0.19182308\n",
      "   0.7448142   0.75522784 -0.46322876]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.08466305  1.6144958  -1.60911335  0.2313973   0.31785805 -2.09120592\n",
      "   1.00411967]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:46 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.3252029]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 47 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.83887743 -0.80166635  0.44110319  0.80010388 -0.32373553 -0.01017655\n",
      "   1.25309498  0.09770999 -0.51421304]\n",
      " [-0.76119792  0.59617592 -0.63302974 -0.81235379  0.23244719 -0.23756569\n",
      "  -0.62695286 -0.26535063  0.67706694]\n",
      " [ 0.6023517  -0.25418593  0.11192026  0.41019244 -0.44792641  0.3441875\n",
      "   0.58878224 -0.09607451 -0.57458561]\n",
      " [ 0.60466061 -0.53188234  0.47185637  0.45424112 -0.35411005  0.21434395\n",
      "   0.8071057   0.73541908 -0.29323599]\n",
      " [-0.95306183  0.79109107 -0.64376467 -0.92912352  0.11251564  0.12131682\n",
      "  -0.95794047 -0.64043443  0.68051222]\n",
      " [ 1.08326519 -0.70296418  0.64046751  0.53259352 -0.20687006 -0.19182308\n",
      "   0.7448142   0.75522784 -0.46713303]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.12034529  1.59369639 -1.62420104  0.21186186  0.29786596 -2.10531113\n",
      "   0.98223429]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:47 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.79330242]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 47 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.84180327 -0.80166635  0.44402903  0.80010388 -0.32373553 -0.01017655\n",
      "   1.25602082  0.09770999 -0.51421304]\n",
      " [-0.76473644  0.59617592 -0.63656826 -0.81235379  0.23244719 -0.23756569\n",
      "  -0.63049138 -0.26535063  0.67706694]\n",
      " [ 0.60606161 -0.25418593  0.11563017  0.41019244 -0.44792641  0.3441875\n",
      "   0.59249215 -0.09607451 -0.57458561]\n",
      " [ 0.60831734 -0.53188234  0.4755131   0.45424112 -0.35411005  0.21434395\n",
      "   0.81076243  0.73541908 -0.29323599]\n",
      " [-0.95595854  0.79109107 -0.64666139 -0.92912352  0.11251564  0.12131682\n",
      "  -0.96083718 -0.64043443  0.68051222]\n",
      " [ 1.08627694 -0.70296418  0.64347927  0.53259352 -0.20687006 -0.19182308\n",
      "   0.74782595  0.75522784 -0.46713303]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.1033988   1.60939609 -1.62221836  0.2251877   0.31257596 -2.10408917\n",
      "   0.99785738]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:47 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.71455342]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 47 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.8256979  -0.81777171  0.42792367  0.80010388 -0.32373553 -0.01017655\n",
      "   1.23991546  0.09770999 -0.51421304]\n",
      " [-0.74849206  0.6124203  -0.62032387 -0.81235379  0.23244719 -0.23756569\n",
      "  -0.614247   -0.26535063  0.67706694]\n",
      " [ 0.59130001 -0.26894753  0.10086857  0.41019244 -0.44792641  0.3441875\n",
      "   0.57773055 -0.09607451 -0.57458561]\n",
      " [ 0.59220367 -0.54799601  0.45939943  0.45424112 -0.35411005  0.21434395\n",
      "   0.79464876  0.73541908 -0.29323599]\n",
      " [-0.93992355  0.80712606 -0.63062639 -0.92912352  0.11251564  0.12131682\n",
      "  -0.94480219 -0.64043443  0.68051222]\n",
      " [ 1.07024721 -0.71899391  0.62744954  0.53259352 -0.20687006 -0.19182308\n",
      "   0.73179622  0.75522784 -0.46713303]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.1762714   1.54740258 -1.63622626  0.17106594  0.2545548  -2.11467379\n",
      "   0.93554904]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:47 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.68280198]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 47 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.83328076 -0.81777171  0.43550652  0.80010388 -0.32373553 -0.01017655\n",
      "   1.23991546  0.10529284 -0.51421304]\n",
      " [-0.75616025  0.6124203  -0.62799207 -0.81235379  0.23244719 -0.23756569\n",
      "  -0.614247   -0.27301882  0.67706694]\n",
      " [ 0.59598987 -0.26894753  0.10555843  0.41019244 -0.44792641  0.3441875\n",
      "   0.57773055 -0.09138466 -0.57458561]\n",
      " [ 0.59974554 -0.54799601  0.4669413   0.45424112 -0.35411005  0.21434395\n",
      "   0.79464876  0.74296096 -0.29323599]\n",
      " [-0.94668385  0.80712606 -0.63738669 -0.92912352  0.11251564  0.12131682\n",
      "  -0.94480219 -0.64719473  0.68051222]\n",
      " [ 1.07639368 -0.71899391  0.633596    0.53259352 -0.20687006 -0.19182308\n",
      "   0.73179622  0.7613743  -0.46713303]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.14192148  1.57468835 -1.63061828  0.19321346  0.28397763 -2.11128111\n",
      "   0.96717759]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:47 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.53102632]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 47 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.84540856 -0.81777171  0.43550652  0.81223168 -0.32373553 -0.01017655\n",
      "   1.23991546  0.10529284 -0.50208524]\n",
      " [-0.76689648  0.6124203  -0.62799207 -0.82309001  0.23244719 -0.23756569\n",
      "  -0.614247   -0.27301882  0.66633072]\n",
      " [ 0.6020062  -0.26894753  0.10555843  0.41620877 -0.44792641  0.3441875\n",
      "   0.57773055 -0.09138466 -0.56856928]\n",
      " [ 0.60938802 -0.54799601  0.4669413   0.4638836  -0.35411005  0.21434395\n",
      "   0.79464876  0.74296096 -0.28359352]\n",
      " [-0.95913232  0.80712606 -0.63738669 -0.941572    0.11251564  0.12131682\n",
      "  -0.94480219 -0.64719473  0.66806375]\n",
      " [ 1.08862445 -0.71899391  0.633596    0.54482429 -0.20687006 -0.19182308\n",
      "   0.73179622  0.7613743  -0.45490226]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.0835255   1.61870929 -1.61363606  0.22861632  0.32377547 -2.09771495\n",
      "   1.01144291]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:47 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.23123295]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 47 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.84536021 -0.81777171  0.43550652  0.81223168 -0.32378389 -0.01022491\n",
      "   1.23991546  0.10529284 -0.50213359]\n",
      " [-0.76635497  0.6124203  -0.62799207 -0.82309001  0.23298869 -0.23702418\n",
      "  -0.614247   -0.27301882  0.66687222]\n",
      " [ 0.60236697 -0.26894753  0.10555843  0.41620877 -0.44756563  0.34454828\n",
      "   0.57773055 -0.09138466 -0.5682085 ]\n",
      " [ 0.6084404  -0.54799601  0.4669413   0.4638836  -0.35505766  0.21339634\n",
      "   0.79464876  0.74296096 -0.28454113]\n",
      " [-0.95883848  0.80712606 -0.63738669 -0.941572    0.11280949  0.12161066\n",
      "  -0.94480219 -0.64719473  0.66835759]\n",
      " [ 1.08743337 -0.71899391  0.633596    0.54482429 -0.20806114 -0.19301416\n",
      "   0.73179622  0.7613743  -0.45609334]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.10407798  1.60838469 -1.62336979  0.21870115  0.31254614 -2.10769718\n",
      "   0.99996459]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:47 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.07907371]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 47 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.84585655 -0.81727537  0.43550652  0.81223168 -0.32328755 -0.00972856\n",
      "   1.23991546  0.10529284 -0.50163725]\n",
      " [-0.76669854  0.61207673 -0.62799207 -0.82309001  0.23264512 -0.23736775\n",
      "  -0.614247   -0.27301882  0.66652865]\n",
      " [ 0.60260331 -0.26871119  0.10555843  0.41620877 -0.44732929  0.34478462\n",
      "   0.57773055 -0.09138466 -0.56797217]\n",
      " [ 0.60869506 -0.54774135  0.4669413   0.4638836  -0.35480301  0.21365099\n",
      "   0.79464876  0.74296096 -0.28428648]\n",
      " [-0.95930947  0.80665506 -0.63738669 -0.941572    0.11233849  0.12113967\n",
      "  -0.94480219 -0.64719473  0.6678866 ]\n",
      " [ 1.08776495 -0.71866233  0.633596    0.54482429 -0.20772956 -0.19268258\n",
      "   0.73179622  0.7613743  -0.45576176]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.10695709  1.60749735 -1.62516794  0.21750245  0.31136695 -2.10965328\n",
      "   0.99886997]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:47 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.48400646]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 47 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.83441326 -0.82871866  0.43550652  0.81223168 -0.33473084 -0.02117186\n",
      "   1.22847217  0.10529284 -0.50163725]\n",
      " [-0.75659784  0.62217743 -0.62799207 -0.82309001  0.24274582 -0.22726706\n",
      "  -0.6041463  -0.27301882  0.66652865]\n",
      " [ 0.59217937 -0.27913513  0.10555843  0.41620877 -0.45775323  0.33436068\n",
      "   0.56730661 -0.09138466 -0.56797217]\n",
      " [ 0.59916822 -0.55726819  0.4669413   0.4638836  -0.36432984  0.20412416\n",
      "   0.78512193  0.74296096 -0.28428648]\n",
      " [-0.94841727  0.81754727 -0.63738669 -0.941572    0.12323069  0.13203187\n",
      "  -0.93390999 -0.64719473  0.6678866 ]\n",
      " [ 1.07837991 -0.72804737  0.633596    0.54482429 -0.2171146  -0.20206762\n",
      "   0.72241118  0.7613743  -0.45576176]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.167396    1.56409098 -1.64424867  0.17568399  0.27078925 -2.12757395\n",
      "   0.95847891]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:47 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.78274231]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 47 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.83709787 -0.82871866  0.43550652  0.8149163  -0.33473084 -0.02117186\n",
      "   1.23115678  0.10529284 -0.50163725]\n",
      " [-0.76026768  0.62217743 -0.62799207 -0.82675985  0.24274582 -0.22726706\n",
      "  -0.60781614 -0.27301882  0.66652865]\n",
      " [ 0.59631345 -0.27913513  0.10555843  0.42034285 -0.45775323  0.33436068\n",
      "   0.57144069 -0.09138466 -0.56797217]\n",
      " [ 0.60318216 -0.55726819  0.4669413   0.46789753 -0.36432984  0.20412416\n",
      "   0.78913586  0.74296096 -0.28428648]\n",
      " [-0.95117712  0.81754727 -0.63738669 -0.94433185  0.12323069  0.13203187\n",
      "  -0.93666984 -0.64719473  0.6678866 ]\n",
      " [ 1.08183655 -0.72804737  0.633596    0.54828094 -0.2171146  -0.20206762\n",
      "   0.72586783  0.7613743  -0.45576176]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.14892293  1.58157764 -1.64237898  0.19099066  0.28674837 -2.1265386\n",
      "   0.97533716]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:47 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.65694899]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 47 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.84566941 -0.82871866  0.43550652  0.8149163  -0.3261593  -0.01260032\n",
      "   1.23972832  0.10529284 -0.50163725]\n",
      " [-0.76880279  0.62217743 -0.62799207 -0.82675985  0.23421071 -0.23580217\n",
      "  -0.61635125 -0.27301882  0.66652865]\n",
      " [ 0.60408683 -0.27913513  0.10555843  0.42034285 -0.44997985  0.34213406\n",
      "   0.57921407 -0.09138466 -0.56797217]\n",
      " [ 0.61150866 -0.55726819  0.4669413   0.46789753 -0.35600334  0.21245066\n",
      "   0.79746236  0.74296096 -0.28428648]\n",
      " [-0.95980747  0.81754727 -0.63738669 -0.94433185  0.11460034  0.12340152\n",
      "  -0.94530019 -0.64719473  0.6678866 ]\n",
      " [ 1.09041307 -0.72804737  0.633596    0.54828094 -0.20853809 -0.1934911\n",
      "   0.73444434  0.7613743  -0.45576176]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.11026674  1.61432489 -1.63443726  0.21958429  0.31667553 -2.12021922\n",
      "   1.00627589]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:47 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57899739]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 47 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.83086551 -0.82871866  0.43550652  0.80011241 -0.3261593  -0.02740422\n",
      "   1.23972832  0.10529284 -0.51644115]\n",
      " [-0.75390436  0.62217743 -0.62799207 -0.81186142  0.23421071 -0.22090373\n",
      "  -0.61635125 -0.27301882  0.68142709]\n",
      " [ 0.59202549 -0.27913513  0.10555843  0.40828151 -0.44997985  0.33007272\n",
      "   0.57921407 -0.09138466 -0.5800335 ]\n",
      " [ 0.59757808 -0.55726819  0.4669413   0.45396695 -0.35600334  0.19852008\n",
      "   0.79746236  0.74296096 -0.29821705]\n",
      " [-0.94518779  0.81754727 -0.63738669 -0.92971216  0.11460034  0.1380212\n",
      "  -0.94530019 -0.64719473  0.68250628]\n",
      " [ 1.0766182  -0.72804737  0.633596    0.53448607 -0.20853809 -0.20728597\n",
      "   0.73444434  0.7613743  -0.46955663]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.18083477  1.56077482 -1.65121761  0.17091541  0.26498132 -2.13767349\n",
      "   0.95483338]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:47 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.68955516]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 47 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.83781612 -0.82176805  0.43550652  0.80706301 -0.3261593  -0.02045361\n",
      "   1.24667893  0.10529284 -0.51644115]\n",
      " [-0.76120681  0.61487498 -0.62799207 -0.81916387  0.23421071 -0.22820619\n",
      "  -0.6236537  -0.27301882  0.68142709]\n",
      " [ 0.59944496 -0.27171567  0.10555843  0.41570098 -0.44997985  0.33749218\n",
      "   0.58663354 -0.09138466 -0.5800335 ]\n",
      " [ 0.60500936 -0.54983691  0.4669413   0.46139823 -0.35600334  0.20595136\n",
      "   0.80489364  0.74296096 -0.29821705]\n",
      " [-0.95238514  0.81034991 -0.63738669 -0.93690952  0.11460034  0.13082385\n",
      "  -0.95249755 -0.64719473  0.68250628]\n",
      " [ 1.08400814 -0.72065744  0.633596    0.54187601 -0.20853809 -0.19989604\n",
      "   0.74183428  0.7613743  -0.46955663]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.14760648  1.59009279 -1.64642612  0.19870198  0.29209949 -2.13321545\n",
      "   0.98154227]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:47 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.72084093]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 47 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.84360108 -0.81598309  0.43550652  0.81284797 -0.3261593  -0.02045361\n",
      "   1.25246388  0.10529284 -0.51644115]\n",
      " [-0.7674902   0.60859158 -0.62799207 -0.82544726  0.23421071 -0.22820619\n",
      "  -0.6299371  -0.27301882  0.68142709]\n",
      " [ 0.60562421 -0.26553642  0.10555843  0.42188022 -0.44997985  0.33749218\n",
      "   0.59281278 -0.09138466 -0.5800335 ]\n",
      " [ 0.61117937 -0.5436669   0.4669413   0.46756824 -0.35600334  0.20595136\n",
      "   0.81106365  0.74296096 -0.29821705]\n",
      " [-0.95823353  0.80450153 -0.63738669 -0.9427579   0.11460034  0.13082385\n",
      "  -0.95834593 -0.64719473  0.68250628]\n",
      " [ 1.09027301 -0.71439256  0.633596    0.54814088 -0.20853809 -0.19989604\n",
      "   0.74809915  0.7613743  -0.46955663]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.11951899  1.61503257 -1.64166519  0.22091501  0.31427246 -2.12995896\n",
      "   1.00509344]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:47 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.80547369]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 47 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.84625866 -0.81598309  0.4381641   0.81284797 -0.3261593  -0.01779603\n",
      "   1.25512147  0.10529284 -0.51644115]\n",
      " [-0.77044402  0.60859158 -0.63094589 -0.82544726  0.23421071 -0.23116001\n",
      "  -0.63289092 -0.27301882  0.68142709]\n",
      " [ 0.6090247  -0.26553642  0.10895892  0.42188022 -0.44997985  0.34089268\n",
      "   0.59621328 -0.09138466 -0.5800335 ]\n",
      " [ 0.61429455 -0.5436669   0.47005649  0.46756824 -0.35600334  0.20906654\n",
      "   0.81417883  0.74296096 -0.29821705]\n",
      " [-0.96099556  0.80450153 -0.64014873 -0.9427579   0.11460034  0.12806181\n",
      "  -0.96110796 -0.64719473  0.68250628]\n",
      " [ 1.09320677 -0.71439256  0.63652976  0.54814088 -0.20853809 -0.19696228\n",
      "   0.7510329   0.7613743  -0.46955663]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.10427924  1.62912809 -1.64021685  0.23368192  0.32784239 -2.12871806\n",
      "   1.01890886]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:47 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.31775515]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 48 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.84351945 -0.8187223   0.4381641   0.81010876 -0.3261593  -0.01779603\n",
      "   1.25512147  0.10529284 -0.51918036]\n",
      " [-0.76787092  0.61116469 -0.63094589 -0.82287416  0.23421071 -0.23116001\n",
      "  -0.63289092 -0.27301882  0.68400019]\n",
      " [ 0.60744248 -0.26711865  0.10895892  0.420298   -0.44997985  0.34089268\n",
      "   0.59621328 -0.09138466 -0.58161573]\n",
      " [ 0.61225765 -0.54570381  0.47005649  0.46553133 -0.35600334  0.20906654\n",
      "   0.81417883  0.74296096 -0.30025396]\n",
      " [-0.95755852  0.80793857 -0.64014873 -0.93932086  0.11460034  0.12806181\n",
      "  -0.96110796 -0.64719473  0.68594332]\n",
      " [ 1.08946729 -0.71813204  0.63652976  0.54440141 -0.20853809 -0.19696228\n",
      "   0.7510329   0.7613743  -0.4732961 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.1387218   1.60911862 -1.65482471  0.21486934  0.3085646  -2.14240195\n",
      "   0.99781634]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:48 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.79653914]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 48 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.84636108 -0.8187223   0.44100573  0.81010876 -0.3261593  -0.01779603\n",
      "   1.25796309  0.10529284 -0.51918036]\n",
      " [-0.77130357  0.61116469 -0.63437854 -0.82287416  0.23421071 -0.23116001\n",
      "  -0.63632357 -0.27301882  0.68400019]\n",
      " [ 0.61105834 -0.26711865  0.11257479  0.420298   -0.44997985  0.34089268\n",
      "   0.59982915 -0.09138466 -0.58161573]\n",
      " [ 0.61580567 -0.54570381  0.47360451  0.46553133 -0.35600334  0.20906654\n",
      "   0.81772686  0.74296096 -0.30025396]\n",
      " [-0.96037137  0.80793857 -0.64296158 -0.93932086  0.11460034  0.12806181\n",
      "  -0.96392082 -0.64719473  0.68594332]\n",
      " [ 1.09238645 -0.71813204  0.63944891  0.54440141 -0.20853809 -0.19696228\n",
      "   0.75395206  0.7613743  -0.4732961 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.1222349   1.62439672 -1.65291366  0.22786019  0.32289986 -2.14121759\n",
      "   1.01302576]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:48 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.71397682]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 48 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.83022233 -0.83486105  0.42486698  0.81010876 -0.3261593  -0.01779603\n",
      "   1.24182435  0.10529284 -0.51918036]\n",
      " [-0.75505907  0.62740919 -0.61813404 -0.82287416  0.23421071 -0.23116001\n",
      "  -0.62007907 -0.27301882  0.68400019]\n",
      " [ 0.59631561 -0.28186138  0.09783206  0.420298   -0.44997985  0.34089268\n",
      "   0.58508642 -0.09138466 -0.58161573]\n",
      " [ 0.59968843 -0.56182105  0.45748727  0.46553133 -0.35600334  0.20906654\n",
      "   0.80160961  0.74296096 -0.30025396]\n",
      " [-0.94430024  0.8240097  -0.62689045 -0.93932086  0.11460034  0.12806181\n",
      "  -0.94784969 -0.64719473  0.68594332]\n",
      " [ 1.0763341  -0.73418438  0.62339657  0.54440141 -0.20853809 -0.19696228\n",
      "   0.73789971  0.7613743  -0.4732961 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.19513691  1.56250453 -1.66698139  0.17376755  0.26487041 -2.15192515\n",
      "   0.95075536]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:48 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.68641095]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 48 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.83768273 -0.83486105  0.43232738  0.81010876 -0.3261593  -0.01779603\n",
      "   1.24182435  0.11275324 -0.51918036]\n",
      " [-0.76258745  0.62740919 -0.62566242 -0.82287416  0.23421071 -0.23116001\n",
      "  -0.62007907 -0.2805472   0.68400019]\n",
      " [ 0.6009661  -0.28186138  0.10248255  0.420298   -0.44997985  0.34089268\n",
      "   0.58508642 -0.08673417 -0.58161573]\n",
      " [ 0.60708336 -0.56182105  0.4648822   0.46553133 -0.35600334  0.20906654\n",
      "   0.80160961  0.75035589 -0.30025396]\n",
      " [-0.95092525  0.8240097  -0.63351545 -0.93932086  0.11460034  0.12806181\n",
      "  -0.94784969 -0.65381973  0.68594332]\n",
      " [ 1.08235179 -0.73418438  0.62941426  0.54440141 -0.20853809 -0.19696228\n",
      "   0.73789971  0.76739199 -0.4732961 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.16138674  1.58936365 -1.66152663  0.19557988  0.29383364 -2.14861387\n",
      "   0.98185175]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:48 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.53253373]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 48 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.84981037 -0.83486105  0.43232738  0.8222364  -0.3261593  -0.01779603\n",
      "   1.24182435  0.11275324 -0.50705272]\n",
      " [-0.77335977  0.62740919 -0.62566242 -0.83364648  0.23421071 -0.23116001\n",
      "  -0.62007907 -0.2805472   0.67322787]\n",
      " [ 0.60706208 -0.28186138  0.10248255  0.42639398 -0.44997985  0.34089268\n",
      "   0.58508642 -0.08673417 -0.57551975]\n",
      " [ 0.61679646 -0.56182105  0.4648822   0.47524443 -0.35600334  0.20906654\n",
      "   0.80160961  0.75035589 -0.29054086]\n",
      " [-0.96336211  0.8240097  -0.63351545 -0.95175772  0.11460034  0.12806181\n",
      "  -0.94784969 -0.65381973  0.67350646]\n",
      " [ 1.0945886  -0.73418438  0.62941426  0.55663822 -0.20853809 -0.19696228\n",
      "   0.73789971  0.76739199 -0.46105929]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.10320085  1.63332789 -1.64472544  0.2309671   0.33363461 -2.13518976\n",
      "   1.02608127]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:48 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.22306648]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 48 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.84981616 -0.83486105  0.43232738  0.8222364  -0.32615352 -0.01779024\n",
      "   1.24182435  0.11275324 -0.50704693]\n",
      " [-0.77289174  0.62740919 -0.62566242 -0.83364648  0.23467874 -0.23069198\n",
      "  -0.62007907 -0.2805472   0.6736959 ]\n",
      " [ 0.60743625 -0.28186138  0.10248255  0.42639398 -0.44960569  0.34126684\n",
      "   0.58508642 -0.08673417 -0.57514558]\n",
      " [ 0.61593685 -0.56182105  0.4648822   0.47524443 -0.35686295  0.20820693\n",
      "   0.80160961  0.75035589 -0.29140047]\n",
      " [-0.96313418  0.8240097  -0.63351545 -0.95175772  0.11482827  0.12828974\n",
      "  -0.94784969 -0.65381973  0.67373439]\n",
      " [ 1.09350088 -0.73418438  0.62941426  0.55663822 -0.20962581 -0.19805001\n",
      "   0.73789971  0.76739199 -0.46214701]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.12253043  1.62366889 -1.65392146  0.22167685  0.3231056  -2.14462654\n",
      "   1.01531931]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:48 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.0730856]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 48 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.85025273 -0.83442447  0.43232738  0.8222364  -0.32571694 -0.01735366\n",
      "   1.24182435  0.11275324 -0.50661035]\n",
      " [-0.77319885  0.62710208 -0.62566242 -0.83364648  0.23437163 -0.23099909\n",
      "  -0.62007907 -0.2805472   0.67338879]\n",
      " [ 0.60765081 -0.28164682  0.10248255  0.42639398 -0.44939113  0.34148141\n",
      "   0.58508642 -0.08673417 -0.57493102]\n",
      " [ 0.61616703 -0.56159088  0.4648822   0.47524443 -0.35663277  0.20843711\n",
      "   0.80160961  0.75035589 -0.29117029]\n",
      " [-0.96354947  0.8235944  -0.63351545 -0.95175772  0.11441297  0.12787445\n",
      "  -0.94784969 -0.65381973  0.67331909]\n",
      " [ 1.0937971  -0.73388816  0.62941426  0.55663822 -0.20932959 -0.19775378\n",
      "   0.73789971  0.76739199 -0.46185079]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.12500599  1.62292035 -1.65548106  0.22065824  0.32210375 -2.14632277\n",
      "   1.01439079]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:48 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.47660705]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 48 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.83914915 -0.84552805  0.43232738  0.8222364  -0.33682052 -0.02845725\n",
      "   1.23072076  0.11275324 -0.50661035]\n",
      " [-0.76336393  0.63693699 -0.62566242 -0.83364648  0.24420654 -0.22116417\n",
      "  -0.61024416 -0.2805472   0.67338879]\n",
      " [ 0.59744995 -0.29184768  0.10248255  0.42639398 -0.45959199  0.33128055\n",
      "   0.57488556 -0.08673417 -0.57493102]\n",
      " [ 0.60686102 -0.57089689  0.4648822   0.47524443 -0.36593878  0.1991311\n",
      "   0.7923036   0.75035589 -0.29117029]\n",
      " [-0.9529869   0.83415697 -0.63351545 -0.95175772  0.12497554  0.13843702\n",
      "  -0.93728712 -0.65381973  0.67331909]\n",
      " [ 1.08466519 -0.74302007  0.62941426  0.55663822 -0.2184615  -0.2068857\n",
      "   0.7287678   0.76739199 -0.46185079]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.18445146  1.58048004 -1.67438697  0.17960196  0.28227785 -2.16417865\n",
      "   0.97479248]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:48 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.78808216]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 48 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.84169705 -0.84552805  0.43232738  0.8247843  -0.33682052 -0.02845725\n",
      "   1.23326866  0.11275324 -0.50661035]\n",
      " [-0.76685118  0.63693699 -0.62566242 -0.83713373  0.24420654 -0.22116417\n",
      "  -0.61373141 -0.2805472   0.67338879]\n",
      " [ 0.60140737 -0.29184768  0.10248255  0.4303514  -0.45959199  0.33128055\n",
      "   0.57884298 -0.08673417 -0.57493102]\n",
      " [ 0.61068663 -0.57089689  0.4648822   0.47907005 -0.36593878  0.1991311\n",
      "   0.79612921  0.75035589 -0.29117029]\n",
      " [-0.95560511  0.83415697 -0.63351545 -0.95437592  0.12497554  0.13843702\n",
      "  -0.93990532 -0.65381973  0.67331909]\n",
      " [ 1.08794397 -0.74302007  0.62941426  0.559917   -0.2184615  -0.2068857\n",
      "   0.73204658  0.76739199 -0.46185079]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.1667554   1.59724625 -1.67263328  0.19432226  0.29761976 -2.16320369\n",
      "   0.99097581]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:48 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.6589608]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 48 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.85019677 -0.84552805  0.43232738  0.8247843  -0.3283208  -0.01995753\n",
      "   1.24176838  0.11275324 -0.50661035]\n",
      " [-0.7753185   0.63693699 -0.62566242 -0.83713373  0.23573923 -0.22963149\n",
      "  -0.62219872 -0.2805472   0.67338879]\n",
      " [ 0.60914105 -0.29184768  0.10248255  0.4303514  -0.4518583   0.33901423\n",
      "   0.58657666 -0.08673417 -0.57493102]\n",
      " [ 0.61895788 -0.57089689  0.4648822   0.47907005 -0.35766754  0.20740235\n",
      "   0.80440046  0.75035589 -0.29117029]\n",
      " [-0.9641609   0.83415697 -0.63351545 -0.95437592  0.11641975  0.12988122\n",
      "  -0.94846111 -0.65381973  0.67331909]\n",
      " [ 1.09645223 -0.74302007  0.62941426  0.559917   -0.20995324 -0.19837743\n",
      "   0.74055485  0.76739199 -0.46185079]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.12843428  1.62969642 -1.66479123  0.22272385  0.32734029 -2.15693653\n",
      "   1.02168377]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:48 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57977704]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 48 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.83536964 -0.84552805  0.43232738  0.80995718 -0.3283208  -0.03478465\n",
      "   1.24176838  0.11275324 -0.52143748]\n",
      " [-0.76038847  0.63693699 -0.62566242 -0.8222037   0.23573923 -0.21470146\n",
      "  -0.62219872 -0.2805472   0.68831882]\n",
      " [ 0.59701728 -0.29184768  0.10248255  0.41822763 -0.4518583   0.32689045\n",
      "   0.58657666 -0.08673417 -0.58705479]\n",
      " [ 0.60496678 -0.57089689  0.4648822   0.46507895 -0.35766754  0.19341125\n",
      "   0.80440046  0.75035589 -0.30516139]\n",
      " [-0.94951471  0.83415697 -0.63351545 -0.93972973  0.11641975  0.14452741\n",
      "  -0.94846111 -0.65381973  0.68796528]\n",
      " [ 1.08259503 -0.74302007  0.62941426  0.54605979 -0.20995324 -0.21223464\n",
      "   0.74055485  0.76739199 -0.475708  ]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.19906145  1.57607479 -1.68153629  0.17393892  0.27551033 -2.17437278\n",
      "   0.97010481]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:48 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.69127117]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 48 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.84227507 -0.83862263  0.43232738  0.8168626  -0.3283208  -0.02787923\n",
      "   1.2486738   0.11275324 -0.52143748]\n",
      " [-0.76762652  0.62969894 -0.62566242 -0.82944175  0.23573923 -0.22193951\n",
      "  -0.62943677 -0.2805472   0.68831882]\n",
      " [ 0.60437032 -0.28449463  0.10248255  0.42558067 -0.4518583   0.3342435\n",
      "   0.5939297  -0.08673417 -0.58705479]\n",
      " [ 0.61233621 -0.56352746  0.4648822   0.47244838 -0.35766754  0.20078068\n",
      "   0.81176989  0.75035589 -0.30516139]\n",
      " [-0.9566582   0.82701349 -0.63351545 -0.94687322  0.11641975  0.13738392\n",
      "  -0.9556046  -0.65381973  0.68796528]\n",
      " [ 1.08992442 -0.73569068  0.62941426  0.55338918 -0.20995324 -0.20490525\n",
      "   0.74788424  0.76739199 -0.475708  ]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.16611772  1.60511493 -1.67679279  0.20151638  0.30242934 -2.1699314\n",
      "   0.99660438]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:48 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.72331273]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 48 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.84797853 -0.83291917  0.43232738  0.82256606 -0.3283208  -0.02787923\n",
      "   1.25437726  0.11275324 -0.52143748]\n",
      " [-0.77381867  0.62350678 -0.62566242 -0.8356339   0.23573923 -0.22193951\n",
      "  -0.63562893 -0.2805472   0.68831882]\n",
      " [ 0.61047087 -0.27839408  0.10248255  0.43168122 -0.4518583   0.3342435\n",
      "   0.60003026 -0.08673417 -0.58705479]\n",
      " [ 0.61843038 -0.55743329  0.4648822   0.47854255 -0.35766754  0.20078068\n",
      "   0.81786406  0.75035589 -0.30516139]\n",
      " [-0.96242209  0.8212496  -0.63351545 -0.95263711  0.11641975  0.13738392\n",
      "  -0.96136849 -0.65381973  0.68796528]\n",
      " [ 1.09609616 -0.72951893  0.62941426  0.55956093 -0.20995324 -0.20490525\n",
      "   0.75405598  0.76739199 -0.475708  ]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.13843081  1.62969737 -1.67212953  0.22345518  0.32433912 -2.16672328\n",
      "   1.01985125]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:48 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.8075532]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 48 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.85059139 -0.83291917  0.43494024  0.82256606 -0.3283208  -0.02526637\n",
      "   1.25699012  0.11275324 -0.52143748]\n",
      " [-0.7767135   0.62350678 -0.62855725 -0.8356339   0.23573923 -0.22483434\n",
      "  -0.63852376 -0.2805472   0.68831882]\n",
      " [ 0.61380634 -0.27839408  0.10581801  0.43168122 -0.4518583   0.33757897\n",
      "   0.60336573 -0.08673417 -0.58705479]\n",
      " [ 0.62148083 -0.55743329  0.46793265  0.47854255 -0.35766754  0.20383113\n",
      "   0.82091451  0.75035589 -0.30516139]\n",
      " [-0.96513607  0.8212496  -0.63622943 -0.95263711  0.11641975  0.13466995\n",
      "  -0.96408247 -0.65381973  0.68796528]\n",
      " [ 1.09897215 -0.72951893  0.63229025  0.55956093 -0.20995324 -0.20202926\n",
      "   0.75693197  0.76739199 -0.475708  ]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.12347663  1.64352426 -1.67071273  0.23599449  0.33766469 -2.16550203\n",
      "   1.03341105]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:48 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.31046145]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 49 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.84800904 -0.83550152  0.43494024  0.81998371 -0.3283208  -0.02526637\n",
      "   1.25699012  0.11275324 -0.52401983]\n",
      " [-0.77427237  0.62594792 -0.62855725 -0.83319277  0.23573923 -0.22483434\n",
      "  -0.63852376 -0.2805472   0.69075995]\n",
      " [ 0.61232268 -0.27987774  0.10581801  0.43019756 -0.4518583   0.33757897\n",
      "   0.60336573 -0.08673417 -0.58853846]\n",
      " [ 0.61953587 -0.55937825  0.46793265  0.47659759 -0.35766754  0.20383113\n",
      "   0.82091451  0.75035589 -0.30710635]\n",
      " [-0.96187965  0.82450601 -0.63622943 -0.9493807   0.11641975  0.13466995\n",
      "  -0.96408247 -0.65381973  0.6912217 ]\n",
      " [ 1.09539322 -0.73309787  0.63229025  0.55598199 -0.20995324 -0.20202926\n",
      "   0.75693197  0.76739199 -0.47928693]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.15670767  1.62428243 -1.6848502   0.21788728  0.31908588 -2.17876978\n",
      "   1.01309276]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:49 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.79966282]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 49 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.8507709  -0.83550152  0.43770211  0.81998371 -0.3283208  -0.02526637\n",
      "   1.25975198  0.11275324 -0.52401983]\n",
      " [-0.77760453  0.62594792 -0.63188941 -0.83319277  0.23573923 -0.22483434\n",
      "  -0.64185592 -0.2805472   0.69075995]\n",
      " [ 0.61584782 -0.27987774  0.10934315  0.43019756 -0.4518583   0.33757897\n",
      "   0.60689086 -0.08673417 -0.58853846]\n",
      " [ 0.62298047 -0.55937825  0.47137726  0.47659759 -0.35766754  0.20383113\n",
      "   0.82435912  0.75035589 -0.30710635]\n",
      " [-0.96461319  0.82450601 -0.63896297 -0.9493807   0.11641975  0.13466995\n",
      "  -0.96681601 -0.65381973  0.6912217 ]\n",
      " [ 1.09822501 -0.73309787  0.63512204  0.55598199 -0.20995324 -0.20202926\n",
      "   0.75976376  0.76739199 -0.47928693]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.14066044  1.63915652 -1.68300591  0.23055548  0.33306041 -2.17762061\n",
      "   1.02790526]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:49 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.71324714]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 49 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.83459831 -0.85167411  0.42152952  0.81998371 -0.3283208  -0.02526637\n",
      "   1.2435794   0.11275324 -0.52401983]\n",
      " [-0.76135955  0.6421929  -0.61564443 -0.83319277  0.23573923 -0.22483434\n",
      "  -0.62561094 -0.2805472   0.69075995]\n",
      " [ 0.60112606 -0.2945995   0.09462139  0.43019756 -0.4518583   0.33757897\n",
      "   0.5921691  -0.08673417 -0.58853846]\n",
      " [ 0.60685996 -0.57549876  0.45525675  0.47659759 -0.35766754  0.20383113\n",
      "   0.80823861  0.75035589 -0.30710635]\n",
      " [-0.94850502  0.84061418 -0.6228548  -0.9493807   0.11641975  0.13466995\n",
      "  -0.95070784 -0.65381973  0.6912217 ]\n",
      " [ 1.08214734 -0.74917553  0.61904437  0.55598199 -0.20995324 -0.20202926\n",
      "   0.7436861   0.76739199 -0.47928693]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Theta two: \n",
      "[[-1.21359911  1.57736367 -1.6971426   0.17649349  0.2750266  -2.18845688\n",
      "   0.96567316]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:49 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.68992885]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 49 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.84193836 -0.85167411  0.42886957  0.81998371 -0.3283208  -0.02526637\n",
      "   1.2435794   0.12009329 -0.52401983]\n",
      " [-0.76875157  0.6421929  -0.62303645 -0.83319277  0.23573923 -0.22483434\n",
      "  -0.62561094 -0.28793922  0.69075995]\n",
      " [ 0.605735   -0.2945995   0.09923034  0.43019756 -0.4518583   0.33757897\n",
      "   0.5921691  -0.08212522 -0.58853846]\n",
      " [ 0.61411226 -0.57549876  0.46250904  0.47659759 -0.35766754  0.20383113\n",
      "   0.80823861  0.75760819 -0.30710635]\n",
      " [-0.95499976  0.84061418 -0.62934954 -0.9493807   0.11641975  0.13466995\n",
      "  -0.95070784 -0.66031447  0.6912217 ]\n",
      " [ 1.08804161 -0.74917553  0.62493864  0.55598199 -0.20995324 -0.20202926\n",
      "   0.7436861   0.77328626 -0.47928693]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.18043281  1.60380382 -1.69183298  0.19797582  0.30353835 -2.18522277\n",
      "   0.9962495 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:49 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.5341253]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 49 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.85406111 -0.85167411  0.42886957  0.83210646 -0.3283208  -0.02526637\n",
      "   1.2435794   0.12009329 -0.51189708]\n",
      " [-0.77955371  0.6421929  -0.62303645 -0.84399491  0.23573923 -0.22483434\n",
      "  -0.62561094 -0.28793922  0.67995782]\n",
      " [ 0.61190419 -0.2945995   0.09923034  0.43636675 -0.4518583   0.33757897\n",
      "   0.5921691  -0.08212522 -0.58236927]\n",
      " [ 0.62388821 -0.57549876  0.46250904  0.48637353 -0.35766754  0.20383113\n",
      "   0.80823861  0.75760819 -0.2973304 ]\n",
      " [-0.96742055  0.84061418 -0.62934954 -0.96180148  0.11641975  0.13466995\n",
      "  -0.95070784 -0.66031447  0.67880091]\n",
      " [ 1.10027846 -0.74917553  0.62493864  0.56821884 -0.20995324 -0.20202926\n",
      "   0.7436861   0.77328626 -0.46705008]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.12246974  1.64769892 -1.67521163  0.23333443  0.34332747 -2.17194096\n",
      "   1.04042851]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:49 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.2152273]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 49 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.85411302 -0.85167411  0.42886957  0.83210646 -0.3282689  -0.02521446\n",
      "   1.2435794   0.12009329 -0.51184517]\n",
      " [-0.77915148  0.6421929  -0.62303645 -0.84399491  0.23614146 -0.22443211\n",
      "  -0.62561094 -0.28793922  0.68036005]\n",
      " [ 0.61228859 -0.2945995   0.09923034  0.43636675 -0.45147391  0.33796336\n",
      "   0.5921691  -0.08212522 -0.58198487]\n",
      " [ 0.62310917 -0.57549876  0.46250904  0.48637353 -0.35844658  0.20305209\n",
      "   0.80823861  0.75760819 -0.29810944]\n",
      " [-0.96725007  0.84061418 -0.62934954 -0.96180148  0.11659023  0.13484043\n",
      "  -0.95070784 -0.66031447  0.67897139]\n",
      " [ 1.09928529 -0.74917553  0.62493864  0.56821884 -0.2109464  -0.20302242\n",
      "   0.7436861   0.77328626 -0.46804324]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.14064617  1.63866261 -1.68389709  0.22463107  0.33345633 -2.18085865\n",
      "   1.03033901]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:49 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.06756521]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 49 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.85449626 -0.85129086  0.42886957  0.83210646 -0.32788565 -0.02483121\n",
      "   1.2435794   0.12009329 -0.51146193]\n",
      " [-0.77942521  0.64191917 -0.62303645 -0.84399491  0.23586773 -0.22470584\n",
      "  -0.62561094 -0.28793922  0.68008632]\n",
      " [ 0.61248261 -0.29440549  0.09923034  0.43636675 -0.45127989  0.33815738\n",
      "   0.5921691  -0.08212522 -0.58179085]\n",
      " [ 0.62331648 -0.57529145  0.46250904  0.48637353 -0.35823927  0.2032594\n",
      "   0.80823861  0.75760819 -0.29790213]\n",
      " [-0.96761547  0.84024877 -0.62934954 -0.96180148  0.11622483  0.13447502\n",
      "  -0.95070784 -0.66031447  0.67860599]\n",
      " [ 1.0995492  -0.74891163  0.62493864  0.56821884 -0.21068249 -0.20275851\n",
      "   0.7436861   0.77328626 -0.46777934]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.14277448  1.63803107 -1.68524924  0.22376558  0.33260523 -2.18232884\n",
      "   1.02955138]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:49 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.46906955]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 49 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.84373783 -0.86204929  0.42886957  0.83210646 -0.33864408 -0.03558964\n",
      "   1.23282097  0.12009329 -0.51146193]\n",
      " [-0.76986171  0.65148266 -0.62303645 -0.84399491  0.24543122 -0.21514234\n",
      "  -0.61604744 -0.28793922  0.68008632]\n",
      " [ 0.60251221 -0.30437588  0.09923034  0.43636675 -0.46125028  0.32818699\n",
      "   0.58219871 -0.08212522 -0.58179085]\n",
      " [ 0.61423871 -0.58436922  0.46250904  0.48637353 -0.36731703  0.19418163\n",
      "   0.79916084  0.75760819 -0.29790213]\n",
      " [-0.95738658  0.85047766 -0.62934954 -0.96180148  0.12645371  0.14470391\n",
      "  -0.94047895 -0.66031447  0.67860599]\n",
      " [ 1.09067554 -0.75778529  0.62493864  0.56821884 -0.21955615 -0.21163218\n",
      "   0.73481244  0.77328626 -0.46777934]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.2011838   1.59657629 -1.70396332  0.1835006   0.29356022 -2.20009698\n",
      "   0.99077135]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:49 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.79324661]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 49 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.84615678 -0.86204929  0.42886957  0.83452541 -0.33864408 -0.03558964\n",
      "   1.23523992  0.12009329 -0.51146193]\n",
      " [-0.7731762   0.65148266 -0.62303645 -0.8473094   0.24543122 -0.21514234\n",
      "  -0.61936193 -0.28793922  0.68008632]\n",
      " [ 0.60629992 -0.30437588  0.09923034  0.44015446 -0.46125028  0.32818699\n",
      "   0.58598642 -0.08212522 -0.58179085]\n",
      " [ 0.61788481 -0.58436922  0.46250904  0.49001964 -0.36731703  0.19418163\n",
      "   0.80280694  0.75760819 -0.29790213]\n",
      " [-0.95987129  0.85047766 -0.62934954 -0.96428619  0.12645371  0.14470391\n",
      "  -0.94296366 -0.66031447  0.67860599]\n",
      " [ 1.09378654 -0.75778529  0.62493864  0.57132984 -0.21955615 -0.21163218\n",
      "   0.73792344  0.77328626 -0.46777934]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.18422935  1.61265373 -1.7023171   0.1976566   0.3083083  -2.19917833\n",
      "   1.00630749]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:49 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.66096468]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 49 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.85458475 -0.86204929  0.42886957  0.83452541 -0.33021612 -0.02716168\n",
      "   1.24366788  0.12009329 -0.51146193]\n",
      " [-0.78157544  0.65148266 -0.62303645 -0.8473094   0.23703198 -0.22354158\n",
      "  -0.62776117 -0.28793922  0.68008632]\n",
      " [ 0.61399217 -0.30437588  0.09923034  0.44015446 -0.45355804  0.33587923\n",
      "   0.59367866 -0.08212522 -0.58179085]\n",
      " [ 0.62609958 -0.58436922  0.46250904  0.49001964 -0.35910226  0.20239641\n",
      "   0.81102171  0.75760819 -0.29790213]\n",
      " [-0.96835276  0.85047766 -0.62934954 -0.96428619  0.11797224  0.13622244\n",
      "  -0.95144513 -0.66031447  0.67860599]\n",
      " [ 1.10222623 -0.75778529  0.62493864  0.57132984 -0.21111646 -0.20319248\n",
      "   0.74636313  0.77328626 -0.46777934]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.14624208  1.64480874 -1.69457244  0.22586375  0.33781973 -2.19296358\n",
      "   1.03678342]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:49 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.5805812]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 49 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.83973411 -0.86204929  0.42886957  0.81967477 -0.33021612 -0.04201232\n",
      "   1.24366788  0.12009329 -0.52631256]\n",
      " [-0.76661475  0.65148266 -0.62303645 -0.83234871  0.23703198 -0.20858089\n",
      "  -0.62776117 -0.28793922  0.69504701]\n",
      " [ 0.60180937 -0.30437588  0.09923034  0.42797167 -0.45355804  0.32369644\n",
      "   0.59367866 -0.08212522 -0.59397364]\n",
      " [ 0.6120507  -0.58436922  0.46250904  0.47597075 -0.35910226  0.18834752\n",
      "   0.81102171  0.75760819 -0.31195102]\n",
      " [-0.95367991  0.85047766 -0.62934954 -0.94961333  0.11797224  0.15089529\n",
      "  -0.95144513 -0.66031447  0.69327884]\n",
      " [ 1.08830892 -0.75778529  0.62493864  0.55741253 -0.21111646 -0.21710979\n",
      "   0.74636313  0.77328626 -0.48169665]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.21692978  1.59111438 -1.71128529  0.17696672  0.28585776 -2.21038228\n",
      "   0.98507067]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:49 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.69291036]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 49 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.84659661 -0.8551868   0.42886957  0.82653726 -0.33021612 -0.03514982\n",
      "   1.25053037  0.12009329 -0.52631256]\n",
      " [-0.77379175  0.64430566 -0.62303645 -0.83952571  0.23703198 -0.21575789\n",
      "  -0.63493817 -0.28793922  0.69504701]\n",
      " [ 0.60909898 -0.29708627  0.09923034  0.43526128 -0.45355804  0.33098605\n",
      "   0.60096827 -0.08212522 -0.59397364]\n",
      " [ 0.61936077 -0.57705915  0.46250904  0.48328082 -0.35910226  0.19565759\n",
      "   0.81833178  0.75760819 -0.31195102]\n",
      " [-0.96077216  0.84338541 -0.62934954 -0.95670558  0.11797224  0.14380304\n",
      "  -0.95853738 -0.66031447  0.69327884]\n",
      " [ 1.09558016 -0.75051404  0.62493864  0.56468377 -0.21111646 -0.20983855\n",
      "   0.75363437  0.77328626 -0.48169665]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.18425765  1.61988851 -1.70658601  0.20434312  0.31258437 -2.20595592\n",
      "   1.01136838]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:49 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.72572527]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 49 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.8522209  -0.8495625   0.42886957  0.83216156 -0.33021612 -0.03514982\n",
      "   1.25615467  0.12009329 -0.52631256]\n",
      " [-0.77989497  0.63820244 -0.62303645 -0.84562893  0.23703198 -0.21575789\n",
      "  -0.6410414  -0.28793922  0.69504701]\n",
      " [ 0.61512202 -0.29106323  0.09923034  0.44128432 -0.45355804  0.33098605\n",
      "   0.60699131 -0.08212522 -0.59397364]\n",
      " [ 0.62537975 -0.57104017  0.46250904  0.4892998  -0.35910226  0.19565759\n",
      "   0.82435076  0.75760819 -0.31195102]\n",
      " [-0.96645407  0.8377035  -0.62934954 -0.96238749  0.11797224  0.14380304\n",
      "  -0.96421929 -0.66031447  0.69327884]\n",
      " [ 1.10166125 -0.74443295  0.62493864  0.57076486 -0.21111646 -0.20983855\n",
      "   0.75971546  0.77328626 -0.48169665]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.15696072  1.64412277 -1.70201567  0.22601257  0.33423446 -2.2027944\n",
      "   1.03431669]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:49 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.80954296]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 49 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.85479175 -0.8495625   0.43144042  0.83216156 -0.33021612 -0.03257897\n",
      "   1.25872552  0.12009329 -0.52631256]\n",
      " [-0.78273443  0.63820244 -0.62587591 -0.84562893  0.23703198 -0.21859735\n",
      "  -0.64388086 -0.28793922  0.69504701]\n",
      " [ 0.61839573 -0.29106323  0.10250404  0.44128432 -0.45355804  0.33425976\n",
      "   0.61026502 -0.08212522 -0.59397364]\n",
      " [ 0.62836934 -0.57104017  0.46549864  0.4892998  -0.35910226  0.19864718\n",
      "   0.82734035  0.75760819 -0.31195102]\n",
      " [-0.96912294  0.8377035  -0.63201841 -0.96238749  0.11797224  0.14113417\n",
      "  -0.96688816 -0.66031447  0.69327884]\n",
      " [ 1.1044831  -0.74443295  0.62776049  0.57076486 -0.21111646 -0.2070167\n",
      "   0.76253732  0.77328626 -0.48169665]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.14227808  1.65769376 -1.70062797  0.23833432  0.3473265  -2.20159125\n",
      "   1.04763252]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:49 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.30332041]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 50 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.8523587  -0.85199556  0.43144042  0.8297285  -0.33021612 -0.03257897\n",
      "   1.25872552  0.12009329 -0.52874561]\n",
      " [-0.78042069  0.64051618 -0.62587591 -0.84331519  0.23703198 -0.21859735\n",
      "  -0.64388086 -0.28793922  0.69736075]\n",
      " [ 0.61700709 -0.29245187  0.10250404  0.43989568 -0.45355804  0.33425976\n",
      "   0.61026502 -0.08212522 -0.59536229]\n",
      " [ 0.62651473 -0.57289478  0.46549864  0.48744519 -0.35910226  0.19864718\n",
      "   0.82734035  0.75760819 -0.31380563]\n",
      " [-0.9660392   0.84078724 -0.63201841 -0.95930376  0.11797224  0.14113417\n",
      "  -0.96688816 -0.66031447  0.69636258]\n",
      " [ 1.10106025 -0.74785581  0.62776049  0.56734201 -0.21111646 -0.2070167\n",
      "   0.76253732  0.77328626 -0.48511951]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.17432649  1.63919707 -1.7143047   0.22091441  0.32943062 -2.2144486\n",
      "   1.02806922]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:50 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.80267537]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 50 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.85504502 -0.85199556  0.43412674  0.8297285  -0.33021612 -0.03257897\n",
      "   1.26141184  0.12009329 -0.52874561]\n",
      " [-0.7836575   0.64051618 -0.62911272 -0.84331519  0.23703198 -0.21859735\n",
      "  -0.64711767 -0.28793922  0.69736075]\n",
      " [ 0.62044484 -0.29245187  0.10594179  0.43989568 -0.45355804  0.33425976\n",
      "   0.61370277 -0.08212522 -0.59536229]\n",
      " [ 0.629861   -0.57289478  0.46884491  0.48744519 -0.35910226  0.19864718\n",
      "   0.83068663  0.75760819 -0.31380563]\n",
      " [-0.96869772  0.84078724 -0.63467693 -0.95930376  0.11797224  0.14113417\n",
      "  -0.96954668 -0.66031447  0.69636258]\n",
      " [ 1.10380961 -0.74785581  0.63050985  0.56734201 -0.21111646 -0.2070167\n",
      "   0.76528667  0.77328626 -0.48511951]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.1586996   1.65368429 -1.71252269  0.23327221  0.3430583  -2.21333239\n",
      "   1.04250118]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:50 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.71236533]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 50 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.83883846 -0.86820212  0.41792018  0.8297285  -0.33021612 -0.03257897\n",
      "   1.24520528  0.12009329 -0.52874561]\n",
      " [-0.76741181  0.65676187 -0.61286702 -0.84331519  0.23703198 -0.21859735\n",
      "  -0.63087197 -0.28793922  0.69736075]\n",
      " [ 0.60574594 -0.30715077  0.09124289  0.43989568 -0.45355804  0.33425976\n",
      "   0.59900387 -0.08212522 -0.59536229]\n",
      " [ 0.61373755 -0.58901824  0.45272146  0.48744519 -0.35910226  0.19864718\n",
      "   0.81456317  0.75760819 -0.31380563]\n",
      " [-0.95255197  0.85693298 -0.61853118 -0.95930376  0.11797224  0.14113417\n",
      "  -0.95340093 -0.66031447  0.69636258]\n",
      " [ 1.08770428 -0.76396113  0.61440452  0.56734201 -0.21111646 -0.2070167\n",
      "   0.74918135  0.77328626 -0.48511951]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.23168177  1.59198879 -1.72673671  0.17924197  0.28502362 -2.22430269\n",
      "   0.98030772]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:50 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.69335257]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 50 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.84606057 -0.86820212  0.4251423   0.8297285  -0.33021612 -0.03257897\n",
      "   1.24520528  0.12731541 -0.52874561]\n",
      " [-0.77467118  0.65676187 -0.62012639 -0.84331519  0.23703198 -0.21859735\n",
      "  -0.63087197 -0.29519859  0.69736075]\n",
      " [ 0.61031162 -0.30715077  0.09580857  0.43989568 -0.45355804  0.33425976\n",
      "   0.59900387 -0.07755955 -0.59536229]\n",
      " [ 0.62085168 -0.58901824  0.45983559  0.48744519 -0.35910226  0.19864718\n",
      "   0.81456317  0.76472231 -0.31380563]\n",
      " [-0.95892146  0.85693298 -0.62490067 -0.95930376  0.11797224  0.14113417\n",
      "  -0.95340093 -0.66668396  0.69636258]\n",
      " [ 1.09348038 -0.76396113  0.62018062  0.56734201 -0.21111646 -0.2070167\n",
      "   0.74918135  0.77906235 -0.48511951]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.19908288  1.61801851 -1.72156451  0.20040026  0.313093   -2.2211417\n",
      "   1.01037689]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:50 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.53578965]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 50 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.85817405 -0.86820212  0.4251423   0.84184198 -0.33021612 -0.03257897\n",
      "   1.24520528  0.12731541 -0.51663213]\n",
      " [-0.78549729  0.65676187 -0.62012639 -0.8541413   0.23703198 -0.21859735\n",
      "  -0.63087197 -0.29519859  0.68653464]\n",
      " [ 0.61654791 -0.30715077  0.09580857  0.44613197 -0.45355804  0.33425976\n",
      "   0.59900387 -0.07755955 -0.589126  ]\n",
      " [ 0.63068317 -0.58901824  0.45983559  0.49727669 -0.35910226  0.19864718\n",
      "   0.81456317  0.76472231 -0.30397413]\n",
      " [-0.97132206  0.85693298 -0.62490067 -0.97170436  0.11797224  0.14113417\n",
      "  -0.95340093 -0.66668396  0.68396197]\n",
      " [ 1.10571175 -0.76396113  0.62018062  0.57957338 -0.21111646 -0.2070167\n",
      "   0.74918135  0.77906235 -0.47288814]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.14135389  1.66183311 -1.70512153  0.23571832  0.35285644 -2.20800207\n",
      "   1.0544919 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:50 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.20770935]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 50 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.85826485 -0.86820212  0.4251423   0.84184198 -0.33012532 -0.03248817\n",
      "   1.24520528  0.12731541 -0.51654133]\n",
      " [-0.78515377  0.65676187 -0.62012639 -0.8541413   0.2373755  -0.21825383\n",
      "  -0.63087197 -0.29519859  0.68687816]\n",
      " [ 0.61693964 -0.30715077  0.09580857  0.44613197 -0.4531663   0.33465149\n",
      "   0.59900387 -0.07755955 -0.58873426]\n",
      " [ 0.6299777  -0.58901824  0.45983559  0.49727669 -0.35980774  0.19794171\n",
      "   0.81456317  0.76472231 -0.3046796 ]\n",
      " [-0.97120136  0.85693298 -0.62490067 -0.97170436  0.11809294  0.14125487\n",
      "  -0.95340093 -0.66668396  0.68408267]\n",
      " [ 1.10480492 -0.76396113  0.62018062  0.57957338 -0.21202329 -0.20792352\n",
      "   0.74918135  0.77906235 -0.47379496]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.15844486  1.65337844 -1.71332313  0.22756512  0.34360222 -2.21642684\n",
      "   1.04503261]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:50 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.06248108]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 50 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.85860074 -0.86786623  0.4251423   0.84184198 -0.32978943 -0.03215229\n",
      "   1.24520528  0.12731541 -0.51620545]\n",
      " [-0.78539715  0.65651849 -0.62012639 -0.8541413   0.23713212 -0.21849721\n",
      "  -0.63087197 -0.29519859  0.68663478]\n",
      " [ 0.61711448 -0.30697593  0.09580857  0.44613197 -0.45299146  0.33482633\n",
      "   0.59900387 -0.07755955 -0.58855942]\n",
      " [ 0.63016385 -0.58883209  0.45983559  0.49727669 -0.35962159  0.19812786\n",
      "   0.81456317  0.76472231 -0.30449346]\n",
      " [-0.9715223   0.85661204 -0.62490067 -0.97170436  0.11777199  0.14093392\n",
      "  -0.95340093 -0.66668396  0.68376173]\n",
      " [ 1.10503948 -0.76372657  0.62018062  0.57957338 -0.21178873 -0.20768896\n",
      "   0.74918135  0.77906235 -0.4735604 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.16027484  1.65284537 -1.71449526  0.22682961  0.34287905 -2.21770084\n",
      "   1.04436431]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:50 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.46142828]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 50 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.84819079 -0.87827618  0.4251423   0.84184198 -0.34019938 -0.04256224\n",
      "   1.23479533  0.12731541 -0.51620545]\n",
      " [-0.77610898  0.66580666 -0.62012639 -0.8541413   0.2464203  -0.20920904\n",
      "  -0.6215838  -0.29519859  0.68663478]\n",
      " [ 0.60738034 -0.31671007  0.09580857  0.44613197 -0.46272561  0.32509219\n",
      "   0.58926973 -0.07755955 -0.58855942]\n",
      " [ 0.62132006 -0.59767587  0.45983559  0.49727669 -0.36846538  0.18928407\n",
      "   0.80571939  0.76472231 -0.30449346]\n",
      " [-0.96162921  0.86650514 -0.62490067 -0.97170436  0.12766509  0.15082702\n",
      "  -0.94350783 -0.66668396  0.68376173]\n",
      " [ 1.09642758 -0.77233847  0.62018062  0.57957338 -0.22040063 -0.21630086\n",
      "   0.74056945  0.77906235 -0.4735604 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.21761012  1.61239091 -1.73300081  0.18738057  0.30463955 -2.23535865\n",
      "   1.00642379]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:50 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.7982406]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 50 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.85048812 -0.87827618  0.4251423   0.84413931 -0.34019938 -0.04256224\n",
      "   1.23709266  0.12731541 -0.51620545]\n",
      " [-0.77926008  0.66580666 -0.62012639 -0.8572924   0.2464203  -0.20920904\n",
      "  -0.6247349  -0.29519859  0.68663478]\n",
      " [ 0.61100526 -0.31671007  0.09580857  0.44975688 -0.46272561  0.32509219\n",
      "   0.59289464 -0.07755955 -0.58855942]\n",
      " [ 0.62479524 -0.59767587  0.45983559  0.50075187 -0.36846538  0.18928407\n",
      "   0.80919457  0.76472231 -0.30449346]\n",
      " [-0.96398809  0.86650514 -0.62490067 -0.97406324  0.12766509  0.15082702\n",
      "  -0.94586672 -0.66668396  0.68376173]\n",
      " [ 1.09938036 -0.77233847  0.62018062  0.58252616 -0.22040063 -0.21630086\n",
      "   0.74352222  0.77906235 -0.4735604 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.20136319  1.62781023 -1.73145424  0.20099409  0.31881693 -2.23449252\n",
      "   1.02133981]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:50 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.66296714]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 50 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.85884414 -0.87827618  0.4251423   0.84413931 -0.33184336 -0.03420622\n",
      "   1.24544867  0.12731541 -0.51620545]\n",
      " [-0.78759082  0.66580666 -0.62012639 -0.8572924   0.23808955 -0.21753978\n",
      "  -0.63306564 -0.29519859  0.68663478]\n",
      " [ 0.61865438 -0.31671007  0.09580857  0.44975688 -0.45507648  0.33274131\n",
      "   0.60054377 -0.07755955 -0.58855942]\n",
      " [ 0.63295229 -0.59767587  0.45983559  0.50075187 -0.36030833  0.19744112\n",
      "   0.81735162  0.76472231 -0.30449346]\n",
      " [-0.97239522  0.86650514 -0.62490067 -0.97406324  0.11925796  0.14241989\n",
      "  -0.95427385 -0.66668396  0.68376173]\n",
      " [ 1.10775101 -0.77233847  0.62018062  0.58252616 -0.21202998 -0.20793021\n",
      "   0.75189288  0.77906235 -0.4735604 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.16370959  1.65967132 -1.72380529  0.22900409  0.34811637 -2.22833074\n",
      "   1.05158191]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:50 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.58140567]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 50 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.84396975 -0.87827618  0.4251423   0.82926493 -0.33184336 -0.0490806\n",
      "   1.24544867  0.12731541 -0.53107983]\n",
      " [-0.77260036  0.66580666 -0.62012639 -0.84230194  0.23808955 -0.20254932\n",
      "  -0.63306564 -0.29519859  0.70162524]\n",
      " [ 0.60641575 -0.31671007  0.09580857  0.43751825 -0.45507648  0.32050268\n",
      "   0.60054377 -0.07755955 -0.60079805]\n",
      " [ 0.61884818 -0.59767587  0.45983559  0.48664776 -0.36030833  0.18333701\n",
      "   0.81735162  0.76472231 -0.31859756]\n",
      " [-0.9576956   0.86650514 -0.62490067 -0.95936362  0.11925796  0.15711951\n",
      "  -0.95427385 -0.66668396  0.69846135]\n",
      " [ 1.09377571 -0.77233847  0.62018062  0.56855086 -0.21202998 -0.22190551\n",
      "   0.75189288  0.77906235 -0.4875357 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.23445885  1.60590327 -1.7404887   0.17999878  0.29602608 -2.24573222\n",
      "   0.99973808]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:50 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.69447861]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 50 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.8507913  -0.87145463  0.4251423   0.83608647 -0.33184336 -0.04225906\n",
      "   1.25227022  0.12731541 -0.53107983]\n",
      " [-0.77971936  0.65868766 -0.62012639 -0.84942094  0.23808955 -0.20966832\n",
      "  -0.64018464 -0.29519859  0.70162524]\n",
      " [ 0.61364469 -0.30948113  0.09580857  0.4447472  -0.45507648  0.32773163\n",
      "   0.60777271 -0.07755955 -0.60079805]\n",
      " [ 0.62610124 -0.59042282  0.45983559  0.49390082 -0.36030833  0.19059007\n",
      "   0.82460467  0.76472231 -0.31859756]\n",
      " [-0.96473897  0.85946177 -0.62490067 -0.96640699  0.11925796  0.15007614\n",
      "  -0.96131722 -0.66668396  0.69846135]\n",
      " [ 1.10099107 -0.76512311  0.62018062  0.57576622 -0.21202998 -0.21469015\n",
      "   0.75910824  0.77906235 -0.4875357 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.20204638  1.63442247 -1.7358303   0.20718173  0.32256671 -2.24131952\n",
      "   1.02584096]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:50 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.72808124]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 50 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.8563386  -0.86590733  0.4251423   0.84163377 -0.33184336 -0.04225906\n",
      "   1.25781752  0.12731541 -0.53107983]\n",
      " [-0.78573589  0.65267113 -0.62012639 -0.85543747  0.23808955 -0.20966832\n",
      "  -0.64620117 -0.29519859  0.70162524]\n",
      " [ 0.61959147 -0.30353436  0.09580857  0.45069397 -0.45507648  0.32773163\n",
      "   0.61371948 -0.07755955 -0.60079805]\n",
      " [ 0.63204582 -0.58447824  0.45983559  0.49984539 -0.36030833  0.19059007\n",
      "   0.83054925  0.76472231 -0.31859756]\n",
      " [-0.97034124  0.8538595  -0.62490067 -0.97200926  0.11925796  0.15007614\n",
      "  -0.96691949 -0.66668396  0.69846135]\n",
      " [ 1.10698391 -0.75913028  0.62018062  0.58175905 -0.21202998 -0.21469015\n",
      "   0.76510107  0.77906235 -0.4875357 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.17512928  1.6583174  -1.7313486   0.22858675  0.34396076 -2.23820302\n",
      "   1.0484964 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:50 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.81144556]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 50 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.85886999 -0.86590733  0.42767369  0.84163377 -0.33184336 -0.03972766\n",
      "   1.26034891  0.12731541 -0.53107983]\n",
      " [-0.78852339  0.65267113 -0.62291389 -0.85543747  0.23808955 -0.21245582\n",
      "  -0.64898867 -0.29519859  0.70162524]\n",
      " [ 0.62280655 -0.30353436  0.09902365  0.45069397 -0.45507648  0.33094671\n",
      "   0.61693456 -0.07755955 -0.60079805]\n",
      " [ 0.6349782  -0.58447824  0.46276797  0.49984539 -0.36030833  0.19352246\n",
      "   0.83348164  0.76472231 -0.31859756]\n",
      " [-0.97296778  0.8538595  -0.6275272  -0.97200926  0.11925796  0.1474496\n",
      "  -0.96954602 -0.66668396  0.69846135]\n",
      " [ 1.10975502 -0.75913028  0.62295173  0.58175905 -0.21202998 -0.21191903\n",
      "   0.76787218  0.77906235 -0.4875357 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.16070471  1.67164475 -1.72998775  0.24070072  0.35682982 -2.23701652\n",
      "   1.06157953]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:50 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.29632983]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 51 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.85657887 -0.86819846  0.42767369  0.83934265 -0.33184336 -0.03972766\n",
      "   1.26034891  0.12731541 -0.53337095]\n",
      " [-0.78633237  0.65486216 -0.62291389 -0.85324645  0.23808955 -0.21245582\n",
      "  -0.64898867 -0.29519859  0.70381626]\n",
      " [ 0.62150923 -0.30483167  0.09902365  0.44939666 -0.45507648  0.33094671\n",
      "   0.61693456 -0.07755955 -0.60209536]\n",
      " [ 0.63321205 -0.58624439  0.46276797  0.49807924 -0.36030833  0.19352246\n",
      "   0.83348164  0.76472231 -0.32036371]\n",
      " [-0.97004895  0.85677833 -0.6275272  -0.96909043  0.11925796  0.1474496\n",
      "  -0.96954602 -0.66668396  0.70138018]\n",
      " [ 1.10648364 -0.76240166  0.62295173  0.57848767 -0.21202998 -0.21191903\n",
      "   0.76787218  0.77906235 -0.49080708]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.19159983  1.65387073 -1.74321352  0.22394964  0.33960026 -2.24946967\n",
      "   1.04275162]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:51 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.8055789]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 51 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.85919364 -0.86819846  0.43028847  0.83934265 -0.33184336 -0.03972766\n",
      "   1.26296369  0.12731541 -0.53337095]\n",
      " [-0.78947873  0.65486216 -0.62606025 -0.85324645  0.23808955 -0.21245582\n",
      "  -0.65213503 -0.29519859  0.70381626]\n",
      " [ 0.62486293 -0.30483167  0.10237735  0.44939666 -0.45507648  0.33094671\n",
      "   0.62028826 -0.07755955 -0.60209536]\n",
      " [ 0.6364649  -0.58624439  0.46602082  0.49807924 -0.36030833  0.19352246\n",
      "   0.83673449  0.76472231 -0.32036371]\n",
      " [-0.9726365   0.85677833 -0.63011476 -0.96909043  0.11925796  0.1474496\n",
      "  -0.97213358 -0.66668396  0.70138018]\n",
      " [ 1.10915523 -0.76240166  0.62562332  0.57848767 -0.21202998 -0.21191903\n",
      "   0.77054377  0.77906235 -0.49080708]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.17637457  1.66798775 -1.74148967  0.23600917  0.35289481 -2.24838434\n",
      "   1.05681905]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:51 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.71133221]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 51 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.8429533  -0.8844388   0.41404813  0.83934265 -0.33184336 -0.03972766\n",
      "   1.24672335  0.12731541 -0.53337095]\n",
      " [-0.77323225  0.67110863 -0.60981378 -0.85324645  0.23808955 -0.21245582\n",
      "  -0.63588856 -0.29519859  0.70381626]\n",
      " [ 0.61018863 -0.31950598  0.08770305  0.44939666 -0.45507648  0.33094671\n",
      "   0.60561396 -0.07755955 -0.60209536]\n",
      " [ 0.62033886 -0.60237043  0.44989478  0.49807924 -0.36030833  0.19352246\n",
      "   0.82060845  0.76472231 -0.32036371]\n",
      " [-0.95645299  0.87296185 -0.61393124 -0.96909043  0.11925796  0.1474496\n",
      "  -0.95595006 -0.66668396  0.70138018]\n",
      " [ 1.09302027 -0.77853663  0.60948835  0.57848767 -0.21202998 -0.21191903\n",
      "   0.75440881  0.77906235 -0.49080708]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.24940658  1.60638772 -1.75578868  0.18201149  0.29486242 -2.25949362\n",
      "   0.99466459]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:51 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.69667982]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 51 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.85006014 -0.8844388   0.42115496  0.83934265 -0.33184336 -0.03972766\n",
      "   1.24672335  0.13442224 -0.53337095]\n",
      " [-0.78036285  0.67110863 -0.61694437 -0.85324645  0.23808955 -0.21245582\n",
      "  -0.63588856 -0.30232918  0.70381626]\n",
      " [ 0.61470972 -0.31950598  0.09222413  0.44939666 -0.45507648  0.33094671\n",
      "   0.60561396 -0.07303846 -0.60209536]\n",
      " [ 0.62731939 -0.60237043  0.45687531  0.49807924 -0.36030833  0.19352246\n",
      "   0.82060845  0.77170284 -0.32036371]\n",
      " [-0.96270217  0.87296185 -0.62018042 -0.96909043  0.11925796  0.1474496\n",
      "  -0.95595006 -0.67293314  0.70138018]\n",
      " [ 1.09868334 -0.77853663  0.61515143  0.57848767 -0.21202998 -0.21191903\n",
      "   0.75440881  0.78472543 -0.49080708]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.21735822  1.63201626 -1.75074657  0.20285235  0.32249936 -2.25640189\n",
      "   1.0242401 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:51 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.53751599]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 51 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.86216031 -0.8844388   0.42115496  0.85144282 -0.33184336 -0.03972766\n",
      "   1.24672335  0.13442224 -0.52127078]\n",
      " [-0.7912075   0.67110863 -0.61694437 -0.86409111  0.23808955 -0.21245582\n",
      "  -0.63588856 -0.30232918  0.6929716 ]\n",
      " [ 0.62100731 -0.31950598  0.09222413  0.45569425 -0.45507648  0.33094671\n",
      "   0.60561396 -0.07303846 -0.59579777]\n",
      " [ 0.63719962 -0.60237043  0.45687531  0.50795947 -0.36030833  0.19352246\n",
      "   0.82060845  0.77170284 -0.31048349]\n",
      " [-0.97507883  0.87296185 -0.62018042 -0.98146709  0.11925796  0.1474496\n",
      "  -0.95595006 -0.67293314  0.68900352]\n",
      " [ 1.11090419 -0.77853663  0.61515143  0.59070852 -0.21202998 -0.21191903\n",
      "   0.75440881  0.78472543 -0.47858623]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.15987318  1.67574001 -1.73448022  0.23811891  0.36222442 -2.24340399\n",
      "   1.06827882]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:51 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.20050514]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 51 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.86228355 -0.8844388   0.42115496  0.85144282 -0.33172012 -0.03960442\n",
      "   1.24672335  0.13442224 -0.52114754]\n",
      " [-0.79091619  0.67110863 -0.61694437 -0.86409111  0.23838086 -0.21216451\n",
      "  -0.63588856 -0.30232918  0.69326291]\n",
      " [ 0.62140377 -0.31950598  0.09222413  0.45569425 -0.45468002  0.33134317\n",
      "   0.60561396 -0.07303846 -0.59540131]\n",
      " [ 0.63656116 -0.60237043  0.45687531  0.50795947 -0.36094678  0.192884\n",
      "   0.82060845  0.77170284 -0.31112195]\n",
      " [-0.97500102  0.87296185 -0.62018042 -0.98146709  0.11933577  0.14752741\n",
      "  -0.95595006 -0.67293314  0.68908132]\n",
      " [ 1.11007605 -0.77853663  0.61515143  0.59070852 -0.21285812 -0.21274717\n",
      "   0.75440881  0.78472543 -0.47941438]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.17594395  1.66782788 -1.74222404  0.23048063  0.35354785 -2.25136156\n",
      "   1.05940929]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:51 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.05780236]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 51 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.86257756 -0.88414479  0.42115496  0.85144282 -0.33142611 -0.03931041\n",
      "   1.24672335  0.13442224 -0.52085354]\n",
      " [-0.79113216  0.67089266 -0.61694437 -0.86409111  0.2381649  -0.21238048\n",
      "  -0.63588856 -0.30232918  0.69304695]\n",
      " [ 0.62156088 -0.31934887  0.09222413  0.45569425 -0.45452292  0.33150027\n",
      "   0.60561396 -0.07303846 -0.5952442 ]\n",
      " [ 0.63672788 -0.60220371  0.45687531  0.50795947 -0.36078007  0.19305071\n",
      "   0.82060845  0.77170284 -0.31095523]\n",
      " [-0.97528252  0.87268034 -0.62018042 -0.98146709  0.11905427  0.1472459\n",
      "  -0.95595006 -0.67293314  0.68879982]\n",
      " [ 1.11028413 -0.77832854  0.61515143  0.59070852 -0.21265004 -0.21253909\n",
      "   0.75440881  0.78472543 -0.47920629]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.17751794  1.66737764 -1.74324018  0.22985532  0.35293313 -2.25246554\n",
      "   1.05884196]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:51 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.45371536]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 51 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.85251741 -0.89420494  0.42115496  0.85144282 -0.34148627 -0.04937057\n",
      "   1.2366632   0.13442224 -0.52085354]\n",
      " [-0.78212158  0.67990325 -0.61694437 -0.86409111  0.24717548 -0.20336989\n",
      "  -0.62687797 -0.30232918  0.69304695]\n",
      " [ 0.61206722 -0.32884253  0.09222413  0.45569425 -0.46401657  0.32200662\n",
      "   0.5961203  -0.07303846 -0.5952442 ]\n",
      " [ 0.62812223 -0.61080936  0.45687531  0.50795947 -0.36938571  0.18444507\n",
      "   0.8120028   0.77170284 -0.31095523]\n",
      " [-0.9657255   0.88223737 -0.62018042 -0.98146709  0.12861129  0.15680293\n",
      "  -0.94639304 -0.67293314  0.68879982]\n",
      " [ 1.10193599 -0.78667668  0.61515143  0.59070852 -0.22099817 -0.22088723\n",
      "   0.74606067  0.78472543 -0.47920629]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.23374637  1.62793376 -1.76152113  0.19124245  0.31551935 -2.26999112\n",
      "   1.02175795]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:51 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.80306898]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 51 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.85470001 -0.89420494  0.42115496  0.85362542 -0.34148627 -0.04937057\n",
      "   1.2388458   0.13442224 -0.52085354]\n",
      " [-0.78511819  0.67990325 -0.61694437 -0.86708772  0.24717548 -0.20336989\n",
      "  -0.62987459 -0.30232918  0.69304695]\n",
      " [ 0.6155362  -0.32884253  0.09222413  0.45916323 -0.46401657  0.32200662\n",
      "   0.59958928 -0.07303846 -0.5952442 ]\n",
      " [ 0.63143483 -0.61080936  0.45687531  0.51127207 -0.36938571  0.18444507\n",
      "   0.8153154   0.77170284 -0.31095523]\n",
      " [-0.96796579  0.88223737 -0.62018042 -0.98370738  0.12861129  0.15680293\n",
      "  -0.94863333 -0.67293314  0.68879982]\n",
      " [ 1.10473959 -0.78667668  0.61515143  0.59351212 -0.22099817 -0.22088723\n",
      "   0.74886428  0.78472543 -0.47920629]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.21817413  1.64272456 -1.76006705  0.20433494  0.32914876 -2.26917402\n",
      "   1.03608023]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:51 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.66497343]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 51 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.86298368 -0.89420494  0.42115496  0.85362542 -0.33320259 -0.0410869\n",
      "   1.24712947  0.13442224 -0.52085354]\n",
      " [-0.79337991  0.67990325 -0.61694437 -0.86708772  0.23891377 -0.21163161\n",
      "  -0.6381363  -0.30232918  0.69304695]\n",
      " [ 0.62314058 -0.32884253  0.09222413  0.45916323 -0.45641219  0.329611\n",
      "   0.60719366 -0.07303846 -0.5952442 ]\n",
      " [ 0.63953288 -0.61080936  0.45687531  0.51127207 -0.36128766  0.19254312\n",
      "   0.82341346  0.77170284 -0.31095523]\n",
      " [-0.97629835  0.88223737 -0.62018042 -0.98370738  0.12027873  0.14847036\n",
      "  -0.95696589 -0.67293314  0.68879982]\n",
      " [ 1.11304061 -0.78667668  0.61515143  0.59351212 -0.21269716 -0.21258621\n",
      "   0.75716529  0.78472543 -0.47920629]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.18085489  1.67429237 -1.75251259  0.23214483  0.35823304 -2.26306605\n",
      "   1.06608627]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:51 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.58224575]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 51 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.8480854  -0.89420494  0.42115496  0.83872714 -0.33320259 -0.05598518\n",
      "   1.24712947  0.13442224 -0.53575182]\n",
      " [-0.77836057  0.67990325 -0.61694437 -0.85206839  0.23891377 -0.19661227\n",
      "  -0.6381363  -0.30232918  0.70806628]\n",
      " [ 0.6108491  -0.32884253  0.09222413  0.44687174 -0.45641219  0.31731952\n",
      "   0.60719366 -0.07303846 -0.60753569]\n",
      " [ 0.62537598 -0.61080936  0.45687531  0.49711517 -0.36128766  0.17838622\n",
      "   0.82341346  0.77170284 -0.32511213]\n",
      " [-0.96157195  0.88223737 -0.62018042 -0.96898097  0.12027873  0.16319677\n",
      "  -0.95696589 -0.67293314  0.70352623]\n",
      " [ 1.09900937 -0.78667668  0.61515143  0.57948088 -0.21269716 -0.22661745\n",
      "   0.75716529  0.78472543 -0.49323753]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.25166635  1.62044998 -1.76916904  0.18303502  0.30601812 -2.28045056\n",
      "   1.01411415]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:51 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.69598097]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 51 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.85486777 -0.88742258  0.42115496  0.84550951 -0.33320259 -0.04920281\n",
      "   1.25391184  0.13442224 -0.53575182]\n",
      " [-0.78542437  0.67283945 -0.61694437 -0.85913219  0.23891377 -0.20367607\n",
      "  -0.64520011 -0.30232918  0.70806628]\n",
      " [ 0.61801995 -0.32167167  0.09222413  0.4540426  -0.45641219  0.32449037\n",
      "   0.61436452 -0.07303846 -0.60753569]\n",
      " [ 0.63257426 -0.60361108  0.45687531  0.50431344 -0.36128766  0.1855845\n",
      "   0.83061173  0.77170284 -0.32511213]\n",
      " [-0.96856858  0.87524073 -0.62018042 -0.97597761  0.12027873  0.15620014\n",
      "  -0.96396253 -0.67293314  0.70352623]\n",
      " [ 1.10617099 -0.77951507  0.61515143  0.5866425  -0.21269716 -0.21945583\n",
      "   0.76432691  0.78472543 -0.49323753]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.21950243  1.64872466 -1.7645486   0.21003177  0.33237892 -2.27605038\n",
      "   1.04002889]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:51 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.73038291]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 51 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.86034011 -0.88195023  0.42115496  0.85098185 -0.33320259 -0.04920281\n",
      "   1.25938419  0.13442224 -0.53575182]\n",
      " [-0.79135638  0.66690744 -0.61694437 -0.8650642   0.23891377 -0.20367607\n",
      "  -0.65113212 -0.30232918  0.70806628]\n",
      " [ 0.62389174 -0.31579988  0.09222413  0.45991439 -0.45641219  0.32449037\n",
      "   0.62023631 -0.07303846 -0.60753569]\n",
      " [ 0.63844534 -0.59774     0.45687531  0.51018452 -0.36128766  0.1855845\n",
      "   0.83648281  0.77170284 -0.32511213]\n",
      " [-0.97409341  0.8697159  -0.62018042 -0.98150244  0.12027873  0.15620014\n",
      "  -0.96948736 -0.67293314  0.70352623]\n",
      " [ 1.11207788 -0.77360817  0.61515143  0.59254939 -0.21269716 -0.21945583\n",
      "   0.7702338   0.78472543 -0.49323753]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.19295543  1.67228886 -1.76015161  0.23117729  0.3535207  -2.27297745\n",
      "   1.06239712]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:51 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.81326346]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 51 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.86283446 -0.88195023  0.42364931  0.85098185 -0.33320259 -0.04670847\n",
      "   1.26187853  0.13442224 -0.53575182]\n",
      " [-0.79409512  0.66690744 -0.61968311 -0.8650642   0.23891377 -0.20641481\n",
      "  -0.65387086 -0.30232918  0.70806628]\n",
      " [ 0.62705121 -0.31579988  0.09538361  0.45991439 -0.45641219  0.32764985\n",
      "   0.62339578 -0.07303846 -0.60753569]\n",
      " [ 0.64132397 -0.59774     0.45975395  0.51018452 -0.36128766  0.18846313\n",
      "   0.83936144  0.77170284 -0.32511213]\n",
      " [-0.97668023  0.8697159  -0.62276724 -0.98150244  0.12027873  0.15361332\n",
      "  -0.97207418 -0.67293314  0.70352623]\n",
      " [ 1.11480147 -0.77360817  0.61787502  0.59254939 -0.21269716 -0.21673225\n",
      "   0.77295739  0.78472543 -0.49323753]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.17877597  1.68538439 -1.75881555  0.24309302  0.36617704 -2.27180626\n",
      "   1.07525845]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:51 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.28948687]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 52 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.86067814 -0.88410655  0.42364931  0.84882553 -0.33320259 -0.04670847\n",
      "   1.26187853  0.13442224 -0.53790814]\n",
      " [-0.79202211  0.66898045 -0.61968311 -0.86299118  0.23891377 -0.20641481\n",
      "  -0.65387086 -0.30232918  0.7101393 ]\n",
      " [ 0.62584146 -0.31700964  0.09538361  0.45870463 -0.45641219  0.32764985\n",
      "   0.62339578 -0.07303846 -0.60874545]\n",
      " [ 0.63964416 -0.59941981  0.45975395  0.50850471 -0.36128766  0.18846313\n",
      "   0.83936144  0.77170284 -0.32679194]\n",
      " [-0.97391874  0.87247739 -0.62276724 -0.97874095  0.12027873  0.15361332\n",
      "  -0.97207418 -0.67293314  0.70628772]\n",
      " [ 1.1116769  -0.77673274  0.61787502  0.58942482 -0.21269716 -0.21673225\n",
      "   0.77295739  0.78472543 -0.49636211]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.20854741  1.6683107  -1.77160024  0.22699213  0.34959684 -2.28386178\n",
      "   1.05714612]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:52 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.80837569]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 52 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.86322516 -0.88410655  0.42619633  0.84882553 -0.33320259 -0.04670847\n",
      "   1.26442555  0.13442224 -0.53790814]\n",
      " [-0.7950827   0.66898045 -0.62274371 -0.86299118  0.23891377 -0.20641481\n",
      "  -0.65693145 -0.30232918  0.7101393 ]\n",
      " [ 0.62911442 -0.31700964  0.09865658  0.45870463 -0.45641219  0.32764985\n",
      "   0.62666875 -0.07303846 -0.60874545]\n",
      " [ 0.64280829 -0.59941981  0.46291807  0.50850471 -0.36128766  0.18846313\n",
      "   0.84252556  0.77170284 -0.32679194]\n",
      " [-0.97643918  0.87247739 -0.62528768 -0.97874095  0.12027873  0.15361332\n",
      "  -0.97459462 -0.67293314  0.70628772]\n",
      " [ 1.11427512 -0.77673274  0.62047324  0.58942482 -0.21269716 -0.21673225\n",
      "   0.77555561  0.78472543 -0.49636211]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.19370568  1.6820737  -1.76993072  0.23876531  0.36257171 -2.28280541\n",
      "   1.07086459]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:52 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.71014849]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 52 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.84695152 -0.90038019  0.4099227   0.84882553 -0.33320259 -0.04670847\n",
      "   1.24815192  0.13442224 -0.53790814]\n",
      " [-0.77883555  0.68522761 -0.60649655 -0.86299118  0.23891377 -0.20641481\n",
      "  -0.6406843  -0.30232918  0.7101393 ]\n",
      " [ 0.61446631 -0.33165776  0.08400847  0.45870463 -0.45641219  0.32764985\n",
      "   0.61202064 -0.07303846 -0.60874545]\n",
      " [ 0.62668011 -0.61554799  0.44678989  0.50850471 -0.36128766  0.18846313\n",
      "   0.82639738  0.77170284 -0.32679194]\n",
      " [-0.96021804  0.88869854 -0.60906654 -0.97874095  0.12027873  0.15361332\n",
      "  -0.95837347 -0.67293314  0.70628772]\n",
      " [ 1.09810889 -0.79289897  0.60430701  0.58942482 -0.21269716 -0.21673225\n",
      "   0.75938938  0.78472543 -0.49636211]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.26679332  1.62056745 -1.78432175  0.18480078  0.30454461 -2.29405826\n",
      "   1.0087497 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:52 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.69990914]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 52 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.85394594 -0.90038019  0.41691712  0.84882553 -0.33320259 -0.04670847\n",
      "   1.24815192  0.14141666 -0.53790814]\n",
      " [-0.78584137  0.68522761 -0.61350238 -0.86299118  0.23891377 -0.20641481\n",
      "  -0.6406843  -0.30933501  0.7101393 ]\n",
      " [ 0.61894184 -0.33165776  0.08848399  0.45870463 -0.45641219  0.32764985\n",
      "   0.61202064 -0.06856293 -0.60874545]\n",
      " [ 0.63353168 -0.61554799  0.45364146  0.50850471 -0.36128766  0.18846313\n",
      "   0.82639738  0.77855441 -0.32679194]\n",
      " [-0.9663518   0.88869854 -0.6152003  -0.97874095  0.12027873  0.15361332\n",
      "  -0.95837347 -0.6790669   0.70628772]\n",
      " [ 1.10366397 -0.79289897  0.6098621   0.58942482 -0.21269716 -0.21673225\n",
      "   0.75938938  0.79028052 -0.49636211]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.23527832  1.64580462 -1.77940277  0.20533134  0.33175968 -2.2910321\n",
      "   1.03784546]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:52 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.53929414]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 52 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.86602908 -0.90038019  0.41691712  0.86090867 -0.33320259 -0.04670847\n",
      "   1.24815192  0.14141666 -0.525825  ]\n",
      " [-0.79669954  0.68522761 -0.61350238 -0.87384936  0.23891377 -0.20641481\n",
      "  -0.6406843  -0.30933501  0.69928113]\n",
      " [ 0.62529526 -0.33165776  0.08848399  0.46505805 -0.45641219  0.32764985\n",
      "   0.61202064 -0.06856293 -0.60239202]\n",
      " [ 0.64345426 -0.61554799  0.45364146  0.51842729 -0.36128766  0.18846313\n",
      "   0.82639738  0.77855441 -0.31686936]\n",
      " [-0.9787011   0.88869854 -0.6152003  -0.99109025  0.12027873  0.15361332\n",
      "  -0.95837347 -0.6790669   0.69393842]\n",
      " [ 1.11586969 -0.79289897  0.6098621   0.60163053 -0.21269716 -0.21673225\n",
      "   0.75938938  0.79028052 -0.48415639]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.17804576  1.68942816 -1.76331101  0.24053644  0.37143477 -2.27817515\n",
      "   1.08179674]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:52 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.19360593]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 52 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.86617905 -0.90038019  0.41691712  0.86090867 -0.33305263 -0.0465585\n",
      "   1.24815192  0.14141666 -0.52567504]\n",
      " [-0.79645452  0.68522761 -0.61350238 -0.87384936  0.23915879 -0.20616978\n",
      "  -0.6406843  -0.30933501  0.69952615]\n",
      " [ 0.62569411 -0.33165776  0.08848399  0.46505805 -0.45601334  0.32804869\n",
      "   0.61202064 -0.06856293 -0.60199317]\n",
      " [ 0.64287673 -0.61554799  0.45364146  0.51842729 -0.36186519  0.1878856\n",
      "   0.82639738  0.77855441 -0.31744689]\n",
      " [-0.97866003  0.88869854 -0.6152003  -0.99109025  0.1203198   0.15365439\n",
      "  -0.95837347 -0.6790669   0.69397949]\n",
      " [ 1.11511317 -0.79289897  0.6098621   0.60163053 -0.21345368 -0.21748877\n",
      "   0.75938938  0.79028052 -0.48491291]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.1931589   1.68202159 -1.77062239  0.23337947  0.3632984  -2.28569064\n",
      "   1.07347848]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:52 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.05349911]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 52 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.86643615 -0.90012308  0.41691712  0.86090867 -0.33279552 -0.0463014\n",
      "   1.24815192  0.14141666 -0.52541793]\n",
      " [-0.79664584  0.68503629 -0.61350238 -0.87384936  0.23896747 -0.20636111\n",
      "  -0.6406843  -0.30933501  0.69933483]\n",
      " [ 0.62583493 -0.33151694  0.08848399  0.46505805 -0.45587252  0.32818951\n",
      "   0.61202064 -0.06856293 -0.60185236]\n",
      " [ 0.64302572 -0.615399    0.45364146  0.51842729 -0.3617162   0.18803459\n",
      "   0.82639738  0.77855441 -0.3172979 ]\n",
      " [-0.97890667  0.88845189 -0.6152003  -0.99109025  0.12007315  0.15340774\n",
      "  -0.95837347 -0.6790669   0.69373285]\n",
      " [ 1.11529747 -0.79271467  0.6098621   0.60163053 -0.21326937 -0.21730447\n",
      "   0.75938938  0.79028052 -0.48472861]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.19451342  1.68164097 -1.77150351  0.23284753  0.36277553 -2.28664748\n",
      "   1.07299655]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:52 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.44596045]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 52 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.85672524 -0.90983399  0.41691712  0.86090867 -0.34250643 -0.05601231\n",
      "   1.23844101  0.14141666 -0.52541793]\n",
      " [-0.78791361  0.69376851 -0.61350238 -0.87384936  0.2476997  -0.19762888\n",
      "  -0.63195207 -0.30933501  0.69933483]\n",
      " [ 0.61658456 -0.3407673   0.08848399  0.46505805 -0.46512289  0.31893915\n",
      "   0.60277027 -0.06856293 -0.60185236]\n",
      " [ 0.6346609  -0.62376381  0.45364146  0.51842729 -0.37008101  0.17966978\n",
      "   0.81803257  0.77855441 -0.3172979 ]\n",
      " [-0.96968433  0.89767424 -0.6152003  -0.99109025  0.1292955   0.16263009\n",
      "  -0.94915113 -0.6790669   0.69373285]\n",
      " [ 1.10721373 -0.80079841  0.6098621   0.60163053 -0.22135311 -0.22538821\n",
      "   0.75130564  0.79028052 -0.48472861]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.24960731  1.6432136  -1.78954466  0.19508671  0.32620337 -2.30401988\n",
      "   1.03678193]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:52 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.80773652]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 52 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.85879963 -0.90983399  0.41691712  0.86298306 -0.34250643 -0.05601231\n",
      "   1.2405154   0.14141666 -0.52541793]\n",
      " [-0.79076421  0.69376851 -0.61350238 -0.87669996  0.2476997  -0.19762888\n",
      "  -0.63480267 -0.30933501  0.69933483]\n",
      " [ 0.61990434 -0.3407673   0.08848399  0.46837783 -0.46512289  0.31893915\n",
      "   0.60609004 -0.06856293 -0.60185236]\n",
      " [ 0.63781898 -0.62376381  0.45364146  0.52158537 -0.37008101  0.17966978\n",
      "   0.82119065  0.77855441 -0.3172979 ]\n",
      " [-0.97181283  0.89767424 -0.6152003  -0.99321875  0.1292955   0.16263009\n",
      "  -0.95127963 -0.6790669   0.69373285]\n",
      " [ 1.10987671 -0.80079841  0.6098621   0.60429351 -0.22135311 -0.22538821\n",
      "   0.75396862  0.79028052 -0.48472861]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.23467822  1.65740444 -1.78817649  0.20767922  0.33930711 -2.30324857\n",
      "   1.0505361 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:52 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.66698766]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 52 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.86701041 -0.90983399  0.41691712  0.86298306 -0.33429566 -0.04780154\n",
      "   1.24872617  0.14141666 -0.52541793]\n",
      " [-0.79895628  0.69376851 -0.61350238 -0.87669996  0.23950763 -0.20582095\n",
      "  -0.64299474 -0.30933501  0.69933483]\n",
      " [ 0.62746241 -0.3407673   0.08848399  0.46837783 -0.45756482  0.32649722\n",
      "   0.61364812 -0.06856293 -0.60185236]\n",
      " [ 0.64585676 -0.62376381  0.45364146  0.52158537 -0.36204323  0.18770756\n",
      "   0.82922843  0.77855441 -0.3172979 ]\n",
      " [-0.98007045  0.89767424 -0.6152003  -0.99321875  0.12103788  0.15437247\n",
      "  -0.95953725 -0.6790669   0.69373285]\n",
      " [ 1.1181074  -0.80079841  0.6098621   0.60429351 -0.21312242 -0.21715751\n",
      "   0.76219931  0.79028052 -0.48472861]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.19769468  1.68867915 -1.78071567  0.23528588  0.36817286 -2.29719545\n",
      "   1.08030356]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:52 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.58309642]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 52 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.85208818 -0.90983399  0.41691712  0.84806083 -0.33429566 -0.06272377\n",
      "   1.24872617  0.14141666 -0.54034016]\n",
      " [-0.78390897  0.69376851 -0.61350238 -0.86165265  0.23950763 -0.19077364\n",
      "  -0.64299474 -0.30933501  0.71438214]\n",
      " [ 0.6151209  -0.3407673   0.08848399  0.45603631 -0.45756482  0.31415571\n",
      "   0.61364812 -0.06856293 -0.61419387]\n",
      " [ 0.6316494  -0.62376381  0.45364146  0.50737801 -0.36204323  0.1735002\n",
      "   0.82922843  0.77855441 -0.33150526]\n",
      " [-0.96531732  0.89767424 -0.6152003  -0.97846563  0.12103788  0.16912559\n",
      "  -0.95953725 -0.6790669   0.70848597]\n",
      " [ 1.10402222 -0.80079841  0.6098621   0.59020833 -0.21312242 -0.2312427\n",
      "   0.76219931  0.79028052 -0.49881379]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.26856859  1.63476211 -1.79734746  0.18607534  0.31583706 -2.31456317\n",
      "   1.02820608]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:52 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.69742179]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 52 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.85883296 -0.90308921  0.41691712  0.85480561 -0.33429566 -0.05597899\n",
      "   1.25547095  0.14141666 -0.54034016]\n",
      " [-0.79092015  0.68675733 -0.61350238 -0.86866383  0.23950763 -0.19778482\n",
      "  -0.65000592 -0.30933501  0.71438214]\n",
      " [ 0.62223607 -0.33365213  0.08848399  0.46315149 -0.45756482  0.32127088\n",
      "   0.62076329 -0.06856293 -0.61419387]\n",
      " [ 0.63879501 -0.6166182   0.45364146  0.51452362 -0.36204323  0.18064581\n",
      "   0.83637404  0.77855441 -0.33150526]\n",
      " [-0.97226918  0.89072238 -0.6152003  -0.98541749  0.12103788  0.16217373\n",
      "  -0.96648911 -0.6790669   0.70848597]\n",
      " [ 1.11113211 -0.79368852  0.6098621   0.59731822 -0.21312242 -0.22413281\n",
      "   0.7693092   0.79028052 -0.49881379]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.23664286  1.66280209 -1.79276235  0.21289281  0.34202391 -2.31017456\n",
      "   1.05393903]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:52 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.73263224]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 52 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.86423228 -0.89768989  0.41691712  0.86020494 -0.33429566 -0.05597899\n",
      "   1.26087028  0.14141666 -0.54034016]\n",
      " [-0.79676976  0.68090773 -0.61350238 -0.87451343  0.23950763 -0.19778482\n",
      "  -0.65585552 -0.30933501  0.71438214]\n",
      " [ 0.6280342  -0.327854    0.08848399  0.46894961 -0.45756482  0.32127088\n",
      "   0.62656142 -0.06856293 -0.61419387]\n",
      " [ 0.64459358 -0.61081963  0.45364146  0.52032219 -0.36204323  0.18064581\n",
      "   0.84217261  0.77855441 -0.33150526]\n",
      " [-0.97771867  0.88527289 -0.6152003  -0.99086697  0.12103788  0.16217373\n",
      "  -0.97193859 -0.6790669   0.70848597]\n",
      " [ 1.11695532 -0.7878653   0.6098621   0.60314143 -0.21312242 -0.22413281\n",
      "   0.77513242  0.79028052 -0.49881379]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.21045657  1.68604391 -1.78844647  0.2337838   0.36291734 -2.30714388\n",
      "   1.07602567]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:52 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.814999]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 52 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.86669186 -0.89768989  0.4193767   0.86020494 -0.33429566 -0.05351941\n",
      "   1.26332986  0.14141666 -0.54034016]\n",
      " [-0.79946276  0.68090773 -0.61619538 -0.87451343  0.23950763 -0.20047782\n",
      "  -0.65854853 -0.30933501  0.71438214]\n",
      " [ 0.63114097 -0.327854    0.09159076  0.46894961 -0.45756482  0.32437765\n",
      "   0.62966819 -0.06856293 -0.61419387]\n",
      " [ 0.64742173 -0.61081963  0.45646961  0.52032219 -0.36204323  0.18347396\n",
      "   0.84500076  0.77855441 -0.33150526]\n",
      " [-0.98026825  0.88527289 -0.61774988 -0.99086697  0.12103788  0.15962416\n",
      "  -0.97448817 -0.6790669   0.70848597]\n",
      " [ 1.11963442 -0.7878653   0.61254119  0.60314143 -0.21312242 -0.22145372\n",
      "   0.77781151  0.79028052 -0.49881379]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.19650974  1.69891902 -1.78713329  0.24551059  0.37537091 -2.30598674\n",
      "   1.08867576]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:52 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.28278817]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 53 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.86466348 -0.89971827  0.4193767   0.85817656 -0.33429566 -0.05351941\n",
      "   1.26332986  0.14141666 -0.54236854]\n",
      " [-0.79750306  0.68286743 -0.61619538 -0.87255373  0.23950763 -0.20047782\n",
      "  -0.65854853 -0.30933501  0.71634184]\n",
      " [ 0.63001495 -0.32898002  0.09159076  0.46782359 -0.45756482  0.32437765\n",
      "   0.62966819 -0.06856293 -0.61531989]\n",
      " [ 0.64582598 -0.61241539  0.45646961  0.51872644 -0.36204323  0.18347396\n",
      "   0.84500076  0.77855441 -0.33310101]\n",
      " [-0.97765679  0.88788435 -0.61774988 -0.98825551  0.12103788  0.15962416\n",
      "  -0.97448817 -0.6790669   0.71109743]\n",
      " [ 1.11665195 -0.79084777  0.61254119  0.60015897 -0.21312242 -0.22145372\n",
      "   0.77781151  0.79028052 -0.50179626]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.22518716  1.68252361 -1.79948683  0.23004117  0.3594229  -2.31765156\n",
      "   1.07125915]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:53 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.81106822]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 53 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.86714634 -0.89971827  0.42185955  0.85817656 -0.33429566 -0.05351941\n",
      "   1.26581271  0.14141666 -0.54236854]\n",
      " [-0.80048234  0.68286743 -0.61917466 -0.87255373  0.23950763 -0.20047782\n",
      "  -0.66152781 -0.30933501  0.71634184]\n",
      " [ 0.63321046 -0.32898002  0.09478627  0.46782359 -0.45756482  0.32437765\n",
      "   0.6328637  -0.06856293 -0.61531989]\n",
      " [ 0.64890588 -0.61241539  0.45954951  0.51872644 -0.36204323  0.18347396\n",
      "   0.84808065  0.77855441 -0.33310101]\n",
      " [-0.98011375  0.88788435 -0.62020684 -0.98825551  0.12103788  0.15962416\n",
      "  -0.97694513 -0.6790669   0.71109743]\n",
      " [ 1.11918096 -0.79084777  0.6150702   0.60015897 -0.21312242 -0.22145372\n",
      "   0.78034052  0.79028052 -0.50179626]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.21071153  1.69594826 -1.79786811  0.24153971  0.37209127 -2.31662232\n",
      "   1.08464377]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:53 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.70881472]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 53 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.85084018 -0.91602442  0.4055534   0.85817656 -0.33429566 -0.05351941\n",
      "   1.24950656  0.14141666 -0.54236854]\n",
      " [-0.7842348   0.69911497 -0.60292712 -0.87255373  0.23950763 -0.20047782\n",
      "  -0.64528027 -0.30933501  0.71634184]\n",
      " [ 0.61859005 -0.34360043  0.08016587  0.46782359 -0.45756482  0.32437765\n",
      "   0.61824329 -0.06856293 -0.61531989]\n",
      " [ 0.63277611 -0.62854515  0.44341974  0.51872644 -0.36204323  0.18347396\n",
      "   0.83195089  0.77855441 -0.33310101]\n",
      " [-0.96385544  0.90414266 -0.60394853 -0.98825551  0.12103788  0.15962416\n",
      "  -0.96068682 -0.6790669   0.71109743]\n",
      " [ 1.1029822  -0.80704654  0.59887144  0.60015897 -0.21312242 -0.22145372\n",
      "   0.76414176  0.79028052 -0.50179626]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.28385993  1.63453438 -1.81235758  0.18760883  0.31407238 -2.32802303\n",
      "   1.02256927]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:53 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.70303973]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 53 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.8577252  -0.91602442  0.41243841  0.85817656 -0.33429566 -0.05351941\n",
      "   1.24950656  0.14830168 -0.54236854]\n",
      " [-0.79111994  0.69911497 -0.60981226 -0.87255373  0.23950763 -0.20047782\n",
      "  -0.64528027 -0.31622014  0.71634184]\n",
      " [ 0.62301936 -0.34360043  0.08459518  0.46782359 -0.45756482  0.32437765\n",
      "   0.61824329 -0.06413362 -0.61531989]\n",
      " [ 0.63950339 -0.62854515  0.45014702  0.51872644 -0.36204323  0.18347396\n",
      "   0.83195089  0.78528169 -0.33310101]\n",
      " [-0.96987858  0.90414266 -0.60997168 -0.98825551  0.12103788  0.15962416\n",
      "  -0.96068682 -0.68509004  0.71109743]\n",
      " [ 1.10843419 -0.80704654  0.60432343  0.60015897 -0.21312242 -0.22145372\n",
      "   0.76414176  0.79573251 -0.50179626]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.25286101  1.65939043 -1.80755513  0.20783665  0.34087665 -2.32505892\n",
      "   1.05119952]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:53 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.54111461]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 53 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.86978789 -0.91602442  0.41243841  0.87023925 -0.33429566 -0.05351941\n",
      "   1.24950656  0.14830168 -0.53030585]\n",
      " [-0.80198697  0.69911497 -0.60981226 -0.88342077  0.23950763 -0.20047782\n",
      "  -0.64528027 -0.31622014  0.70547481]\n",
      " [ 0.62942344 -0.34360043  0.08459518  0.47422767 -0.45756482  0.32437765\n",
      "   0.61824329 -0.06413362 -0.60891581]\n",
      " [ 0.64946237 -0.62854515  0.45014702  0.52868542 -0.36204323  0.18347396\n",
      "   0.83195089  0.78528169 -0.32314203]\n",
      " [-0.9821974   0.90414266 -0.60997168 -1.00057433  0.12103788  0.15962416\n",
      "  -0.96068682 -0.68509004  0.69877861]\n",
      " [ 1.12062059 -0.80704654  0.60432343  0.61234537 -0.21312242 -0.22145372\n",
      "   0.76414176  0.79573251 -0.48960986]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.19588819  1.70290531 -1.79163568  0.24297123  0.38049122 -2.31234182\n",
      "   1.0950533 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:53 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.18700197]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 53 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.86995955 -0.91602442  0.41243841  0.87023925 -0.334124   -0.05334774\n",
      "   1.24950656  0.14830168 -0.53013418]\n",
      " [-0.80178286  0.69911497 -0.60981226 -0.88342077  0.23971174 -0.20027371\n",
      "  -0.64528027 -0.31622014  0.70567892]\n",
      " [ 0.62982261 -0.34360043  0.08459518  0.47422767 -0.45716564  0.32477682\n",
      "   0.61824329 -0.06413362 -0.60851664]\n",
      " [ 0.64894015 -0.62854515  0.45014702  0.52868542 -0.36256545  0.18295174\n",
      "   0.83195089  0.78528169 -0.32366425]\n",
      " [-0.98218761  0.90414266 -0.60997168 -1.00057433  0.12104768  0.15963395\n",
      "  -0.96068682 -0.68509004  0.69878841]\n",
      " [ 1.11992919 -0.80704654  0.60432343  0.61234537 -0.21381382 -0.22214512\n",
      "   0.76414176  0.79573251 -0.49030126]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.21010336  1.69596946 -1.79853904  0.23626367  0.37285951 -2.3194396\n",
      "   1.08724987]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:53 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.04954263]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 53 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.87018424 -0.91579974  0.41243841  0.87023925 -0.33389931 -0.05312305\n",
      "   1.24950656  0.14830168 -0.5299095 ]\n",
      " [-0.80195213  0.6989457  -0.60981226 -0.88342077  0.23954247 -0.20044298\n",
      "  -0.64528027 -0.31622014  0.70550965]\n",
      " [ 0.62994857 -0.34347447  0.08459518  0.47422767 -0.45703968  0.32490279\n",
      "   0.61824329 -0.06413362 -0.60839068]\n",
      " [ 0.64907306 -0.62841223  0.45014702  0.52868542 -0.36243254  0.18308465\n",
      "   0.83195089  0.78528169 -0.32353134]\n",
      " [-0.98240355  0.90392672 -0.60997168 -1.00057433  0.12083173  0.15941801\n",
      "  -0.96068682 -0.68509004  0.69857246]\n",
      " [ 1.12009223 -0.80688349  0.60432343  0.61234537 -0.21365077 -0.22198207\n",
      "   0.76414176  0.79573251 -0.49013821]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.21126979  1.69564735 -1.79930341  0.2358108   0.37241444 -2.32026922\n",
      "   1.08684011]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:53 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.4381906]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 53 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.86082031 -0.92516366  0.41243841  0.87023925 -0.34326324 -0.06248698\n",
      "   1.24014263  0.14830168 -0.5299095 ]\n",
      " [-0.79349766  0.70740018 -0.60981226 -0.88342077  0.24799695 -0.1919885\n",
      "  -0.63682579 -0.31622014  0.70550965]\n",
      " [ 0.62094295 -0.35248009  0.08459518  0.47422767 -0.4660453   0.31589716\n",
      "   0.60923767 -0.06413362 -0.60839068]\n",
      " [ 0.6409504  -0.63653489  0.45014702  0.52868542 -0.3705552   0.174962\n",
      "   0.82382824  0.78528169 -0.32353134]\n",
      " [-0.97351297  0.9128173  -0.60997168 -1.00057433  0.12972231  0.16830859\n",
      "  -0.95179624 -0.68509004  0.69857246]\n",
      " [ 1.11227226 -0.81470346  0.60432343  0.61234537 -0.22147074 -0.22980204\n",
      "   0.75632179  0.79573251 -0.49013821]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.26520659  1.65823834 -1.81709061  0.19891378  0.33669568 -2.3374687\n",
      "   1.05150388]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:53 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.81224802]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 53 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.86279264 -0.92516366  0.41243841  0.87221157 -0.34326324 -0.06248698\n",
      "   1.24211496  0.14830168 -0.5299095 ]\n",
      " [-0.79621027  0.70740018 -0.60981226 -0.88613338  0.24799695 -0.1919885\n",
      "  -0.63953841 -0.31622014  0.70550965]\n",
      " [ 0.62412011 -0.35248009  0.08459518  0.47740483 -0.4660453   0.31589716\n",
      "   0.61241483 -0.06413362 -0.60839068]\n",
      " [ 0.64396174 -0.63653489  0.45014702  0.53169675 -0.3705552   0.174962\n",
      "   0.82683957  0.78528169 -0.32353134]\n",
      " [-0.9755361   0.9128173  -0.60997168 -1.00259746  0.12972231  0.16830859\n",
      "  -0.95381937 -0.68509004  0.69857246]\n",
      " [ 1.1148027  -0.81470346  0.60432343  0.61487581 -0.22147074 -0.22980204\n",
      "   0.75885223  0.79573251 -0.49013821]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.25089039  1.67185671 -1.81580228  0.21102685  0.3492955  -2.33674017\n",
      "   1.06471479]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:53 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.66901287]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 53 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.87092985 -0.92516366  0.41243841  0.87221157 -0.33512603 -0.05434977\n",
      "   1.25025216  0.14830168 -0.5299095 ]\n",
      " [-0.80433201  0.70740018 -0.60981226 -0.88613338  0.23987521 -0.20011024\n",
      "  -0.64766014 -0.31622014  0.70550965]\n",
      " [ 0.63163037 -0.35248009  0.08459518  0.47740483 -0.45853504  0.32340742\n",
      "   0.61992509 -0.06413362 -0.60839068]\n",
      " [ 0.65193797 -0.63653489  0.45014702  0.53169675 -0.36257897  0.18293823\n",
      "   0.8348158   0.78528169 -0.32353134]\n",
      " [-0.98371828  0.9128173  -0.60997168 -1.00259746  0.12154013  0.16012641\n",
      "  -0.96200155 -0.68509004  0.69857246]\n",
      " [ 1.12296231 -0.81470346  0.60432343  0.61487581 -0.21331113 -0.22164243\n",
      "   0.76701184  0.79573251 -0.49013821]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.21424438  1.70283815 -1.80843457  0.2384271   0.37793922 -2.33074308\n",
      "   1.0942409 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:53 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.58395249]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 53 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.85598373 -0.92516366  0.41243841  0.85726546 -0.33512603 -0.06929588\n",
      "   1.25025216  0.14830168 -0.54485561]\n",
      " [-0.78925765  0.70740018 -0.60981226 -0.87105903  0.23987521 -0.18503588\n",
      "  -0.64766014 -0.31622014  0.72058401]\n",
      " [ 0.61924153 -0.35248009  0.08459518  0.46501599 -0.45853504  0.31101858\n",
      "   0.61992509 -0.06413362 -0.62077952]\n",
      " [ 0.63768241 -0.63653489  0.45014702  0.5174412  -0.36257897  0.16868267\n",
      "   0.8348158   0.78528169 -0.33778689]\n",
      " [-0.96893862  0.9128173  -0.60997168 -0.98781781  0.12154013  0.17490607\n",
      "  -0.96200155 -0.68509004  0.71335212]\n",
      " [ 1.10882515 -0.81470346  0.60432343  0.60073865 -0.21331113 -0.23577959\n",
      "   0.76701184  0.79573251 -0.50427537]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.28518058  1.64884651 -1.82504375  0.18911964  0.32548641 -2.34809416\n",
      "   1.04202115]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:53 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.69880481]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 53 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.86269238 -0.91845501  0.41243841  0.86397411 -0.33512603 -0.06258723\n",
      "   1.25696082  0.14830168 -0.54485561]\n",
      " [-0.79621861  0.70043922 -0.60981226 -0.87801999  0.23987521 -0.19199685\n",
      "  -0.6546211  -0.31622014  0.72058401]\n",
      " [ 0.62630329 -0.34541833  0.08459518  0.47207774 -0.45853504  0.31808034\n",
      "   0.62698684 -0.06413362 -0.62077952]\n",
      " [ 0.64477738 -0.62943992  0.45014702  0.52453617 -0.36257897  0.17577764\n",
      "   0.84191077  0.78528169 -0.33778689]\n",
      " [-0.97584752  0.9059084  -0.60997168 -0.99472671  0.12154013  0.16799717\n",
      "  -0.96891045 -0.68509004  0.71335212]\n",
      " [ 1.11588524 -0.80764338  0.60432343  0.60779873 -0.21331113 -0.2287195\n",
      "   0.77407193  0.79573251 -0.50427537]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.25348331  1.67666113 -1.82049166  0.21576447  0.35150498 -2.34371629\n",
      "   1.06757841]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:53 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.73483095]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 53 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.86802052 -0.91312687  0.41243841  0.86930225 -0.33512603 -0.06258723\n",
      "   1.26228895  0.14830168 -0.54485561]\n",
      " [-0.80198786  0.69466997 -0.60981226 -0.88378924  0.23987521 -0.19199685\n",
      "  -0.66039035 -0.31622014  0.72058401]\n",
      " [ 0.6320291  -0.33969253  0.08459518  0.47780355 -0.45853504  0.31808034\n",
      "   0.63271265 -0.06413362 -0.62077952]\n",
      " [ 0.65050451 -0.62371279  0.45014702  0.5302633  -0.36257897  0.17577764\n",
      "   0.8476379   0.78528169 -0.33778689]\n",
      " [-0.98122365  0.90053227 -0.60997168 -1.00010284  0.12154013  0.16799717\n",
      "  -0.97428658 -0.68509004  0.71335212]\n",
      " [ 1.12162697 -0.80190165  0.60432343  0.61354046 -0.21331113 -0.2287195\n",
      "   0.77981366  0.79573251 -0.50427537]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.22764863  1.69958868 -1.81625356  0.23640594  0.37215406 -2.34072664\n",
      "   1.08938905]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:53 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.81665445]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 53 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.87044751 -0.91312687  0.4148654   0.86930225 -0.33512603 -0.06016025\n",
      "   1.26471594  0.14830168 -0.54485561]\n",
      " [-0.80463799  0.69466997 -0.61246238 -0.88378924  0.23987521 -0.19464697\n",
      "  -0.66304048 -0.31622014  0.72058401]\n",
      " [ 0.63508594 -0.33969253  0.08765202  0.47780355 -0.45853504  0.32113719\n",
      "   0.6357695  -0.06413362 -0.62077952]\n",
      " [ 0.65328527 -0.62371279  0.45292778  0.5302633  -0.36257897  0.1785584\n",
      "   0.85041866  0.78528169 -0.33778689]\n",
      " [-0.98373834  0.90053227 -0.61248636 -1.00010284  0.12154013  0.16548248\n",
      "  -0.97680127 -0.68509004  0.71335212]\n",
      " [ 1.12426444 -0.80190165  0.60696089  0.61354046 -0.21331113 -0.22608204\n",
      "   0.78245113  0.79573251 -0.50427537]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.21392247  1.71225435 -1.81496148  0.24795281  0.38441454 -2.33958236\n",
      "   1.1018381 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:53 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.27623002]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 54 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.8685405  -0.91503388  0.4148654   0.86739525 -0.33512603 -0.06016025\n",
      "   1.26471594  0.14830168 -0.54676261]\n",
      " [-0.80278696  0.696521   -0.61246238 -0.88193821  0.23987521 -0.19464697\n",
      "  -0.66304048 -0.31622014  0.72243504]\n",
      " [ 0.63403986 -0.34073861  0.08765202  0.47675747 -0.45853504  0.32113719\n",
      "   0.6357695  -0.06413362 -0.6218256 ]\n",
      " [ 0.65177116 -0.62522691  0.45292778  0.52874919 -0.36257897  0.1785584\n",
      "   0.85041866  0.78528169 -0.339301  ]\n",
      " [-0.98126987  0.90300073 -0.61248636 -0.99763437  0.12154013  0.16548248\n",
      "  -0.97680127 -0.68509004  0.71582058]\n",
      " [ 1.12141939 -0.80474669  0.60696089  0.61069542 -0.21331113 -0.22608204\n",
      "   0.78245113  0.79573251 -0.50712042]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.24153539  1.69651555 -1.82689381  0.23309621  0.3690815  -2.35086366\n",
      "   1.08509743]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:54 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.81365912]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 54 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.8709626  -0.91503388  0.4172875   0.86739525 -0.33512603 -0.06016025\n",
      "   1.26713804  0.14830168 -0.54676261]\n",
      " [-0.80568917  0.696521   -0.61536459 -0.88193821  0.23987521 -0.19464697\n",
      "  -0.66594269 -0.31622014  0.72243504]\n",
      " [ 0.63716113 -0.34073861  0.09077329  0.47675747 -0.45853504  0.32113719\n",
      "   0.63889077 -0.06413362 -0.6218256 ]\n",
      " [ 0.65477113 -0.62522691  0.45592775  0.52874919 -0.36257897  0.1785584\n",
      "   0.85341863  0.78528169 -0.339301  ]\n",
      " [-0.9836668   0.90300073 -0.61488329 -0.99763437  0.12154013  0.16548248\n",
      "  -0.97919819 -0.68509004  0.71582058]\n",
      " [ 1.12388313 -0.80474669  0.60942463  0.61069542 -0.21331113 -0.22608204\n",
      "   0.78491486  0.79573251 -0.50712042]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.22740907  1.70961696 -1.82532261  0.24433153  0.3814562  -2.34985988\n",
      "   1.09816282]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:54 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.70733134]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 54 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.85462499 -0.93137149  0.40094988  0.86739525 -0.33512603 -0.06016025\n",
      "   1.25080042  0.14830168 -0.54676261]\n",
      " [-0.78944174  0.71276842 -0.59911717 -0.88193821  0.23987521 -0.19464697\n",
      "  -0.64969526 -0.31622014  0.72243504]\n",
      " [ 0.62256988 -0.35532986  0.07618204  0.47675747 -0.45853504  0.32113719\n",
      "   0.62429952 -0.06413362 -0.6218256 ]\n",
      " [ 0.63864048 -0.64135755  0.4397971   0.52874919 -0.36257897  0.1785584\n",
      "   0.83728798  0.78528169 -0.339301  ]\n",
      " [-0.9673721   0.91929543 -0.59858859 -0.99763437  0.12154013  0.16548248\n",
      "  -0.9629035  -0.68509004  0.71582058]\n",
      " [ 1.10765092 -0.8209789   0.59319242  0.61069542 -0.21331113 -0.22608204\n",
      "   0.76868265  0.79573251 -0.50712042]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.30062272  1.64829445 -1.83991642  0.19043483  0.32344848 -2.36141242\n",
      "   1.03612988]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:54 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.70607152]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 54 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.86140371 -0.93137149  0.4077286   0.86739525 -0.33512603 -0.06016025\n",
      "   1.25080042  0.1550804  -0.54676261]\n",
      " [-0.79621031  0.71276842 -0.60588573 -0.88193821  0.23987521 -0.19464697\n",
      "  -0.64969526 -0.32298871  0.72243504]\n",
      " [ 0.62695258 -0.35532986  0.08056475  0.47675747 -0.45853504  0.32113719\n",
      "   0.62429952 -0.05975092 -0.6218256 ]\n",
      " [ 0.64524809 -0.64135755  0.44640471  0.52874919 -0.36257897  0.1785584\n",
      "   0.83728798  0.79188929 -0.339301  ]\n",
      " [-0.97328932  0.91929543 -0.60450581 -0.99763437  0.12154013  0.16548248\n",
      "  -0.9629035  -0.69100726  0.71582058]\n",
      " [ 1.11300456 -0.8209789   0.59854606  0.61069542 -0.21331113 -0.22608204\n",
      "   0.76868265  0.80108614 -0.50712042]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.27012256  1.67277992 -1.83522421  0.21036777  0.34985338 -2.35850701\n",
      "   1.06430902]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:54 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.54296861]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 54 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.87344283 -0.93137149  0.4077286   0.87943437 -0.33512603 -0.06016025\n",
      "   1.25080042  0.1550804  -0.53472349]\n",
      " [-0.80708192  0.71276842 -0.60588573 -0.89280982  0.23987521 -0.19464697\n",
      "  -0.64969526 -0.32298871  0.71156343]\n",
      " [ 0.6334024  -0.35532986  0.08056475  0.48320729 -0.45853504  0.32113719\n",
      "   0.62429952 -0.05975092 -0.61537578]\n",
      " [ 0.65523795 -0.64135755  0.44640471  0.53873904 -0.36257897  0.1785584\n",
      "   0.83728798  0.79188929 -0.32931115]\n",
      " [-0.98557485  0.91929543 -0.60450581 -1.00991991  0.12154013  0.16548248\n",
      "  -0.9629035  -0.69100726  0.70353505]\n",
      " [ 1.12516786 -0.8209789   0.59854606  0.62285872 -0.21331113 -0.22608204\n",
      "   0.76868265  0.80108614 -0.49495712]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.21341555  1.71617858 -1.81947456  0.24542364  0.38939784 -2.34592838\n",
      "   1.10805627]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:54 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.18068276]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 54 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.87363181 -0.93137149  0.4077286   0.87943437 -0.33493705 -0.05997127\n",
      "   1.25080042  0.1550804  -0.53453451]\n",
      " [-0.80691388  0.71276842 -0.60588573 -0.89280982  0.24004325 -0.19447893\n",
      "  -0.64969526 -0.32298871  0.71173146]\n",
      " [ 0.6338001  -0.35532986  0.08056475  0.48320729 -0.45813735  0.32153488\n",
      "   0.62429952 -0.05975092 -0.61497808]\n",
      " [ 0.65476587 -0.64135755  0.44640471  0.53873904 -0.36305104  0.17808633\n",
      "   0.83728798  0.79188929 -0.32978323]\n",
      " [-0.98559151  0.91929543 -0.60450581 -1.00991991  0.12152347  0.16546582\n",
      "  -0.9629035  -0.69100726  0.70351839]\n",
      " [ 1.12453564 -0.8209789   0.59854606  0.62285872 -0.21394335 -0.22671426\n",
      "   0.76868265  0.80108614 -0.49558934]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.22678937  1.70968074 -1.82599336  0.23913537  0.38223727 -2.35263195\n",
      "   1.1007333 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:54 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.04590565]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 54 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.87382808 -0.93117522  0.4077286   0.87943437 -0.33474078 -0.059775\n",
      "   1.25080042  0.1550804  -0.53433824]\n",
      " [-0.80706349  0.71261882 -0.60588573 -0.89280982  0.23989365 -0.19462854\n",
      "  -0.64969526 -0.32298871  0.71158186]\n",
      " [ 0.63391259 -0.35521737  0.08056475  0.48320729 -0.45802486  0.32164737\n",
      "   0.62429952 -0.05975092 -0.61486559]\n",
      " [ 0.65488427 -0.64123915  0.44640471  0.53873904 -0.36293264  0.17820473\n",
      "   0.83728798  0.79188929 -0.32966482]\n",
      " [-0.98578048  0.91910646 -0.60450581 -1.00991991  0.1213345   0.16527685\n",
      "  -0.9629035  -0.69100726  0.70332942]\n",
      " [ 1.12467974 -0.8208348   0.59854606  0.62285872 -0.21379925 -0.22657015\n",
      "   0.76868265  0.80108614 -0.49544523]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.22779467  1.70940784 -1.82665683  0.23874945  0.38185805 -2.35335164\n",
      "   1.10038456]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:54 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.43043016]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 54 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.86480737 -0.94019593  0.4077286   0.87943437 -0.34376149 -0.06879571\n",
      "   1.24177971  0.1550804  -0.53433824]\n",
      " [-0.79888493  0.72079738 -0.60588573 -0.89280982  0.24807221 -0.18644998\n",
      "  -0.6415167  -0.32298871  0.71158186]\n",
      " [ 0.62515195 -0.36397802  0.08056475  0.48320729 -0.4667855   0.31288673\n",
      "   0.61553888 -0.05975092 -0.61486559]\n",
      " [ 0.64700388 -0.64911954  0.44640471  0.53873904 -0.37081304  0.17032434\n",
      "   0.82940759  0.79188929 -0.32966482]\n",
      " [-0.97721743  0.92766951 -0.60450581 -1.00991991  0.12989756  0.17383991\n",
      "  -0.95434044 -0.69100726  0.70332942]\n",
      " [ 1.11712181 -0.82839273  0.59854606  0.62285872 -0.22135719 -0.23412809\n",
      "   0.76112471  0.80108614 -0.49544523]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.2805568   1.6730153  -1.84417711  0.20272406  0.34700059 -2.37035982\n",
      "   1.06593201]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:54 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.81660827]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 54 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.86668341 -0.94019593  0.4077286   0.8813104  -0.34376149 -0.06879571\n",
      "   1.24365574  0.1550804  -0.53433824]\n",
      " [-0.80146716  0.72079738 -0.60588573 -0.89539206  0.24807221 -0.18644998\n",
      "  -0.64409894 -0.32298871  0.71158186]\n",
      " [ 0.62819292 -0.36397802  0.08056475  0.48624826 -0.4667855   0.31288673\n",
      "   0.61857985 -0.05975092 -0.61486559]\n",
      " [ 0.64987593 -0.64911954  0.44640471  0.54161109 -0.37081304  0.17032434\n",
      "   0.83227964  0.79188929 -0.32966482]\n",
      " [-0.97914123  0.92766951 -0.60450581 -1.01184371  0.12989756  0.17383991\n",
      "  -0.95626424 -0.69100726  0.70332942]\n",
      " [ 1.11952733 -0.82839273  0.59854606  0.62526424 -0.22135719 -0.23412809\n",
      "   0.76353023  0.80108614 -0.49544523]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.2668245   1.68608761 -1.84296305  0.21437771  0.35911767 -2.36967127\n",
      "   1.07862367]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:54 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.67105122]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 54 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.87474632 -0.94019593  0.4077286   0.8813104  -0.33569858 -0.0607328\n",
      "   1.25171866  0.1550804  -0.53433824]\n",
      " [-0.80951785  0.72079738 -0.60588573 -0.89539206  0.24002152 -0.19450066\n",
      "  -0.65214962 -0.32298871  0.71158186]\n",
      " [ 0.63565392 -0.36397802  0.08056475  0.48624826 -0.4593245   0.32034773\n",
      "   0.62604085 -0.05975092 -0.61486559]\n",
      " [ 0.65778935 -0.64911954  0.44640471  0.54161109 -0.36289961  0.17823776\n",
      "   0.84019306  0.79188929 -0.32966482]\n",
      " [-0.9872474   0.92766951 -0.60450581 -1.01184371  0.12179139  0.16573374\n",
      "  -0.96437041 -0.69100726  0.70332942]\n",
      " [ 1.12761506 -0.82839273  0.59854606  0.62526424 -0.21326945 -0.22604036\n",
      "   0.77161797  0.80108614 -0.49544523]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.23051818  1.71677534 -1.83568812  0.24156833  0.38753584 -2.36373149\n",
      "   1.10790554]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:54 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.58480871]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 54 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.85977651 -0.94019593  0.4077286   0.86634059 -0.33569858 -0.07570261\n",
      "   1.25171866  0.1550804  -0.54930805]\n",
      " [-0.79441741  0.72079738 -0.60588573 -0.88029162  0.24002152 -0.17940023\n",
      "  -0.65214962 -0.32298871  0.72668229]\n",
      " [ 0.62322036 -0.36397802  0.08056475  0.4738147  -0.4593245   0.30791417\n",
      "   0.62604085 -0.05975092 -0.62729916]\n",
      " [ 0.6434878  -0.64911954  0.44640471  0.52730954 -0.36289961  0.16393621\n",
      "   0.84019306  0.79188929 -0.34396637]\n",
      " [-0.9724415   0.92766951 -0.60450581 -0.99703782  0.12179139  0.18053963\n",
      "  -0.96437041 -0.69100726  0.71813531]\n",
      " [ 1.11342788 -0.82839273  0.59854606  0.61107706 -0.21326945 -0.24022753\n",
      "   0.77161797  0.80108614 -0.50963241]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.30151615  1.66270955 -1.85227662  0.19216783  0.33496998 -2.38106606\n",
      "   1.05556684]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:54 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.70013323]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 54 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.86645038 -0.93352206  0.4077286   0.87301447 -0.33569858 -0.06902874\n",
      "   1.25839253  0.1550804  -0.54930805]\n",
      " [-0.80133038  0.71388441 -0.60588573 -0.88720459  0.24002152 -0.1863132\n",
      "  -0.65906259 -0.32298871  0.72668229]\n",
      " [ 0.63023082 -0.35696755  0.08056475  0.48082516 -0.4593245   0.31492463\n",
      "   0.63305131 -0.05975092 -0.62729916]\n",
      " [ 0.65053406 -0.64207329  0.44640471  0.5343558  -0.36289961  0.17098247\n",
      "   0.84723931  0.79188929 -0.34396637]\n",
      " [-0.97930913  0.92080188 -0.60450581 -1.00390545  0.12179139  0.173672\n",
      "  -0.97123804 -0.69100726  0.71813531]\n",
      " [ 1.12044    -0.82138061  0.59854606  0.61808918 -0.21326945 -0.23321541\n",
      "   0.77863009  0.80108614 -0.50963241]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.27003813  1.69030767 -1.84775543  0.21864642  0.36082577 -2.37669822\n",
      "   1.08095423]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:54 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.73698057]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 54 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.8717091  -0.92826334  0.4077286   0.87827318 -0.33569858 -0.06902874\n",
      "   1.26365125  0.1550804  -0.54930805]\n",
      " [-0.80702129  0.70819351 -0.60588573 -0.8928955   0.24002152 -0.1863132\n",
      "  -0.6647535  -0.32298871  0.72668229]\n",
      " [ 0.63588568 -0.35131269  0.08056475  0.48648002 -0.4593245   0.31492463\n",
      "   0.63870617 -0.05975092 -0.62729916]\n",
      " [ 0.65619088 -0.63641646  0.44640471  0.54001263 -0.36289961  0.17098247\n",
      "   0.85289614  0.79188929 -0.34396637]\n",
      " [-0.98461382  0.9154972  -0.60450581 -1.00921013  0.12179139  0.173672\n",
      "  -0.97654273 -0.69100726  0.71813531]\n",
      " [ 1.12610239 -0.81571823  0.59854606  0.62375157 -0.21326945 -0.23321541\n",
      "   0.78429247  0.80108614 -0.50963241]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.24454626  1.71292885 -1.84359204  0.23904336  0.38123459 -2.37374843\n",
      "   1.10249442]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:54 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.81823202]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 54 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.87410555 -0.92826334  0.41012505  0.87827318 -0.33569858 -0.06663229\n",
      "   1.2660477   0.1550804  -0.54930805]\n",
      " [-0.80963124  0.70819351 -0.60849569 -0.8928955   0.24002152 -0.18892315\n",
      "  -0.66736345 -0.32298871  0.72668229]\n",
      " [ 0.63889528 -0.35131269  0.08357435  0.48648002 -0.4593245   0.31793424\n",
      "   0.64171577 -0.05975092 -0.62729916]\n",
      " [ 0.6589272  -0.63641646  0.44914102  0.54001263 -0.36289961  0.17371878\n",
      "   0.85563245  0.79188929 -0.34396637]\n",
      " [-0.98709585  0.9154972  -0.60698783 -1.00921013  0.12179139  0.17118997\n",
      "  -0.97902476 -0.69100726  0.71813531]\n",
      " [ 1.12870095 -0.81571823  0.60114462  0.62375157 -0.21326945 -0.23061685\n",
      "   0.78689103  0.80108614 -0.50963241]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.23102923  1.72539565 -1.8423194   0.25041912  0.39331138 -2.3726159\n",
      "   1.11475229]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:54 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.26980849]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 55 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.87231365 -0.93005524  0.41012505  0.87648129 -0.33569858 -0.06663229\n",
      "   1.2660477   0.1550804  -0.55109995]\n",
      " [-0.80788432  0.70994042 -0.60849569 -0.89114858  0.24002152 -0.18892315\n",
      "  -0.66736345 -0.32298871  0.72842921]\n",
      " [ 0.63792535 -0.35228262  0.08357435  0.48551009 -0.4593245   0.31793424\n",
      "   0.64171577 -0.05975092 -0.62826908]\n",
      " [ 0.65749221 -0.63785145  0.44914102  0.53857764 -0.36289961  0.17371878\n",
      "   0.85563245  0.79188929 -0.34540135]\n",
      " [-0.98476362  0.91782942 -0.60698783 -1.00687791  0.12179139  0.17118997\n",
      "  -0.97902476 -0.69100726  0.72046754]\n",
      " [ 1.12598868 -0.8184305   0.60114462  0.6210393  -0.21326945 -0.23061685\n",
      "   0.78689103  0.80108614 -0.51234468]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.25760697  1.71029225 -1.85384042  0.23615683  0.37857607 -2.38352108\n",
      "   1.09866794]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:55 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.81615114]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 55 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.87467822 -0.93005524  0.41248963  0.87648129 -0.33569858 -0.06663229\n",
      "   1.26841227  0.1550804  -0.55109995]\n",
      " [-0.81071349  0.70994042 -0.61132485 -0.89114858  0.24002152 -0.18892315\n",
      "  -0.67019261 -0.32298871  0.72842921]\n",
      " [ 0.64097552 -0.35228262  0.08662452  0.48551009 -0.4593245   0.31793424\n",
      "   0.64476594 -0.05975092 -0.62826908]\n",
      " [ 0.66041635 -0.63785145  0.45206516  0.53857764 -0.36289961  0.17371878\n",
      "   0.85855659  0.79188929 -0.34540135]\n",
      " [-0.98710377  0.91782942 -0.60932799 -1.00687791  0.12179139  0.17118997\n",
      "  -0.98136491 -0.69100726  0.72046754]\n",
      " [ 1.12839083 -0.8184305   0.60354678  0.6210393  -0.21326945 -0.23061685\n",
      "   0.78929319  0.80108614 -0.51234468]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.24381385  1.72308498 -1.8523137   0.24714004  0.39066958 -2.38254118\n",
      "   1.11142822]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:55 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.7056987]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 55 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.85831048 -0.94642298  0.39612189  0.87648129 -0.33569858 -0.06663229\n",
      "   1.25204453  0.1550804  -0.55109995]\n",
      " [-0.79446689  0.72618702 -0.59507826 -0.89114858  0.24002152 -0.18892315\n",
      "  -0.65394602 -0.32298871  0.72842921]\n",
      " [ 0.62641485 -0.36684329  0.07206385  0.48551009 -0.4593245   0.31793424\n",
      "   0.63020527 -0.05975092 -0.62826908]\n",
      " [ 0.64428569 -0.65398211  0.4359345   0.53857764 -0.36289961  0.17371878\n",
      "   0.84242593  0.79188929 -0.34540135]\n",
      " [-0.97077379  0.93415941 -0.592998   -1.00687791  0.12179139  0.17118997\n",
      "  -0.96503492 -0.69100726  0.72046754]\n",
      " [ 1.11212464 -0.83469669  0.58728058  0.6210393  -0.21326945 -0.23061685\n",
      "   0.77302699  0.80108614 -0.51234468]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.31709644  1.66185329 -1.86701722  0.19327816  0.33267613 -2.39424927\n",
      "   1.04943846]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:55 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.70900501]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 55 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.86498611 -0.94642298  0.40279751  0.87648129 -0.33569858 -0.06663229\n",
      "   1.25204453  0.16175603 -0.55109995]\n",
      " [-0.80112301  0.72618702 -0.60173438 -0.89114858  0.24002152 -0.18892315\n",
      "  -0.65394602 -0.32964483  0.72842921]\n",
      " [ 0.6307508  -0.36684329  0.0763998   0.48551009 -0.4593245   0.31793424\n",
      "   0.63020527 -0.05541497 -0.62826908]\n",
      " [ 0.65077823 -0.65398211  0.44242703  0.53857764 -0.36289961  0.17371878\n",
      "   0.84242593  0.79838183 -0.34540135]\n",
      " [-0.97658963  0.93415941 -0.59881384 -1.00687791  0.12179139  0.17118997\n",
      "  -0.96503492 -0.6968231   0.72046754]\n",
      " [ 1.11738452 -0.83469669  0.59254046  0.6210393  -0.21326945 -0.23061685\n",
      "   0.77302699  0.80634602 -0.51234468]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.28707785  1.68597891 -1.86242932  0.2129243   0.35869331 -2.39139937\n",
      "   1.07718094]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:55 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.54484807]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 55 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.87699882 -0.94642298  0.40279751  0.888494   -0.33569858 -0.06663229\n",
      "   1.25204453  0.16175603 -0.53908724]\n",
      " [-0.81199524  0.72618702 -0.60173438 -0.90202081  0.24002152 -0.18892315\n",
      "  -0.65394602 -0.32964483  0.71755698]\n",
      " [ 0.63724174 -0.36684329  0.0763998   0.49200104 -0.4593245   0.31793424\n",
      "   0.63020527 -0.05541497 -0.62177814]\n",
      " [ 0.66079381 -0.65398211  0.44242703  0.54859323 -0.36289961  0.17371878\n",
      "   0.84242593  0.79838183 -0.33538577]\n",
      " [-0.98883937  0.93415941 -0.59881384 -1.01912764  0.12179139  0.17118997\n",
      "  -0.96503492 -0.6968231   0.7082178 ]\n",
      " [ 1.12952132 -0.83469669  0.59254046  0.6331761  -0.21326945 -0.23061685\n",
      "   0.77302699  0.80634602 -0.50020788]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.23064159  1.72925461 -1.84684673  0.24789408  0.39815904 -2.37895755\n",
      "   1.12081359]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:55 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.17463727]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 55 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.87720131 -0.94642298  0.40279751  0.888494   -0.33549609 -0.0664298\n",
      "   1.25204453  0.16175603 -0.53888475]\n",
      " [-0.81185893  0.72618702 -0.60173438 -0.90202081  0.24015783 -0.18878685\n",
      "  -0.65394602 -0.32964483  0.71769329]\n",
      " [ 0.63763641 -0.36684329  0.0763998   0.49200104 -0.45892984  0.3183289\n",
      "   0.63020527 -0.05541497 -0.62138348]\n",
      " [ 0.66036715 -0.65398211  0.44242703  0.54859323 -0.36332627  0.17329212\n",
      "   0.84242593  0.79838183 -0.33581242]\n",
      " [-0.98887826  0.93415941 -0.59881384 -1.01912764  0.1217525   0.17115108\n",
      "  -0.96503492 -0.6968231   0.70817891]\n",
      " [ 1.12894286 -0.83469669  0.59254046  0.6331761  -0.21384791 -0.23119531\n",
      "   0.77302699  0.80634602 -0.50078634]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.24322762  1.72316422 -1.85300339  0.24199677  0.39143805 -2.38528945\n",
      "   1.11393879]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:55 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.04256244]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 55 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.87737272 -0.94625156  0.40279751  0.888494   -0.33532468 -0.06625839\n",
      "   1.25204453  0.16175603 -0.53871333]\n",
      " [-0.81199106  0.72605489 -0.60173438 -0.90202081  0.2400257  -0.18891898\n",
      "  -0.65394602 -0.32964483  0.71756116]\n",
      " [ 0.63773673 -0.36674297  0.0763998   0.49200104 -0.45882952  0.31842922\n",
      "   0.63020527 -0.05541497 -0.62128316]\n",
      " [ 0.66047251 -0.65387676  0.44242703  0.54859323 -0.36322092  0.17339748\n",
      "   0.84242593  0.79838183 -0.33570707]\n",
      " [-0.98904358  0.93399409 -0.59881384 -1.01912764  0.12158718  0.17098577\n",
      "  -0.96503492 -0.6968231   0.70801359]\n",
      " [ 1.12907013 -0.83456942  0.59254046  0.6331761  -0.21372064 -0.23106804\n",
      "   0.77302699  0.80634602 -0.50065907]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.24409485  1.72293269 -1.85357966  0.24166756  0.39111459 -2.38591419\n",
      "   1.11364165]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:55 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.42270082]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 55 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.86869012 -0.95493416  0.40279751  0.888494   -0.34400728 -0.07494099\n",
      "   1.24336192  0.16175603 -0.53871333]\n",
      " [-0.80408549  0.73396046 -0.60173438 -0.90202081  0.24793127 -0.1810134\n",
      "  -0.64604045 -0.32964483  0.71756116]\n",
      " [ 0.6292202  -0.3752595   0.0763998   0.49200104 -0.46734604  0.30991269\n",
      "   0.62168874 -0.05541497 -0.62128316]\n",
      " [ 0.65283338 -0.66151588  0.44242703  0.54859323 -0.37086004  0.16575835\n",
      "   0.8347868   0.79838183 -0.33570707]\n",
      " [-0.98080265  0.94223502 -0.59881384 -1.01912764  0.12982811  0.1792267\n",
      "  -0.95679399 -0.6968231   0.70801359]\n",
      " [ 1.1217715  -0.84186805  0.59254046  0.6331761  -0.22101927 -0.23836666\n",
      "   0.76572836  0.80634602 -0.50065907]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.2956696   1.68755133 -1.87082139  0.20651797  0.35712269 -2.4027142\n",
      "   1.08007468]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:55 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.82082209]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 55 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.87047532 -0.95493416  0.40279751  0.8902792  -0.34400728 -0.07494099\n",
      "   1.24514713  0.16175603 -0.53871333]\n",
      " [-0.80654455  0.73396046 -0.60173438 -0.90447987  0.24793127 -0.1810134\n",
      "  -0.64849951 -0.32964483  0.71756116]\n",
      " [ 0.63213121 -0.3752595   0.0763998   0.49491205 -0.46734604  0.30991269\n",
      "   0.62459975 -0.05541497 -0.62128316]\n",
      " [ 0.6555733  -0.66151588  0.44242703  0.55133314 -0.37086004  0.16575835\n",
      "   0.83752672  0.79838183 -0.33570707]\n",
      " [-0.9826328   0.94223502 -0.59881384 -1.0209578   0.12982811  0.1792267\n",
      "  -0.95862414 -0.6968231   0.70801359]\n",
      " [ 1.12405929 -0.84186805  0.59254046  0.63546388 -0.22101927 -0.23836666\n",
      "   0.76801615  0.80634602 -0.50065907]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.28249347  1.7001029  -1.86967644  0.2177316   0.3687776  -2.40206304\n",
      "   1.09227022]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:55 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.67310407]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 55 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.87846317 -0.95493416  0.40279751  0.8902792  -0.33601944 -0.06695315\n",
      "   1.25313497  0.16175603 -0.53871333]\n",
      " [-0.81452344  0.73396046 -0.60173438 -0.90447987  0.23995238 -0.1889923\n",
      "  -0.6564784  -0.32964483  0.71756116]\n",
      " [ 0.63954156 -0.3752595   0.0763998   0.49491205 -0.45993569  0.31732305\n",
      "   0.6320101  -0.05541497 -0.62128316]\n",
      " [ 0.66342268 -0.66151588  0.44242703  0.55133314 -0.36301066  0.17360773\n",
      "   0.8453761   0.79838183 -0.33570707]\n",
      " [-0.99066234  0.94223502 -0.59881384 -1.0209578   0.12179857  0.17119716\n",
      "  -0.96665368 -0.6968231   0.70801359]\n",
      " [ 1.13207433 -0.84186805  0.59254046  0.63546388 -0.21300423 -0.23035162\n",
      "   0.77603119  0.80634602 -0.50065907]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.2465292   1.7304963  -1.86249412  0.24470942  0.39696669 -2.39618184\n",
      "   1.1213049 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m\u001b[1m----------Processing on batch:55 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.58565988]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 55 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.86346996 -0.95493416  0.40279751  0.875286   -0.33601944 -0.08194635\n",
      "   1.25313497  0.16175603 -0.55370653]\n",
      " [-0.79939794  0.73396046 -0.60173438 -0.88935437  0.23995238 -0.17386679\n",
      "  -0.6564784  -0.32964483  0.73268666]\n",
      " [ 0.6270658  -0.3752595   0.0763998   0.48243629 -0.45993569  0.30484729\n",
      "   0.6320101  -0.05541497 -0.63375891]\n",
      " [ 0.64907731 -0.66151588  0.44242703  0.53698777 -0.36301066  0.15926236\n",
      "   0.8453761   0.79838183 -0.35005244]\n",
      " [-0.97583063  0.94223502 -0.59881384 -1.00612608  0.12179857  0.18602887\n",
      "  -0.96665368 -0.6968231   0.7228453 ]\n",
      " [ 1.1178391  -0.84186805  0.59254046  0.62122866 -0.21300423 -0.24458685\n",
      "   0.77603119  0.80634602 -0.5148943 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.31758801  1.67635721 -1.87906371  0.19521986  0.34429188 -2.41350007\n",
      "   1.06885075]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:55 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.70140982]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 55 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.87011033 -0.9482938   0.40279751  0.88192637 -0.33601944 -0.07530598\n",
      "   1.25977534  0.16175603 -0.55370653]\n",
      " [-0.80626501  0.72709339 -0.60173438 -0.89622144  0.23995238 -0.18073387\n",
      "  -0.66334547 -0.32964483  0.73268666]\n",
      " [ 0.634027   -0.36829831  0.0763998   0.48939748 -0.45993569  0.31180848\n",
      "   0.6389713  -0.05541497 -0.63375891]\n",
      " [ 0.65607671 -0.65451649  0.44242703  0.54398717 -0.36301066  0.16626176\n",
      "   0.8523755   0.79838183 -0.35005244]\n",
      " [-0.98265859  0.93540706 -0.59881384 -1.01295404  0.12179857  0.17920091\n",
      "  -0.97348165 -0.6968231   0.7228453 ]\n",
      " [ 1.12480501 -0.83490214  0.59254046  0.62819457 -0.21300423 -0.23762094\n",
      "   0.7829971   0.80634602 -0.5148943 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.28632053  1.70374728 -1.87457151  0.2215384   0.36999023 -2.40914162\n",
      "   1.09407391]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:55 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.73908252]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 55 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.87530132 -0.94310281  0.40279751  0.88711736 -0.33601944 -0.07530598\n",
      "   1.26496633  0.16175603 -0.55370653]\n",
      " [-0.81187952  0.72147888 -0.60173438 -0.90183594  0.23995238 -0.18073387\n",
      "  -0.66895998 -0.32964483  0.73268666]\n",
      " [ 0.63961228 -0.36271303  0.0763998   0.49498276 -0.45993569  0.31180848\n",
      "   0.64455657 -0.05541497 -0.63375891]\n",
      " [ 0.66166441 -0.64892879  0.44242703  0.54957487 -0.36301066  0.16626176\n",
      "   0.8579632   0.79838183 -0.35005244]\n",
      " [-0.98789366  0.93017199 -0.59881384 -1.01818912  0.12179857  0.17920091\n",
      "  -0.97871672 -0.6968231   0.7228453 ]\n",
      " [ 1.13039012 -0.82931703  0.59254046  0.63377967 -0.21300423 -0.23762094\n",
      "   0.78858221  0.80634602 -0.5148943 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.26116292  1.72606978 -1.87047996  0.24169582  0.39016296 -2.4062306\n",
      "   1.11534913]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:55 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.81973388]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 55 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.8776692  -0.94310281  0.40516539  0.88711736 -0.33601944 -0.0729381\n",
      "   1.26733421  0.16175603 -0.55370653]\n",
      " [-0.81445187  0.72147888 -0.60430673 -0.90183594  0.23995238 -0.18330622\n",
      "  -0.67153233 -0.32964483  0.73268666]\n",
      " [ 0.64257721 -0.36271303  0.07936473  0.49498276 -0.45993569  0.31477341\n",
      "   0.6475215  -0.05541497 -0.63375891]\n",
      " [ 0.66435905 -0.64892879  0.44512168  0.54957487 -0.36301066  0.16895641\n",
      "   0.86065784  0.79838183 -0.35005244]\n",
      " [-0.99034516  0.93017199 -0.60126535 -1.01818912  0.12179857  0.1767494\n",
      "  -0.98116822 -0.6968231   0.7228453 ]\n",
      " [ 1.13295237 -0.82931703  0.59510271  0.63377967 -0.21300423 -0.23505869\n",
      "   0.79114446  0.80634602 -0.5148943 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.24784394  1.73834789 -1.8692252   0.25290897  0.40206514 -2.40510876\n",
      "   1.12742534]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:55 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.26351951]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 56 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.87598645 -0.94478556  0.40516539  0.8854346  -0.33601944 -0.0729381\n",
      "   1.26733421  0.16175603 -0.55538929]\n",
      " [-0.81280461  0.72312614 -0.60430673 -0.90018869  0.23995238 -0.18330622\n",
      "  -0.67153233 -0.32964483  0.73433392]\n",
      " [ 0.64167972 -0.36361051  0.07936473  0.49408528 -0.45993569  0.31477341\n",
      "   0.6475215  -0.05541497 -0.6346564 ]\n",
      " [ 0.66300064 -0.6502872   0.44512168  0.54821645 -0.36301066  0.16895641\n",
      "   0.86065784  0.79838183 -0.35141086]\n",
      " [-0.98814271  0.93237444 -0.60126535 -1.01598667  0.12179857  0.1767494\n",
      "  -0.98116822 -0.6968231   0.72504775]\n",
      " [ 1.1303683  -0.8319011   0.59510271  0.6311956  -0.21300423 -0.23505869\n",
      "   0.79114446  0.80634602 -0.51747837]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.27341547  1.72385916 -1.88034478  0.23922274  0.38791047 -2.41564538\n",
      "   1.11197793]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:56 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.81854717]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 56 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.87829655 -0.94478556  0.4074755   0.8854346  -0.33601944 -0.0729381\n",
      "   1.26964432  0.16175603 -0.55538929]\n",
      " [-0.81556454  0.72312614 -0.60706666 -0.90018869  0.23995238 -0.18330622\n",
      "  -0.67429226 -0.32964483  0.73433392]\n",
      " [ 0.64466184 -0.36361051  0.08234684  0.49408528 -0.45993569  0.31477341\n",
      "   0.65050362 -0.05541497 -0.6346564 ]\n",
      " [ 0.66585285 -0.6502872   0.4479739   0.54821645 -0.36301066  0.16895641\n",
      "   0.86351006  0.79838183 -0.35141086]\n",
      " [-0.99042917  0.93237444 -0.6035518  -1.01598667  0.12179857  0.1767494\n",
      "  -0.98345468 -0.6968231   0.72504775]\n",
      " [ 1.13271236 -0.8319011   0.59744677  0.6311956  -0.21300423 -0.23505869\n",
      "   0.79348853  0.80634602 -0.51747837]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.25994009  1.7363572  -1.87885972  0.24996462  0.39973485 -2.41468787\n",
      "   1.12444668]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:56 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.70391706]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 56 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.86190031 -0.96118181  0.39107925  0.8854346  -0.33601944 -0.0729381\n",
      "   1.25324807  0.16175603 -0.55538929]\n",
      " [-0.79931974  0.73937095 -0.59082185 -0.90018869  0.23995238 -0.18330622\n",
      "  -0.65804745 -0.32964483  0.73433392]\n",
      " [ 0.63013317 -0.37813918  0.06781818  0.49408528 -0.45993569  0.31477341\n",
      "   0.63597496 -0.05541497 -0.6346564 ]\n",
      " [ 0.64972322 -0.66641684  0.43184426  0.54821645 -0.36301066  0.16895641\n",
      "   0.84738043  0.79838183 -0.35141086]\n",
      " [-0.9740653   0.94873831 -0.58718794 -1.01598667  0.12179857  0.1767494\n",
      "  -0.96709081 -0.6968231   0.72504775]\n",
      " [ 1.116412   -0.84820146  0.58114641  0.6311956  -0.21300423 -0.23505869\n",
      "   0.77718817  0.80634602 -0.51747837]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.33329452  1.67521635 -1.89367783  0.19613838  0.34175902 -2.42655498\n",
      "   1.06250226]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:56 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.7118413]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 56 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.86847605 -0.96118181  0.397655    0.8854346  -0.33601944 -0.0729381\n",
      "   1.25324807  0.16833177 -0.55538929]\n",
      " [-0.8058675   0.73937095 -0.59736961 -0.90018869  0.23995238 -0.18330622\n",
      "  -0.65804745 -0.33619259  0.73433392]\n",
      " [ 0.63442241 -0.37813918  0.07210742  0.49408528 -0.45993569  0.31477341\n",
      "   0.63597496 -0.05112573 -0.6346564 ]\n",
      " [ 0.6561052  -0.66641684  0.43822625  0.54821645 -0.36301066  0.16895641\n",
      "   0.84738043  0.80476382 -0.35141086]\n",
      " [-0.97978421  0.94873831 -0.59290685 -1.01598667  0.12179857  0.1767494\n",
      "  -0.96709081 -0.70254201  0.72504775]\n",
      " [ 1.12158255 -0.84820146  0.58631696  0.6311956  -0.21300423 -0.23505869\n",
      "   0.77718817  0.81151657 -0.51747837]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.30374049  1.69899292 -1.88918862  0.21550592  0.36740025 -2.42375754\n",
      "   1.08982245]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:56 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.54674568]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 56 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.88045978 -0.96118181  0.397655    0.89741834 -0.33601944 -0.0729381\n",
      "   1.25324807  0.16833177 -0.54340556]\n",
      " [-0.81673671  0.73937095 -0.59736961 -0.91105789  0.23995238 -0.18330622\n",
      "  -0.65804745 -0.33619259  0.72346471]\n",
      " [ 0.64095011 -0.37813918  0.07210742  0.50061298 -0.45993569  0.31477341\n",
      "   0.63597496 -0.05112573 -0.62812869]\n",
      " [ 0.66614175 -0.66641684  0.43822625  0.558253   -0.36301066  0.16895641\n",
      "   0.84738043  0.80476382 -0.34137431]\n",
      " [-0.9919959   0.94873831 -0.59290685 -1.02819836  0.12179857  0.1767494\n",
      "  -0.96709081 -0.70254201  0.71283606]\n",
      " [ 1.13368979 -0.84820146  0.58631696  0.64330284 -0.21300423 -0.23505869\n",
      "   0.77718817  0.81151657 -0.50537113]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.24757892  1.7421397  -1.87377017  0.250383    0.40677947 -2.41145065\n",
      "   1.13333334]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:56 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.16885405]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 56 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.8806725  -0.96118181  0.397655    0.89741834 -0.33580672 -0.07272538\n",
      "   1.25324807  0.16833177 -0.54319284]\n",
      " [-0.81662825  0.73937095 -0.59736961 -0.91105789  0.24006084 -0.18319776\n",
      "  -0.65804745 -0.33619259  0.72357317]\n",
      " [ 0.64134042 -0.37813918  0.07210742  0.50061298 -0.45954539  0.31516372\n",
      "   0.63597496 -0.05112573 -0.62773839]\n",
      " [ 0.6657562  -0.66641684  0.43822625  0.558253   -0.3633962   0.16857086\n",
      "   0.84738043  0.80476382 -0.34175986]\n",
      " [-0.99205333  0.94873831 -0.59290685 -1.02819836  0.12174115  0.17669198\n",
      "  -0.96709081 -0.70254201  0.71277864]\n",
      " [ 1.13316016 -0.84820146  0.58631696  0.64330284 -0.21353386 -0.23558833\n",
      "   0.77718817  0.81151657 -0.50590076]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.25942761  1.73642825 -1.87958603  0.2448501   0.40046848 -2.41743243\n",
      "   1.12687648]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:56 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.03948889]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 56 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.88082221 -0.9610321   0.397655    0.89741834 -0.33565701 -0.07257567\n",
      "   1.25324807  0.16833177 -0.54304313]\n",
      " [-0.81674488  0.73925432 -0.59736961 -0.91105789  0.23994421 -0.18331439\n",
      "  -0.65804745 -0.33619259  0.72345654]\n",
      " [ 0.64142979 -0.37804981  0.07210742  0.50061298 -0.45945602  0.31525309\n",
      "   0.63597496 -0.05112573 -0.62764902]\n",
      " [ 0.66584986 -0.66632318  0.43822625  0.558253   -0.36330254  0.16866452\n",
      "   0.84738043  0.80476382 -0.34166619]\n",
      " [-0.99219794  0.94859369 -0.59290685 -1.02819836  0.12159653  0.17654736\n",
      "  -0.96709081 -0.70254201  0.71263402]\n",
      " [ 1.13327252 -0.8480891   0.58631696  0.64330284 -0.2134215  -0.23547597\n",
      "   0.77718817  0.81151657 -0.5057884 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.2601765   1.73623156 -1.88008696  0.24456893  0.40019227 -2.41797515\n",
      "   1.126623  ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:56 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.41502167]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 56 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.87247148 -0.96938283  0.397655    0.89741834 -0.34400774 -0.08092641\n",
      "   1.24489734  0.16833177 -0.54304313]\n",
      " [-0.80910843  0.74689077 -0.59736961 -0.91105789  0.24758066 -0.17567794\n",
      "  -0.650411   -0.33619259  0.72345654]\n",
      " [ 0.63315554 -0.38632406  0.07210742  0.50061298 -0.46773027  0.30697884\n",
      "   0.62770071 -0.05112573 -0.62764902]\n",
      " [ 0.65845004 -0.673723    0.43822625  0.558253   -0.37070236  0.1612647\n",
      "   0.8399806   0.80476382 -0.34166619]\n",
      " [-0.98427276  0.95651888 -0.59290685 -1.02819836  0.12952172  0.18447255\n",
      "  -0.95916562 -0.70254201  0.71263402]\n",
      " [ 1.12622963 -0.85513199  0.58631696  0.64330284 -0.22046439 -0.24251886\n",
      "   0.77014527  0.81151657 -0.5057884 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.31055571  1.70185298 -1.89703982  0.21029594  0.36706685 -2.43455169\n",
      "   1.09394035]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:56 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.82489438]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 56 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.87417098 -0.96938283  0.397655    0.89911784 -0.34400774 -0.08092641\n",
      "   1.24659684  0.16833177 -0.54304313]\n",
      " [-0.81145112  0.74689077 -0.59736961 -0.91340058  0.24758066 -0.17567794\n",
      "  -0.65275369 -0.33619259  0.72345654]\n",
      " [ 0.6359426  -0.38632406  0.07210742  0.50340004 -0.46773027  0.30697884\n",
      "   0.63048777 -0.05112573 -0.62764902]\n",
      " [ 0.66106465 -0.673723    0.43822625  0.5608676  -0.37070236  0.1612647\n",
      "   0.84259521  0.80476382 -0.34166619]\n",
      " [-0.98601461  0.95651888 -0.59290685 -1.02994021  0.12952172  0.18447255\n",
      "  -0.96090748 -0.70254201  0.71263402]\n",
      " [ 1.12840645 -0.85513199  0.58631696  0.64547967 -0.22046439 -0.24251886\n",
      "   0.7723221   0.81151657 -0.5057884 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.29790926  1.71390805 -1.89595925  0.22108834  0.37827946 -2.43393551\n",
      "   1.10566205]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:56 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.67517216]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 56 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.88208298 -0.96938283  0.397655    0.89911784 -0.33609575 -0.07301441\n",
      "   1.25450884  0.16833177 -0.54304313]\n",
      " [-0.81935748  0.74689077 -0.59736961 -0.91340058  0.2396743  -0.1835843\n",
      "  -0.66066005 -0.33619259  0.72345654]\n",
      " [ 0.64330098 -0.38632406  0.07210742  0.50340004 -0.46037188  0.31433723\n",
      "   0.63784616 -0.05112573 -0.62764902]\n",
      " [ 0.66884881 -0.673723    0.43822625  0.5608676  -0.3629182   0.16904886\n",
      "   0.85037937  0.80476382 -0.34166619]\n",
      " [-0.99396689  0.95651888 -0.59290685 -1.02994021  0.12156945  0.17652028\n",
      "  -0.96885975 -0.70254201  0.71263402]\n",
      " [ 1.13634798 -0.85513199  0.58631696  0.64547967 -0.21252287 -0.23457733\n",
      "   0.78026363  0.81151657 -0.5057884 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.2622895   1.74400639 -1.88886948  0.24785027  0.40623602 -2.42811419\n",
      "   1.13444657]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:56 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.58650097]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 56 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.86706681 -0.96938283  0.397655    0.88410167 -0.33609575 -0.08803058\n",
      "   1.25450884  0.16833177 -0.5580593 ]\n",
      " [-0.80420797  0.74689077 -0.59736961 -0.89825107  0.2396743  -0.16843479\n",
      "  -0.66066005 -0.33619259  0.73860605]\n",
      " [ 0.6307855  -0.38632406  0.07210742  0.49088455 -0.46037188  0.30182174\n",
      "   0.63784616 -0.05112573 -0.6401645 ]\n",
      " [ 0.65446174 -0.673723    0.43822625  0.54648054 -0.3629182   0.1546618\n",
      "   0.85037937  0.80476382 -0.35605326]\n",
      " [-0.97910989  0.95651888 -0.59290685 -1.01508322  0.12156945  0.19137727\n",
      "  -0.96885975 -0.70254201  0.72749102]\n",
      " [ 1.12206666 -0.85513199  0.58631696  0.63119835 -0.21252287 -0.24885865\n",
      "   0.78026363  0.81151657 -0.52006972]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.3334079   1.68979525 -1.9054218   0.19827569  0.35345651 -2.44541623\n",
      "   1.08188068]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:56 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.70263699]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 56 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.87367486 -0.96277478  0.397655    0.89070973 -0.33609575 -0.08142252\n",
      "   1.26111689  0.16833177 -0.5580593 ]\n",
      " [-0.81103112  0.74006762 -0.59736961 -0.90507422  0.2396743  -0.17525794\n",
      "  -0.6674832  -0.33619259  0.73860605]\n",
      " [ 0.63769934 -0.37941022  0.07210742  0.49779839 -0.46037188  0.30873558\n",
      "   0.64475999 -0.05112573 -0.6401645 ]\n",
      " [ 0.66141606 -0.66676868  0.43822625  0.55343486 -0.3629182   0.16161612\n",
      "   0.85733369  0.80476382 -0.35605326]\n",
      " [-0.98589969  0.94972908 -0.59290685 -1.02187302  0.12156945  0.18458747\n",
      "  -0.97564955 -0.70254201  0.72749102]\n",
      " [ 1.12898805 -0.8482106   0.58631696  0.63811974 -0.21252287 -0.24193725\n",
      "   0.78718502  0.81151657 -0.52006972]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.30234265  1.71698533 -1.90095685  0.22444019  0.37900261 -2.4410666\n",
      "   1.10694504]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:56 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.74113816]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 56 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.87879977 -0.95764987  0.397655    0.89583463 -0.33609575 -0.08142252\n",
      "   1.2662418   0.16833177 -0.5580593 ]\n",
      " [-0.81657113  0.73452761 -0.59736961 -0.91061423  0.2396743  -0.17525794\n",
      "  -0.67302321 -0.33619259  0.73860605]\n",
      " [ 0.64321641 -0.37389314  0.07210742  0.50331547 -0.46037188  0.30873558\n",
      "   0.65027707 -0.05112573 -0.6401645 ]\n",
      " [ 0.66693585 -0.66124889  0.43822625  0.55895464 -0.3629182   0.16161612\n",
      "   0.86285347  0.80476382 -0.35605326]\n",
      " [-0.99106693  0.94456185 -0.59290685 -1.02704025  0.12156945  0.18458747\n",
      "  -0.98081678 -0.70254201  0.72749102]\n",
      " [ 1.13449791 -0.84270074  0.58631696  0.6436296  -0.21252287 -0.24193725\n",
      "   0.79269487  0.81151657 -0.52006972]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.27751102  1.73901663 -1.89693446  0.24436306  0.39894345 -2.43819332\n",
      "   1.12796073]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:56 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.82116219]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 56 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.88114097 -0.95764987  0.3999962   0.89583463 -0.33609575 -0.07908132\n",
      "   1.268583    0.16833177 -0.5580593 ]\n",
      " [-0.8191083   0.73452761 -0.59990679 -0.91061423  0.2396743  -0.17779512\n",
      "  -0.67556038 -0.33619259  0.73860605]\n",
      " [ 0.64613914 -0.37389314  0.07503014  0.50331547 -0.46037188  0.3116583\n",
      "   0.65319979 -0.05112573 -0.6401645 ]\n",
      " [ 0.66959148 -0.66124889  0.44088187  0.55895464 -0.3629182   0.16427175\n",
      "   0.8655091   0.80476382 -0.35605326]\n",
      " [-0.99348993  0.94456185 -0.59532985 -1.02704025  0.12156945  0.18216446\n",
      "  -0.98323979 -0.70254201  0.72749102]\n",
      " [ 1.1370263  -0.84270074  0.58884535  0.6436296  -0.21252287 -0.23940886\n",
      "   0.79522326  0.81151657 -0.52006972]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.26437942  1.75111585 -1.89569614  0.25542188  0.41067982 -2.43708114\n",
      "   1.13986446]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:56 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.25735902]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 57 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.87956169 -0.95922916  0.3999962   0.89425535 -0.33609575 -0.07908132\n",
      "   1.268583    0.16833177 -0.55963859]\n",
      " [-0.81755636  0.73607956 -0.59990679 -0.90906228  0.2396743  -0.17779512\n",
      "  -0.67556038 -0.33619259  0.74015799]\n",
      " [ 0.64531045 -0.37472183  0.07503014  0.50248678 -0.46037188  0.3116583\n",
      "   0.65319979 -0.05112573 -0.64099319]\n",
      " [ 0.66830703 -0.66253334  0.44088187  0.5576702  -0.3629182   0.16427175\n",
      "   0.8655091   0.80476382 -0.3573377 ]\n",
      " [-0.99141108  0.9466407  -0.59532985 -1.0249614   0.12156945  0.18216446\n",
      "  -0.98323979 -0.70254201  0.72956987]\n",
      " [ 1.13456592 -0.84516112  0.58884535  0.64116922 -0.21252287 -0.23940886\n",
      "   0.79522326  0.81151657 -0.52253009]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.28897333  1.73722158 -1.90642407  0.24229371  0.39708884 -2.44725691\n",
      "   1.12503489]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:57 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.82085019]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 57 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.88182022 -0.95922916  0.40225473  0.89425535 -0.33609575 -0.07908132\n",
      "   1.27084153  0.16833177 -0.55963859]\n",
      " [-0.82025068  0.73607956 -0.60260112 -0.90906228  0.2396743  -0.17779512\n",
      "  -0.67825471 -0.33619259  0.74015799]\n",
      " [ 0.64822748 -0.37472183  0.07794716  0.50248678 -0.46037188  0.3116583\n",
      "   0.65611682 -0.05112573 -0.64099319]\n",
      " [ 0.67109103 -0.66253334  0.44366587  0.5576702  -0.3629182   0.16427175\n",
      "   0.8682931   0.80476382 -0.3573377 ]\n",
      " [-0.99364676  0.9466407  -0.59756553 -1.0249614   0.12156945  0.18216446\n",
      "  -0.98547547 -0.70254201  0.72956987]\n",
      " [ 1.13685519 -0.84516112  0.59113462  0.64116922 -0.21252287 -0.23940886\n",
      "   0.79751253  0.81151657 -0.52253009]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.27580088  1.74943835 -1.90497804  0.25280469  0.40865577 -2.44632041\n",
      "   1.13722518]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:57 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.70198664]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 57 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.86539737 -0.97565201  0.38583188  0.89425535 -0.33609575 -0.07908132\n",
      "   1.25441869  0.16833177 -0.55963859]\n",
      " [-0.80400885  0.75232138 -0.58635929 -0.90906228  0.2396743  -0.17779512\n",
      "  -0.66201288 -0.33619259  0.74015799]\n",
      " [ 0.63373228 -0.38921703  0.06345196  0.50248678 -0.46037188  0.3116583\n",
      "   0.64162161 -0.05112573 -0.64099319]\n",
      " [ 0.65496369 -0.67866068  0.42753853  0.5576702  -0.3629182   0.16427175\n",
      "   0.85216576  0.80476382 -0.3573377 ]\n",
      " [-0.97725073  0.96303672 -0.58116951 -1.0249614   0.12156945  0.18216446\n",
      "  -0.96907945 -0.70254201  0.72956987]\n",
      " [ 1.12052086 -0.86149545  0.57480029  0.64116922 -0.21252287 -0.23940886\n",
      "   0.7811782   0.81151657 -0.52253009]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.34922917  1.68838898 -1.91991515  0.19901521  0.35070119 -2.45834972\n",
      "   1.07532882]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:57 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.71458195]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 57 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.87187646 -0.97565201  0.39231097  0.89425535 -0.33609575 -0.07908132\n",
      "   1.25441869  0.17481086 -0.55963859]\n",
      " [-0.81045229  0.75232138 -0.59280273 -0.90906228  0.2396743  -0.17779512\n",
      "  -0.66201288 -0.34263603  0.74015799]\n",
      " [ 0.63797501 -0.38921703  0.06769469  0.50248678 -0.46037188  0.3116583\n",
      "   0.64162161 -0.046883   -0.64099319]\n",
      " [ 0.66123953 -0.67866068  0.43381437  0.5576702  -0.3629182   0.16427175\n",
      "   0.85216576  0.81103966 -0.3573377 ]\n",
      " [-0.982877    0.96303672 -0.58679577 -1.0249614   0.12156945  0.18216446\n",
      "  -0.96907945 -0.70816827  0.72956987]\n",
      " [ 1.12560634 -0.86149545  0.57988577  0.64116922 -0.21252287 -0.23940886\n",
      "   0.7811782   0.81660205 -0.52253009]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.32012301  1.71182729 -1.91551931  0.2181124   0.37597825 -2.45560186\n",
      "   1.10224095]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:57 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.54865482]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 57 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.8838289  -0.97565201  0.39231097  0.90620778 -0.33609575 -0.07908132\n",
      "   1.25441869  0.17481086 -0.54768615]\n",
      " [-0.82131515  0.75232138 -0.59280273 -0.91992514  0.2396743  -0.17779512\n",
      "  -0.66201288 -0.34263603  0.72929514]\n",
      " [ 0.64453535 -0.38921703  0.06769469  0.50904712 -0.46037188  0.3116583\n",
      "   0.64162161 -0.046883   -0.63443285]\n",
      " [ 0.67129262 -0.67866068  0.43381437  0.56772328 -0.3629182   0.16427175\n",
      "   0.85216576  0.81103966 -0.34728461]\n",
      " [-0.99504865  0.96303672 -0.58679577 -1.03713305  0.12156945  0.18216446\n",
      "  -0.96907945 -0.70816827  0.71739822]\n",
      " [ 1.13768129 -0.86149545  0.57988577  0.65324418 -0.21252287 -0.23940886\n",
      "   0.7811782   0.81660205 -0.51045514]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.2642391   1.7548399  -1.90026191  0.25289088  0.41526402 -2.44342783\n",
      "   1.14562375]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:57 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.16332153]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 57 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.88404903 -0.97565201  0.39231097  0.90620778 -0.33587561 -0.07886118\n",
      "   1.25441869  0.17481086 -0.54746602]\n",
      " [-0.82123109  0.75232138 -0.59280273 -0.91992514  0.23975836 -0.17771105\n",
      "  -0.66201288 -0.34263603  0.7293792 ]\n",
      " [ 0.64492018 -0.38921703  0.06769469  0.50904712 -0.45998705  0.31204313\n",
      "   0.64162161 -0.046883   -0.63404802]\n",
      " [ 0.67094427 -0.67866068  0.43381437  0.56772328 -0.36326655  0.1639234\n",
      "   0.85216576  0.81103966 -0.34763296]\n",
      " [-0.9951214   0.96303672 -0.58679577 -1.03713305  0.12149669  0.18209171\n",
      "  -0.96907945 -0.70816827  0.71732546]\n",
      " [ 1.13719601 -0.86149545  0.57988577  0.65324418 -0.21300815 -0.23989414\n",
      "   0.7811782   0.81660205 -0.51094042]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.27539785  1.74948089 -1.90575721  0.24769757  0.40933539 -2.44907996\n",
      "   1.13955661]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:57 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.03666254]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 57 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.88417981 -0.97552123  0.39231097  0.90620778 -0.33574484 -0.07873041\n",
      "   1.25441869  0.17481086 -0.54733524]\n",
      " [-0.821334    0.75221846 -0.59280273 -0.91992514  0.23965544 -0.17781397\n",
      "  -0.66201288 -0.34263603  0.72927628]\n",
      " [ 0.64499973 -0.38913748  0.06769469  0.50904712 -0.4599075   0.31212268\n",
      "   0.64162161 -0.046883   -0.63396847]\n",
      " [ 0.67102748 -0.67857747  0.43381437  0.56772328 -0.36318334  0.16400661\n",
      "   0.85216576  0.81103966 -0.34754975]\n",
      " [-0.99524792  0.9629102  -0.58679577 -1.03713305  0.12137017  0.18196519\n",
      "  -0.96907945 -0.70816827  0.71719895]\n",
      " [ 1.13729518 -0.86139628  0.57988577  0.65324418 -0.21290898 -0.23979497\n",
      "   0.7811782   0.81660205 -0.51084125]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.27604528  1.74931354 -1.90619303  0.24745713  0.40909922 -2.44955184\n",
      "   1.13934008]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:57 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.40740933]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 57 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.87615375 -0.98354729  0.39231097  0.90620778 -0.3437709  -0.08675647\n",
      "   1.24639263  0.17481086 -0.54733524]\n",
      " [-0.813962    0.75959047 -0.59280273 -0.91992514  0.24702745 -0.17044196\n",
      "  -0.65464088 -0.34263603  0.72927628]\n",
      " [ 0.63696506 -0.39717215  0.06769469  0.50904712 -0.46794216  0.30408801\n",
      "   0.63358695 -0.046883   -0.63396847]\n",
      " [ 0.66386415 -0.6857408   0.43381437  0.56772328 -0.37034667  0.15684328\n",
      "   0.84500242  0.81103966 -0.34754975]\n",
      " [-0.98763127  0.97052685 -0.58679577 -1.03713305  0.12898682  0.18958184\n",
      "  -0.9614628  -0.70816827  0.71719895]\n",
      " [ 1.13050373 -0.86818773  0.57988577  0.65324418 -0.21970043 -0.24658643\n",
      "   0.77438674  0.81660205 -0.51084125]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.32522508  1.71592665 -1.92284809  0.21405848  0.37683816 -2.46589123\n",
      "   1.10753766]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:57 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.82883005]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 57 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.87777238 -0.98354729  0.39231097  0.90782642 -0.3437709  -0.08675647\n",
      "   1.24801126  0.17481086 -0.54733524]\n",
      " [-0.81619474  0.75959047 -0.59280273 -0.92215788  0.24702745 -0.17044196\n",
      "  -0.65687362 -0.34263603  0.72927628]\n",
      " [ 0.63963396 -0.39717215  0.06769469  0.51171602 -0.46794216  0.30408801\n",
      "   0.63625585 -0.046883   -0.63396847]\n",
      " [ 0.66635995 -0.6857408   0.43381437  0.57021908 -0.37034667  0.15684328\n",
      "   0.84749822  0.81103966 -0.34754975]\n",
      " [-0.98928986  0.97052685 -0.58679577 -1.03879164  0.12898682  0.18958184\n",
      "  -0.96312138 -0.70816827  0.71719895]\n",
      " [ 1.13257595 -0.86818773  0.57988577  0.6553164  -0.21970043 -0.24658643\n",
      "   0.77645897  0.81660205 -0.51084125]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.31308307  1.72750838 -1.92182754  0.22444776  0.38762763 -2.46530778\n",
      "   1.11880688]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:57 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.67725569]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 57 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.88560776 -0.98354729  0.39231097  0.90782642 -0.33593552 -0.07892109\n",
      "   1.25584664  0.17481086 -0.54733524]\n",
      " [-0.82402783  0.75959047 -0.59280273 -0.92215788  0.23919435 -0.17827506\n",
      "  -0.66470671 -0.34263603  0.72927628]\n",
      " [ 0.64693913 -0.39717215  0.06769469  0.51171602 -0.46063699  0.31139318\n",
      "   0.64356102 -0.046883   -0.63396847]\n",
      " [ 0.67407775 -0.6857408   0.43381437  0.57021908 -0.36262887  0.16456108\n",
      "   0.85521602  0.81103966 -0.34754975]\n",
      " [-0.99716423  0.97052685 -0.58679577 -1.03879164  0.12111245  0.18170747\n",
      "  -0.97099576 -0.70816827  0.71719895]\n",
      " [ 1.14044317 -0.86818773  0.57988577  0.6553164  -0.21183322 -0.23871922\n",
      "   0.78432618  0.81660205 -0.51084125]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.27781028  1.75731086 -1.9148303   0.25099083  0.41534833 -2.45954757\n",
      "   1.14733833]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:57 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.58732716]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 57 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.87056917 -0.98354729  0.39231097  0.89278783 -0.33593552 -0.09395968\n",
      "   1.25584664  0.17481086 -0.56237383]\n",
      " [-0.80885542  0.75959047 -0.59280273 -0.90698547  0.23919435 -0.16310264\n",
      "  -0.66470671 -0.34263603  0.7444487 ]\n",
      " [ 0.63438634 -0.39717215  0.06769469  0.49916323 -0.46063699  0.29884039\n",
      "   0.64356102 -0.046883   -0.64652126]\n",
      " [ 0.6596511  -0.6857408   0.43381437  0.55579244 -0.36262887  0.15013444\n",
      "   0.85521602  0.81103966 -0.36197639]\n",
      " [-0.98228259  0.97052685 -0.58679577 -1.02391     0.12111245  0.1965891\n",
      "  -0.97099576 -0.70816827  0.73208058]\n",
      " [ 1.12611774 -0.86818773  0.57988577  0.64099098 -0.21183322 -0.25304464\n",
      "   0.78432618  0.81660205 -0.52516667]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.34898668  1.70302928 -1.93136693  0.2013354   0.3624685  -2.47683365\n",
      "   1.09466463]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:57 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.70381693]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 57 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.87714607 -0.97697039  0.39231097  0.89936473 -0.33593552 -0.08738278\n",
      "   1.26242354  0.17481086 -0.56237383]\n",
      " [-0.81563652  0.75280937 -0.59280273 -0.91376657  0.23919435 -0.16988374\n",
      "  -0.67148781 -0.34263603  0.7444487 ]\n",
      " [ 0.64125464 -0.39030385  0.06769469  0.50603153 -0.46063699  0.30570869\n",
      "   0.65042932 -0.046883   -0.64652126]\n",
      " [ 0.66656206 -0.67882984  0.43381437  0.5627034  -0.36262887  0.1570454\n",
      "   0.86212698  0.81103966 -0.36197639]\n",
      " [-0.98903569  0.96377376 -0.58679577 -1.03066309  0.12111245  0.18983601\n",
      "  -0.97774885 -0.70816827  0.73208058]\n",
      " [ 1.13299624 -0.86130924  0.57988577  0.64786947 -0.21183322 -0.24616614\n",
      "   0.79120468  0.81660205 -0.52516667]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.31811572  1.73002712 -1.92692761  0.22735168  0.38786741 -2.47249228\n",
      "   1.11957542]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:57 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.74314883]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 57 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.88220648 -0.97190998  0.39231097  0.90442513 -0.33593552 -0.08738278\n",
      "   1.26748395  0.17481086 -0.56237383]\n",
      " [-0.82110387  0.74734201 -0.59280273 -0.91923392  0.23919435 -0.16988374\n",
      "  -0.67695517 -0.34263603  0.7444487 ]\n",
      " [ 0.64670489 -0.38485359  0.06769469  0.51148178 -0.46063699  0.30570869\n",
      "   0.65587957 -0.046883   -0.64652126]\n",
      " [ 0.67201517 -0.67337673  0.43381437  0.5681565  -0.36262887  0.1570454\n",
      "   0.86758009  0.81103966 -0.36197639]\n",
      " [-0.99413679  0.95867266 -0.58679577 -1.03576419  0.12111245  0.18983601\n",
      "  -0.98284995 -0.70816827  0.73208058]\n",
      " [ 1.1384328  -0.85587268  0.57988577  0.65330604 -0.21183322 -0.24616614\n",
      "   0.79664124  0.81660205 -0.52516667]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.29360202  1.75177447 -1.92297187  0.24704494  0.40758059 -2.46965574\n",
      "   1.14033694]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:57 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.82251911]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 57 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.8845228  -0.97190998  0.3946273   0.90442513 -0.33593552 -0.08506646\n",
      "   1.26980027  0.17481086 -0.56237383]\n",
      " [-0.82360819  0.74734201 -0.59530704 -0.91923392  0.23919435 -0.17238806\n",
      "  -0.67945948 -0.34263603  0.7444487 ]\n",
      " [ 0.64958777 -0.38485359  0.07057757  0.51148178 -0.46063699  0.30859157\n",
      "   0.65876245 -0.046883   -0.64652126]\n",
      " [ 0.67463429 -0.67337673  0.43643349  0.5681565  -0.36262887  0.15966451\n",
      "   0.8701992   0.81103966 -0.36197639]\n",
      " [-0.99653324  0.95867266 -0.58919222 -1.03576419  0.12111245  0.18743956\n",
      "  -0.9852464  -0.70816827  0.73208058]\n",
      " [ 1.14092966 -0.85587268  0.58238263  0.65330604 -0.21183322 -0.24366928\n",
      "   0.7991381   0.81660205 -0.52516667]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.28064756  1.76370421 -1.92174862  0.25795742  0.41915965 -2.46855227\n",
      "   1.15207703]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:57 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.25132301]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 58 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.8830416  -0.97339119  0.3946273   0.90294393 -0.33593552 -0.08506646\n",
      "   1.26980027  0.17481086 -0.56385503]\n",
      " [-0.82214733  0.74880287 -0.59530704 -0.91777307  0.23919435 -0.17238806\n",
      "  -0.67945948 -0.34263603  0.74590955]\n",
      " [ 0.64882434 -0.38561702  0.07057757  0.51071836 -0.46063699  0.30859157\n",
      "   0.65876245 -0.046883   -0.64728469]\n",
      " [ 0.6734212  -0.67458982  0.43643349  0.56694341 -0.36262887  0.15966451\n",
      "   0.8701992   0.81103966 -0.36318948]\n",
      " [-0.99457208  0.96063381 -0.58919222 -1.03380304  0.12111245  0.18743956\n",
      "  -0.9852464  -0.70816827  0.73404173]\n",
      " [ 1.13858857 -0.85821377  0.58238263  0.65096494 -0.21183322 -0.24366928\n",
      "   0.7991381   0.81660205 -0.52750776]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.304292    1.75038473 -1.9320946   0.24536963  0.40611562 -2.47837498\n",
      "   1.13784653]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:58 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.82306327]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 58 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.8852513  -0.97339119  0.396837    0.90294393 -0.33593552 -0.08506646\n",
      "   1.27200997  0.17481086 -0.56385503]\n",
      " [-0.82477948  0.74880287 -0.59793919 -0.91777307  0.23919435 -0.17238806\n",
      "  -0.68209163 -0.34263603  0.74590955]\n",
      " [ 0.65167913 -0.38561702  0.07343236  0.51071836 -0.46063699  0.30859157\n",
      "   0.66161724 -0.046883   -0.64728469]\n",
      " [ 0.6761405  -0.67458982  0.4391528   0.56694341 -0.36262887  0.15966451\n",
      "   0.87291851  0.81103966 -0.36318948]\n",
      " [-0.99675975  0.96063381 -0.59137988 -1.03380304  0.12111245  0.18743956\n",
      "  -0.98743406 -0.70816827  0.73404173]\n",
      " [ 1.14082615 -0.85821377  0.58462021  0.65096494 -0.21183322 -0.24366928\n",
      "   0.80137568  0.81660205 -0.52750776]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.29140834  1.76233308 -1.93068516  0.25565976  0.41743633 -2.47745818\n",
      "   1.14977087]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:58 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.69990763]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 58 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.86880404 -0.98983845  0.38038974  0.90294393 -0.33593552 -0.08506646\n",
      "   1.25556271  0.17481086 -0.56385503]\n",
      " [-0.80854208  0.76504026 -0.58170179 -0.91777307  0.23919435 -0.17238806\n",
      "  -0.66585423 -0.34263603  0.74590955]\n",
      " [ 0.63721889 -0.40007726  0.05897212  0.51071836 -0.46063699  0.30859157\n",
      "   0.64715699 -0.046883   -0.64728469]\n",
      " [ 0.66001693 -0.69071339  0.42302922  0.56694341 -0.36262887  0.15966451\n",
      "   0.85679494  0.81103966 -0.36318948]\n",
      " [-0.98033361  0.97705995 -0.57495374 -1.03380304  0.12111245  0.18743956\n",
      "  -0.97100792 -0.70816827  0.73404173]\n",
      " [ 1.12445843 -0.8745815   0.56825249  0.65096494 -0.21183322 -0.24366928\n",
      "   0.78500796  0.81660205 -0.52750776]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.36491157  1.70137651 -1.94574523  0.20190853  0.35950706 -2.48965266\n",
      "   1.08792597]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:58 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.71722901]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 58 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.87518968 -0.98983845  0.38677538  0.90294393 -0.33593552 -0.08506646\n",
      "   1.25556271  0.1811965  -0.56385503]\n",
      " [-0.81488515  0.76504026 -0.58804486 -0.91777307  0.23919435 -0.17238806\n",
      "  -0.66585423 -0.3489791   0.74590955]\n",
      " [ 0.64141547 -0.40007726  0.0631687   0.51071836 -0.46063699  0.30859157\n",
      "   0.64715699 -0.04268641 -0.64728469]\n",
      " [ 0.66619093 -0.69071339  0.42920322  0.56694341 -0.36262887  0.15966451\n",
      "   0.85679494  0.81721365 -0.36318948]\n",
      " [-0.98587135  0.97705995 -0.58049148 -1.03380304  0.12111245  0.18743956\n",
      "  -0.97100792 -0.71370601  0.73404173]\n",
      " [ 1.12946292 -0.8745815   0.57325698  0.65096494 -0.21183322 -0.24366928\n",
      "   0.78500796  0.82160654 -0.52750776]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.33623696  1.72448726 -1.94143775  0.22074358  0.38443163 -2.48695165\n",
      "   1.11444403]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:58 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.55056961]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 58 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.88710873 -0.98983845  0.38677538  0.91486299 -0.33593552 -0.08506646\n",
      "   1.25556271  0.1811965  -0.55193598]\n",
      " [-0.8257386   0.76504026 -0.58804486 -0.92862652  0.23919435 -0.17238806\n",
      "  -0.66585423 -0.3489791   0.7350561 ]\n",
      " [ 0.64800456 -0.40007726  0.0631687   0.51730744 -0.46063699  0.30859157\n",
      "   0.64715699 -0.04268641 -0.6406956 ]\n",
      " [ 0.67625647 -0.69071339  0.42920322  0.57700896 -0.36262887  0.15966451\n",
      "   0.85679494  0.81721365 -0.35312394]\n",
      " [-0.99800121  0.97705995 -0.58049148 -1.0459329   0.12111245  0.18743956\n",
      "  -0.97100792 -0.71370601  0.72191187]\n",
      " [ 1.14150317 -0.8745815   0.57325698  0.66300519 -0.21183322 -0.24366928\n",
      "   0.78500796  0.82160654 -0.51546751]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.28063282  1.76736111 -1.92633818  0.25541822  0.42361777 -2.47490818\n",
      "   1.1576932 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:58 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.15802804]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 58 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.8873339  -0.98983845  0.38677538  0.91486299 -0.33571035 -0.08484128\n",
      "   1.25556271  0.1811965  -0.55171081]\n",
      " [-0.82567586  0.76504026 -0.58804486 -0.92862652  0.2392571  -0.17232531\n",
      "  -0.66585423 -0.3489791   0.73511884]\n",
      " [ 0.64838299 -0.40007726  0.0631687   0.51730744 -0.46025857  0.30896999\n",
      "   0.64715699 -0.04268641 -0.64031717]\n",
      " [ 0.67594177 -0.69071339  0.42920322  0.57700896 -0.36294357  0.15934981\n",
      "   0.85679494  0.81721365 -0.35343864]\n",
      " [-0.99808651  0.97705995 -0.58049148 -1.0459329   0.12102715  0.18735426\n",
      "  -0.97100792 -0.71370601  0.72182657]\n",
      " [ 1.1410582  -0.8745815   0.57325698  0.66300519 -0.21227819 -0.24411425\n",
      "   0.78500796  0.82160654 -0.51591248]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.29114605  1.76232995 -1.93153204  0.25054136  0.4180457  -2.48025011\n",
      "   1.15198946]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:58 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.03406259]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 58 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.88744817 -0.98972418  0.38677538  0.91486299 -0.33559608 -0.08472702\n",
      "   1.25556271  0.1811965  -0.55159654]\n",
      " [-0.82576666  0.76494946 -0.58804486 -0.92862652  0.23916629 -0.17241612\n",
      "  -0.66585423 -0.3489791   0.73502804]\n",
      " [ 0.64845375 -0.4000065   0.0631687   0.51730744 -0.46018781  0.30904075\n",
      "   0.64715699 -0.04268641 -0.64024641]\n",
      " [ 0.67601567 -0.6906395   0.42920322  0.57700896 -0.36286967  0.15942371\n",
      "   0.85679494  0.81721365 -0.35336474]\n",
      " [-0.99819722  0.97694924 -0.58049148 -1.0459329   0.12091644  0.18724354\n",
      "  -0.97100792 -0.71370601  0.72171586]\n",
      " [ 1.14114572 -0.87449398  0.57325698  0.66300519 -0.21219067 -0.24402673\n",
      "   0.78500796  0.82160654 -0.51582496]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.29170642  1.76218733 -1.93191158  0.25033547  0.4178435  -2.48066078\n",
      "   1.15180425]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:58 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.39987814]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 58 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.87973881 -0.99743354  0.38677538  0.91486299 -0.34330544 -0.09243638\n",
      "   1.24785335  0.1811965  -0.55159654]\n",
      " [-0.81865375  0.77206237 -0.58804486 -0.92862652  0.2462792  -0.16530321\n",
      "  -0.65874132 -0.3489791   0.73502804]\n",
      " [ 0.64065524 -0.40780501  0.0631687   0.51730744 -0.46798631  0.30124224\n",
      "   0.63935849 -0.04268641 -0.64024641]\n",
      " [ 0.66908529 -0.69756988  0.42920322  0.57700896 -0.36980006  0.15249333\n",
      "   0.84986456  0.81721365 -0.35336474]\n",
      " [-0.99088126  0.9842652  -0.58049148 -1.0459329   0.1282324   0.19455951\n",
      "  -0.96369196 -0.71370601  0.72171586]\n",
      " [ 1.13460079 -0.88103891  0.57325698  0.66300519 -0.2187356  -0.25057166\n",
      "   0.77846303  0.82160654 -0.51582496]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.33968692  1.72977865 -1.94826129  0.21780614  0.38644194 -2.49675094\n",
      "   1.1208754 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:58 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.83263409]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 58 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.88128112 -0.99743354  0.38677538  0.9164053  -0.34330544 -0.09243638\n",
      "   1.24939566  0.1811965  -0.55159654]\n",
      " [-0.8207826   0.77206237 -0.58804486 -0.93075537  0.2462792  -0.16530321\n",
      "  -0.66087017 -0.3489791   0.73502804]\n",
      " [ 0.64321154 -0.40780501  0.0631687   0.51986375 -0.46798631  0.30124224\n",
      "   0.64191479 -0.04268641 -0.64024641]\n",
      " [ 0.67146846 -0.69756988  0.42920322  0.57939213 -0.36980006  0.15249333\n",
      "   0.85224773  0.81721365 -0.35336474]\n",
      " [-0.99246129  0.9842652  -0.58049148 -1.04751294  0.1282324   0.19455951\n",
      "  -0.965272   -0.71370601  0.72171586]\n",
      " [ 1.13657439 -0.88103891  0.57325698  0.6649788  -0.2187356  -0.25057166\n",
      "   0.78043663  0.82160654 -0.51582496]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.32802531  1.74090913 -1.9472967   0.22780972  0.39682671 -2.49619814\n",
      "   1.13171259]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:58 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.67935447]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 58 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.88903914 -0.99743354  0.38677538  0.9164053  -0.33554742 -0.08467836\n",
      "   1.25715368  0.1811965  -0.55159654]\n",
      " [-0.82854172  0.77206237 -0.58804486 -0.93075537  0.23852008 -0.17306233\n",
      "  -0.66862929 -0.3489791   0.73502804]\n",
      " [ 0.6504623  -0.40780501  0.0631687   0.51986375 -0.46073555  0.30849301\n",
      "   0.64916555 -0.04268641 -0.64024641]\n",
      " [ 0.67911882 -0.69756988  0.42920322  0.57939213 -0.3621497   0.16014369\n",
      "   0.85989809  0.81721365 -0.35336474]\n",
      " [-1.00025714  0.9842652  -0.58049148 -1.04751294  0.12043655  0.18676366\n",
      "  -0.97306785 -0.71370601  0.72171586]\n",
      " [ 1.14436651 -0.88103891  0.57325698  0.6649788  -0.21094348 -0.24277954\n",
      "   0.78822875  0.82160654 -0.51582496]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.29310189  1.77041496 -1.94039204  0.25413111  0.42430836 -2.49050023\n",
      "   1.15998815]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:58 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.58813391]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 58 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.87397878 -0.99743354  0.38677538  0.90134494 -0.33554742 -0.09973872\n",
      "   1.25715368  0.1811965  -0.5666569 ]\n",
      " [-0.81334755  0.77206237 -0.58804486 -0.9155612   0.23852008 -0.15786816\n",
      "  -0.66862929 -0.3489791   0.75022221]\n",
      " [ 0.63787458 -0.40780501  0.0631687   0.50727602 -0.46073555  0.29590528\n",
      "   0.64916555 -0.04268641 -0.65283414]\n",
      " [ 0.66465468 -0.69756988  0.42920322  0.56492799 -0.3621497   0.14567955\n",
      "   0.85989809  0.81721365 -0.36782888]\n",
      " [-0.98535162  0.9842652  -0.58049148 -1.03260742  0.12043655  0.20166918\n",
      "  -0.97306785 -0.71370601  0.73662138]\n",
      " [ 1.12999897 -0.88103891  0.57325698  0.65061126 -0.21094348 -0.25714708\n",
      "   0.78822875  0.82160654 -0.5301925 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.36433444  1.71606489 -1.95691445  0.20439907  0.3713327  -2.5077706\n",
      "   1.10721074]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:58 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.70495161]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 58 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.88052563 -0.9908867   0.38677538  0.90789179 -0.33554742 -0.09319187\n",
      "   1.26370053  0.1811965  -0.5666569 ]\n",
      " [-0.82008837  0.76532155 -0.58804486 -0.92230202  0.23852008 -0.16460898\n",
      "  -0.67537011 -0.3489791   0.75022221]\n",
      " [ 0.64469909 -0.4009805   0.0631687   0.51410053 -0.46073555  0.30272979\n",
      "   0.65599006 -0.04268641 -0.65283414]\n",
      " [ 0.67152393 -0.69070062  0.42920322  0.57179725 -0.3621497   0.1525488\n",
      "   0.86676734  0.81721365 -0.36782888]\n",
      " [-0.99206939  0.97754744 -0.58049148 -1.03932518  0.12043655  0.19495142\n",
      "  -0.97978561 -0.71370601  0.73662138]\n",
      " [ 1.13683613 -0.87420175  0.57325698  0.65744842 -0.21094348 -0.25030992\n",
      "   0.79506591  0.82160654 -0.5301925 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.33365017  1.74287789 -1.95249926  0.23027279  0.39658936 -2.50343699\n",
      "   1.13197304]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:58 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.74511591]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 58 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.88552307 -0.98588925  0.38677538  0.91288923 -0.33554742 -0.09319187\n",
      "   1.26869797  0.1811965  -0.5666569 ]\n",
      " [-0.82548487  0.75992505 -0.58804486 -0.92769851  0.23852008 -0.16460898\n",
      "  -0.68076661 -0.3489791   0.75022221]\n",
      " [ 0.65008387 -0.39559572  0.0631687   0.51948531 -0.46073555  0.30272979\n",
      "   0.66137484 -0.04268641 -0.65283414]\n",
      " [ 0.67691161 -0.68531295  0.42920322  0.57718492 -0.3621497   0.1525488\n",
      "   0.87215501  0.81721365 -0.36782888]\n",
      " [-0.99710601  0.97251082 -0.58049148 -1.0443618   0.12043655  0.19495142\n",
      "  -0.98482223 -0.71370601  0.73662138]\n",
      " [ 1.14220131 -0.86883657  0.57325698  0.66281359 -0.21094348 -0.25030992\n",
      "   0.80043109  0.82160654 -0.5301925 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.30944661  1.76434833 -1.94860782  0.24974131  0.41607909 -2.50063624\n",
      "   1.15248563]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:58 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.8238068]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 58 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.88781624 -0.98588925  0.38906855  0.91288923 -0.33554742 -0.0908987\n",
      "   1.27099114  0.1811965  -0.5666569 ]\n",
      " [-0.82795852  0.75992505 -0.59051851 -0.92769851  0.23852008 -0.16708263\n",
      "  -0.68324026 -0.3489791   0.75022221]\n",
      " [ 0.65292917 -0.39559572  0.066014    0.51948531 -0.46073555  0.30557509\n",
      "   0.66422014 -0.04268641 -0.65283414]\n",
      " [ 0.6794966  -0.68531295  0.43178821  0.57718492 -0.3621497   0.1551338\n",
      "   0.87474001  0.81721365 -0.36782888]\n",
      " [-0.99947776  0.97251082 -0.58286323 -1.0443618   0.12043655  0.19257967\n",
      "  -0.98719398 -0.71370601  0.73662138]\n",
      " [ 1.14466887 -0.86883657  0.57572454  0.66281359 -0.21094348 -0.24784236\n",
      "   0.80289865  0.82160654 -0.5301925 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.29665946  1.77611763 -1.94739836  0.26051518  0.42750905 -2.49954054\n",
      "   1.1640706 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:58 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.24540761]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 59 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.88642802 -0.98727748  0.38906855  0.91150101 -0.33554742 -0.0908987\n",
      "   1.27099114  0.1811965  -0.56804513]\n",
      " [-0.82658466  0.7612989  -0.59051851 -0.92632466  0.23852008 -0.16708263\n",
      "  -0.68324026 -0.3489791   0.75159606]\n",
      " [ 0.65222755 -0.39629734  0.066014    0.51878369 -0.46073555  0.30557509\n",
      "   0.66422014 -0.04268641 -0.65353575]\n",
      " [ 0.67835225 -0.68645729  0.43178821  0.57604058 -0.3621497   0.1551338\n",
      "   0.87474001  0.81721365 -0.36897322]\n",
      " [-0.99762867  0.9743599  -0.58286323 -1.04251272  0.12043655  0.19257967\n",
      "  -0.98719398 -0.71370601  0.73847047]\n",
      " [ 1.14244273 -0.87106271  0.57572454  0.66058746 -0.21094348 -0.24784236\n",
      "   0.80289865  0.82160654 -0.53241864]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.31938208  1.7633538  -1.95737198  0.24845045  0.41499547 -2.50901805\n",
      "   1.15042074]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:59 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: [1.] Net Result: [[0.82518955]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 59 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.88859148 -0.98727748  0.39123201  0.91150101 -0.33554742 -0.0908987\n",
      "   1.27315461  0.1811965  -0.56804513]\n",
      " [-0.82915787  0.7612989  -0.59309171 -0.92632466  0.23852008 -0.16708263\n",
      "  -0.68581346 -0.3489791   0.75159606]\n",
      " [ 0.65502286 -0.39629734  0.06880931  0.51878369 -0.46073555  0.30557509\n",
      "   0.66701544 -0.04268641 -0.65353575]\n",
      " [ 0.6810102  -0.68645729  0.43444616  0.57604058 -0.3621497   0.1551338\n",
      "   0.87739795  0.81721365 -0.36897322]\n",
      " [-0.99977092  0.9743599  -0.58500548 -1.04251272  0.12043655  0.19257967\n",
      "  -0.98933623 -0.71370601  0.73847047]\n",
      " [ 1.14463155 -0.87106271  0.57791335  0.66058746 -0.21094348 -0.24784236\n",
      "   0.80508746  0.82160654 -0.53241864]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.30677372  1.775046   -1.95599687  0.2585294   0.42608075 -2.50811974\n",
      "   1.1620911 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:59 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.69768026]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 59 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.87212229 -1.00374667  0.37476282  0.91150101 -0.33554742 -0.0908987\n",
      "   1.25668541  0.1811965  -0.56804513]\n",
      " [-0.81292662  0.77753016 -0.57686046 -0.92632466  0.23852008 -0.16708263\n",
      "  -0.66958221 -0.3489791   0.75159606]\n",
      " [ 0.64059914 -0.41072105  0.05438559  0.51878369 -0.46073555  0.30557509\n",
      "   0.65259173 -0.04268641 -0.65353575]\n",
      " [ 0.66489212 -0.70257537  0.41832808  0.57604058 -0.3621497   0.1551338\n",
      "   0.86127987  0.81721365 -0.36897322]\n",
      " [-0.98331704  0.99081378 -0.5685516  -1.04251272  0.12043655  0.19257967\n",
      "  -0.97288235 -0.71370601  0.73847047]\n",
      " [ 1.12823139 -0.88746287  0.5615132   0.66058746 -0.21094348 -0.24784236\n",
      "   0.78868731  0.82160654 -0.53241864]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.38035196  1.7141843  -1.97118341  0.20481834  0.36818132 -2.52048207\n",
      "   1.10030179]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:59 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.7197849]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 59 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.87841764 -1.00374667  0.38105817  0.91150101 -0.33554742 -0.0908987\n",
      "   1.25668541  0.18749185 -0.56804513]\n",
      " [-0.81917316  0.77753016 -0.583107   -0.92632466  0.23852008 -0.16708263\n",
      "  -0.66958221 -0.35522565  0.75159606]\n",
      " [ 0.64475003 -0.41072105  0.05853648  0.51878369 -0.46073555  0.30557509\n",
      "   0.65259173 -0.03853552 -0.65353575]\n",
      " [ 0.67096843 -0.70257537  0.42440439  0.57604058 -0.3621497   0.1551338\n",
      "   0.86127987  0.82328996 -0.36897322]\n",
      " [-0.98877023  0.99081378 -0.57400478 -1.04251272  0.12043655  0.19257967\n",
      "  -0.97288235 -0.7191592   0.73847047]\n",
      " [ 1.1331588  -0.88746287  0.56644061  0.66058746 -0.21094348 -0.24784236\n",
      "   0.78868731  0.82653395 -0.53241864]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.35209303  1.73697802 -1.96695956  0.22339938  0.39276492 -2.51782532\n",
      "   1.12643944]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:59 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.55248483]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 59 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89030146 -1.00374667  0.38105817  0.92338482 -0.33554742 -0.0908987\n",
      "   1.25668541  0.18749185 -0.55616131]\n",
      " [-0.83001443  0.77753016 -0.583107   -0.93716593  0.23852008 -0.16708263\n",
      "  -0.66958221 -0.35522565  0.7407548 ]\n",
      " [ 0.6513642  -0.41072105  0.05853648  0.52539787 -0.46073555  0.30557509\n",
      "   0.65259173 -0.03853552 -0.64692158]\n",
      " [ 0.68104266 -0.70257537  0.42440439  0.58611481 -0.3621497   0.1551338\n",
      "   0.86127987  0.82328996 -0.35889899]\n",
      " [-1.00085677  0.99081378 -0.57400478 -1.05459926  0.12043655  0.19257967\n",
      "  -0.97288235 -0.7191592   0.72638393]\n",
      " [ 1.14516222 -0.88746287  0.56644061  0.67259087 -0.21094348 -0.24784236\n",
      "   0.78868731  0.82653395 -0.52041522]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.29677001  1.77970914 -1.95201448  0.25796554  0.43184593 -2.50591001\n",
      "   1.16955015]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:59 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.15296203]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 59 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89052964 -1.00374667  0.38105817  0.92338482 -0.33531924 -0.09067052\n",
      "   1.25668541  0.18749185 -0.55593312]\n",
      " [-0.82997028  0.77753016 -0.583107   -0.93716593  0.23856423 -0.16703848\n",
      "  -0.66958221 -0.35522565  0.74079894]\n",
      " [ 0.65173546 -0.41072105  0.05853648  0.52539787 -0.46036429  0.30594635\n",
      "   0.65259173 -0.03853552 -0.64655032]\n",
      " [ 0.68075839 -0.70257537  0.42440439  0.58611481 -0.36243396  0.15484953\n",
      "   0.86127987  0.82328996 -0.35918326]\n",
      " [-1.00095221  0.99081378 -0.57400478 -1.05459926  0.1203411   0.19248422\n",
      "  -0.97288235 -0.7191592   0.72628848]\n",
      " [ 1.1447539  -0.88746287  0.56644061  0.67259087 -0.2113518  -0.24825068\n",
      "   0.78868731  0.82653395 -0.52082354]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.30667924  1.77498303 -1.95692495  0.25338359  0.42660642 -2.5109601\n",
      "   1.16418534]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:59 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.0316698]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 59 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89062953 -1.00364678  0.38105817  0.92338482 -0.33521935 -0.09057063\n",
      "   1.25668541  0.18749185 -0.55583324]\n",
      " [-0.8300504   0.77745004 -0.583107   -0.93716593  0.23848411 -0.1671186\n",
      "  -0.66958221 -0.35522565  0.74071883]\n",
      " [ 0.65179838 -0.41065814  0.05853648  0.52539787 -0.46030137  0.30600927\n",
      "   0.65259173 -0.03853552 -0.6464874 ]\n",
      " [ 0.680824   -0.70250977  0.42440439  0.58611481 -0.36236836  0.15491514\n",
      "   0.86127987  0.82328996 -0.35911765]\n",
      " [-1.00104913  0.99071686 -0.57400478 -1.05459926  0.12024418  0.1923873\n",
      "  -0.97288235 -0.7191592   0.72619156]\n",
      " [ 1.14483114 -0.88738562  0.56644061  0.67259087 -0.21127455 -0.24817344\n",
      "   0.78868731  0.82653395 -0.52074629]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.30716485  1.77486131 -1.95725581  0.25320704  0.42643308 -2.51131785\n",
      "   1.16402669]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:59 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.39244031]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 59 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.88322827 -1.01104804  0.38105817  0.92338482 -0.34262061 -0.09797188\n",
      "   1.24928416  0.18749185 -0.55583324]\n",
      " [-0.82319067  0.78430977 -0.583107   -0.93716593  0.24534384 -0.16025887\n",
      "  -0.66272248 -0.35522565  0.74071883]\n",
      " [ 0.64423198 -0.41822453  0.05853648  0.52539787 -0.46786777  0.29844287\n",
      "   0.64502533 -0.03853552 -0.6464874 ]\n",
      " [ 0.67412242 -0.70921135  0.42440439  0.58611481 -0.36906994  0.14821355\n",
      "   0.85457829  0.82328996 -0.35911765]\n",
      " [-0.99402547  0.99774052 -0.57400478 -1.05459926  0.12726784  0.19941096\n",
      "  -0.96585869 -0.7191592   0.72619156]\n",
      " [ 1.13852732 -0.89368945  0.56644061  0.67259087 -0.21757838 -0.25447726\n",
      "   0.78238348  0.82653395 -0.52074629]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.3539498   1.74341529 -1.97329394  0.22153956  0.39588369 -2.5271483\n",
      "   1.13396249]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:59 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.83631147]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 59 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.88469853 -1.01104804  0.38105817  0.92485508 -0.34262061 -0.09797188\n",
      "   1.25075442  0.18749185 -0.55583324]\n",
      " [-0.82522133  0.78430977 -0.583107   -0.93919659  0.24534384 -0.16025887\n",
      "  -0.66475314 -0.35522565  0.74071883]\n",
      " [ 0.64668101 -0.41822453  0.05853648  0.52784689 -0.46786777  0.29844287\n",
      "   0.64747435 -0.03853552 -0.6464874 ]\n",
      " [ 0.67639882 -0.70921135  0.42440439  0.58839121 -0.36906994  0.14821355\n",
      "   0.8568547   0.82328996 -0.35911765]\n",
      " [-0.9955314   0.99774052 -0.57400478 -1.05610519  0.12726784  0.19941096\n",
      "  -0.96736462 -0.7191592   0.72619156]\n",
      " [ 1.14040792 -0.89368945  0.56644061  0.67447147 -0.21757838 -0.25447726\n",
      "   0.78426408  0.82653395 -0.52074629]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.34274576  1.75411556 -1.9723816   0.23117417  0.40588144 -2.52662422\n",
      "   1.1443872 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:59 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.68146803]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 59 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89237849 -1.01104804  0.38105817  0.92485508 -0.33494065 -0.09029193\n",
      "   1.25843438  0.18749185 -0.55583324]\n",
      " [-0.83290581  0.78430977 -0.583107   -0.93919659  0.23765936 -0.16794335\n",
      "  -0.67243762 -0.35522565  0.74071883]\n",
      " [ 0.65387624 -0.41822453  0.05853648  0.52784689 -0.46067254  0.3056381\n",
      "   0.65466958 -0.03853552 -0.6464874 ]\n",
      " [ 0.68398072 -0.70921135  0.42440439  0.58839121 -0.36148805  0.15579545\n",
      "   0.86443659  0.82328996 -0.35911765]\n",
      " [-1.00324814  0.99774052 -0.57400478 -1.05610519  0.11955111  0.19169422\n",
      "  -0.97508135 -0.7191592   0.72619156]\n",
      " [ 1.14812421 -0.89368945  0.56644061  0.67447147 -0.20986208 -0.24676096\n",
      "   0.79198038  0.82653395 -0.52074629]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.308174    1.78332394 -1.96556953  0.25727119  0.43312102 -2.5209897\n",
      "   1.17240418]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:59 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.58891701]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 59 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.87729711 -1.01104804  0.38105817  0.90977371 -0.33494065 -0.1053733\n",
      "   1.25843438  0.18749185 -0.57091461]\n",
      " [-0.81769107  0.78430977 -0.583107   -0.92398184  0.23765936 -0.1527286\n",
      "  -0.67243762 -0.35522565  0.75593357]\n",
      " [ 0.64125593 -0.41822453  0.05853648  0.51522658 -0.46067254  0.29301779\n",
      "   0.65466958 -0.03853552 -0.65910771]\n",
      " [ 0.66948116 -0.70921135  0.42440439  0.57389165 -0.36148805  0.14129589\n",
      "   0.86443659  0.82328996 -0.37361721]\n",
      " [-0.98831957  0.99774052 -0.57400478 -1.04117662  0.11955111  0.20662279\n",
      "  -0.97508135 -0.7191592   0.74112013]\n",
      " [ 1.13371655 -0.89368945  0.56644061  0.66006381 -0.20986208 -0.26116863\n",
      "   0.79198038  0.82653395 -0.53515396]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.37946056  1.72890767 -1.98207914  0.20746689  0.38005415 -2.53824469\n",
      "   1.11952731]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:59 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.70604286]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 59 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.88381498 -1.00453017  0.38105817  0.91629157 -0.33494065 -0.09885544\n",
      "   1.26495224  0.18749185 -0.57091461]\n",
      " [-0.8243933   0.77760753 -0.583107   -0.93068408  0.23765936 -0.15943084\n",
      "  -0.67913985 -0.35522565  0.75593357]\n",
      " [ 0.6480383  -0.41144216  0.05853648  0.52200896 -0.46067254  0.29980016\n",
      "   0.66145196 -0.03853552 -0.65910771]\n",
      " [ 0.6763103  -0.70238221  0.42440439  0.5807208  -0.36148805  0.14812503\n",
      "   0.87126573  0.82328996 -0.37361721]\n",
      " [-0.99500334  0.99105674 -0.57400478 -1.04786039  0.11955111  0.19993902\n",
      "  -0.98176513 -0.7191592   0.74112013]\n",
      " [ 1.14051388 -0.88689212  0.56644061  0.66686114 -0.20986208 -0.2543713\n",
      "   0.79877771  0.82653395 -0.53515396]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.3489557   1.75554293 -1.97768672  0.23320354  0.40517335 -2.53391834\n",
      "   1.14414602]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:59 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.7470408]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 59 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.88875094 -0.99959421  0.38105817  0.92122754 -0.33494065 -0.09885544\n",
      "   1.26988821  0.18749185 -0.57091461]\n",
      " [-0.82972067  0.77228017 -0.583107   -0.93601144  0.23765936 -0.15943084\n",
      "  -0.68446722 -0.35522565  0.75593357]\n",
      " [ 0.65335895 -0.40612151  0.05853648  0.52732961 -0.46067254  0.29980016\n",
      "   0.66677261 -0.03853552 -0.65910771]\n",
      " [ 0.68163379 -0.69705872  0.42440439  0.58604429 -0.36148805  0.14812503\n",
      "   0.87658922  0.82328996 -0.37361721]\n",
      " [-0.99997708  0.98608301 -0.57400478 -1.05283413  0.11955111  0.19993902\n",
      "  -0.98673886 -0.7191592   0.74112013]\n",
      " [ 1.14580951 -0.88159649  0.56644061  0.67215677 -0.20986208 -0.2543713\n",
      "   0.80407334  0.82653395 -0.53515396]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.32505475  1.77674325 -1.97385736  0.25245211  0.42444383 -2.53115245\n",
      "   1.16441483]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:59 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.82502744]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 59 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89102261 -0.99959421  0.38332984  0.92122754 -0.33494065 -0.09658377\n",
      "   1.27215987  0.18749185 -0.57091461]\n",
      " [-0.83216573  0.77228017 -0.58555207 -0.93601144  0.23765936 -0.16187591\n",
      "  -0.68691229 -0.35522565  0.75593357]\n",
      " [ 0.65616884 -0.40612151  0.06134637  0.52732961 -0.46067254  0.30261005\n",
      "   0.6695825  -0.03853552 -0.65910771]\n",
      " [ 0.68418692 -0.69705872  0.42695752  0.58604429 -0.36148805  0.15067816\n",
      "   0.87914235  0.82328996 -0.37361721]\n",
      " [-1.0023259   0.98608301 -0.57635361 -1.05283413  0.11955111  0.19759019\n",
      "  -0.98908768 -0.7191592   0.74112013]\n",
      " [ 1.14824987 -0.88159649  0.56888097  0.67215677 -0.20986208 -0.25193093\n",
      "   0.8065137   0.82653395 -0.53515396]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.31242547  1.78836078 -1.97266047  0.26309482  0.43573259 -2.53006365\n",
      "   1.17585286]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:59 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.23960912]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 60 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.88972251 -1.0008943   0.38332984  0.91992744 -0.33494065 -0.09658377\n",
      "   1.27215987  0.18749185 -0.57221471]\n",
      " [-0.83087492  0.77357097 -0.58555207 -0.93472064  0.23765936 -0.16187591\n",
      "  -0.68691229 -0.35522565  0.75722438]\n",
      " [ 0.6555257  -0.40676465  0.06134637  0.52668646 -0.46067254  0.30261005\n",
      "   0.6695825  -0.03853552 -0.65975086]\n",
      " [ 0.68310872 -0.69813692  0.42695752  0.58496608 -0.36148805  0.15067816\n",
      "   0.87914235  0.82328996 -0.37469541]\n",
      " [-1.00058351  0.98782539 -0.57635361 -1.05109174  0.11955111  0.19759019\n",
      "  -0.98908768 -0.7191592   0.74286252]\n",
      " [ 1.14613447 -0.88371189  0.56888097  0.67004136 -0.20986208 -0.25193093\n",
      "   0.8065137   0.82653395 -0.53726936]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.33425346  1.776134   -1.98227123  0.25153618  0.42373323 -2.53920385\n",
      "   1.16276555]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:60 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.82723219]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 60 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89184219 -1.0008943   0.38544952  0.91992744 -0.33494065 -0.09658377\n",
      "   1.27427955  0.18749185 -0.57221471]\n",
      " [-0.83339226  0.77357097 -0.5880694  -0.93472064  0.23765936 -0.16187591\n",
      "  -0.68942962 -0.35522565  0.75722438]\n",
      " [ 0.65826416 -0.40676465  0.06408483  0.52668646 -0.46067254  0.30261005\n",
      "   0.67232096 -0.03853552 -0.65975086]\n",
      " [ 0.68570846 -0.69813692  0.42955726  0.58496608 -0.36148805  0.15067816\n",
      "   0.88174209  0.82328996 -0.37469541]\n",
      " [-1.00268279  0.98782539 -0.57845289 -1.05109174  0.11955111  0.19759019\n",
      "  -0.99118697 -0.7191592   0.74286252]\n",
      " [ 1.14827727 -0.88371189  0.57102378  0.67004136 -0.20986208 -0.25193093\n",
      "   0.80865651  0.82653395 -0.53726936]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.32190755  1.78758175 -1.98092834  0.26141323  0.43459342 -2.53832288\n",
      "   1.17419339]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:60 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.69530482]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 60 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.87535385 -1.01738264  0.36896117  0.91992744 -0.33494065 -0.09658377\n",
      "   1.25779121  0.18749185 -0.57221471]\n",
      " [-0.81716913  0.78979411 -0.57184627 -0.93472064  0.23765936 -0.16187591\n",
      "  -0.67320649 -0.35522565  0.75722438]\n",
      " [ 0.64387862 -0.42115019  0.0496993   0.52668646 -0.46067254  0.30261005\n",
      "   0.65793542 -0.03853552 -0.65975086]\n",
      " [ 0.66959784 -0.71424754  0.41344664  0.58496608 -0.36148805  0.15067816\n",
      "   0.86563147  0.82328996 -0.37469541]\n",
      " [-0.98620389  1.0043043  -0.56197398 -1.05109174  0.11955111  0.19759019\n",
      "  -0.97470806 -0.7191592   0.74286252]\n",
      " [ 1.13184605 -0.90014311  0.55459255  0.67004136 -0.20986208 -0.25193093\n",
      "   0.79222528  0.82653395 -0.53726936]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.39555981  1.7268178  -1.9962444   0.20774474  0.3767289  -2.55085551\n",
      "   1.11246456]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:60 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.7222524]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 60 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.881562   -1.01738264  0.37516932  0.91992744 -0.33494065 -0.09658377\n",
      "   1.25779121  0.1937     -0.57221471]\n",
      " [-0.82332289  0.78979411 -0.57800003 -0.93472064  0.23765936 -0.16187591\n",
      "  -0.67320649 -0.36137941  0.75722438]\n",
      " [ 0.64798437 -0.42115019  0.05380504  0.52668646 -0.46067254  0.30261005\n",
      "   0.65793542 -0.03442978 -0.65975086]\n",
      " [ 0.67558047 -0.71424754  0.41942927  0.58496608 -0.36148805  0.15067816\n",
      "   0.86563147  0.82927259 -0.37469541]\n",
      " [-0.99157631  1.0043043  -0.5673464  -1.05109174  0.11955111  0.19759019\n",
      "  -0.97470806 -0.72453162  0.74286252]\n",
      " [ 1.1367001  -0.90014311  0.5594466   0.67004136 -0.20986208 -0.25193093\n",
      "   0.79222528  0.83138799 -0.53726936]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.36770118  1.74930479 -1.99209973  0.22607978  0.40098281 -2.54824058\n",
      "   1.13823507]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:60 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.55439593]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 60 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89340893 -1.01738264  0.37516932  0.93177437 -0.33494065 -0.09658377\n",
      "   1.25779121  0.1937     -0.56036778]\n",
      " [-0.83414942  0.78979411 -0.57800003 -0.94554717  0.23765936 -0.16187591\n",
      "  -0.67320649 -0.36137941  0.74639784]\n",
      " [ 0.65462017 -0.42115019  0.05380504  0.53332226 -0.46067254  0.30261005\n",
      "   0.65793542 -0.03442978 -0.65311506]\n",
      " [ 0.68565991 -0.71424754  0.41942927  0.59504552 -0.36148805  0.15067816\n",
      "   0.86563147  0.82927259 -0.36461598]\n",
      " [-1.0036182   1.0043043  -0.5673464  -1.06313364  0.11955111  0.19759019\n",
      "  -0.97470806 -0.72453162  0.73082062]\n",
      " [ 1.14866481 -0.90014311  0.5594466   0.68200607 -0.20986208 -0.25193093\n",
      "   0.79222528  0.83138799 -0.52530465]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.31265993  1.79188975 -1.97730572  0.26053336  0.43995385 -2.53645086\n",
      "   1.18120316]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:60 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.14811213]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 60 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89363843 -1.01738264  0.37516932  0.93177437 -0.33471115 -0.09635427\n",
      "   1.25779121  0.1937     -0.56013828]\n",
      " [-0.83412146  0.78979411 -0.57800003 -0.94554717  0.23768732 -0.16184795\n",
      "  -0.67320649 -0.36137941  0.7464258 ]\n",
      " [ 0.65498365 -0.42115019  0.05380504  0.53332226 -0.46030906  0.30297354\n",
      "   0.65793542 -0.03442978 -0.65275157]\n",
      " [ 0.68540318 -0.71424754  0.41942927  0.59504552 -0.36174477  0.15042143\n",
      "   0.86563147  0.82927259 -0.3648727 ]\n",
      " [-1.00372174  1.0043043  -0.5673464  -1.06313364  0.11944757  0.19748666\n",
      "  -0.97470806 -0.72453162  0.73071708]\n",
      " [ 1.14828984 -0.90014311  0.5594466   0.68200607 -0.21023705 -0.2523059\n",
      "   0.79222528  0.83138799 -0.52567962]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.32200395  1.78744761 -1.98194978  0.25622632  0.43502459 -2.54122644\n",
      "   1.17615455]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:60 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.02946649]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 60 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89372579 -1.01729528  0.37516932  0.93177437 -0.33462379 -0.09626691\n",
      "   1.25779121  0.1937     -0.56005092]\n",
      " [-0.83419216  0.78972341 -0.57800003 -0.94554717  0.23761662 -0.16191865\n",
      "  -0.67320649 -0.36137941  0.7463551 ]\n",
      " [ 0.65503958 -0.42109426  0.05380504  0.53332226 -0.46025313  0.30302947\n",
      "   0.65793542 -0.03442978 -0.65269564]\n",
      " [ 0.68546142 -0.7141893   0.41942927  0.59504552 -0.36168654  0.15047967\n",
      "   0.86563147  0.82927259 -0.36481447]\n",
      " [-1.00380663  1.00421942 -0.5673464  -1.06313364  0.11936269  0.19740177\n",
      "  -0.97470806 -0.72453162  0.7306322 ]\n",
      " [ 1.14835803 -0.90007493  0.5594466   0.68200607 -0.21016886 -0.25223772\n",
      "   0.79222528  0.83138799 -0.52561144]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.32242529  1.78734356 -1.9822385   0.25607472  0.43487577 -2.54153842\n",
      "   1.17601846]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:60 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.38510615]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 60 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.88662357 -1.0243975   0.37516932  0.93177437 -0.34172601 -0.10336913\n",
      "   1.25068899  0.1937     -0.56005092]\n",
      " [-0.82757926  0.7963363  -0.57800003 -0.94554717  0.24422951 -0.15530575\n",
      "  -0.66659359 -0.36137941  0.7463551 ]\n",
      " [ 0.64770072 -0.42843312  0.05380504  0.53332226 -0.46759199  0.29569061\n",
      "   0.65059656 -0.03442978 -0.65269564]\n",
      " [ 0.67898396 -0.72066676  0.41942927  0.59504552 -0.36816399  0.14400221\n",
      "   0.85915402  0.82927259 -0.36481447]\n",
      " [-0.9970665   1.01095954 -0.5673464  -1.06313364  0.12610281  0.20414189\n",
      "  -0.96796794 -0.72453162  0.7306322 ]\n",
      " [ 1.14228949 -0.90614347  0.5594466   0.68200607 -0.2162374  -0.25830626\n",
      "   0.78615674  0.83138799 -0.52561144]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.36802174  1.75684291 -1.99796014  0.22525944  0.40516909 -2.55710024\n",
      "   1.14680798]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:60 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.83986718]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 60 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.88802579 -1.0243975   0.37516932  0.93317659 -0.34172601 -0.10336913\n",
      "   1.25209121  0.1937     -0.56005092]\n",
      " [-0.8295171   0.7963363  -0.57800003 -0.94748501  0.24422951 -0.15530575\n",
      "  -0.66853143 -0.36137941  0.7463551 ]\n",
      " [ 0.65004756 -0.42843312  0.05380504  0.5356691  -0.46759199  0.29569061\n",
      "   0.6529434  -0.03442978 -0.65269564]\n",
      " [ 0.68115916 -0.72066676  0.41942927  0.59722072 -0.36816399  0.14400221\n",
      "   0.86132922  0.82927259 -0.36481447]\n",
      " [-0.99850248  1.01095954 -0.5673464  -1.06456961  0.12610281  0.20414189\n",
      "  -0.96940391 -0.72453162  0.7306322 ]\n",
      " [ 1.14408235 -0.90614347  0.5594466   0.68379893 -0.2162374  -0.25830626\n",
      "   0.78794961  0.83138799 -0.52561144]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.35725359  1.76713296 -1.9970966   0.2345411   0.41479675 -2.55660308\n",
      "   1.15683885]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:60 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.68359564]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 60 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89562703 -1.0243975   0.37516932  0.93317659 -0.33412477 -0.09576789\n",
      "   1.25969245  0.1937     -0.56005092]\n",
      " [-0.83712629  0.7963363  -0.57800003 -0.94748501  0.23662032 -0.16291494\n",
      "  -0.67614062 -0.36137941  0.7463551 ]\n",
      " [ 0.65718619 -0.42843312  0.05380504  0.5356691  -0.46045336  0.30282924\n",
      "   0.66008203 -0.03442978 -0.65269564]\n",
      " [ 0.68867163 -0.72066676  0.41942927  0.59722072 -0.36065153  0.15151468\n",
      "   0.86884169  0.82927259 -0.36481447]\n",
      " [-1.00613954  1.01095954 -0.5673464  -1.06456961  0.11846574  0.19650483\n",
      "  -0.97704098 -0.72453162  0.7306322 ]\n",
      " [ 1.15172212 -0.90614347  0.5594466   0.68379893 -0.20859763 -0.25066648\n",
      "   0.79558938  0.83138799 -0.52561144]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.32303562  1.79604315 -1.99037715  0.26041125  0.44179142 -2.55103296\n",
      "   1.18459469]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:60 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.58967258]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 60 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.88052548 -1.0243975   0.37516932  0.91807504 -0.33412477 -0.11086943\n",
      "   1.25969245  0.1937     -0.57515246]\n",
      " [-0.82189221  0.7963363  -0.57800003 -0.93225092  0.23662032 -0.14768086\n",
      "  -0.67614062 -0.36137941  0.76158919]\n",
      " [ 0.64453561 -0.42843312  0.05380504  0.52301852 -0.46045336  0.29017866\n",
      "   0.66008203 -0.03442978 -0.66534622]\n",
      " [ 0.6741387  -0.72066676  0.41942927  0.58268779 -0.36065153  0.13698175\n",
      "   0.86884169  0.82927259 -0.3793474 ]\n",
      " [-0.99118886  1.01095954 -0.5673464  -1.04961893  0.11846574  0.21145551\n",
      "  -0.97704098 -0.72453162  0.74558288]\n",
      " [ 1.13727634 -0.90614347  0.5594466   0.66935315 -0.20859763 -0.26511227\n",
      "   0.79558938  0.83138799 -0.54005722]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.39437387  1.74156327 -2.00687533  0.21053908  0.38863803 -2.56827293\n",
      "   1.13162278]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:60 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.70709248]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 60 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.8870154  -1.01790758  0.37516932  0.92456496 -0.33412477 -0.10437951\n",
      "   1.26618237  0.1937     -0.57515246]\n",
      " [-0.82855746  0.78967105 -0.57800003 -0.93891618  0.23662032 -0.15434611\n",
      "  -0.68280588 -0.36137941  0.76158919]\n",
      " [ 0.65127744 -0.42169129  0.05380504  0.52976035 -0.46045336  0.29692049\n",
      "   0.66682386 -0.03442978 -0.66534622]\n",
      " [ 0.68092926 -0.71387619  0.41942927  0.58947835 -0.36065153  0.14377232\n",
      "   0.87563225  0.82927259 -0.3793474 ]\n",
      " [-0.99783993  1.00430847 -0.5673464  -1.05627     0.11846574  0.20480444\n",
      "  -0.98369205 -0.72453162  0.74558288]\n",
      " [ 1.14403528 -0.89938453  0.5594466   0.67611209 -0.20859763 -0.25835333\n",
      "   0.80234832  0.83138799 -0.54005722]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.36404143  1.76802757 -2.00250436  0.23614399  0.41362446 -2.56395335\n",
      "   1.15610261]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:60 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.74892499]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 60 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89189132 -1.01303166  0.37516932  0.92944088 -0.33412477 -0.10437951\n",
      "   1.27105829  0.1937     -0.57515246]\n",
      " [-0.83381738  0.78441113 -0.57800003 -0.94417609  0.23662032 -0.15434611\n",
      "  -0.68806579 -0.36137941  0.76158919]\n",
      " [ 0.65653527 -0.41643346  0.05380504  0.53501818 -0.46045336  0.29692049\n",
      "   0.67208169 -0.03442978 -0.66534622]\n",
      " [ 0.68618981 -0.70861565  0.41942927  0.5947389  -0.36065153  0.14377232\n",
      "   0.88089279  0.82927259 -0.3793474 ]\n",
      " [-1.00275231  0.99939609 -0.5673464  -1.06118238  0.11846574  0.20480444\n",
      "  -0.98860443 -0.72453162  0.74558288]\n",
      " [ 1.14926314 -0.89415666  0.5594466   0.68133995 -0.20859763 -0.25835333\n",
      "   0.80757619  0.83138799 -0.54005722]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.34043582  1.78896433 -1.998735    0.25517729  0.43267981 -2.56122145\n",
      "   1.17613266]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:60 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.82618321]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 60 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89414307 -1.01303166  0.37742107  0.92944088 -0.33412477 -0.10212777\n",
      "   1.27331003  0.1937     -0.57515246]\n",
      " [-0.83623584  0.78441113 -0.5804185  -0.94417609  0.23662032 -0.15676458\n",
      "  -0.69048425 -0.36137941  0.76158919]\n",
      " [ 0.65931183 -0.41643346  0.0565816   0.53501818 -0.46045336  0.29969705\n",
      "   0.67485825 -0.03442978 -0.66534622]\n",
      " [ 0.68871323 -0.70861565  0.42195269  0.5947389  -0.36065153  0.14629573\n",
      "   0.88341621  0.82927259 -0.3793474 ]\n",
      " [-1.0050799   0.99939609 -0.56967399 -1.06118238  0.11846574  0.20247686\n",
      "  -0.99093201 -0.72453162  0.74558288]\n",
      " [ 1.15167832 -0.89415666  0.56186178  0.68133995 -0.20859763 -0.25593815\n",
      "   0.80999137  0.83138799 -0.54005722]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.32795538  1.8004384  -1.99754954  0.26569602  0.44383498 -2.56013869\n",
      "   1.18743161]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:60 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.23392405]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 61 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.8929265  -1.01424823  0.37742107  0.92822431 -0.33412477 -0.10212777\n",
      "   1.27331003  0.1937     -0.57636903]\n",
      " [-0.83502424  0.78562273 -0.5804185  -0.94296449  0.23662032 -0.15676458\n",
      "  -0.69048425 -0.36137941  0.76280079]\n",
      " [ 0.65872393 -0.41702136  0.0565816   0.53443028 -0.46045336  0.29969705\n",
      "   0.67485825 -0.03442978 -0.66593412]\n",
      " [ 0.68769859 -0.70963029  0.42195269  0.59372426 -0.36065153  0.14629573\n",
      "   0.88341621  0.82927259 -0.38036204]\n",
      " [-1.00343908  1.00103691 -0.56967399 -1.05954156  0.11846574  0.20247686\n",
      "  -0.99093201 -0.72453162  0.7472237 ]\n",
      " [ 1.14966952 -0.89616546  0.56186178  0.67933115 -0.20859763 -0.25593815\n",
      "   0.80999137  0.83138799 -0.54206602]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.34891539  1.78873056 -2.00680683  0.25462687  0.43233386 -2.56894951\n",
      "   1.17488913]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:61 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.82919442]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 61 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89500471 -1.01424823  0.37949928  0.92822431 -0.33412477 -0.10212777\n",
      "   1.27538824  0.1937     -0.57636903]\n",
      " [-0.83748859  0.78562273 -0.58288285 -0.94296449  0.23662032 -0.15676458\n",
      "  -0.6929486  -0.36137941  0.76280079]\n",
      " [ 0.66140808 -0.41702136  0.05926575  0.53443028 -0.46045336  0.29969705\n",
      "   0.6775424  -0.03442978 -0.66593412]\n",
      " [ 0.69024311 -0.70963029  0.42449721  0.59372426 -0.36065153  0.14629573\n",
      "   0.88596073  0.82927259 -0.38036204]\n",
      " [-1.00549772  1.00103691 -0.57173263 -1.05954156  0.11846574  0.20247686\n",
      "  -0.99299065 -0.72453162  0.7472237 ]\n",
      " [ 1.15176891 -0.89616546  0.56396116  0.67933115 -0.20859763 -0.25593815\n",
      "   0.81209075  0.83138799 -0.54206602]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.33681971  1.79994503 -2.00549418  0.2643109   0.44297887 -2.56808481\n",
      "   1.18608534]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:61 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.69278169]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 61 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.87850031 -1.03075262  0.36299488  0.92822431 -0.33412477 -0.10212777\n",
      "   1.25888385  0.1937     -0.57636903]\n",
      " [-0.82127585  0.80183548 -0.5666701  -0.94296449  0.23662032 -0.15676458\n",
      "  -0.67673586 -0.36137941  0.76280079]\n",
      " [ 0.64706247 -0.43136697  0.04492015  0.53443028 -0.46045336  0.29969705\n",
      "   0.66319679 -0.03442978 -0.66593412]\n",
      " [ 0.67414218 -0.72573121  0.40839628  0.59372426 -0.36065153  0.14629573\n",
      "   0.86985981  0.82927259 -0.38036204]\n",
      " [-0.98899683  1.0175378  -0.55523174 -1.05954156  0.11846574  0.20247686\n",
      "  -0.97648976 -0.72453162  0.7472237 ]\n",
      " [ 1.13530838 -0.91262598  0.54750064  0.67933115 -0.20859763 -0.25593815\n",
      "   0.79563023  0.83138799 -0.54206602]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.41054388  1.73928252 -2.02094238  0.21068797  0.38515491 -2.58078989\n",
      "   1.12442272]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:61 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.72463457]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 61 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.88462427 -1.03075262  0.36911884  0.92822431 -0.33412477 -0.10212777\n",
      "   1.25888385  0.19982395 -0.57636903]\n",
      " [-0.82734042  0.80183548 -0.57273468 -0.94296449  0.23662032 -0.15676458\n",
      "  -0.67673586 -0.36744398  0.76280079]\n",
      " [ 0.65112369 -0.43136697  0.04898137  0.53443028 -0.46045336  0.29969705\n",
      "   0.66319679 -0.03036856 -0.66593412]\n",
      " [ 0.68003498 -0.72573121  0.41428909  0.59372426 -0.36065153  0.14629573\n",
      "   0.86985981  0.8351654  -0.38036204]\n",
      " [-0.9942921   1.0175378  -0.56052701 -1.05954156  0.11846574  0.20247686\n",
      "  -0.97648976 -0.7298269   0.7472237 ]\n",
      " [ 1.14009261 -0.91262598  0.55228487  0.67933115 -0.20859763 -0.25593815\n",
      "   0.79563023  0.83617222 -0.54206602]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.38307077  1.76147282 -2.01687272  0.2287848   0.40909011 -2.57821447\n",
      "   1.14983895]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:61 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.55629894]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 61 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89643286 -1.03075262  0.36911884  0.9400329  -0.33412477 -0.10212777\n",
      "   1.25888385  0.19982395 -0.56456044]\n",
      " [-0.83814991  0.80183548 -0.57273468 -0.95377398  0.23662032 -0.15676458\n",
      "  -0.67673586 -0.36744398  0.7519913 ]\n",
      " [ 0.65777785 -0.43136697  0.04898137  0.54108444 -0.46045336  0.29969705\n",
      "   0.66319679 -0.03036856 -0.65927996]\n",
      " [ 0.69011641 -0.72573121  0.41428909  0.60380569 -0.36065153  0.14629573\n",
      "   0.86985981  0.8351654  -0.3702806 ]\n",
      " [-1.00628822  1.0175378  -0.56052701 -1.07153768  0.11846574  0.20247686\n",
      "  -0.97648976 -0.7298269   0.73522758]\n",
      " [ 1.15201698 -0.91262598  0.55228487  0.69125552 -0.20859763 -0.25593815\n",
      "   0.79563023  0.83617222 -0.53014165]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.3283113   1.8039087  -2.00222629  0.26312221  0.44794692 -2.56654771\n",
      "   1.19266083]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:61 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.14346723]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 61 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89666226 -1.03075262  0.36911884  0.9400329  -0.33389537 -0.10189837\n",
      "   1.25888385  0.19982395 -0.56433104]\n",
      " [-0.83813602  0.80183548 -0.57273468 -0.95377398  0.23663421 -0.15675069\n",
      "  -0.67673586 -0.36744398  0.75200519]\n",
      " [ 0.65813308 -0.43136697  0.04898137  0.54108444 -0.46009813  0.30005228\n",
      "   0.66319679 -0.03036856 -0.65892473]\n",
      " [ 0.68988461 -0.72573121  0.41428909  0.60380569 -0.36088333  0.14606393\n",
      "   0.86985981  0.8351654  -0.37051241]\n",
      " [-1.00639808  1.0175378  -0.56052701 -1.07153768  0.11835588  0.202367\n",
      "  -0.97648976 -0.7298269   0.73511772]\n",
      " [ 1.15167238 -0.91262598  0.55228487  0.69125552 -0.20894223 -0.25628275\n",
      "   0.79563023  0.83617222 -0.53048625]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.33712625  1.79973105 -2.00661987  0.25907153  0.44330721 -2.57106508\n",
      "   1.18790734]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:61 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.02743644]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 61 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89673871 -1.03067617  0.36911884  0.9400329  -0.33381891 -0.10182191\n",
      "   1.25888385  0.19982395 -0.56425459]\n",
      " [-0.83819842  0.80177308 -0.57273468 -0.95377398  0.23657181 -0.15681309\n",
      "  -0.67673586 -0.36744398  0.75194279]\n",
      " [ 0.65818279 -0.43131726  0.04898137  0.54108444 -0.46004842  0.30010198\n",
      "   0.66319679 -0.03036856 -0.65887502]\n",
      " [ 0.68993631 -0.72567952  0.41428909  0.60380569 -0.36083163  0.14611562\n",
      "   0.86985981  0.8351654  -0.37046071]\n",
      " [-1.00647247  1.01746341 -0.56052701 -1.07153768  0.1182815   0.20229261\n",
      "  -0.97648976 -0.7298269   0.73504333]\n",
      " [ 1.15173259 -0.91256577  0.55228487  0.69125552 -0.20888202 -0.25622254\n",
      "   0.79563023  0.83617222 -0.53042604]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.3374923   1.79964195 -2.00687209  0.25894117  0.44317927 -2.57133743\n",
      "   1.18779043]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:61 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.37788427]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 61 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.88992611 -1.03748877  0.36911884  0.9400329  -0.34063151 -0.10863451\n",
      "   1.25207125  0.19982395 -0.56425459]\n",
      " [-0.83182565  0.80814585 -0.57273468 -0.95377398  0.24294458 -0.15044032\n",
      "  -0.67036309 -0.36744398  0.75194279]\n",
      " [ 0.65106648 -0.43843357  0.04898137  0.54108444 -0.46716473  0.29298567\n",
      "   0.65608048 -0.03036856 -0.65887502]\n",
      " [ 0.68367789 -0.73193794  0.41428909  0.60380569 -0.36709005  0.1398572\n",
      "   0.86360138  0.8351654  -0.37046071]\n",
      " [-1.00000684  1.02392904 -0.56052701 -1.07153768  0.12474713  0.20875824\n",
      "  -0.97002413 -0.7298269   0.73504333]\n",
      " [ 1.14589319 -0.91840517  0.55228487  0.69125552 -0.21472142 -0.26206194\n",
      "   0.78979083  0.83617222 -0.53042604]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.38191028  1.77006791 -2.02227356  0.22896655  0.41430393 -2.58662318\n",
      "   1.15942101]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:61 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.8433062]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 61 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89126405 -1.03748877  0.36911884  0.94137083 -0.34063151 -0.10863451\n",
      "   1.25340918  0.19982395 -0.56425459]\n",
      " [-0.83367571  0.80814585 -0.57273468 -0.95562403  0.24294458 -0.15044032\n",
      "  -0.67221315 -0.36744398  0.75194279]\n",
      " [ 0.65331597 -0.43843357  0.04898137  0.54333393 -0.46716473  0.29298567\n",
      "   0.65832998 -0.03036856 -0.65887502]\n",
      " [ 0.68575713 -0.73193794  0.41428909  0.60588493 -0.36709005  0.1398572\n",
      "   0.86568063  0.8351654  -0.37046071]\n",
      " [-1.00137676  1.02392904 -0.56052701 -1.0729076   0.12474713  0.20875824\n",
      "  -0.97139406 -0.7298269   0.73504333]\n",
      " [ 1.14760325 -0.91840517  0.55228487  0.69296558 -0.21472142 -0.26206194\n",
      "   0.79150089  0.83617222 -0.53042604]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.37155745  1.77996672 -2.02145565  0.23791055  0.42357768 -2.58615127\n",
      "   1.1690758 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:61 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.68573647]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 61 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89878597 -1.03748877  0.36911884  0.94137083 -0.33310959 -0.10111259\n",
      "   1.2609311   0.19982395 -0.56425459]\n",
      " [-0.84120902  0.80814585 -0.57273468 -0.95562403  0.23541127 -0.15797363\n",
      "  -0.67974645 -0.36744398  0.75194279]\n",
      " [ 0.660397   -0.43843357  0.04898137  0.54333393 -0.46008371  0.30006669\n",
      "   0.665411   -0.03036856 -0.65887502]\n",
      " [ 0.69319928 -0.73193794  0.41428909  0.60588493 -0.35964791  0.14729935\n",
      "   0.87312277  0.8351654  -0.37046071]\n",
      " [-1.00893365  1.02392904 -0.56052701 -1.0729076   0.11719024  0.20120136\n",
      "  -0.97895094 -0.7298269   0.73504333]\n",
      " [ 1.15516586 -0.91840517  0.55228487  0.69296558 -0.20715881 -0.25449933\n",
      "   0.7990635   0.83617222 -0.53042604]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.33769525  1.80857804 -2.01482879  0.2635515   0.45032477 -2.58064646\n",
      "   1.1965681 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:61 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.59039709]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 61 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.88366517 -1.03748877  0.36911884  0.92625004 -0.33310959 -0.11623338\n",
      "   1.2609311   0.19982395 -0.57937538]\n",
      " [-0.82595684  0.80814585 -0.57273468 -0.94037186  0.23541127 -0.14272145\n",
      "  -0.67974645 -0.36744398  0.76719496]\n",
      " [ 0.64771844 -0.43843357  0.04898137  0.53065538 -0.46008371  0.28738814\n",
      "   0.665411   -0.03036856 -0.67155358]\n",
      " [ 0.67863501 -0.73193794  0.41428909  0.59132066 -0.35964791  0.13273508\n",
      "   0.87312277  0.8351654  -0.38502498]\n",
      " [-0.99396186  1.02392904 -0.56052701 -1.05793582  0.11719024  0.21617314\n",
      "  -0.97895094 -0.7298269   0.75001512]\n",
      " [ 1.14068395 -0.91840517  0.55228487  0.67848367 -0.20715881 -0.26898124\n",
      "   0.7990635   0.83617222 -0.54490795]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.40908263  1.75403739 -2.03131686  0.21361596  0.39708966 -2.59787183\n",
      "   1.14350566]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:61 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.70810219]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 61 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89012815 -1.03102579  0.36911884  0.93271301 -0.33310959 -0.10977041\n",
      "   1.26739408  0.19982395 -0.57937538]\n",
      " [-0.83258665  0.80151604 -0.57273468 -0.94700167  0.23541127 -0.14935126\n",
      "  -0.68637626 -0.36744398  0.76719496]\n",
      " [ 0.65442124 -0.43173078  0.04898137  0.53735818 -0.46008371  0.29409094\n",
      "   0.6721138  -0.03036856 -0.67155358]\n",
      " [ 0.68538847 -0.72518447  0.41428909  0.59807412 -0.35964791  0.13948855\n",
      "   0.87987624  0.8351654  -0.38502498]\n",
      " [-1.00058145  1.01730945 -0.56052701 -1.06455541  0.11719024  0.20955355\n",
      "  -0.98557053 -0.7298269   0.75001512]\n",
      " [ 1.14740589 -0.91168323  0.55228487  0.68520561 -0.20715881 -0.2622593\n",
      "   0.80578544  0.83617222 -0.54490795]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.37891595  1.78033723 -2.02696613  0.23909426  0.42194784 -2.59355856\n",
      "   1.16785117]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:61 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.75077004]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 61 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.8949454  -1.02620854  0.36911884  0.93753027 -0.33310959 -0.10977041\n",
      "   1.27221133  0.19982395 -0.57937538]\n",
      " [-0.83778072  0.79632197 -0.57273468 -0.95219574  0.23541127 -0.14935126\n",
      "  -0.69157033 -0.36744398  0.76719496]\n",
      " [ 0.65961752 -0.42653449  0.04898137  0.54255446 -0.46008371  0.29409094\n",
      "   0.67731008 -0.03036856 -0.67155358]\n",
      " [ 0.69058729 -0.71998566  0.41428909  0.60327294 -0.35964791  0.13948855\n",
      "   0.88507506  0.8351654  -0.38502498]\n",
      " [-1.00543395  1.01245695 -0.56052701 -1.06940791  0.11719024  0.20955355\n",
      "  -0.99042303 -0.7298269   0.75001512]\n",
      " [ 1.1525677  -0.90652142  0.55228487  0.69036743 -0.20715881 -0.2622593\n",
      "   0.81094725  0.83617222 -0.54490795]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.35559869  1.80101672 -2.02325482  0.25791683  0.44079211 -2.5908598\n",
      "   1.18764732]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:61 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.8272763]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 61 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89717874 -1.02620854  0.37135217  0.93753027 -0.33310959 -0.10753707\n",
      "   1.27444467  0.19982395 -0.57937538]\n",
      " [-0.84017447  0.79632197 -0.57512842 -0.95219574  0.23541127 -0.15174501\n",
      "  -0.69396408 -0.36744398  0.76719496]\n",
      " [ 0.66236273 -0.42653449  0.05172658  0.54255446 -0.46008371  0.29683615\n",
      "   0.68005529 -0.03036856 -0.67155358]\n",
      " [ 0.69308303 -0.71998566  0.41678483  0.60327294 -0.35964791  0.14198429\n",
      "   0.8875708   0.8351654  -0.38502498]\n",
      " [-1.00774192  1.01245695 -0.56283498 -1.06940791  0.11719024  0.20724558\n",
      "  -0.992731   -0.7298269   0.75001512]\n",
      " [ 1.15495961 -0.90652142  0.55467678  0.69036743 -0.20715881 -0.2598674\n",
      "   0.81333916  0.83617222 -0.54490795]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.34325843  1.81235528 -2.02207971  0.2683185   0.451821   -2.58978227\n",
      "   1.19881471]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:61 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.22834915]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 62 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89604134 -1.02734594  0.37135217  0.93639286 -0.33310959 -0.10753707\n",
      "   1.27444467  0.19982395 -0.58051278]\n",
      " [-0.83903838  0.79745806 -0.57512842 -0.95105965  0.23541127 -0.15174501\n",
      "  -0.69396408 -0.36744398  0.76833105]\n",
      " [ 0.66182696 -0.42707027  0.05172658  0.54201869 -0.46008371  0.29683615\n",
      "   0.68005529 -0.03036856 -0.67208935]\n",
      " [ 0.6921294  -0.72093928  0.41678483  0.60231931 -0.35964791  0.14198429\n",
      "   0.8875708   0.8351654  -0.38597861]\n",
      " [-1.00619777  1.0140011  -0.56283498 -1.06786376  0.11719024  0.20724558\n",
      "  -0.992731   -0.7298269   0.75155927]\n",
      " [ 1.15305338 -0.90842765  0.55467678  0.6884612  -0.20715881 -0.2598674\n",
      "   0.81333916  0.83617222 -0.54681418]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.36337665  1.80114879 -2.03099279  0.25772259  0.44080243 -2.59827163\n",
      "   1.18679969]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:62 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.83107945]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 62 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89808026 -1.02734594  0.3733911   0.93639286 -0.33310959 -0.10753707\n",
      "   1.27648359  0.19982395 -0.58051278]\n",
      " [-0.84145247  0.79745806 -0.57754252 -0.95105965  0.23541127 -0.15174501\n",
      "  -0.69637817 -0.36744398  0.76833105]\n",
      " [ 0.66445922 -0.42707027  0.05435884  0.54201869 -0.46008371  0.29683615\n",
      "   0.68268755 -0.03036856 -0.67208935]\n",
      " [ 0.69462152 -0.72093928  0.41927695  0.60231931 -0.35964791  0.14198429\n",
      "   0.89006292  0.8351654  -0.38597861]\n",
      " [-1.00821795  1.0140011  -0.56485516 -1.06786376  0.11719024  0.20724558\n",
      "  -0.99475118 -0.7298269   0.75155927]\n",
      " [ 1.15511177 -0.90842765  0.55673517  0.6884612  -0.20715881 -0.2598674\n",
      "   0.81539755  0.83617222 -0.54681418]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.35151958  1.81214057 -2.02970857  0.2672221   0.45124172 -2.59742219\n",
      "   1.19777467]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:62 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.6901114]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 62 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.88156321 -1.04386299  0.35687405  0.93639286 -0.33310959 -0.10753707\n",
      "   1.25996655  0.19982395 -0.58051278]\n",
      " [-0.82525266  0.81365788 -0.5613427  -0.95105965  0.23541127 -0.15174501\n",
      "  -0.68017836 -0.36744398  0.76833105]\n",
      " [ 0.65015541 -0.44137407  0.04005503  0.54201869 -0.46008371  0.29683615\n",
      "   0.66838374 -0.03036856 -0.67208935]\n",
      " [ 0.67853278 -0.73702802  0.40318821  0.60231931 -0.35964791  0.14198429\n",
      "   0.87397418  0.8351654  -0.38597861]\n",
      " [-0.99169847  1.03052058 -0.54833568 -1.06786376  0.11719024  0.20724558\n",
      "  -0.9782317  -0.7298269   0.75155927]\n",
      " [ 1.13862413 -0.92491529  0.54024753  0.6884612  -0.20715881 -0.2598674\n",
      "   0.79890991  0.83617222 -0.54681418]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.42531238  1.75158412 -2.04529105  0.21364833  0.39346463 -2.61030159\n",
      "   1.13618484]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:62 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.72693475]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 62 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.88760589 -1.04386299  0.36291672  0.93639286 -0.33310959 -0.10753707\n",
      "   1.25996655  0.20586662 -0.58051278]\n",
      " [-0.83123151  0.81365788 -0.56732155 -0.95105965  0.23541127 -0.15174501\n",
      "  -0.68017836 -0.37342283  0.76833105]\n",
      " [ 0.65417276 -0.44137407  0.04407238  0.54201869 -0.46008371  0.29683615\n",
      "   0.66838374 -0.02635121 -0.67208935]\n",
      " [ 0.68433944 -0.73702802  0.40899486  0.60231931 -0.35964791  0.14198429\n",
      "   0.87397418  0.84097205 -0.38597861]\n",
      " [-0.99692004  1.03052058 -0.55355725 -1.06786376  0.11719024  0.20724558\n",
      "  -0.9782317  -0.73504847  0.75155927]\n",
      " [ 1.14334189 -0.92491529  0.54496529  0.6884612  -0.20715881 -0.2598674\n",
      "   0.79890991  0.84088998 -0.54681418]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Theta two: \n",
      "[[-1.39821057  1.7734874  -2.04129249  0.23151451  0.41709174 -2.60776351\n",
      "   1.16125916]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:62 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.55819048]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 62 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89937485 -1.04386299  0.36291672  0.94816183 -0.33310959 -0.10753707\n",
      "   1.25996655  0.20586662 -0.56874382]\n",
      " [-0.84202184  0.81365788 -0.56732155 -0.96184999  0.23541127 -0.15174501\n",
      "  -0.68017836 -0.37342283  0.75754072]\n",
      " [ 0.66084222 -0.44137407  0.04407238  0.54868814 -0.46008371  0.29683615\n",
      "   0.66838374 -0.02635121 -0.66541989]\n",
      " [ 0.69441991 -0.73702802  0.40899486  0.61239979 -0.35964791  0.14198429\n",
      "   0.87397418  0.84097205 -0.37589814]\n",
      " [-1.00886943  1.03052058 -0.55355725 -1.07981314  0.11719024  0.20724558\n",
      "  -0.9782317  -0.73504847  0.73960989]\n",
      " [ 1.15522451 -0.92491529  0.54496529  0.70034382 -0.20715881 -0.2598674\n",
      "   0.79890991  0.84088998 -0.53493156]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.34373239  1.81577174 -2.0267901   0.26573261  0.45583059 -2.59621698\n",
      "   1.2039318 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:62 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.13901657]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 62 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89960298 -1.04386299  0.36291672  0.94816183 -0.33288146 -0.10730894\n",
      "   1.25996655  0.20586662 -0.56851569]\n",
      " [-0.84202015  0.81365788 -0.56732155 -0.96184999  0.23541297 -0.15174332\n",
      "  -0.68017836 -0.37342283  0.75754241]\n",
      " [ 0.66118883 -0.44137407  0.04407238  0.54868814 -0.4597371   0.29718276\n",
      "   0.66838374 -0.02635121 -0.66507328]\n",
      " [ 0.69421067 -0.73702802  0.40899486  0.61239979 -0.35985715  0.14177505\n",
      "   0.87397418  0.84097205 -0.37610738]\n",
      " [-1.0089841   1.03052058 -0.55355725 -1.07981314  0.11707557  0.20713091\n",
      "  -0.9782317  -0.73504847  0.73949521]\n",
      " [ 1.1549076  -0.92491529  0.54496529  0.70034382 -0.20747572 -0.26018431\n",
      "   0.79890991  0.84088998 -0.53524847]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.35205191  1.81184057 -2.03094816  0.26192109  0.45146124 -2.60049147\n",
      "   1.19945388]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:62 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.02556478]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 62 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89966994 -1.04379603  0.36291672  0.94816183 -0.33281451 -0.10724199\n",
      "   1.25996655  0.20586662 -0.56844874]\n",
      " [-0.84207524  0.81360278 -0.56732155 -0.96184999  0.23535787 -0.15179841\n",
      "  -0.68017836 -0.37342283  0.75748732]\n",
      " [ 0.66123301 -0.44132989  0.04407238  0.54868814 -0.45969292  0.29722694\n",
      "   0.66838374 -0.02635121 -0.66502911]\n",
      " [ 0.69425657 -0.73698213  0.40899486  0.61239979 -0.35981125  0.14182094\n",
      "   0.87397418  0.84097205 -0.37606148]\n",
      " [-1.00904932  1.03045535 -0.55355725 -1.07981314  0.11701034  0.20706569\n",
      "  -0.9782317  -0.73504847  0.73942999]\n",
      " [ 1.15496078 -0.92486211  0.54496529  0.70034382 -0.20742254 -0.26013112\n",
      "   0.79890991  0.84088998 -0.53519529]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.35237033  1.81176417 -2.03116874  0.26180883  0.45135109 -2.60072949\n",
      "   1.19935331]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:62 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.37078177]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 62 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89313731 -1.05032866  0.36291672  0.94816183 -0.33934713 -0.11377461\n",
      "   1.25343392  0.20586662 -0.56844874]\n",
      " [-0.83593563  0.81974239 -0.56732155 -0.96184999  0.24149748 -0.1456588\n",
      "  -0.67403875 -0.37342283  0.75748732]\n",
      " [ 0.65433391 -0.44822899  0.04407238  0.54868814 -0.46659202  0.29032784\n",
      "   0.66148464 -0.02635121 -0.66502911]\n",
      " [ 0.68821175 -0.74302694  0.40899486  0.61239979 -0.36585606  0.13577613\n",
      "   0.86792936  0.84097205 -0.37606148]\n",
      " [-1.00284897  1.0366557  -0.55355725 -1.07981314  0.1232107   0.21326604\n",
      "  -0.97203134 -0.73504847  0.73942999]\n",
      " [ 1.14934413 -0.93047876  0.54496529  0.70034382 -0.21303919 -0.26574778\n",
      "   0.79329326  0.84088998 -0.53519529]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.39562252  1.78309674 -2.0462475   0.23266172  0.4232941  -2.61573312\n",
      "   1.17181079]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:62 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.84663345]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 62 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89441449 -1.05032866  0.36291672  0.94943901 -0.33934713 -0.11377461\n",
      "   1.25471111  0.20586662 -0.56844874]\n",
      " [-0.83770265  0.81974239 -0.56732155 -0.96361701  0.24149748 -0.1456588\n",
      "  -0.67580576 -0.37342283  0.75748732]\n",
      " [ 0.65649067 -0.44822899  0.04407238  0.5508449  -0.46659202  0.29032784\n",
      "   0.66364141 -0.02635121 -0.66502911]\n",
      " [ 0.6902     -0.74302694  0.40899486  0.61438803 -0.36585606  0.13577613\n",
      "   0.86991761  0.84097205 -0.37606148]\n",
      " [-1.0041565   1.0366557  -0.55355725 -1.08112067  0.1232107   0.21326604\n",
      "  -0.97333887 -0.73504847  0.73942999]\n",
      " [ 1.15097601 -0.93047876  0.54496529  0.7019757  -0.21303919 -0.26574778\n",
      "   0.79492514  0.84088998 -0.53519529]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.38566556  1.79262232 -2.04547228  0.24128267  0.43222936 -2.61528491\n",
      "   1.18110638]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:62 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.6878896]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 62 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90185655 -1.05032866  0.36291672  0.94943901 -0.33190508 -0.10633256\n",
      "   1.26215316  0.20586662 -0.56844874]\n",
      " [-0.84515952  0.81974239 -0.56732155 -0.96361701  0.23404061 -0.15311567\n",
      "  -0.68326264 -0.37342283  0.75748732]\n",
      " [ 0.66351314 -0.44822899  0.04407238  0.5508449  -0.45956956  0.2973503\n",
      "   0.67066387 -0.02635121 -0.66502911]\n",
      " [ 0.69757099 -0.74302694  0.40899486  0.61438803 -0.35848507  0.14314713\n",
      "   0.8772886   0.84097205 -0.37606148]\n",
      " [-1.01163273  1.0366557  -0.55355725 -1.08112067  0.11573446  0.20578981\n",
      "  -0.98081511 -0.73504847  0.73942999]\n",
      " [ 1.15846086 -0.93047876  0.54496529  0.7019757  -0.20555434 -0.25826293\n",
      "   0.80240999  0.84088998 -0.53519529]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.3521609   1.82093416 -2.03893796  0.26669224  0.45872642 -2.60984621\n",
      "   1.20833288]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:62 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.59108733]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 62 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.8867175  -1.05032866  0.36291672  0.93429997 -0.33190508 -0.1214716\n",
      "   1.26215316  0.20586662 -0.58358778]\n",
      " [-0.82989055  0.81974239 -0.56732155 -0.94834803  0.23404061 -0.1378467\n",
      "  -0.68326264 -0.37342283  0.77275629]\n",
      " [ 0.65080887 -0.44822899  0.04407238  0.53814064 -0.45956956  0.28464604\n",
      "   0.67066387 -0.02635121 -0.67773337]\n",
      " [ 0.6829774  -0.74302694  0.40899486  0.59979444 -0.35848507  0.12855353\n",
      "   0.8772886   0.84097205 -0.39065508]\n",
      " [-0.99664092  1.0366557  -0.55355725 -1.06612886  0.11573446  0.22078162\n",
      "  -0.98081511 -0.73504847  0.7544218 ]\n",
      " [ 1.14394482 -0.93047876  0.54496529  0.68745966 -0.20555434 -0.27277897\n",
      "   0.80240999  0.84088998 -0.54971133]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.42359472  1.76633583 -2.05541721  0.21669785  0.40541442 -2.62705747\n",
      "   1.15518456]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:62 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.70907374]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 62 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.8931545  -1.04389166  0.36291672  0.94073697 -0.33190508 -0.11503461\n",
      "   1.26859016  0.20586662 -0.58358778]\n",
      " [-0.83648636  0.81314658 -0.56732155 -0.95494385  0.23404061 -0.14444251\n",
      "  -0.68985845 -0.37342283  0.77275629]\n",
      " [ 0.65747408 -0.44156378  0.04407238  0.54480585 -0.45956956  0.29131125\n",
      "   0.67732908 -0.02635121 -0.67773337]\n",
      " [ 0.68969517 -0.73630917  0.40899486  0.60651221 -0.35848507  0.1352713\n",
      "   0.88400638  0.84097205 -0.39065508]\n",
      " [-1.00323021  1.03006641 -0.55355725 -1.07271815  0.11573446  0.21419232\n",
      "  -0.9874044  -0.73504847  0.7544218 ]\n",
      " [ 1.15063109 -0.92379249  0.54496529  0.69414593 -0.20555434 -0.2660927\n",
      "   0.80909625  0.84088998 -0.54971133]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.39358739  1.7924774  -2.0510856   0.2420545   0.43014872 -2.62275004\n",
      "   1.17940011]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:62 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.75257759]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 62 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89791441 -1.03913175  0.36291672  0.94549688 -0.33190508 -0.11503461\n",
      "   1.27335007  0.20586662 -0.58358778]\n",
      " [-0.84161614  0.8080168  -0.56732155 -0.96007362  0.23404061 -0.14444251\n",
      "  -0.69498822 -0.37342283  0.77275629]\n",
      " [ 0.66261005 -0.43642781  0.04407238  0.54994182 -0.45956956  0.29131125\n",
      "   0.68246505 -0.02635121 -0.67773337]\n",
      " [ 0.69483345 -0.73117088  0.40899486  0.6116505  -0.35848507  0.1352713\n",
      "   0.88914466  0.84097205 -0.39065508]\n",
      " [-1.00802425  1.02527238 -0.55355725 -1.07751218  0.11573446  0.21419232\n",
      "  -0.99219843 -0.73504847  0.7544218 ]\n",
      " [ 1.15572849 -0.91869509  0.54496529  0.69924333 -0.20555434 -0.2660927\n",
      "   0.81419365  0.84088998 -0.54971133]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.3705518   1.81290566 -2.0474305   0.26067075  0.44878586 -2.62008363\n",
      "   1.19896706]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:62 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.82830892]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 62 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90013079 -1.03913175  0.3651331   0.94549688 -0.33190508 -0.11281823\n",
      "   1.27556645  0.20586662 -0.58358778]\n",
      " [-0.84398695  0.8080168  -0.56969237 -0.96007362  0.23404061 -0.14681333\n",
      "  -0.69735904 -0.37342283  0.77275629]\n",
      " [ 0.66532581 -0.43642781  0.04678814  0.54994182 -0.45956956  0.29402701\n",
      "   0.68518081 -0.02635121 -0.67773337]\n",
      " [ 0.69730346 -0.73117088  0.41146487  0.6116505  -0.35848507  0.13774131\n",
      "   0.89161467  0.84097205 -0.39065508]\n",
      " [-1.01031414  1.02527238 -0.55584714 -1.07751218  0.11573446  0.21190243\n",
      "  -0.99448832 -0.73504847  0.7544218 ]\n",
      " [ 1.15809894 -0.91869509  0.54733574  0.69924333 -0.20555434 -0.26372225\n",
      "   0.8165641   0.84088998 -0.54971133]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.35834343  1.82411631 -2.04626472  0.27096199  0.45969548 -2.61901055\n",
      "   1.21001011]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:62 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.22288143]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 63 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89906839 -1.04019415  0.3651331   0.94443448 -0.33190508 -0.11281823\n",
      "   1.27556645  0.20586662 -0.58465018]\n",
      " [-0.84292279  0.80908096 -0.56969237 -0.95900946  0.23404061 -0.14681333\n",
      "  -0.69735904 -0.37342283  0.77382045]\n",
      " [ 0.66483916 -0.43691446  0.04678814  0.54945517 -0.45956956  0.29402701\n",
      "   0.68518081 -0.02635121 -0.67822002]\n",
      " [ 0.69640833 -0.73206602  0.41146487  0.61075536 -0.35848507  0.13774131\n",
      "   0.89161467  0.84097205 -0.39155021]\n",
      " [-1.00886198  1.02672454 -0.55584714 -1.07606003  0.11573446  0.21190243\n",
      "  -0.99448832 -0.73504847  0.75587396]\n",
      " [ 1.15629134 -0.92050268  0.54733574  0.69743574 -0.20555434 -0.26372225\n",
      "   0.8165641   0.84088998 -0.55151892]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.37764555  1.81339403 -2.05484275  0.26082345  0.44914406 -2.62718636\n",
      "   1.19850553]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:63 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.83289053]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 63 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90107008 -1.04019415  0.36713479  0.94443448 -0.33190508 -0.11281823\n",
      "   1.27756814  0.20586662 -0.58465018]\n",
      " [-0.84528919  0.80908096 -0.57205877 -0.95900946  0.23404061 -0.14681333\n",
      "  -0.69972545 -0.37342283  0.77382045]\n",
      " [ 0.66742184 -0.43691446  0.04937081  0.54945517 -0.45956956  0.29402701\n",
      "   0.68776348 -0.02635121 -0.67822002]\n",
      " [ 0.6988507  -0.73206602  0.41390724  0.61075536 -0.35848507  0.13774131\n",
      "   0.89405704  0.84097205 -0.39155021]\n",
      " [-1.01084575  1.02672454 -0.55783091 -1.07606003  0.11573446  0.21190243\n",
      "  -0.99647209 -0.73504847  0.75587396]\n",
      " [ 1.15831103 -0.92050268  0.54935542  0.69743574 -0.20555434 -0.26372225\n",
      "   0.81858379  0.84088998 -0.55151892]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.36601608  1.82417319 -2.05358524  0.27014653  0.45938664 -2.62635124\n",
      "   1.20926913]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:63 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.68729465]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 63 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.88454412 -1.05672011  0.35060883  0.94443448 -0.33190508 -0.11281823\n",
      "   1.26104217  0.20586662 -0.58465018]\n",
      " [-0.82910514  0.82526502 -0.55587471 -0.95900946  0.23404061 -0.14681333\n",
      "  -0.68354139 -0.37342283  0.77382045]\n",
      " [ 0.65316182 -0.45117448  0.0351108   0.54945517 -0.45956956  0.29402701\n",
      "   0.67350347 -0.02635121 -0.67822002]\n",
      " [ 0.68277691 -0.7481398   0.39783346  0.61075536 -0.35848507  0.13774131\n",
      "   0.87798325  0.84097205 -0.39155021]\n",
      " [-0.99431143  1.04325886 -0.54129659 -1.07606003  0.11573446  0.21190243\n",
      "  -0.97993777 -0.73504847  0.75587396]\n",
      " [ 1.14179888 -0.93701484  0.53284327  0.69743574 -0.20555434 -0.26372225\n",
      "   0.80207164  0.84088998 -0.55151892]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.43987301  1.76372831 -2.06930369  0.21662617  0.40166342 -2.63940652\n",
      "   1.1477596 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:63 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.72915647]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 63 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89050831 -1.05672011  0.35657302  0.94443448 -0.33190508 -0.11281823\n",
      "   1.26104217  0.21183081 -0.58465018]\n",
      " [-0.83500157  0.82526502 -0.56177114 -0.95900946  0.23404061 -0.14681333\n",
      "  -0.68354139 -0.37931926  0.77382045]\n",
      " [ 0.657136   -0.45117448  0.03908498  0.54945517 -0.45956956  0.29402701\n",
      "   0.67350347 -0.02237703 -0.67822002]\n",
      " [ 0.68850093 -0.7481398   0.40355747  0.61075536 -0.35848507  0.13774131\n",
      "   0.87798325  0.84669606 -0.39155021]\n",
      " [-0.99946255  1.04325886 -0.54644772 -1.07606003  0.11573446  0.21190243\n",
      "  -0.97993777 -0.74019959  0.75587396]\n",
      " [ 1.14645333 -0.93701484  0.53749773  0.69743574 -0.20555434 -0.26372225\n",
      "   0.80207164  0.84554444 -0.55151892]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.41312893  1.7853539  -2.06537257  0.23426902  0.42499267 -2.63690374\n",
      "   1.17250385]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:63 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.56006768]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 63 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90223654 -1.05672011  0.35657302  0.95616271 -0.33190508 -0.11281823\n",
      "   1.26104217  0.21183081 -0.57292194]\n",
      " [-0.84577085  0.82526502 -0.56177114 -0.96977874  0.23404061 -0.14681333\n",
      "  -0.68354139 -0.37931926  0.76305117]\n",
      " [ 0.66381785 -0.45117448  0.03908498  0.55613702 -0.45956956  0.29402701\n",
      "   0.67350347 -0.02237703 -0.67153817]\n",
      " [ 0.69857773 -0.7481398   0.40355747  0.62083217 -0.35848507  0.13774131\n",
      "   0.87798325  0.84669606 -0.38147341]\n",
      " [-1.01136439  1.04325886 -0.54644772 -1.08796187  0.11573446  0.21190243\n",
      "  -0.97993777 -0.74019959  0.74397212]\n",
      " [ 1.15829299 -0.93701484  0.53749773  0.70927539 -0.20555434 -0.26372225\n",
      "   0.80207164  0.84554444 -0.53967927]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.35893105  1.82748465 -2.05101066  0.26836507  0.46361031 -2.62547467\n",
      "   1.21502469]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:63 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.13474974]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 63 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90246243 -1.05672011  0.35657302  0.95616271 -0.33167919 -0.11259233\n",
      "   1.26104217  0.21183081 -0.57269605]\n",
      " [-0.8457797   0.82526502 -0.56177114 -0.96977874  0.23403176 -0.14682218\n",
      "  -0.68354139 -0.37931926  0.76304232]\n",
      " [ 0.66415557 -0.45117448  0.03908498  0.55613702 -0.45923183  0.29436473\n",
      "   0.67350347 -0.02237703 -0.67120045]\n",
      " [ 0.69838893 -0.7481398   0.40355747  0.62083217 -0.35867387  0.13755252\n",
      "   0.87798325  0.84669606 -0.38166221]\n",
      " [-1.0114826   1.04325886 -0.54644772 -1.08796187  0.11561626  0.21178423\n",
      "  -0.97993777 -0.74019959  0.74385391]\n",
      " [ 1.15800134 -0.93701484  0.53749773  0.70927539 -0.20584599 -0.2640139\n",
      "   0.80207164  0.84554444 -0.53997092]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.36678644  1.82378335 -2.0549472   0.26477679  0.45949353 -2.62952064\n",
      "   1.21080426]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:63 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.02383795]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 63 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90252111 -1.05666143  0.35657302  0.95616271 -0.33162051 -0.11253365\n",
      "   1.26104217  0.21183081 -0.57263737]\n",
      " [-0.84582836  0.82521636 -0.56177114 -0.96977874  0.2339831  -0.14687084\n",
      "  -0.68354139 -0.37931926  0.76299366]\n",
      " [ 0.66419483 -0.45113521  0.03908498  0.55613702 -0.45919257  0.29440399\n",
      "   0.67350347 -0.02237703 -0.67116119]\n",
      " [ 0.69842968 -0.74809905  0.40355747  0.62083217 -0.35863312  0.13759327\n",
      "   0.87798325  0.84669606 -0.38162146]\n",
      " [-1.01153983  1.04320162 -0.54644772 -1.08796187  0.11555902  0.21172699\n",
      "  -0.97993777 -0.74019959  0.74379668]\n",
      " [ 1.15804834 -0.93696784  0.53749773  0.70927539 -0.20579899 -0.2639669\n",
      "   0.80207164  0.84554444 -0.53992392]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.36706379  1.82371772 -2.05514032  0.26467997  0.45939856 -2.62972888\n",
      "   1.21071761]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:63 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.36380443]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 63 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89625867 -1.06292387  0.35657302  0.95616271 -0.33788295 -0.1187961\n",
      "   1.25477973  0.21183081 -0.57263737]\n",
      " [-0.83991476  0.83112996 -0.56177114 -0.96977874  0.2398967  -0.14095724\n",
      "  -0.67762779 -0.37931926  0.76299366]\n",
      " [ 0.65750735 -0.45782269  0.03908498  0.55613702 -0.46588005  0.28771651\n",
      "   0.66681599 -0.02237703 -0.67116119]\n",
      " [ 0.69259278 -0.75393594  0.40355747  0.62083217 -0.36447001  0.13175637\n",
      "   0.87214636  0.84669606 -0.38162146]\n",
      " [-1.00559543  1.04914602 -0.54644772 -1.08796187  0.12150342  0.21767139\n",
      "  -0.97399337 -0.74019959  0.74379668]\n",
      " [ 1.15264787 -0.94236831  0.53749773  0.70927539 -0.21119946 -0.26936737\n",
      "   0.79667117  0.84554444 -0.53992392]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.4091652   1.79593595 -2.06989496  0.23634583  0.43214553 -2.64444569\n",
      "   1.18398655]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:63 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.84985379]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 63 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89747841 -1.06292387  0.35657302  0.95738245 -0.33788295 -0.1187961\n",
      "   1.25599947  0.21183081 -0.57263737]\n",
      " [-0.84160319  0.83112996 -0.56177114 -0.97146716  0.2398967  -0.14095724\n",
      "  -0.67931621 -0.37931926  0.76299366]\n",
      " [ 0.65957576 -0.45782269  0.03908498  0.55820543 -0.46588005  0.28771651\n",
      "   0.6688844  -0.02237703 -0.67116119]\n",
      " [ 0.69449471 -0.75393594  0.40355747  0.62273409 -0.36447001  0.13175637\n",
      "   0.87404829  0.84669606 -0.38162146]\n",
      " [-1.00684398  1.04914602 -0.54644772 -1.08921042  0.12150342  0.21767139\n",
      "  -0.97524192 -0.74019959  0.74379668]\n",
      " [ 1.15420591 -0.94236831  0.53749773  0.71083343 -0.21119946 -0.26936737\n",
      "   0.7982292   0.84554444 -0.53992392]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.39958569  1.80510535 -2.06915971  0.24465761  0.44075699 -2.64401973\n",
      "   1.19293897]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:63 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.69005406]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 63 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90484011 -1.06292387  0.35657302  0.95738245 -0.33052125 -0.1114344\n",
      "   1.26336117  0.21183081 -0.57263737]\n",
      " [-0.84898311  0.83112996 -0.56177114 -0.97146716  0.23251678 -0.14833717\n",
      "  -0.68669613 -0.37931926  0.76299366]\n",
      " [ 0.66653878 -0.45782269  0.03908498  0.55820543 -0.45891703  0.29467953\n",
      "   0.67584742 -0.02237703 -0.67116119]\n",
      " [ 0.70179378 -0.75393594  0.40355747  0.62273409 -0.35717095  0.13905544\n",
      "   0.88134735  0.84669606 -0.38162146]\n",
      " [-1.01423915  1.04914602 -0.54644772 -1.08921042  0.11410826  0.21027623\n",
      "  -0.98263709 -0.74019959  0.74379668]\n",
      " [ 1.16161244 -0.94236831  0.53749773  0.71083343 -0.20379293 -0.26196083\n",
      "   0.80563574  0.84554444 -0.53992392]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.36644016  1.83311717 -2.06271783  0.26983381  0.46700173 -2.63864786\n",
      "   1.21989759]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:63 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.59174043]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 63 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.88968388 -1.06292387  0.35657302  0.94222622 -0.33052125 -0.12659063\n",
      "   1.26336117  0.21183081 -0.5877936 ]\n",
      " [-0.83369864  0.83112996 -0.56177114 -0.95618269  0.23251678 -0.1330527\n",
      "  -0.68669613 -0.37931926  0.77827813]\n",
      " [ 0.65381105 -0.45782269  0.03908498  0.5454777  -0.45891703  0.2819518\n",
      "   0.67584742 -0.02237703 -0.68388891]\n",
      " [ 0.68717284 -0.75393594  0.40355747  0.60811316 -0.35717095  0.1244345\n",
      "   0.88134735  0.84669606 -0.39624239]\n",
      " [-0.99922844  1.04914602 -0.54644772 -1.07419971  0.11410826  0.22528693\n",
      "  -0.98263709 -0.74019959  0.75880738]\n",
      " [ 1.14706427 -0.94236831  0.53749773  0.69628525 -0.20379293 -0.27650901\n",
      "   0.80563574  0.84554444 -0.55447209]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.43791758  1.77846446 -2.07918953  0.21978513  0.41361774 -2.65584554\n",
      "   1.1666681 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:63 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.71000888]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 63 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89609583 -1.05651192  0.35657302  0.94863817 -0.33052125 -0.12017868\n",
      "   1.26977312  0.21183081 -0.5877936 ]\n",
      " [-0.84026184  0.82456675 -0.56177114 -0.9627459   0.23251678 -0.1396159\n",
      "  -0.69325934 -0.37931926  0.77827813]\n",
      " [ 0.66044004 -0.4511937   0.03908498  0.55210669 -0.45891703  0.28858079\n",
      "   0.6824764  -0.02237703 -0.68388891]\n",
      " [ 0.69385628 -0.7472525   0.40355747  0.6147966  -0.35717095  0.13111794\n",
      "   0.88803079  0.84669606 -0.39624239]\n",
      " [-1.00578858  1.04258589 -0.54644772 -1.08075984  0.11410826  0.2187268\n",
      "  -0.98919722 -0.74019959  0.75880738]\n",
      " [ 1.15371613 -0.93571645  0.53749773  0.70293711 -0.20379293 -0.26985715\n",
      "   0.81228759  0.84554444 -0.55447209]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.40806353  1.80445365 -2.07487597  0.2450249   0.43823237 -2.65154352\n",
      "   1.19075787]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:63 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.7543494]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 63 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90079966 -1.05180809  0.35657302  0.953342   -0.33052125 -0.12017868\n",
      "   1.27447695  0.21183081 -0.5877936 ]\n",
      " [-0.8453288   0.8194998  -0.56177114 -0.96781285  0.23251678 -0.1396159\n",
      "  -0.69832629 -0.37931926  0.77827813]\n",
      " [ 0.66551689 -0.44611685  0.03908498  0.55718354 -0.45891703  0.28858079\n",
      "   0.68755325 -0.02237703 -0.68388891]\n",
      " [ 0.69893519 -0.7421736   0.40355747  0.61987551 -0.35717095  0.13111794\n",
      "   0.8931097   0.84669606 -0.39624239]\n",
      " [-1.01052548  1.03784899 -0.54644772 -1.08549675  0.11410826  0.2187268\n",
      "  -0.99393413 -0.74019959  0.75880738]\n",
      " [ 1.15875069 -0.93068189  0.53749773  0.70797167 -0.20379293 -0.26985715\n",
      "   0.81732215  0.84554444 -0.55447209]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.38530322  1.82463642 -2.07127534  0.26343906  0.4566662  -2.64890867\n",
      "   1.21010014]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:63 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.8292833]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 63 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90300047 -1.05180809  0.35877383  0.953342   -0.33052125 -0.11797787\n",
      "   1.27667776  0.21183081 -0.5877936 ]\n",
      " [-0.84767838  0.8194998  -0.56412072 -0.96781285  0.23251678 -0.14196548\n",
      "  -0.70067587 -0.37931926  0.77827813]\n",
      " [ 0.668205   -0.44611685  0.04177309  0.55718354 -0.45891703  0.2912689\n",
      "   0.69024136 -0.02237703 -0.68388891]\n",
      " [ 0.7013813  -0.7421736   0.40600358  0.61987551 -0.35717095  0.13356405\n",
      "   0.89555581  0.84669606 -0.39624239]\n",
      " [-1.01279877  1.03784899 -0.548721   -1.08549675  0.11410826  0.21645351\n",
      "  -0.99620742 -0.74019959  0.75880738]\n",
      " [ 1.1611014  -0.93068189  0.53984844  0.70797167 -0.20379293 -0.26750644\n",
      "   0.81967287  0.84554444 -0.55447209]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.37321883  1.83572643 -2.07011792  0.27362623  0.46746328 -2.64783929\n",
      "   1.22102573]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:63 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.21751812]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 64 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90200912 -1.05279944  0.35877383  0.95235066 -0.33052125 -0.11797787\n",
      "   1.27667776  0.21183081 -0.58878495]\n",
      " [-0.84668269  0.82049549 -0.56412072 -0.96681716  0.23251678 -0.14196548\n",
      "  -0.70067587 -0.37931926  0.77927382]\n",
      " [ 0.66776459 -0.44655726  0.04177309  0.55674313 -0.45891703  0.2912689\n",
      "   0.69024136 -0.02237703 -0.68432932]\n",
      " [ 0.70054218 -0.74301272  0.40600358  0.61903639 -0.35717095  0.13356405\n",
      "   0.89555581  0.84669606 -0.39708151]\n",
      " [-1.01143412  1.03921364 -0.548721   -1.0841321   0.11410826  0.21645351\n",
      "  -0.99620742 -0.74019959  0.76017203]\n",
      " [ 1.15938859 -0.9323947   0.53984844  0.70625886 -0.20379293 -0.26750644\n",
      "   0.81967287  0.84554444 -0.5561849 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.39173005  1.82547169 -2.07836996  0.26392954  0.45736387 -2.65570946\n",
      "   1.21001492]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:64 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.83463087]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 64 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90397551 -1.05279944  0.36074022  0.95235066 -0.33052125 -0.11797787\n",
      "   1.27864415  0.21183081 -0.58878495]\n",
      " [-0.84900382  0.82049549 -0.56644185 -0.96681716  0.23251678 -0.14196548\n",
      "  -0.702997   -0.37931926  0.77927382]\n",
      " [ 0.67029987 -0.44655726  0.04430837  0.55674313 -0.45891703  0.2912689\n",
      "   0.69277665 -0.02237703 -0.68432932]\n",
      " [ 0.7029373  -0.74301272  0.40839869  0.61903639 -0.35717095  0.13356405\n",
      "   0.89795092  0.84669606 -0.39708151]\n",
      " [-1.01338341  1.03921364 -0.5506703  -1.0841321   0.11410826  0.21645351\n",
      "  -0.99815671 -0.74019959  0.76017203]\n",
      " [ 1.16137171 -0.9323947   0.54183156  0.70625886 -0.20379293 -0.26750644\n",
      "   0.82165599  0.84554444 -0.5561849 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.38031775  1.83604777 -2.07713759  0.27308391  0.46741831 -2.65488777\n",
      "   1.22057651]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:64 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.68433239]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 64 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.88744468 -1.06933027  0.34420939  0.95235066 -0.33052125 -0.11797787\n",
      "   1.26211332  0.21183081 -0.58878495]\n",
      " [-0.83283864  0.83666067 -0.55027668 -0.96681716  0.23251678 -0.14196548\n",
      "  -0.68683182 -0.37931926  0.77927382]\n",
      " [ 0.65608577 -0.46077137  0.03009427  0.55674313 -0.45891703  0.2912689\n",
      "   0.67856254 -0.02237703 -0.68432932]\n",
      " [ 0.68688152 -0.7590685   0.39234291  0.61903639 -0.35717095  0.13356405\n",
      "   0.88189514  0.84669606 -0.39708151]\n",
      " [-0.99683835  1.0557587  -0.53412523 -1.0841321   0.11410826  0.21645351\n",
      "  -0.98161164 -0.74019959  0.76017203]\n",
      " [ 1.14483808 -0.94892833  0.52529793  0.70625886 -0.20379293 -0.26750644\n",
      "   0.80512236  0.84554444 -0.5561849 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.45423303  1.77572091 -2.09299321  0.21962191  0.40975668 -2.66812014\n",
      "   1.15915568]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:64 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.73130344]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 64 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89333307 -1.06933027  0.35009778  0.95235066 -0.33052125 -0.11797787\n",
      "   1.26211332  0.21771921 -0.58878495]\n",
      " [-0.8386558   0.83666067 -0.55609383 -0.96681716  0.23251678 -0.14196548\n",
      "  -0.68683182 -0.38513642  0.77927382]\n",
      " [ 0.6600175  -0.46077137  0.03402599  0.55674313 -0.45891703  0.2912689\n",
      "   0.67856254 -0.01844531 -0.68432932]\n",
      " [ 0.69252621 -0.7590685   0.39798761  0.61903639 -0.35717095  0.13356405\n",
      "   0.88189514  0.85234076 -0.39708151]\n",
      " [-1.0019221   1.0557587  -0.53920898 -1.0841321   0.11410826  0.21645351\n",
      "  -0.98161164 -0.74528334  0.76017203]\n",
      " [ 1.14943222 -0.94892833  0.52989207  0.70625886 -0.20379293 -0.26750644\n",
      "   0.80512236  0.85013858 -0.5561849 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.42783376  1.79707774 -2.08912609  0.23704844  0.43279788 -2.66565076\n",
      "   1.18358117]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:64 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.56192816]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 64 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90501961 -1.06933027  0.35009778  0.96403719 -0.33052125 -0.11797787\n",
      "   1.26211332  0.21771921 -0.57709842]\n",
      " [-0.84940229  0.83666067 -0.55609383 -0.97756365  0.23251678 -0.14196548\n",
      "  -0.68683182 -0.38513642  0.76852733]\n",
      " [ 0.66670899 -0.46077137  0.03402599  0.56343462 -0.45891703  0.2912689\n",
      "   0.67856254 -0.01844531 -0.67763783]\n",
      " [ 0.70259684 -0.7590685   0.39798761  0.62910702 -0.35717095  0.13356405\n",
      "   0.88189514  0.85234076 -0.38701088]\n",
      " [-1.01377574  1.0557587  -0.53920898 -1.09598574  0.11410826  0.21645351\n",
      "  -0.98161164 -0.74528334  0.74831839]\n",
      " [ 1.16122788 -0.94892833  0.52989207  0.71805452 -0.20379293 -0.26750644\n",
      "   0.80512236  0.85013858 -0.54438924]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.37391481  1.83905321 -2.0749011   0.27102008  0.4712915  -2.65433634\n",
      "   1.22594809]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:64 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.13065678]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 64 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90524248 -1.06933027  0.35009778  0.96403719 -0.33029838 -0.117755\n",
      "   1.26211332  0.21771921 -0.57687555]\n",
      " [-0.84942024  0.83666067 -0.55609383 -0.97756365  0.23249883 -0.14198343\n",
      "  -0.68683182 -0.38513642  0.76850938]\n",
      " [ 0.66703764 -0.46077137  0.03402599  0.56343462 -0.45858838  0.29159755\n",
      "   0.67856254 -0.01844531 -0.67730918]\n",
      " [ 0.70242657 -0.7590685   0.39798761  0.62910702 -0.35734122  0.13339378\n",
      "   0.88189514  0.85234076 -0.38718115]\n",
      " [-1.01389638  1.0557587  -0.53920898 -1.09598574  0.11398761  0.21633287\n",
      "  -0.98161164 -0.74528334  0.74819775]\n",
      " [ 1.16095932 -0.94892833  0.52989207  0.71805452 -0.20406149 -0.267775\n",
      "   0.80512236  0.85013858 -0.5446578 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.38133517  1.83556644 -2.07862924  0.26764029  0.46741081 -2.65816725\n",
      "   1.2219684 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:64 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.02224356]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 64 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90529394 -1.0692788   0.35009778  0.96403719 -0.33024692 -0.11770354\n",
      "   1.26211332  0.21771921 -0.57682408]\n",
      " [-0.84946324  0.83661766 -0.55609383 -0.97756365  0.23245582 -0.14202644\n",
      "  -0.68683182 -0.38513642  0.76846637]\n",
      " [ 0.66707255 -0.46073646  0.03402599  0.56343462 -0.45855348  0.29163246\n",
      "   0.67856254 -0.01844531 -0.67727427]\n",
      " [ 0.70246277 -0.7590323   0.39798761  0.62910702 -0.35730502  0.13342998\n",
      "   0.88189514  0.85234076 -0.38714495]\n",
      " [-1.01394664  1.05570845 -0.53920898 -1.09598574  0.11393735  0.21628261\n",
      "  -0.98161164 -0.74528334  0.74814749]\n",
      " [ 1.16100087 -0.94888678  0.52989207  0.71805452 -0.20401994 -0.26773345\n",
      "   0.80512236  0.85013858 -0.54461625]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.38157705  1.83550998 -2.0787985   0.26755668  0.46732882 -2.65834964\n",
      "   1.22189364]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:64 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.3569569]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 64 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89929185 -1.0752809   0.35009778  0.96403719 -0.33624901 -0.12370563\n",
      "   1.25611122  0.21771921 -0.57682408]\n",
      " [-0.84376838  0.84231253 -0.55609383 -0.97756365  0.23815069 -0.13633157\n",
      "  -0.68113696 -0.38513642  0.76846637]\n",
      " [ 0.66059089 -0.46721812  0.03402599  0.56343462 -0.46503513  0.2851508\n",
      "   0.67208089 -0.01844531 -0.67727427]\n",
      " [ 0.69682789 -0.76466717  0.39798761  0.62910702 -0.3629399   0.12779511\n",
      "   0.87626027  0.85234076 -0.38714495]\n",
      " [-1.00824885  1.06140624 -0.53920898 -1.09598574  0.11963514  0.2219804\n",
      "  -0.97591385 -0.74528334  0.74814749]\n",
      " [ 1.15580989 -0.95407776  0.52989207  0.71805452 -0.20921092 -0.27292443\n",
      "   0.79993138  0.85013858 -0.54461625]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.42254476  1.80859212 -2.09322859  0.24001976  0.44086416 -2.67277616\n",
      "   1.19595752]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:64 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.85297198]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 64 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90045724 -1.0752809   0.35009778  0.96520259 -0.33624901 -0.12370563\n",
      "   1.25727662  0.21771921 -0.57682408]\n",
      " [-0.84538238  0.84231253 -0.55609383 -0.97917765  0.23815069 -0.13633157\n",
      "  -0.68275097 -0.38513642  0.76846637]\n",
      " [ 0.6625751  -0.46721812  0.03402599  0.56541884 -0.46503513  0.2851508\n",
      "   0.6740651  -0.01844531 -0.67727427]\n",
      " [ 0.69864791 -0.76466717  0.39798761  0.63092704 -0.3629399   0.12779511\n",
      "   0.87808029  0.85234076 -0.38714495]\n",
      " [-1.00944163  1.06140624 -0.53920898 -1.09717852  0.11963514  0.2219804\n",
      "  -0.97710664 -0.74528334  0.74814749]\n",
      " [ 1.15729812 -0.95407776  0.52989207  0.71954276 -0.20921092 -0.27292443\n",
      "   0.80141961  0.85013858 -0.54461625]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.41332531  1.81742147 -2.09253081  0.24803561  0.44916579 -2.67237112\n",
      "   1.20458197]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:64 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.69222891]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 64 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90773815 -1.0752809   0.35009778  0.96520259 -0.32896811 -0.11642473\n",
      "   1.26455752  0.21771921 -0.57682408]\n",
      " [-0.85268489  0.84231253 -0.55609383 -0.97917765  0.23084818 -0.14363408\n",
      "  -0.69005347 -0.38513642  0.76846637]\n",
      " [ 0.66947782 -0.46721812  0.03402599  0.56541884 -0.45813242  0.29205352\n",
      "   0.68096782 -0.01844531 -0.67727427]\n",
      " [ 0.70587434 -0.76466717  0.39798761  0.63092704 -0.35571347  0.13502153\n",
      "   0.88530672  0.85234076 -0.38714495]\n",
      " [-1.01675536  1.06140624 -0.53920898 -1.09717852  0.11232141  0.21466667\n",
      "  -0.98442036 -0.74528334  0.74814749]\n",
      " [ 1.16462586 -0.95407776  0.52989207  0.71954276 -0.20188318 -0.26559669\n",
      "   0.80874735  0.85013858 -0.54461625]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.3805403   1.84513283 -2.08618122  0.27297658  0.47515611 -2.66706667\n",
      "   1.2312708 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:64 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.59235382]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 64 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89256585 -1.0752809   0.35009778  0.95003029 -0.32896811 -0.13159703\n",
      "   1.26455752  0.21771921 -0.59199638]\n",
      " [-0.83738625  0.84231253 -0.55609383 -0.96387902  0.23084818 -0.12833544\n",
      "  -0.69005347 -0.38513642  0.78376501]\n",
      " [ 0.65672886 -0.46721812  0.03402599  0.55266988 -0.45813242  0.27930456\n",
      "   0.68096782 -0.01844531 -0.69002323]\n",
      " [ 0.69122804 -0.76466717  0.39798761  0.61628074 -0.35571347  0.12037524\n",
      "   0.88530672  0.85234076 -0.40179125]\n",
      " [-1.00172695  1.06140624 -0.53920898 -1.08215011  0.11232141  0.22969508\n",
      "  -0.98442036 -0.74528334  0.7631759 ]\n",
      " [ 1.15004753 -0.95407776  0.52989207  0.70496443 -0.20188318 -0.28017502\n",
      "   0.80874735  0.85013858 -0.55919458]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.45205836  1.7904292  -2.10264661  0.22287823  0.42170507 -2.68425136\n",
      "   1.17796492]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:64 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.71090943]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 64 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89895364 -1.06889311  0.35009778  0.95641808 -0.32896811 -0.12520924\n",
      "   1.27094531  0.21771921 -0.59199638]\n",
      " [-0.84391815  0.83578063 -0.55609383 -0.97041092  0.23084818 -0.13486734\n",
      "  -0.69658537 -0.38513642  0.78376501]\n",
      " [ 0.66332292 -0.46062405  0.03402599  0.55926394 -0.45813242  0.28589862\n",
      "   0.68756188 -0.01844531 -0.69002323]\n",
      " [ 0.69787843 -0.75801678  0.39798761  0.62293113 -0.35571347  0.12702563\n",
      "   0.89195711  0.85234076 -0.40179125]\n",
      " [-1.00825899  1.05487419 -0.53920898 -1.08868215  0.11232141  0.22316304\n",
      "  -0.99095241 -0.74528334  0.7631759 ]\n",
      " [ 1.15666618 -0.94745911  0.52989207  0.71158308 -0.20188318 -0.27355637\n",
      "   0.815366    0.85013858 -0.55919458]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.42235182  1.8162716  -2.0983501   0.24800565  0.44620407 -2.67995432\n",
      "   1.20193288]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:64 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.75608729]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 64 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.9036026  -1.06424415  0.35009778  0.96106704 -0.32896811 -0.12520924\n",
      "   1.27559427  0.21771921 -0.59199638]\n",
      " [-0.84892369  0.83077509 -0.55609383 -0.97541645  0.23084818 -0.13486734\n",
      "  -0.70159091 -0.38513642  0.78376501]\n",
      " [ 0.66834179 -0.45560519  0.03402599  0.56428281 -0.45813242  0.28589862\n",
      "   0.69258075 -0.01844531 -0.69002323]\n",
      " [ 0.70289908 -0.75299613  0.39798761  0.62795178 -0.35571347  0.12702563\n",
      "   0.89697776  0.85234076 -0.40179125]\n",
      " [-1.01294005  1.05019313 -0.53920898 -1.09336322  0.11232141  0.22316304\n",
      "  -0.99563347 -0.74528334  0.7631759 ]\n",
      " [ 1.16163939 -0.9424859   0.52989207  0.71655629 -0.20188318 -0.27355637\n",
      "   0.82033921  0.85013858 -0.55919458]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.39986071  1.83621436 -2.09480233  0.26622177  0.46443824 -2.67735031\n",
      "   1.22105478]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:64 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.83020164]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 64 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90578915 -1.06424415  0.35228433  0.96106704 -0.32896811 -0.12302269\n",
      "   1.27778082  0.21771921 -0.59199638]\n",
      " [-0.85125364  0.83077509 -0.55842379 -0.97541645  0.23084818 -0.1371973\n",
      "  -0.70392086 -0.38513642  0.78376501]\n",
      " [ 0.67100398 -0.45560519  0.03668818  0.56428281 -0.45813242  0.28856081\n",
      "   0.69524293 -0.01844531 -0.69002323]\n",
      " [ 0.70532303 -0.75299613  0.40041155  0.62795178 -0.35571347  0.12944957\n",
      "   0.8994017   0.85234076 -0.40179125]\n",
      " [-1.01519814  1.05019313 -0.54146706 -1.09336322  0.11232141  0.22090495\n",
      "  -0.99789155 -0.74528334  0.7631759 ]\n",
      " [ 1.16397201 -0.9424859   0.53222468  0.71655629 -0.20188318 -0.27122376\n",
      "   0.82267182  0.85013858 -0.55919458]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.38789274  1.84719065 -2.09365236  0.27631097  0.47512922 -2.67628391\n",
      "   1.23186949]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:64 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.21225675]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 65 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90486509 -1.06516821  0.35228433  0.96014298 -0.32896811 -0.12302269\n",
      "   1.27778082  0.21771921 -0.59292044]\n",
      " [-0.85032308  0.83170565 -0.55842379 -0.97448589  0.23084818 -0.1371973\n",
      "  -0.70392086 -0.38513642  0.78469557]\n",
      " [ 0.67060703 -0.45600214  0.03668818  0.56388586 -0.45813242  0.28856081\n",
      "   0.69524293 -0.01844531 -0.69042018]\n",
      " [ 0.70453748 -0.75378168  0.40041155  0.62716624 -0.35571347  0.12944957\n",
      "   0.8994017   0.85234076 -0.40257679]\n",
      " [-1.01391671  1.05147456 -0.54146706 -1.09208178  0.11232141  0.22090495\n",
      "  -0.99789155 -0.74528334  0.76445734]\n",
      " [ 1.16235023 -0.94410768  0.53222468  0.71493451 -0.20188318 -0.27122376\n",
      "   0.82267182  0.85013858 -0.56081636]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.40563781  1.8373872  -2.10158734  0.26704095  0.46546696 -2.68385632\n",
      "   1.2213361 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:65 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.83630367]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 65 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90679801 -1.06516821  0.35421725  0.96014298 -0.32896811 -0.12302269\n",
      "   1.27971374  0.21771921 -0.59292044]\n",
      " [-0.85260121  0.83170565 -0.56070191 -0.97448589  0.23084818 -0.1371973\n",
      "  -0.70619899 -0.38513642  0.78469557]\n",
      " [ 0.67309701 -0.45600214  0.03917816  0.56388586 -0.45813242  0.28856081\n",
      "   0.69773291 -0.01844531 -0.69042018]\n",
      " [ 0.7068877  -0.75378168  0.40276177  0.62716624 -0.35571347  0.12944957\n",
      "   0.90175192  0.85234076 -0.40257679]\n",
      " [-1.01583334  1.05147456 -0.5433837  -1.09208178  0.11232141  0.22090495\n",
      "  -0.99980818 -0.74528334  0.76445734]\n",
      " [ 1.1642988  -0.94410768  0.53417325  0.71493451 -0.20188318 -0.27122376\n",
      "   0.82462039  0.85013858 -0.56081636]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.39443281  1.84776925 -2.10037864  0.27603393  0.4753414  -2.68304724\n",
      "   1.23170456]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:65 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.68122581]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 65 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89026667 -1.08169954  0.33768592  0.96014298 -0.32896811 -0.12302269\n",
      "   1.26318241  0.21771921 -0.59292044]\n",
      " [-0.83645831  0.84784854 -0.54455902 -0.97448589  0.23084818 -0.1371973\n",
      "  -0.69005609 -0.38513642  0.78469557]\n",
      " [ 0.65893107 -0.47016807  0.02501223  0.56388586 -0.45813242  0.28856081\n",
      "   0.68356698 -0.01844531 -0.69042018]\n",
      " [ 0.69085325 -0.76981613  0.38672732  0.62716624 -0.35571347  0.12944957\n",
      "   0.88571747  0.85234076 -0.40257679]\n",
      " [-0.99928198  1.06802592 -0.52683233 -1.09208178  0.11232141  0.22090495\n",
      "  -0.98325682 -0.74528334  0.76445734]\n",
      " [ 1.14774714 -0.96065934  0.51762159  0.71493451 -0.20188318 -0.27122376\n",
      "   0.80806873  0.85013858 -0.56081636]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.46839936  1.78756779 -2.11637213  0.222636    0.41774986 -2.69645753\n",
      "   1.17038181]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:65 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.7333795]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 65 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89608183 -1.08169954  0.34350107  0.96014298 -0.32896811 -0.12302269\n",
      "   1.26318241  0.22353436 -0.59292044]\n",
      " [-0.84219918  0.84784854 -0.55029989 -0.97448589  0.23084818 -0.1371973\n",
      "  -0.69005609 -0.39087729  0.78469557]\n",
      " [ 0.66282106 -0.47016807  0.02890222  0.56388586 -0.45813242  0.28856081\n",
      "   0.68356698 -0.01455531 -0.69042018]\n",
      " [ 0.69642176 -0.76981613  0.39229583  0.62716624 -0.35571347  0.12944957\n",
      "   0.88571747  0.85790927 -0.40257679]\n",
      " [-1.00430124  1.06802592 -0.5318516  -1.09208178  0.11232141  0.22090495\n",
      "  -0.98325682 -0.75030261  0.76445734]\n",
      " [ 1.15228375 -0.96065934  0.52215821  0.71493451 -0.20188318 -0.27122376\n",
      "   0.80806873  0.8546752  -0.56081636]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.44233267  1.80866438 -2.11256584  0.2398529   0.44051235 -2.69401978\n",
      "   1.19449927]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:65 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.56376993]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 65 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90772583 -1.08169954  0.34350107  0.97178699 -0.32896811 -0.12302269\n",
      "   1.26318241  0.22353436 -0.58127643]\n",
      " [-0.85292132  0.84784854 -0.55029989 -0.98520803  0.23084818 -0.1371973\n",
      "  -0.69005609 -0.39087729  0.77397344]\n",
      " [ 0.66951962 -0.47016807  0.02890222  0.57058441 -0.45813242  0.28856081\n",
      "   0.68356698 -0.01455531 -0.68372162]\n",
      " [ 0.70648392 -0.76981613  0.39229583  0.6372284  -0.35571347  0.12944957\n",
      "   0.88571747  0.85790927 -0.39251463]\n",
      " [-1.01610615  1.06802592 -0.5318516  -1.10388669  0.11232141  0.22090495\n",
      "  -0.98325682 -0.75030261  0.75265243]\n",
      " [ 1.16403455 -0.96065934  0.52215821  0.72668531 -0.20188318 -0.27122376\n",
      "   0.80806873  0.8546752  -0.54906556]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.3886909   1.85048321 -2.09847422  0.27369808  0.47887953 -2.68281718\n",
      "   1.23671054]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:65 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.12672813]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 65 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90794505 -1.08169954  0.34350107  0.97178699 -0.32874889 -0.12280347\n",
      "   1.26318241  0.22353436 -0.58105721]\n",
      " [-0.85294709  0.84784854 -0.55029989 -0.98520803  0.2308224  -0.13722307\n",
      "  -0.69005609 -0.39087729  0.77394766]\n",
      " [ 0.66983909 -0.47016807  0.02890222  0.57058441 -0.45781295  0.28888027\n",
      "   0.68356698 -0.01455531 -0.68340216]\n",
      " [ 0.70633046 -0.76981613  0.39229583  0.6372284  -0.35586693  0.12929611\n",
      "   0.88571747  0.85790927 -0.39266809]\n",
      " [-1.01622832  1.06802592 -0.5318516  -1.10388669  0.11219924  0.22078279\n",
      "  -0.98325682 -0.75030261  0.75253026]\n",
      " [ 1.1637871  -0.96065934  0.52215821  0.72668531 -0.20213063 -0.27147121\n",
      "   0.80806873  0.8546752  -0.54931301]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.39570328  1.84719681 -2.10200619  0.27051315  0.47521968 -2.68644564\n",
      "   1.23295607]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:65 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.02077032]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 65 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90799023 -1.08165436  0.34350107  0.97178699 -0.32870371 -0.12275829\n",
      "   1.26318241  0.22353436 -0.58101204]\n",
      " [-0.85298511  0.84781052 -0.55029989 -0.98520803  0.23078438 -0.13726109\n",
      "  -0.69005609 -0.39087729  0.77390964]\n",
      " [ 0.66987013 -0.47013703  0.02890222  0.57058441 -0.45778191  0.28891131\n",
      "   0.68356698 -0.01455531 -0.68337112]\n",
      " [ 0.70636262 -0.76978397  0.39229583  0.6372284  -0.35583477  0.12932827\n",
      "   0.88571747  0.85790927 -0.39263593]\n",
      " [-1.01627249  1.06798176 -0.5318516  -1.10388669  0.11215508  0.22073862\n",
      "  -0.98325682 -0.75030261  0.75248609]\n",
      " [ 1.16382386 -0.96062258  0.52215821  0.72668531 -0.20209387 -0.27143445\n",
      "   0.80806873  0.8546752  -0.54927625]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.3959145   1.84714816 -2.10215472  0.27044083  0.47514878 -2.68660557\n",
      "   1.23289148]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:65 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.35024285]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 65 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90223866 -1.08740594  0.34350107  0.97178699 -0.33445529 -0.12850987\n",
      "   1.25743084  0.22353436 -0.58101204]\n",
      " [-0.84750165  0.85329398 -0.55029989 -0.98520803  0.23626784 -0.13177763\n",
      "  -0.68457263 -0.39087729  0.77390964]\n",
      " [ 0.66358836 -0.4764188   0.02890222  0.57058441 -0.46406368  0.28262954\n",
      "   0.67728521 -0.01455531 -0.68337112]\n",
      " [ 0.70092374 -0.77522285  0.39229583  0.6372284  -0.36127366  0.12388938\n",
      "   0.88027859  0.85790927 -0.39263593]\n",
      " [-1.010812    1.07344225 -0.5318516  -1.10388669  0.11761557  0.22619911\n",
      "  -0.97779633 -0.75030261  0.75248609]\n",
      " [ 1.15883561 -0.96561083  0.52215821  0.72668531 -0.20708213 -0.2764227\n",
      "   0.80308048  0.8546752  -0.54927625]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Theta two: \n",
      "[[-1.43576738  1.82107187 -2.11626077  0.24368444  0.44945594 -2.70073946\n",
      "   1.20773288]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:65 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.85599267]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 65 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90335261 -1.08740594  0.34350107  0.97290094 -0.33445529 -0.12850987\n",
      "   1.25854479  0.22353436 -0.58101204]\n",
      " [-0.84904515  0.85329398 -0.55029989 -0.98675153  0.23626784 -0.13177763\n",
      "  -0.68611613 -0.39087729  0.77390964]\n",
      " [ 0.66549232 -0.4764188   0.02890222  0.57248837 -0.46406368  0.28262954\n",
      "   0.67918917 -0.01455531 -0.68337112]\n",
      " [ 0.702666   -0.77522285  0.39229583  0.63897066 -0.36127366  0.12388938\n",
      "   0.88202085  0.85790927 -0.39263593]\n",
      " [-1.01195201  1.07344225 -0.5318516  -1.1050267   0.11761557  0.22619911\n",
      "  -0.97893635 -0.75030261  0.75248609]\n",
      " [ 1.16025783 -0.96561083  0.52215821  0.72810753 -0.20708213 -0.2764227\n",
      "   0.8045027   0.8546752  -0.54927625]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.42689154  1.82957644 -2.11559814  0.25141692  0.45746099 -2.70035409\n",
      "   1.21604378]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:65 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.69441322]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 65 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91055234 -1.08740594  0.34350107  0.97290094 -0.32725556 -0.12131014\n",
      "   1.26574452  0.22353436 -0.58101204]\n",
      " [-0.85626982  0.85329398 -0.55029989 -0.98675153  0.22904318 -0.1390023\n",
      "  -0.6933408  -0.39087729  0.77390964]\n",
      " [ 0.67233393 -0.4764188   0.02890222  0.57248837 -0.45722206  0.28947116\n",
      "   0.68603078 -0.01455531 -0.68337112]\n",
      " [ 0.70981913 -0.77522285  0.39229583  0.63897066 -0.35412053  0.13104251\n",
      "   0.88917398  0.85790927 -0.39263593]\n",
      " [-1.01918397  1.07344225 -0.5318516  -1.1050267   0.1103836   0.21896715\n",
      "  -0.98616831 -0.75030261  0.75248609]\n",
      " [ 1.16750632 -0.96561083  0.52215821  0.72810753 -0.19983363 -0.26917421\n",
      "   0.81175119  0.8546752  -0.54927625]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.39446825  1.85698694 -2.10934063  0.27612095  0.48319497 -2.69511758\n",
      "   1.24246105]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:65 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.59292523]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 65 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89536513 -1.08740594  0.34350107  0.95771374 -0.32725556 -0.13649735\n",
      "   1.26574452  0.22353436 -0.59619924]\n",
      " [-0.84095835  0.85329398 -0.55029989 -0.97144006  0.22904318 -0.12369083\n",
      "  -0.6933408  -0.39087729  0.7892211 ]\n",
      " [ 0.65956594 -0.4764188   0.02890222  0.55972038 -0.45722206  0.27670317\n",
      "   0.68603078 -0.01455531 -0.6961391 ]\n",
      " [ 0.69514941 -0.77522285  0.39229583  0.62430094 -0.35412053  0.1163728\n",
      "   0.88917398  0.85790927 -0.40730565]\n",
      " [-1.00413908  1.07344225 -0.5318516  -1.08998181  0.1103836   0.23401204\n",
      "  -0.98616831 -0.75030261  0.76753099]\n",
      " [ 1.1528998  -0.96561083  0.52215821  0.71350101 -0.19983363 -0.28378074\n",
      "   0.81175119  0.8546752  -0.56388278]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.46602392  1.80223604 -2.12580094  0.22597756  0.42968182 -2.71228992\n",
      "   1.18908361]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:65 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.71177724]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 65 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90172961 -1.08104146  0.34350107  0.96407822 -0.32725556 -0.13013286\n",
      "   1.272109    0.22353436 -0.59619924]\n",
      " [-0.84746018  0.84679215 -0.55029989 -0.97794189  0.22904318 -0.13019266\n",
      "  -0.69984263 -0.39087729  0.7892211 ]\n",
      " [ 0.6661263  -0.46985844  0.02890222  0.56628074 -0.45722206  0.28326353\n",
      "   0.69259114 -0.01455531 -0.6961391 ]\n",
      " [ 0.70176797 -0.76860429  0.39229583  0.6309195  -0.35412053  0.12299136\n",
      "   0.89579253  0.85790927 -0.40730565]\n",
      " [-1.01064405  1.06693727 -0.5318516  -1.09648678  0.1103836   0.22750707\n",
      "  -0.99267328 -0.75030261  0.76753099]\n",
      " [ 1.15948638 -0.95902425  0.52215821  0.72008759 -0.19983363 -0.27719415\n",
      "   0.81833778  0.8546752  -0.56388278]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.43645941  1.82793692 -2.12152056  0.25099696  0.45406903 -2.70799744\n",
      "   1.21293354]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:65 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.75779318]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 65 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90632484 -1.07644623  0.34350107  0.96867344 -0.32725556 -0.13013286\n",
      "   1.27670422  0.22353436 -0.59619924]\n",
      " [-0.85240562  0.84184671 -0.55029989 -0.98288733  0.22904318 -0.13019266\n",
      "  -0.70478807 -0.39087729  0.7892211 ]\n",
      " [ 0.67108827 -0.46489647  0.02890222  0.57124271 -0.45722206  0.28326353\n",
      "   0.6975531  -0.01455531 -0.6961391 ]\n",
      " [ 0.70673144 -0.76364083  0.39229583  0.63588297 -0.35412053  0.12299136\n",
      "   0.900756    0.85790927 -0.40730565]\n",
      " [-1.01527047  1.06231085 -0.5318516  -1.1011132   0.1103836   0.22750707\n",
      "  -0.99729971 -0.75030261  0.76753099]\n",
      " [ 1.16439966 -0.95411098  0.52215821  0.72500087 -0.19983363 -0.27719415\n",
      "   0.82325105  0.8546752  -0.56388278]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.41423177  1.84764485 -2.11802414  0.26901887  0.47210706 -2.70542359\n",
      "   1.23183916]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:65 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.83106618]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 65 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90849839 -1.07644623  0.34567462  0.96867344 -0.32725556 -0.12795931\n",
      "   1.27887778  0.22353436 -0.59619924]\n",
      " [-0.85471747  0.84184671 -0.55261174 -0.98288733  0.22904318 -0.13250451\n",
      "  -0.70709992 -0.39087729  0.7892211 ]\n",
      " [ 0.67372617 -0.46489647  0.03154012  0.57124271 -0.45722206  0.28590143\n",
      "   0.700191   -0.01455531 -0.6961391 ]\n",
      " [ 0.70913486 -0.76364083  0.39469926  0.63588297 -0.35412053  0.12539479\n",
      "   0.90315943  0.85790927 -0.40730565]\n",
      " [-1.01751469  1.06231085 -0.53409582 -1.1011132   0.1103836   0.22526285\n",
      "  -0.99954392 -0.75030261  0.76753099]\n",
      " [ 1.16671572 -0.95411098  0.52447427  0.72500087 -0.19983363 -0.27487809\n",
      "   0.82556711  0.8546752  -0.56388278]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.40237302  1.85851401 -2.11688076  0.27901593  0.48269807 -2.70435949\n",
      "   1.24254928]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:65 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.20709508]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 66 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90763802 -1.0773066   0.34567462  0.96781307 -0.32725556 -0.12795931\n",
      "   1.27887778  0.22353436 -0.59705962]\n",
      " [-0.85384882  0.84271536 -0.55261174 -0.98201868  0.22904318 -0.13250451\n",
      "  -0.70709992 -0.39087729  0.79008976]\n",
      " [ 0.67337001 -0.46525263  0.03154012  0.57088655 -0.45722206  0.28590143\n",
      "   0.700191   -0.01455531 -0.69649526]\n",
      " [ 0.7084005  -0.76437519  0.39469926  0.6351486  -0.35412053  0.12539479\n",
      "   0.90315943  0.85790927 -0.40804001]\n",
      " [-1.01631236  1.06351318 -0.53409582 -1.09991087  0.1103836   0.22526285\n",
      "  -0.99954392 -0.75030261  0.76873332]\n",
      " [ 1.1651813  -0.95564539  0.52447427  0.72346645 -0.19983363 -0.27487809\n",
      "   0.82556711  0.8546752  -0.56541719]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.41937622  1.84914603 -2.12450752  0.27015776  0.47345839 -2.71164198\n",
      "   1.23247729]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:66 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.83791211]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 66 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90953917 -1.0773066   0.34757577  0.96781307 -0.32725556 -0.12795931\n",
      "   1.28077893  0.22353436 -0.59705962]\n",
      " [-0.85608607  0.84271536 -0.55484899 -0.98201868  0.22904318 -0.13250451\n",
      "  -0.70933717 -0.39087729  0.79008976]\n",
      " [ 0.67581666 -0.46525263  0.03398677  0.57088655 -0.45722206  0.28590143\n",
      "   0.70263766 -0.01455531 -0.69649526]\n",
      " [ 0.71070803 -0.76437519  0.39700679  0.6351486  -0.35412053  0.12539479\n",
      "   0.90546696  0.85790927 -0.40804001]\n",
      " [-1.01819803  1.06351318 -0.53598148 -1.09991087  0.1103836   0.22526285\n",
      "  -1.00142959 -0.75030261  0.76873332]\n",
      " [ 1.1670972  -0.95564539  0.52639017  0.72346645 -0.19983363 -0.27487809\n",
      "   0.82748301  0.8546752  -0.56541719]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.4083692   1.85934258 -2.12332112  0.2789963   0.48316056 -2.71084474\n",
      "   1.242661  ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:66 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.67797642]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 66 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89301204 -1.09383373  0.33104864  0.96781307 -0.32725556 -0.12795931\n",
      "   1.26425179  0.22353436 -0.59705962]\n",
      " [-0.83996915  0.85883228 -0.53873207 -0.98201868  0.22904318 -0.13250451\n",
      "  -0.69322025 -0.39087729  0.79008976]\n",
      " [ 0.6617013  -0.47936799  0.01987141  0.57088655 -0.45722206  0.28590143\n",
      "   0.68852229 -0.01455531 -0.69649526]\n",
      " [ 0.6946985  -0.78038472  0.38099726  0.6351486  -0.35412053  0.12539479\n",
      "   0.88945743  0.85790927 -0.40804001]\n",
      " [-1.00164518  1.08006603 -0.51942864 -1.09991087  0.1103836   0.22526285\n",
      "  -0.98487674 -0.75030261  0.76873332]\n",
      " [ 1.1505314  -0.97221119  0.50982437  0.72346645 -0.19983363 -0.27487809\n",
      "   0.81091721  0.8546752  -0.56541719]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.4823786   1.79927489 -2.13945268  0.22566888  0.42564836 -2.72443341\n",
      "   1.18144664]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:66 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.73538859]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 66 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89875637 -1.09383373  0.33679297  0.96781307 -0.32725556 -0.12795931\n",
      "   1.26425179  0.22927869 -0.59705962]\n",
      " [-0.84563655  0.85883228 -0.54439946 -0.98201868  0.22904318 -0.13250451\n",
      "  -0.69322025 -0.39654468  0.79008976]\n",
      " [ 0.66555028 -0.47936799  0.02372038  0.57088655 -0.45722206  0.28590143\n",
      "   0.68852229 -0.01070634 -0.69649526]\n",
      " [ 0.70019379 -0.78038472  0.38649255  0.6351486  -0.35412053  0.12539479\n",
      "   0.88945743  0.86340456 -0.40804001]\n",
      " [-1.00660267  1.08006603 -0.52438613 -1.09991087  0.1103836   0.22526285\n",
      "  -0.98487674 -0.7552601   0.76873332]\n",
      " [ 1.15501312 -0.97221119  0.51430609  0.72346645 -0.19983363 -0.27487809\n",
      "   0.81091721  0.85915691 -0.56541719]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.45663294  1.8201193  -2.13570424  0.24268251  0.448141   -2.72202564\n",
      "   1.20526624]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:66 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.5655914]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 66 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91035715 -1.09383373  0.33679297  0.97941385 -0.32725556 -0.12795931\n",
      "   1.26425179  0.22927869 -0.58545884]\n",
      " [-0.85633291  0.85883228 -0.54439946 -0.99271504  0.22904318 -0.13250451\n",
      "  -0.69322025 -0.39654468  0.7793934 ]\n",
      " [ 0.67225345 -0.47936799  0.02372038  0.57758972 -0.45722206  0.28590143\n",
      "   0.68852229 -0.01070634 -0.68979209]\n",
      " [ 0.71024538 -0.78038472  0.38649255  0.64520019 -0.35412053  0.12539479\n",
      "   0.88945743  0.86340456 -0.39798843]\n",
      " [-1.01835844  1.08006603 -0.52438613 -1.11166664  0.1103836   0.22526285\n",
      "  -0.98487674 -0.7552601   0.75697755]\n",
      " [ 1.16671833 -0.97221119  0.51430609  0.73517166 -0.19983363 -0.27487809\n",
      "   0.81091721  0.85915691 -0.55371198]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.40326633  1.86178041 -2.12174247  0.27639947  0.48637966 -2.71093204\n",
      "   1.24732047]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:66 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.12295471]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 66 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91057222 -1.09383373  0.33679297  0.97941385 -0.32704049 -0.12774424\n",
      "   1.26425179  0.22927869 -0.58524377]\n",
      " [-0.85636539  0.85883228 -0.54439946 -0.99271504  0.2290107  -0.13253699\n",
      "  -0.69322025 -0.39654468  0.77936092]\n",
      " [ 0.67256369 -0.47936799  0.02372038  0.57758972 -0.45691183  0.28621167\n",
      "   0.68852229 -0.01070634 -0.68948185]\n",
      " [ 0.71010718 -0.78038472  0.38649255  0.64520019 -0.35425873  0.12525658\n",
      "   0.88945743  0.86340456 -0.39812663]\n",
      " [-1.01848135  1.08006603 -0.52438613 -1.11166664  0.11026069  0.22513994\n",
      "  -0.98487674 -0.7552601   0.75685464]\n",
      " [ 1.16649022 -0.97221119  0.51430609  0.73517166 -0.20006175 -0.2751062\n",
      "   0.81091721  0.85915691 -0.55394009]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.40989585  1.85868133 -2.12508971  0.2733968   0.48292653 -2.71436983\n",
      "   1.24377687]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:66 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.01940798]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 66 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.9106119  -1.09379405  0.33679297  0.97941385 -0.32700081 -0.12770456\n",
      "   1.26425179  0.22927869 -0.58520408]\n",
      " [-0.85639902  0.85879864 -0.54439946 -0.99271504  0.22897706 -0.13257062\n",
      "  -0.69322025 -0.39654468  0.77932728]\n",
      " [ 0.67259129 -0.47934038  0.02372038  0.57758972 -0.45688422  0.28623928\n",
      "   0.68852229 -0.01070634 -0.68945424]\n",
      " [ 0.71013576 -0.78035614  0.38649255  0.64520019 -0.35423015  0.12528516\n",
      "   0.88945743  0.86340456 -0.39809805]\n",
      " [-1.0185202   1.08002718 -0.52438613 -1.11166664  0.11022184  0.22510109\n",
      "  -0.98487674 -0.7552601   0.75681579]\n",
      " [ 1.16652275 -0.97217865  0.51430609  0.73517166 -0.20002921 -0.27507366\n",
      "   0.81091721  0.85915691 -0.55390755]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.41008053  1.85863934 -2.12522019  0.27333416  0.48286515 -2.71451022\n",
      "   1.24372098]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:66 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.34366513]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 66 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.9051011  -1.09930485  0.33679297  0.97941385 -0.33251161 -0.13321536\n",
      "   1.25874099  0.22927869 -0.58520408]\n",
      " [-0.85111962  0.86407805 -0.54439946 -0.99271504  0.23425647 -0.12729122\n",
      "  -0.68794084 -0.39654468  0.77932728]\n",
      " [ 0.66650339 -0.48542829  0.02372038  0.57758972 -0.46297212  0.28015137\n",
      "   0.68243439 -0.01070634 -0.68945424]\n",
      " [ 0.70488674 -0.78560516  0.38649255  0.64520019 -0.35947917  0.12003614\n",
      "   0.88420841  0.86340456 -0.39809805]\n",
      " [-1.01328778  1.0852596  -0.52438613 -1.11166664  0.11545426  0.23033351\n",
      "  -0.97964432 -0.7552601   0.75681579]\n",
      " [ 1.16173043 -0.97697098  0.51430609  0.73517166 -0.20482154 -0.27986599\n",
      "   0.80612489  0.85915691 -0.55390755]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.44883898  1.83338183 -2.13900357  0.24734078  0.45792671 -2.72835021\n",
      "   1.21932175]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:66 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.85892039]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 66 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90616633 -1.09930485  0.33679297  0.98047909 -0.33251161 -0.13321536\n",
      "   1.25980622  0.22927869 -0.58520408]\n",
      " [-0.85259628  0.86407805 -0.54439946 -0.9941917   0.23425647 -0.12729122\n",
      "  -0.6894175  -0.39654468  0.77932728]\n",
      " [ 0.66833082 -0.48542829  0.02372038  0.57941715 -0.46297212  0.28015137\n",
      "   0.68426182 -0.01070634 -0.68945424]\n",
      " [ 0.70655515 -0.78560516  0.38649255  0.6468686  -0.35947917  0.12003614\n",
      "   0.88587681  0.86340456 -0.39809805]\n",
      " [-1.01437782  1.0852596  -0.52438613 -1.11275669  0.11545426  0.23033351\n",
      "  -0.98073437 -0.7552601   0.75681579]\n",
      " [ 1.16309018 -0.97697098  0.51430609  0.73653142 -0.20482154 -0.27986599\n",
      "   0.80748464  0.85915691 -0.55390755]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.44029124  1.84157606 -2.13837394  0.25480181  0.46564778 -2.72798336\n",
      "   1.22733274]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:66 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.69660611]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 66 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91328455 -1.09930485  0.33679297  0.98047909 -0.3253934  -0.12609715\n",
      "   1.26692444  0.22927869 -0.58520408]\n",
      " [-0.85974272  0.86407805 -0.54439946 -0.9941917   0.22711003 -0.13443766\n",
      "  -0.69656394 -0.39654468  0.77932728]\n",
      " [ 0.67511057 -0.48542829  0.02372038  0.57941715 -0.45619237  0.28693113\n",
      "   0.69104157 -0.01070634 -0.68945424]\n",
      " [ 0.71363438 -0.78560516  0.38649255  0.6468686  -0.35239993  0.12711538\n",
      "   0.89295605  0.86340456 -0.39809805]\n",
      " [-1.02152774  1.0852596  -0.52438613 -1.11275669  0.10830434  0.22318359\n",
      "  -0.98788429 -0.7552601   0.75681579]\n",
      " [ 1.17025904 -0.97697098  0.51430609  0.73653142 -0.19765268 -0.27269714\n",
      "   0.8146535   0.85915691 -0.55390755]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.40823069  1.86868539 -2.13220827  0.27926733  0.49112366 -2.72281519\n",
      "   1.25347684]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:66 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.59345261]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 66 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89808364 -1.09930485  0.33679297  0.96527818 -0.3253934  -0.14129806\n",
      "   1.26692444  0.22927869 -0.60040499]\n",
      " [-0.84441978  0.86407805 -0.54439946 -0.97886876  0.22711003 -0.11911471\n",
      "  -0.69656394 -0.39654468  0.79465022]\n",
      " [ 0.66232575 -0.48542829  0.02372038  0.56663232 -0.45619237  0.2741463\n",
      "   0.69104157 -0.01070634 -0.70223907]\n",
      " [ 0.69894317 -0.78560516  0.38649255  0.63217739 -0.35239993  0.11242417\n",
      "   0.89295605  0.86340456 -0.41278925]\n",
      " [-1.00646762  1.0852596  -0.52438613 -1.09769657  0.10830434  0.23824371\n",
      "  -0.98788429 -0.7552601   0.77187591]\n",
      " [ 1.15562627 -0.97697098  0.51430609  0.72189865 -0.19765268 -0.28732991\n",
      "   0.8146535   0.85915691 -0.56854032]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.47982084  1.81389099 -2.14866471  0.22908353  0.43755337 -2.73997587\n",
      "   1.20003271]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:66 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.71261425]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 66 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90442561 -1.09296289  0.33679297  0.97162014 -0.3253934  -0.13495609\n",
      "   1.27326641  0.22927869 -0.60040499]\n",
      " [-0.85089269  0.85760514 -0.54439946 -0.98534167  0.22711003 -0.12558762\n",
      "  -0.70303685 -0.39654468  0.79465022]\n",
      " [ 0.66885355 -0.47890049  0.02372038  0.57316012 -0.45619237  0.2806741\n",
      "   0.69756937 -0.01070634 -0.70223907]\n",
      " [ 0.70553105 -0.77901729  0.38649255  0.63876527 -0.35239993  0.11901205\n",
      "   0.89954392  0.86340456 -0.41278925]\n",
      " [-1.01294649  1.07878074 -0.52438613 -1.10417543  0.10830434  0.23176485\n",
      "  -0.99436315 -0.7552601   0.77187591]\n",
      " [ 1.16218185 -0.9704154   0.51430609  0.72845423 -0.19765268 -0.28077433\n",
      "   0.82120908  0.85915691 -0.56854032]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.45039323  1.83945532 -2.1443996   0.253999    0.46183241 -2.73568758\n",
      "   1.22376815]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:66 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.75946906]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 66 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90896817 -1.08842032  0.33679297  0.97616271 -0.3253934  -0.13495609\n",
      "   1.27780897  0.22927869 -0.60040499]\n",
      " [-0.85577929  0.85271854 -0.54439946 -0.99022828  0.22711003 -0.12558762\n",
      "  -0.70792346 -0.39654468  0.79465022]\n",
      " [ 0.67375963 -0.4739944   0.02372038  0.57806621 -0.45619237  0.2806741\n",
      "   0.70247546 -0.01070634 -0.70223907]\n",
      " [ 0.71043835 -0.77410999  0.38649255  0.64367256 -0.35239993  0.11901205\n",
      "   0.90445122  0.86340456 -0.41278925]\n",
      " [-1.01751941  1.07420782 -0.52438613 -1.10874835  0.10830434  0.23176485\n",
      "  -0.99893607 -0.7552601   0.77187591]\n",
      " [ 1.16703652 -0.96556073  0.51430609  0.7333089  -0.19765268 -0.28077433\n",
      "   0.82606375  0.85915691 -0.56854032]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.42842364  1.85893329 -2.14095313  0.27183034  0.47967764 -2.73314325\n",
      "   1.24246138]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:66 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.83187914]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 66 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91112992 -1.08842032  0.33895472  0.97616271 -0.3253934  -0.13279434\n",
      "   1.27997072  0.22927869 -0.60040499]\n",
      " [-0.85807448  0.85271854 -0.54669465 -0.99022828  0.22711003 -0.12788281\n",
      "  -0.71021865 -0.39654468  0.79465022]\n",
      " [ 0.6763748  -0.4739944   0.02633555  0.57806621 -0.45619237  0.28328927\n",
      "   0.70509063 -0.01070634 -0.70223907]\n",
      " [ 0.71282281 -0.77410999  0.38887702  0.64367256 -0.35239993  0.12139651\n",
      "   0.90683569  0.86340456 -0.41278925]\n",
      " [-1.01975102  1.07420782 -0.52661774 -1.10874835  0.10830434  0.22953324\n",
      "  -1.00116768 -0.7552601   0.77187591]\n",
      " [ 1.1693375  -0.96556073  0.51660707  0.7333089  -0.19765268 -0.27847334\n",
      "   0.82836473  0.85915691 -0.56854032]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.41666727  1.8697016  -2.13981551  0.28174082  0.49017455 -2.73208078\n",
      "   1.25307287]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:66 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.20203109]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 67 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91032981 -1.08922044  0.33895472  0.97536259 -0.3253934  -0.13279434\n",
      "   1.27997072  0.22927869 -0.60120511]\n",
      " [-0.85726462  0.85352839 -0.54669465 -0.98941842  0.22711003 -0.12788281\n",
      "  -0.71021865 -0.39654468  0.79546008]\n",
      " [ 0.67605688 -0.47431232  0.02633555  0.57774829 -0.45619237  0.28328927\n",
      "   0.70509063 -0.01070634 -0.70255699]\n",
      " [ 0.71213728 -0.77479552  0.38887702  0.64298703 -0.35239993  0.12139651\n",
      "   0.90683569  0.86340456 -0.41347479]\n",
      " [-1.01862384  1.07533499 -0.52661774 -1.10762118  0.10830434  0.22953324\n",
      "  -1.00116768 -0.7552601   0.77300308]\n",
      " [ 1.16788686 -0.96701137  0.51660707  0.73185826 -0.19765268 -0.27847334\n",
      "   0.82836473  0.85915691 -0.56999096]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.43295244  1.86075364 -2.14714278  0.27327999  0.48134314 -2.73908115\n",
      "   1.24344658]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:67 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.83945933]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 67 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91220079 -1.08922044  0.34082571  0.97536259 -0.3253934  -0.13279434\n",
      "   1.28184171  0.22927869 -0.60120511]\n",
      " [-0.85946299  0.85352839 -0.54889303 -0.98941842  0.22711003 -0.12788281\n",
      "  -0.71241702 -0.39654468  0.79546008]\n",
      " [ 0.67846208 -0.47431232  0.02874075  0.57774829 -0.45619237  0.28328927\n",
      "   0.70749582 -0.01070634 -0.70255699]\n",
      " [ 0.71440419 -0.77479552  0.39114393  0.64298703 -0.35239993  0.12139651\n",
      "   0.9091026   0.86340456 -0.41347479]\n",
      " [-1.02048014  1.07533499 -0.52847403 -1.10762118  0.10830434  0.22953324\n",
      "  -1.00302397 -0.7552601   0.77300308]\n",
      " [ 1.16977184 -0.96701137  0.51849205  0.73185826 -0.19765268 -0.27847334\n",
      "   0.83024971  0.85915691 -0.56999096]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.42213462  1.87077275 -2.14597741  0.28197069  0.49088033 -2.73829504\n",
      "   1.25345348]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:67 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.67458607]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 67 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89568287 -1.10573836  0.32430779  0.97536259 -0.3253934  -0.13279434\n",
      "   1.26532379  0.22927869 -0.60120511]\n",
      " [-0.84337601  0.86961538 -0.53280604 -0.98941842  0.22711003 -0.12788281\n",
      "  -0.69633004 -0.39654468  0.79546008]\n",
      " [ 0.66439982 -0.48837458  0.01467849  0.57774829 -0.45619237  0.28328927\n",
      "   0.69343357 -0.01070634 -0.70255699]\n",
      " [ 0.69842346 -0.79077625  0.3751632   0.64298703 -0.35239993  0.12139651\n",
      "   0.89312187  0.86340456 -0.41347479]\n",
      " [-1.00393095  1.09188418 -0.51192484 -1.10762118  0.10830434  0.22953324\n",
      "  -0.98647479 -0.7552601   0.77300308]\n",
      " [ 1.15319622 -0.98358699  0.50191643  0.73185826 -0.19765268 -0.27847334\n",
      "   0.81367409  0.85915691 -0.56999096]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.49617709  1.81084813 -2.16224671  0.228721    0.43345752 -2.75206212\n",
      "   1.19235878]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:67 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.7373347]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 67 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90135867 -1.10573836  0.32998359  0.97536259 -0.3253934  -0.13279434\n",
      "   1.26532379  0.23495449 -0.60120511]\n",
      " [-0.84897258  0.86961538 -0.53840261 -0.98941842  0.22711003 -0.12788281\n",
      "  -0.69633004 -0.40214125  0.79546008]\n",
      " [ 0.66820849 -0.48837458  0.01848716  0.57774829 -0.45619237  0.28328927\n",
      "   0.69343357 -0.00689767 -0.70255699]\n",
      " [ 0.7038483  -0.79077625  0.38058804  0.64298703 -0.35239993  0.12139651\n",
      "   0.89312187  0.8688294  -0.41347479]\n",
      " [-1.00882919  1.09188418 -0.51682309 -1.10762118  0.10830434  0.22953324\n",
      "  -0.98647479 -0.76015834  0.77300308]\n",
      " [ 1.15762547 -0.98358699  0.50634568  0.73185826 -0.19765268 -0.27847334\n",
      "   0.81367409  0.86358616 -0.56999096]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.4707416   1.83144797 -2.15855339  0.24553738  0.45568868 -2.7496828\n",
      "   1.21589007]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:67 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.56739133]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 67 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91291564 -1.10573836  0.32998359  0.98691955 -0.3253934  -0.13279434\n",
      "   1.26532379  0.23495449 -0.58964814]\n",
      " [-0.85964189  0.86961538 -0.53840261 -1.00008773  0.22711003 -0.12788281\n",
      "  -0.69633004 -0.40214125  0.78479077]\n",
      " [ 0.67491396 -0.48837458  0.01848716  0.58445376 -0.45619237  0.28328927\n",
      "   0.69343357 -0.00689767 -0.69585151]\n",
      " [ 0.71388737 -0.79077625  0.38058804  0.6530261  -0.35239993  0.12139651\n",
      "   0.89312187  0.8688294  -0.40343571]\n",
      " [-1.0205355   1.09188418 -0.51682309 -1.11932749  0.10830434  0.22953324\n",
      "  -0.98647479 -0.76015834  0.76129677]\n",
      " [ 1.16928451 -0.98358699  0.50634568  0.7435173  -0.19765268 -0.27847334\n",
      "   0.81367409  0.86358616 -0.55833192]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.41764788  1.87295057 -2.14471797  0.27912462  0.49379705 -2.73869538\n",
      "   1.25778618]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:67 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.11932786]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 67 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91312616 -1.10573836  0.32998359  0.98691955 -0.32518287 -0.13258381\n",
      "   1.26532379  0.23495449 -0.58943762]\n",
      " [-0.85968009  0.86961538 -0.53840261 -1.00008773  0.22707183 -0.12792101\n",
      "  -0.69633004 -0.40214125  0.78475257]\n",
      " [ 0.67521497 -0.48837458  0.01848716  0.58445376 -0.45589136  0.28359028\n",
      "   0.69343357 -0.00689767 -0.6955505 ]\n",
      " [ 0.71376303 -0.79077625  0.38058804  0.6530261  -0.35252427  0.12127217\n",
      "   0.89312187  0.8688294  -0.40356005]\n",
      " [-1.02065852  1.09188418 -0.51682309 -1.11932749  0.10818133  0.22941022\n",
      "  -0.98647479 -0.76015834  0.76117376]\n",
      " [ 1.16907413 -0.98358699  0.50634568  0.7435173  -0.19786307 -0.27868373\n",
      "   0.81367409  0.86358616 -0.5585423 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.42391789  1.87002673 -2.14789118  0.27629251  0.49053757 -2.74195353\n",
      "   1.25444015]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:67 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.01814716]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 67 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91316106 -1.10570346  0.32998359  0.98691955 -0.32514798 -0.13254892\n",
      "   1.26532379  0.23495449 -0.58940272]\n",
      " [-0.85970986  0.8695856  -0.53840261 -1.00008773  0.22704205 -0.12795079\n",
      "  -0.69633004 -0.40214125  0.7847228 ]\n",
      " [ 0.67523954 -0.48835001  0.01848716  0.58445376 -0.45586679  0.28361484\n",
      "   0.69343357 -0.00689767 -0.69552594]\n",
      " [ 0.71378845 -0.79075084  0.38058804  0.6530261  -0.35249886  0.12129758\n",
      "   0.89312187  0.8688294  -0.40353464]\n",
      " [-1.02069271  1.09184999 -0.51682309 -1.11932749  0.10814714  0.22937603\n",
      "  -0.98647479 -0.76015834  0.76113957]\n",
      " [ 1.16910295 -0.98355817  0.50634568  0.7435173  -0.19783425 -0.27865491\n",
      "   0.81367409  0.86358616 -0.55851348]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.42407956  1.86999044 -2.14800592  0.27623819  0.49048435 -2.74207691\n",
      "   1.25439173]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:67 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.33722588]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 67 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90788138 -1.11098314  0.32998359  0.98691955 -0.33042765 -0.13782859\n",
      "   1.26004411  0.23495449 -0.58940272]\n",
      " [-0.85462718  0.87466829 -0.53840261 -1.00008773  0.23212473 -0.12286811\n",
      "  -0.69124736 -0.40214125  0.7847228 ]\n",
      " [ 0.66933943 -0.49425012  0.01848716  0.58445376 -0.4617669   0.27771474\n",
      "   0.68753346 -0.00689767 -0.69552594]\n",
      " [ 0.7087231  -0.79581618  0.38058804  0.6530261  -0.3575642   0.11623224\n",
      "   0.88805652  0.8688294  -0.40353464]\n",
      " [-1.01567925  1.09686344 -0.51682309 -1.11932749  0.11316059  0.23438949\n",
      "  -0.98146133 -0.76015834  0.76113957]\n",
      " [ 1.16449975 -0.98816136  0.50634568  0.7435173  -0.20243744 -0.2832581\n",
      "   0.8090709   0.86358616 -0.55851348]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.46176533  1.84552862 -2.16146878  0.25098967  0.46628227 -2.75562269\n",
      "   1.23073314]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:67 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.86175951]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 67 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90890044 -1.11098314  0.32998359  0.98793862 -0.33042765 -0.13782859\n",
      "   1.26106317  0.23495449 -0.58940272]\n",
      " [-0.85604045  0.87466829 -0.53840261 -1.00150099  0.23212473 -0.12286811\n",
      "  -0.69266062 -0.40214125  0.7847228 ]\n",
      " [ 0.67109386 -0.49425012  0.01848716  0.58620819 -0.4617669   0.27771474\n",
      "   0.68928789 -0.00689767 -0.69552594]\n",
      " [ 0.71032133 -0.79581618  0.38058804  0.65462433 -0.3575642   0.11623224\n",
      "   0.88965475  0.8688294  -0.40353464]\n",
      " [-1.01672196  1.09686344 -0.51682309 -1.1203702   0.11316059  0.23438949\n",
      "  -0.98250404 -0.76015834  0.76113957]\n",
      " [ 1.16580036 -0.98816136  0.50634568  0.74481791 -0.20243744 -0.2832581\n",
      "   0.8103715   0.86358616 -0.55851348]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.45353103  1.85342613 -2.16087016  0.25819055  0.47373129 -2.75527328\n",
      "   1.23845714]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:67 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.69880677]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 67 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91593687 -1.11098314  0.32998359  0.98793862 -0.32339122 -0.13079217\n",
      "   1.2680996   0.23495449 -0.58940272]\n",
      " [-0.86310832  0.87466829 -0.53840261 -1.00150099  0.22505687 -0.12993598\n",
      "  -0.69972849 -0.40214125  0.7847228 ]\n",
      " [ 0.67781103 -0.49425012  0.01848716  0.58620819 -0.45504973  0.28443191\n",
      "   0.69600506 -0.00689767 -0.69552594]\n",
      " [ 0.71732612 -0.79581618  0.38058804  0.65462433 -0.35055942  0.12323703\n",
      "   0.89665954  0.8688294  -0.40353464]\n",
      " [-1.0237896   1.09686344 -0.51682309 -1.1203702   0.10609296  0.22732185\n",
      "  -0.98957168 -0.76015834  0.76113957]\n",
      " [ 1.17288922 -0.98816136  0.50634568  0.74481791 -0.19534857 -0.27616923\n",
      "   0.81746037  0.86358616 -0.55851348]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.42183407  1.88023407 -2.15479603  0.28241612  0.49894746 -2.7501738\n",
      "   1.2643266 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:67 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.59393415]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 67 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90072349 -1.11098314  0.32998359  0.97272524 -0.32339122 -0.14600554\n",
      "   1.2680996   0.23495449 -0.6046161 ]\n",
      " [-0.84777526  0.87466829 -0.53840261 -0.98616794  0.22505687 -0.11460292\n",
      "  -0.69972849 -0.40214125  0.80005585]\n",
      " [ 0.66501153 -0.49425012  0.01848716  0.57340869 -0.45504973  0.27163241\n",
      "   0.69600506 -0.00689767 -0.70832543]\n",
      " [ 0.70261532 -0.79581618  0.38058804  0.63991354 -0.35055942  0.10852623\n",
      "   0.89665954  0.8688294  -0.41824543]\n",
      " [-1.00871555  1.09686344 -0.51682309 -1.10529615  0.10609296  0.2423959\n",
      "  -0.98957168 -0.76015834  0.77621362]\n",
      " [ 1.15823214 -0.98816136  0.50634568  0.73016082 -0.19534857 -0.29082632\n",
      "   0.81746037  0.86358616 -0.57317057]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.49345552  1.82540005 -2.1712498   0.23219657  0.445325   -2.76732353\n",
      "   1.21082066]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:67 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.71342245]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 67 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90704369 -1.10466294  0.32998359  0.97904544 -0.32339122 -0.13968535\n",
      "   1.27441979  0.23495449 -0.6046161 ]\n",
      " [-0.85422032  0.86822322 -0.53840261 -0.992613    0.22505687 -0.12104798\n",
      "  -0.70617355 -0.40214125  0.80005585]\n",
      " [ 0.67150784 -0.48775381  0.01848716  0.579905   -0.45504973  0.27812872\n",
      "   0.70250137 -0.00689767 -0.70832543]\n",
      " [ 0.70917359 -0.78925791  0.38058804  0.6464718  -0.35055942  0.1150845\n",
      "   0.9032178   0.8688294  -0.41824543]\n",
      " [-1.01516919  1.0904098  -0.51682309 -1.11174979  0.10609296  0.23594226\n",
      "  -0.99602532 -0.76015834  0.77621362]\n",
      " [ 1.16475771 -0.98163579  0.50634568  0.7366864  -0.19534857 -0.28430074\n",
      "   0.82398594  0.86358616 -0.57317057]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.46416     1.85083248 -2.16699919  0.25701194  0.4694993  -2.76303908\n",
      "   1.23444493]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:67 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.761117]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 67 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91153459 -1.10017204  0.32998359  0.98353634 -0.32339122 -0.13968535\n",
      "   1.2789107   0.23495449 -0.6046161 ]\n",
      " [-0.85904927  0.86339428 -0.53840261 -0.99744194  0.22505687 -0.12104798\n",
      "  -0.71100249 -0.40214125  0.80005585]\n",
      " [ 0.67635901 -0.48290264  0.01848716  0.58475617 -0.45504973  0.27812872\n",
      "   0.70735253 -0.00689767 -0.70832543]\n",
      " [ 0.71402568 -0.78440582  0.38058804  0.6513239  -0.35055942  0.1150845\n",
      "   0.9080699   0.8688294  -0.41824543]\n",
      " [-1.01968967  1.08588933 -0.51682309 -1.11627026  0.10609296  0.23594226\n",
      "  -1.00054579 -0.76015834  0.77621362]\n",
      " [ 1.16955502 -0.97683848  0.50634568  0.74148371 -0.19534857 -0.28430074\n",
      "   0.82878325  0.86358616 -0.57317057]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.4424434   1.87008505 -2.16360134  0.2746561   0.48715484 -2.76052366\n",
      "   1.25292941]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:67 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.83264274]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 67 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91368568 -1.10017204  0.33213468  0.98353634 -0.32339122 -0.13753426\n",
      "   1.28106178  0.23495449 -0.6046161 ]\n",
      " [-0.86132915  0.86339428 -0.5406825  -0.99744194  0.22505687 -0.12332787\n",
      "  -0.71328238 -0.40214125  0.80005585]\n",
      " [ 0.67895292 -0.48290264  0.02108108  0.58475617 -0.45504973  0.28072264\n",
      "   0.70994645 -0.00689767 -0.70832543]\n",
      " [ 0.71639265 -0.78440582  0.38295501  0.6513239  -0.35055942  0.11745147\n",
      "   0.91043687  0.8688294  -0.41824543]\n",
      " [-1.02190987  1.08588933 -0.51904329 -1.11627026  0.10609296  0.23372205\n",
      "  -1.002766   -0.76015834  0.77621362]\n",
      " [ 1.17184231 -0.97683848  0.50863297  0.74148371 -0.19534857 -0.28201345\n",
      "   0.83107054  0.86358616 -0.57317057]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.43078288  1.88075847 -2.16246873  0.2844853   0.49756323 -2.7594622\n",
      "   1.26344796]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:67 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.19706302]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 68 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91294254 -1.10091518  0.33213468  0.9827932  -0.32339122 -0.13753426\n",
      "   1.28106178  0.23495449 -0.60535924]\n",
      " [-0.86057508  0.86414836 -0.5406825  -0.99668787  0.22505687 -0.12332787\n",
      "  -0.71328238 -0.40214125  0.80080993]\n",
      " [ 0.67867079 -0.48318478  0.02108108  0.58447403 -0.45504973  0.28072264\n",
      "   0.70994645 -0.00689767 -0.70860757]\n",
      " [ 0.71575365 -0.78504482  0.38295501  0.6506849  -0.35055942  0.11745147\n",
      "   0.91043687  0.8688294  -0.41888444]\n",
      " [-1.02085407  1.08694513 -0.51904329 -1.11521446  0.10609296  0.23372205\n",
      "  -1.002766   -0.76015834  0.77726942]\n",
      " [ 1.17047196 -0.97820883  0.50863297  0.74011335 -0.19534857 -0.28201345\n",
      "   0.83107054  0.86358616 -0.57454093]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.44637344  1.87221546 -2.16950513  0.27640764  0.48912605 -2.76618821\n",
      "   1.25425197]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:68 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.84094843]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 68 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91478485 -1.10091518  0.33397699  0.9827932  -0.32339122 -0.13753426\n",
      "   1.2829041   0.23495449 -0.60535924]\n",
      " [-0.86273644  0.86414836 -0.54284386 -0.99668787  0.22505687 -0.12332787\n",
      "  -0.71544375 -0.40214125  0.80080993]\n",
      " [ 0.6810363  -0.48318478  0.02344659  0.58447403 -0.45504973  0.28072264\n",
      "   0.71231196 -0.00689767 -0.70860757]\n",
      " [ 0.71798188 -0.78504482  0.38518324  0.6506849  -0.35055942  0.11745147\n",
      "   0.9126651   0.8688294  -0.41888444]\n",
      " [-1.02268247  1.08694513 -0.5208717  -1.11521446  0.10609296  0.23372205\n",
      "  -1.0045944  -0.76015834  0.77726942]\n",
      " [ 1.17232766 -0.97820883  0.51048867  0.74011335 -0.19534857 -0.28201345\n",
      "   0.83292625  0.86358616 -0.57454093]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.43573654  1.88206475 -2.16835962  0.28495672  0.49850518 -2.76541256\n",
      "   1.26408954]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:68 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.67105698]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 68 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.89828147 -1.11741856  0.3174736   0.9827932  -0.32339122 -0.13753426\n",
      "   1.26640071  0.23495449 -0.60535924]\n",
      " [-0.84668363  0.88020117 -0.52679105 -0.99668787  0.22505687 -0.12332787\n",
      "  -0.69939093 -0.40214125  0.80080993]\n",
      " [ 0.66702983 -0.49719125  0.00944012  0.58447403 -0.45504973  0.28072264\n",
      "   0.69830549 -0.00689767 -0.70860757]\n",
      " [ 0.70203407 -0.80099263  0.36923543  0.6506849  -0.35055942  0.11745147\n",
      "   0.89671729  0.8688294  -0.41888444]\n",
      " [-1.00614244  1.10348517 -0.50433166 -1.11521446  0.10609296  0.23372205\n",
      "  -0.98805437 -0.76015834  0.77726942]\n",
      " [ 1.15574695 -0.99478954  0.49390796  0.74011335 -0.19534857 -0.28201345\n",
      "   0.81634554  0.86358616 -0.57454093]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.50980093  1.82229343 -2.18476579  0.23179277  0.4411826  -2.77935765\n",
      "   1.2031267 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:68 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.73922189]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 68 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90389089 -1.11741856  0.32308302  0.9827932  -0.32339122 -0.13753426\n",
      "   1.26640071  0.24056391 -0.60535924]\n",
      " [-0.85221185  0.88020117 -0.53231927 -0.99668787  0.22505687 -0.12332787\n",
      "  -0.69939093 -0.40766947  0.80080993]\n",
      " [ 0.67079887 -0.49719125  0.01320916  0.58447403 -0.45504973  0.28072264\n",
      "   0.69830549 -0.00312863 -0.70860757]\n",
      " [ 0.70739106 -0.80099263  0.37459241  0.6506849  -0.35055942  0.11745147\n",
      "   0.89671729  0.87418638 -0.41888444]\n",
      " [-1.01098377  1.10348517 -0.509173   -1.11521446  0.10609296  0.23372205\n",
      "  -0.98805437 -0.76499968  0.77726942]\n",
      " [ 1.16012599 -0.99478954  0.498287    0.74011335 -0.19534857 -0.28201345\n",
      "   0.81634554  0.8679652  -0.57454093]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.48466546  1.84265587 -2.18112503  0.24841756  0.46316012 -2.77700536\n",
      "   1.22637864]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:68 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.56916872]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 68 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91540355 -1.11741856  0.32308302  0.99430586 -0.32339122 -0.13753426\n",
      "   1.26640071  0.24056391 -0.59384658]\n",
      " [-0.86285294  0.88020117 -0.53231927 -1.00732896  0.22505687 -0.12332787\n",
      "  -0.69939093 -0.40766947  0.79016883]\n",
      " [ 0.67750446 -0.49719125  0.01320916  0.59117962 -0.45504973  0.28072264\n",
      "   0.69830549 -0.00312863 -0.70190198]\n",
      " [ 0.71741584 -0.80099263  0.37459241  0.66070968 -0.35055942  0.11745147\n",
      "   0.89671729  0.87418638 -0.40885966]\n",
      " [-1.02264041  1.10348517 -0.509173   -1.1268711   0.10609296  0.23372205\n",
      "  -0.98805437 -0.76499968  0.76561278]\n",
      " [ 1.17173839 -0.99478954  0.498287    0.75172575 -0.19534857 -0.28201345\n",
      "   0.81634554  0.8679652  -0.56292853]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.43184216  1.88399935 -2.16741253  0.2818738   0.50113671 -2.76612135\n",
      "   1.26811582]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:68 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.11583939]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 68 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91560923 -1.11741856  0.32308302  0.99430586 -0.32318554 -0.13732857\n",
      "   1.26640071  0.24056391 -0.59364089]\n",
      " [-0.86289601  0.88020117 -0.53231927 -1.00732896  0.2250138  -0.12337094\n",
      "  -0.69939093 -0.40766947  0.79012576]\n",
      " [ 0.67779628 -0.49719125  0.01320916  0.59117962 -0.4547579   0.28101447\n",
      "   0.69830549 -0.00312863 -0.70161016]\n",
      " [ 0.7173041  -0.80099263  0.37459241  0.66070968 -0.35067115  0.11733973\n",
      "   0.89671729  0.87418638 -0.40897139]\n",
      " [-1.02276298  1.10348517 -0.509173   -1.1268711   0.10597039  0.23359949\n",
      "  -0.98805437 -0.76499968  0.76549021]\n",
      " [ 1.17154428 -0.99478954  0.498287    0.75172575 -0.19554268 -0.28220756\n",
      "   0.81634554  0.8679652  -0.56312263]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.43777433  1.88123962 -2.17042168  0.27920147  0.49805878 -2.76921015\n",
      "   1.26495507]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:68 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.01697938]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 68 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91563994 -1.11738785  0.32308302  0.99430586 -0.32315483 -0.13729786\n",
      "   1.26640071  0.24056391 -0.59361018]\n",
      " [-0.86292238  0.8801748  -0.53231927 -1.00732896  0.22498743 -0.12339731\n",
      "  -0.69939093 -0.40766947  0.79009939]\n",
      " [ 0.67781815 -0.49716939  0.01320916  0.59117962 -0.45473604  0.28103633\n",
      "   0.69830549 -0.00312863 -0.70158829]\n",
      " [ 0.71732671 -0.80097002  0.37459241  0.66070968 -0.35064854  0.11736234\n",
      "   0.89671729  0.87418638 -0.40894879]\n",
      " [-1.0227931   1.10345505 -0.509173   -1.1268711   0.10594027  0.23356937\n",
      "  -0.98805437 -0.76499968  0.76546009]\n",
      " [ 1.17156982 -0.994764    0.498287    0.75172575 -0.19551714 -0.28218202\n",
      "   0.81634554  0.8679652  -0.56309709]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.43791604  1.88120821 -2.17052269  0.27915428  0.49801258 -2.76931869\n",
      "   1.26491306]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:68 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.33092664]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 68 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91058192 -1.12244588  0.32308302  0.99430586 -0.32821285 -0.14235589\n",
      "   1.26134269  0.24056391 -0.59361018]\n",
      " [-0.85802916  0.88506803 -0.53231927 -1.00732896  0.22988065 -0.11850409\n",
      "  -0.69449771 -0.40766947  0.79009939]\n",
      " [ 0.67209976 -0.50288778  0.01320916  0.59117962 -0.46045443  0.27531794\n",
      "   0.6925871  -0.00312863 -0.70158829]\n",
      " [ 0.71243885 -0.80585788  0.37459241  0.66070968 -0.35553641  0.11247448\n",
      "   0.89182943  0.87418638 -0.40894879]\n",
      " [-1.01798966  1.10825849 -0.509173   -1.1268711   0.11074371  0.23837281\n",
      "  -0.98325092 -0.76499968  0.76546009]\n",
      " [ 1.16714901 -0.99918482  0.498287    0.75172575 -0.19993795 -0.28660283\n",
      "   0.81192472  0.8679652  -0.56309709]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.47455196  1.85751881 -2.1836679   0.25463196  0.47452827 -2.78257084\n",
      "   1.2419759 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:68 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.86451426]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 68 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91155718 -1.12244588  0.32308302  0.99528113 -0.32821285 -0.14235589\n",
      "   1.26231796  0.24056391 -0.59361018]\n",
      " [-0.85938225  0.88506803 -0.53231927 -1.00868206  0.22988065 -0.11850409\n",
      "  -0.6958508  -0.40766947  0.79009939]\n",
      " [ 0.67378452 -0.50288778  0.01320916  0.59286438 -0.46045443  0.27531794\n",
      "   0.69427186 -0.00312863 -0.70158829]\n",
      " [ 0.71397036 -0.80585788  0.37459241  0.66224118 -0.35553641  0.11247448\n",
      "   0.89336094  0.87418638 -0.40894879]\n",
      " [-1.01898748  1.10825849 -0.509173   -1.12786893  0.11074371  0.23837281\n",
      "  -0.98424875 -0.76499968  0.76546009]\n",
      " [ 1.16839356 -0.99918482  0.498287    0.7529703  -0.19993795 -0.28660283\n",
      "   0.81316927  0.8679652  -0.56309709]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.46661729  1.86513248 -2.18309845  0.26158341  0.48171657 -2.78223788\n",
      "   1.24942513]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:68 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.70101445]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 68 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91851158 -1.12244588  0.32308302  0.99528113 -0.32125846 -0.13540149\n",
      "   1.26927235  0.24056391 -0.59361018]\n",
      " [-0.86637124  0.88506803 -0.53231927 -1.00868206  0.22289166 -0.12549308\n",
      "  -0.70283979 -0.40766947  0.79009939]\n",
      " [ 0.68043842 -0.50288778  0.01320916  0.59286438 -0.45380053  0.28197184\n",
      "   0.70092576 -0.00312863 -0.70158829]\n",
      " [ 0.72090019 -0.80585788  0.37459241  0.66224118 -0.34860657  0.11940431\n",
      "   0.90029078  0.87418638 -0.40894879]\n",
      " [-1.02597264  1.10825849 -0.509173   -1.12786893  0.10375856  0.23138766\n",
      "  -0.9912339  -0.76499968  0.76546009]\n",
      " [ 1.17540213 -0.99918482  0.498287    0.7529703  -0.19292938 -0.27959426\n",
      "   0.82017784  0.8679652  -0.56309709]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.43528462  1.89163885 -2.17711551  0.28556771  0.50667156 -2.77720733\n",
      "   1.27501862]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:68 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.59436825]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 68 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.903287   -1.12244588  0.32308302  0.98005655 -0.32125846 -0.15062607\n",
      "   1.26927235  0.24056391 -0.60883476]\n",
      " [-0.85102944  0.88506803 -0.53231927 -0.99334026  0.22289166 -0.11015128\n",
      "  -0.70283979 -0.40766947  0.80544119]\n",
      " [ 0.66762641 -0.50288778  0.01320916  0.58005237 -0.45380053  0.26915983\n",
      "   0.70092576 -0.00312863 -0.7144003 ]\n",
      " [ 0.70617169 -0.80585788  0.37459241  0.64751268 -0.34860657  0.10467581\n",
      "   0.90029078  0.87418638 -0.42367729]\n",
      " [-1.01088597  1.10825849 -0.509173   -1.11278226  0.10375856  0.24647432\n",
      "  -0.9912339  -0.76499968  0.78054676]\n",
      " [ 1.16072264 -0.99918482  0.498287    0.73829081 -0.19292938 -0.29427375\n",
      "   0.82017784  0.8679652  -0.57777659]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.50693412  1.83676921 -2.19356781  0.23531706  0.4530019  -2.79434689\n",
      "   1.22145578]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:68 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.71420391]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 68 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90958612 -1.11614676  0.32308302  0.98635566 -0.32125846 -0.14432696\n",
      "   1.27557147  0.24056391 -0.60883476]\n",
      " [-0.85744765  0.87864981 -0.53231927 -0.99975847  0.22289166 -0.11656949\n",
      "  -0.709258   -0.40766947  0.80544119]\n",
      " [ 0.67409222 -0.49642197  0.01320916  0.58651818 -0.45380053  0.27562564\n",
      "   0.70739156 -0.00312863 -0.7144003 ]\n",
      " [ 0.71270134 -0.79932823  0.37459241  0.65404233 -0.34860657  0.11120546\n",
      "   0.90682043  0.87418638 -0.42367729]\n",
      " [-1.01731522  1.10182924 -0.509173   -1.11921151  0.10375856  0.24004507\n",
      "  -0.99766316 -0.76499968  0.78054676]\n",
      " [ 1.16721913 -0.99268833  0.498287    0.7447873  -0.19292938 -0.28777726\n",
      "   0.82667433  0.8679652  -0.57777659]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.47776624  1.86207406 -2.18933096  0.26003589  0.47707463 -2.79006594\n",
      "   1.24497196]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:68 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.76273913]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 68 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91402629 -1.11170659  0.32308302  0.99079583 -0.32125846 -0.14432696\n",
      "   1.28001164  0.24056391 -0.60883476]\n",
      " [-0.86222002  0.87387745 -0.53231927 -1.00453084  0.22289166 -0.11656949\n",
      "  -0.71403037 -0.40766947  0.80544119]\n",
      " [ 0.67888935 -0.49162484  0.01320916  0.59131531 -0.45380053  0.27562564\n",
      "   0.7121887  -0.00312863 -0.7144003 ]\n",
      " [ 0.71749914 -0.79453043  0.37459241  0.65884013 -0.34860657  0.11120546\n",
      "   0.91161823  0.87418638 -0.42367729]\n",
      " [-1.02178423  1.09736023 -0.509173   -1.12368052  0.10375856  0.24004507\n",
      "  -1.00213216 -0.76499968  0.78054676]\n",
      " [ 1.17196024 -0.98794721  0.498287    0.74952842 -0.19292938 -0.28777726\n",
      "   0.83141545  0.8679652  -0.57777659]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.45629791  1.88110549 -2.18598053  0.27749603  0.49454342 -2.7875789\n",
      "   1.2632511 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:68 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.83335918]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 68 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91616779 -1.11170659  0.32522453  0.99079583 -0.32125846 -0.14218545\n",
      "   1.28215314  0.24056391 -0.60883476]\n",
      " [-0.8644859   0.87387745 -0.53458514 -1.00453084  0.22289166 -0.11883536\n",
      "  -0.71629625 -0.40766947  0.80544119]\n",
      " [ 0.68146342 -0.49162484  0.01578323  0.59131531 -0.45380053  0.2781997\n",
      "   0.71476276 -0.00312863 -0.7144003 ]\n",
      " [ 0.71985    -0.79453043  0.37694328  0.65884013 -0.34860657  0.11355632\n",
      "   0.91396909  0.87418638 -0.42367729]\n",
      " [-1.02399416  1.09736023 -0.51138293 -1.12368052  0.10375856  0.23783513\n",
      "  -1.00434209 -0.76499968  0.78054676]\n",
      " [ 1.17423515 -0.98794721  0.5005619   0.74952842 -0.19292938 -0.28550236\n",
      "   0.83369035  0.8679652  -0.57777659]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.44472707  1.89168968 -2.1848522   0.28724901  0.5048686  -2.78651785\n",
      "   1.27368209]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:68 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.19218929]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 69 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91547849 -1.11239589  0.32522453  0.99010653 -0.32125846 -0.14218545\n",
      "   1.28215314  0.24056391 -0.60952406]\n",
      " [-0.8637847   0.87457864 -0.53458514 -1.00382964  0.22289166 -0.11883536\n",
      "  -0.71629625 -0.40766947  0.80614238]\n",
      " [ 0.68121472 -0.49187353  0.01578323  0.59106662 -0.45380053  0.2781997\n",
      "   0.71476276 -0.00312863 -0.714649  ]\n",
      " [ 0.71925528 -0.79512515  0.37694328  0.65824541 -0.34860657  0.11355632\n",
      "   0.91396909  0.87418638 -0.42427201]\n",
      " [-1.02300609  1.09834831 -0.51138293 -1.12269244  0.10375856  0.23783513\n",
      "  -1.00434209 -0.76499968  0.78153483]\n",
      " [ 1.17294165 -0.98924071  0.5005619   0.74823492 -0.19292938 -0.28550236\n",
      "   0.83369035  0.8679652  -0.57907008]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.459646    1.88353691 -2.19160626  0.27954066  0.49681185 -2.79297722\n",
      "   1.26490131]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:69 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.84238245]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 69 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91729353 -1.11239589  0.32703956  0.99010653 -0.32125846 -0.14218545\n",
      "   1.28396818  0.24056391 -0.60952406]\n",
      " [-0.8659108   0.87457864 -0.53671124 -1.00382964  0.22289166 -0.11883536\n",
      "  -0.71842235 -0.40766947  0.80614238]\n",
      " [ 0.68354222 -0.49187353  0.01811072  0.59106662 -0.45380053  0.2781997\n",
      "   0.71709026 -0.00312863 -0.714649  ]\n",
      " [ 0.72144665 -0.79512515  0.37913465  0.65824541 -0.34860657  0.11355632\n",
      "   0.91616046  0.87418638 -0.42427201]\n",
      " [-1.02480799  1.09834831 -0.51318484 -1.12269244  0.10375856  0.23783513\n",
      "  -1.006144   -0.76499968  0.78153483]\n",
      " [ 1.17476961 -0.98924071  0.50238986  0.74823492 -0.19292938 -0.28550236\n",
      "   0.83551831  0.8679652  -0.57907008]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.44918223  1.89322353 -2.19047952  0.28795399  0.50603945 -2.7922114\n",
      "   1.27457659]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:69 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.66739182]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 69 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.9008103  -1.12887913  0.31055633  0.99010653 -0.32125846 -0.14218545\n",
      "   1.26748495  0.24056391 -0.60952406]\n",
      " [-0.84989663  0.89059281 -0.52069707 -1.00382964  0.22289166 -0.11883536\n",
      "  -0.70240818 -0.40766947  0.80614238]\n",
      " [ 0.66959433 -0.50582142  0.00416284  0.59106662 -0.45380053  0.2781997\n",
      "   0.70314237 -0.00312863 -0.714649  ]\n",
      " [ 0.70553613 -0.81103567  0.36322413  0.65824541 -0.34860657  0.11355632\n",
      "   0.90024994  0.87418638 -0.42427201]\n",
      " [-1.00828291  1.11487339 -0.49665975 -1.12269244  0.10375856  0.23783513\n",
      "  -0.98961892 -0.76499968  0.78153483]\n",
      " [ 1.15818895 -1.00582137  0.4858092   0.74823492 -0.19292938 -0.28550236\n",
      "   0.81893765  0.8679652  -0.57907008]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.52325604  1.83361665 -2.20702113  0.23488457  0.44882869 -2.80633361\n",
      "   1.21375872]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:69 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.74105419]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 69 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 9.06355339e-01 -1.12887913e+00  3.16101374e-01  9.90106534e-01\n",
      "  -3.21258458e-01 -1.42185454e-01  1.26748495e+00  2.46108956e-01\n",
      "  -6.09524063e-01]\n",
      " [-8.55358812e-01  8.90592812e-01 -5.26159252e-01 -1.00382964e+00\n",
      "   2.22891661e-01 -1.18835364e-01 -7.02408178e-01 -4.13131650e-01\n",
      "   8.06142385e-01]\n",
      " [ 6.73324410e-01 -5.05821418e-01  7.89291486e-03  5.91066615e-01\n",
      "  -4.53800528e-01  2.78199702e-01  7.03142374e-01  6.01451914e-04\n",
      "  -7.14648996e-01]\n",
      " [ 7.10827667e-01 -8.11035667e-01  3.68515665e-01  6.58245410e-01\n",
      "  -3.48606571e-01  1.13556322e-01  9.00249944e-01  8.79477918e-01\n",
      "  -4.24272012e-01]\n",
      " [-1.01306952e+00  1.11487339e+00 -5.01446360e-01 -1.12269244e+00\n",
      "   1.03758560e-01  2.37835134e-01 -9.89618919e-01 -7.69786287e-01\n",
      "   7.81534834e-01]\n",
      " [ 1.16251987e+00 -1.00582137e+00  4.90140121e-01  7.48234920e-01\n",
      "  -1.92929383e-01 -2.85502355e-01  8.18937650e-01  8.72296119e-01\n",
      "  -5.79070081e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.49841111  1.85374838 -2.2034306   0.25132306  0.47055994 -2.80400706\n",
      "   1.23673969]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:69 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.57092284]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 69 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 9.17823298e-01 -1.12887913e+00  3.16101374e-01  1.00157449e+00\n",
      "  -3.21258458e-01 -1.42185454e-01  1.26748495e+00  2.46108956e-01\n",
      "  -5.98056104e-01]\n",
      " [-8.65970662e-01  8.90592812e-01 -5.26159252e-01 -1.01444149e+00\n",
      "   2.22891661e-01 -1.18835364e-01 -7.02408178e-01 -4.13131650e-01\n",
      "   7.95530535e-01]\n",
      " [ 6.80028031e-01 -5.05821418e-01  7.89291486e-03  5.97770237e-01\n",
      "  -4.53800528e-01  2.78199702e-01  7.03142374e-01  6.01451914e-04\n",
      "  -7.07945375e-01]\n",
      " [ 7.20836519e-01 -8.11035667e-01  3.68515665e-01  6.68254261e-01\n",
      "  -3.48606571e-01  1.13556322e-01  9.00249944e-01  8.79477918e-01\n",
      "  -4.14263161e-01]\n",
      " [-1.02467635e+00  1.11487339e+00 -5.01446360e-01 -1.13429928e+00\n",
      "   1.03758560e-01  2.37835134e-01 -9.89618919e-01 -7.69786287e-01\n",
      "   7.69928000e-01]\n",
      " [ 1.17408526e+00 -1.00582137e+00  4.90140121e-01  7.59800306e-01\n",
      "  -1.92929383e-01 -2.85502355e-01  8.18937650e-01  8.72296119e-01\n",
      "  -5.67504695e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.44585561  1.89493237 -2.18983761  0.28464723  0.5084035  -2.79322371\n",
      "   1.27831736]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:69 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.11248153]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 69 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 9.18023929e-01 -1.12887913e+00  3.16101374e-01  1.00157449e+00\n",
      "  -3.21057827e-01 -1.41984823e-01  1.26748495e+00  2.46108956e-01\n",
      "  -5.97855473e-01]\n",
      " [-8.66017833e-01  8.90592812e-01 -5.26159252e-01 -1.01444149e+00\n",
      "   2.22844490e-01 -1.18882535e-01 -7.02408178e-01 -4.13131650e-01\n",
      "   7.95483364e-01]\n",
      " [ 6.80310756e-01 -5.05821418e-01  7.89291486e-03  5.97770237e-01\n",
      "  -4.53517804e-01  2.78482426e-01  7.03142374e-01  6.01451914e-04\n",
      "  -7.07662650e-01]\n",
      " [ 7.20736256e-01 -8.11035667e-01  3.68515665e-01  6.68254261e-01\n",
      "  -3.48706835e-01  1.13456059e-01  9.00249944e-01  8.79477918e-01\n",
      "  -4.14363424e-01]\n",
      " [-1.02479802e+00  1.11487339e+00 -5.01446360e-01 -1.13429928e+00\n",
      "   1.03636891e-01  2.37713466e-01 -9.89618919e-01 -7.69786287e-01\n",
      "   7.69806332e-01]\n",
      " [ 1.17390612e+00 -1.00582137e+00  4.90140121e-01  7.59800306e-01\n",
      "  -1.93108519e-01 -2.85681491e-01  8.18937650e-01  8.72296119e-01\n",
      "  -5.67683831e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.45147009  1.89232645 -2.19269204  0.28212467  0.50549591 -2.79615278\n",
      "   1.27533049]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:69 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.01589688]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 69 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 9.18050973e-01 -1.12885208e+00  3.16101374e-01  1.00157449e+00\n",
      "  -3.21030783e-01 -1.41957779e-01  1.26748495e+00  2.46108956e-01\n",
      "  -5.97828429e-01]\n",
      " [-8.66041204e-01  8.90569441e-01 -5.26159252e-01 -1.01444149e+00\n",
      "   2.22821119e-01 -1.18905906e-01 -7.02408178e-01 -4.13131650e-01\n",
      "   7.95459993e-01]\n",
      " [ 6.80330227e-01 -5.05801948e-01  7.89291486e-03  5.97770237e-01\n",
      "  -4.53498333e-01  2.78501897e-01  7.03142374e-01  6.01451914e-04\n",
      "  -7.07643179e-01]\n",
      " [ 7.20756375e-01 -8.11015547e-01  3.68515665e-01  6.68254261e-01\n",
      "  -3.48686715e-01  1.13476179e-01  9.00249944e-01  8.79477918e-01\n",
      "  -4.14343304e-01]\n",
      " [-1.02482457e+00  1.11484683e+00 -5.01446360e-01 -1.13429928e+00\n",
      "   1.03610337e-01  2.37686912e-01 -9.89618919e-01 -7.69786287e-01\n",
      "   7.69779777e-01]\n",
      " [ 1.17392877e+00 -1.00579872e+00  4.90140121e-01  7.59800306e-01\n",
      "  -1.93085870e-01 -2.85658842e-01  8.18937650e-01  8.72296119e-01\n",
      "  -5.67661182e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.45159444  1.89229922 -2.19278105  0.28208364  0.50545574 -2.79624837\n",
      "   1.27529399]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:69 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.32476845]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 69 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 9.13205303e-01 -1.13369775e+00  3.16101374e-01  1.00157449e+00\n",
      "  -3.25876453e-01 -1.46803449e-01  1.26263928e+00  2.46108956e-01\n",
      "  -5.97828429e-01]\n",
      " [-8.61330259e-01  8.95280386e-01 -5.26159252e-01 -1.01444149e+00\n",
      "   2.27532065e-01 -1.14194961e-01 -6.97697232e-01 -4.13131650e-01\n",
      "   7.95459993e-01]\n",
      " [ 6.74787494e-01 -5.11344681e-01  7.89291486e-03  5.97770237e-01\n",
      "  -4.59041066e-01  2.72959164e-01  6.97599641e-01  6.01451914e-04\n",
      "  -7.07643179e-01]\n",
      " [ 7.16039812e-01 -8.15732111e-01  3.68515665e-01  6.68254261e-01\n",
      "  -3.53403278e-01  1.08759616e-01  8.95533381e-01  8.79477918e-01\n",
      "  -4.14343304e-01]\n",
      " [-1.02022237e+00  1.11944904e+00 -5.01446360e-01 -1.13429928e+00\n",
      "   1.08212542e-01  2.42289117e-01 -9.85016714e-01 -7.69786287e-01\n",
      "   7.69779777e-01]\n",
      " [ 1.16968363e+00 -1.01004386e+00  4.90140121e-01  7.59800306e-01\n",
      "  -1.97331015e-01 -2.89903987e-01  8.14692505e-01  8.72296119e-01\n",
      "  -5.67661182e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.48720431  1.86935889 -2.20561209  0.25826845  0.48267022 -2.80920826\n",
      "   1.25305872]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:69 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.86718868]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 69 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 9.14139016e-01 -1.13369775e+00  3.16101374e-01  1.00250821e+00\n",
      "  -3.25876453e-01 -1.46803449e-01  1.26357299e+00  2.46108956e-01\n",
      "  -5.97828429e-01]\n",
      " [-8.62626204e-01  8.95280386e-01 -5.26159252e-01 -1.01573744e+00\n",
      "   2.27532065e-01 -1.14194961e-01 -6.98993177e-01 -4.13131650e-01\n",
      "   7.95459993e-01]\n",
      " [ 6.76405749e-01 -5.11344681e-01  7.89291486e-03  5.99388493e-01\n",
      "  -4.59041066e-01  2.72959164e-01  6.99217897e-01  6.01451914e-04\n",
      "  -7.07643179e-01]\n",
      " [ 7.17507853e-01 -8.15732111e-01  3.68515665e-01  6.69722302e-01\n",
      "  -3.53403278e-01  1.08759616e-01  8.97001422e-01  8.79477918e-01\n",
      "  -4.14343304e-01]\n",
      " [-1.02117761e+00  1.11944904e+00 -5.01446360e-01 -1.13525452e+00\n",
      "   1.08212542e-01  2.42289117e-01 -9.85971959e-01 -7.69786287e-01\n",
      "   7.69779777e-01]\n",
      " [ 1.17087502e+00 -1.01004386e+00  4.90140121e-01  7.60991702e-01\n",
      "  -1.97331015e-01 -2.89903987e-01  8.15883901e-01  8.72296119e-01\n",
      "  -5.67661182e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.4795562   1.87670089 -2.2050701   0.26498064  0.48960852 -2.80889082\n",
      "   1.26024475]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:69 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.70322846]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 69 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 9.21011189e-01 -1.13369775e+00  3.16101374e-01  1.00250821e+00\n",
      "  -3.19004280e-01 -1.39931276e-01  1.27044516e+00  2.46108956e-01\n",
      "  -5.97828429e-01]\n",
      " [-8.69536041e-01  8.95280386e-01 -5.26159252e-01 -1.01573744e+00\n",
      "   2.20622227e-01 -1.21104798e-01 -7.05903015e-01 -4.13131650e-01\n",
      "   7.95459993e-01]\n",
      " [ 6.82995725e-01 -5.11344681e-01  7.89291486e-03  5.99388493e-01\n",
      "  -4.52451091e-01  2.79549139e-01  7.05807872e-01  6.01451914e-04\n",
      "  -7.07643179e-01]\n",
      " [ 7.24362278e-01 -8.15732111e-01  3.68515665e-01  6.69722302e-01\n",
      "  -3.46548853e-01  1.15614041e-01  9.03855847e-01  8.79477918e-01\n",
      "  -4.14343304e-01]\n",
      " [-1.02808012e+00  1.11944904e+00 -5.01446360e-01 -1.13525452e+00\n",
      "   1.01310034e-01  2.35386609e-01 -9.92874467e-01 -7.69786287e-01\n",
      "   7.69779777e-01]\n",
      " [ 1.17780303e+00 -1.01004386e+00  4.90140121e-01  7.60991702e-01\n",
      "  -1.90403004e-01 -2.82975976e-01  8.22811912e-01  8.72296119e-01\n",
      "  -5.67661182e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.44858836  1.90290559 -2.19917797  0.28872242  0.51430099 -2.80392939\n",
      "   1.28556107]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:69 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.59475343]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 69 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 9.05776696e-01 -1.13369775e+00  3.16101374e-01  9.87273714e-01\n",
      "  -3.19004280e-01 -1.55165768e-01  1.27044516e+00  2.46108956e-01\n",
      "  -6.13062922e-01]\n",
      " [-8.54186877e-01  8.95280386e-01 -5.26159252e-01 -1.00038827e+00\n",
      "   2.20622227e-01 -1.05755634e-01 -7.05903015e-01 -4.13131650e-01\n",
      "   8.10809157e-01]\n",
      " [ 6.70173340e-01 -5.11344681e-01  7.89291486e-03  5.86566108e-01\n",
      "  -4.52451091e-01  2.66726754e-01  7.05807872e-01  6.01451914e-04\n",
      "  -7.20465565e-01]\n",
      " [ 7.09617918e-01 -8.15732111e-01  3.68515665e-01  6.54977942e-01\n",
      "  -3.46548853e-01  1.00869680e-01  9.03855847e-01  8.79477918e-01\n",
      "  -4.29087664e-01]\n",
      " [-1.01298218e+00  1.11944904e+00 -5.01446360e-01 -1.12015658e+00\n",
      "   1.01310034e-01  2.50484553e-01 -9.92874467e-01 -7.69786287e-01\n",
      "   7.84877722e-01]\n",
      " [ 1.16310302e+00 -1.01004386e+00  4.90140121e-01  7.46291692e-01\n",
      "  -1.90403004e-01 -2.97675986e-01  8.22811912e-01  8.72296119e-01\n",
      "  -5.82361191e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.52026263  1.8480044  -2.21562998  0.23844534  0.4605891  -2.82105956\n",
      "   1.23194624]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:69 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.71496073]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 69 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 9.12055354e-01 -1.12741909e+00  3.16101374e-01  9.93552371e-01\n",
      "  -3.19004280e-01 -1.48887111e-01  1.27672382e+00  2.46108956e-01\n",
      "  -6.13062922e-01]\n",
      " [-8.60579146e-01  8.88888118e-01 -5.26159252e-01 -1.00678054e+00\n",
      "   2.20622227e-01 -1.12147903e-01 -7.12295283e-01 -4.13131650e-01\n",
      "   8.10809157e-01]\n",
      " [ 6.76609545e-01 -5.04908475e-01  7.89291486e-03  5.93002313e-01\n",
      "  -4.52451091e-01  2.73162960e-01  7.12244078e-01  6.01451914e-04\n",
      "  -7.20465565e-01]\n",
      " [ 7.16119883e-01 -8.09230145e-01  3.68515665e-01  6.61479908e-01\n",
      "  -3.46548853e-01  1.07371646e-01  9.10357812e-01  8.79477918e-01\n",
      "  -4.29087664e-01]\n",
      " [-1.01938779e+00  1.11304342e+00 -5.01446360e-01 -1.12656220e+00\n",
      "   1.01310034e-01  2.44078937e-01 -9.99280084e-01 -7.69786287e-01\n",
      "   7.84877722e-01]\n",
      " [ 1.16957128e+00 -1.00357560e+00  4.90140121e-01  7.52759950e-01\n",
      "  -1.90403004e-01 -2.91207727e-01  8.29280170e-01  8.72296119e-01\n",
      "  -5.82361191e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.49121828  1.87318566 -2.21140624  0.26307093  0.48456321 -2.81678185\n",
      "   1.25535716]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:69 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.76433759]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 69 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 9.16445649e-01 -1.12302880e+00  3.16101374e-01  9.97942666e-01\n",
      "  -3.19004280e-01 -1.48887111e-01  1.28111412e+00  2.46108956e-01\n",
      "  -6.13062922e-01]\n",
      " [-8.65295953e-01  8.84171311e-01 -5.26159252e-01 -1.01149735e+00\n",
      "   2.20622227e-01 -1.12147903e-01 -7.17012091e-01 -4.13131650e-01\n",
      "   8.10809157e-01]\n",
      " [ 6.81353473e-01 -5.00164547e-01  7.89291486e-03  5.97746241e-01\n",
      "  -4.52451091e-01  2.73162960e-01  7.16988006e-01  6.01451914e-04\n",
      "  -7.20465565e-01]\n",
      " [ 7.20864218e-01 -8.04485811e-01  3.68515665e-01  6.66224242e-01\n",
      "  -3.46548853e-01  1.07371646e-01  9.15102147e-01  8.79477918e-01\n",
      "  -4.29087664e-01]\n",
      " [-1.02380624e+00  1.10862498e+00 -5.01446360e-01 -1.13098064e+00\n",
      "   1.01310034e-01  2.44078937e-01 -1.00369853e+00 -7.69786287e-01\n",
      "   7.84877722e-01]\n",
      " [ 1.17425728e+00 -9.98889605e-01  4.90140121e-01  7.57445949e-01\n",
      "  -1.90403004e-01 -2.91207727e-01  8.33966169e-01  8.72296119e-01\n",
      "  -5.82361191e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.46999386  1.8919999  -2.20810213  0.28034997  0.50184796 -2.81432268\n",
      "   1.27343411]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:69 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.83403068]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 69 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 9.18578585e-01 -1.12302880e+00  3.18234309e-01  9.97942666e-01\n",
      "  -3.19004280e-01 -1.46754175e-01  1.28324705e+00  2.46108956e-01\n",
      "  -6.13062922e-01]\n",
      " [-8.67549027e-01  8.84171311e-01 -5.28412327e-01 -1.01149735e+00\n",
      "   2.20622227e-01 -1.14400977e-01 -7.19265165e-01 -4.13131650e-01\n",
      "   8.10809157e-01]\n",
      " [ 6.83909013e-01 -5.00164547e-01  1.04484547e-02  5.97746241e-01\n",
      "  -4.52451091e-01  2.75718500e-01  7.19543546e-01  6.01451914e-04\n",
      "  -7.20465565e-01]\n",
      " [ 7.23200275e-01 -8.04485811e-01  3.70851722e-01  6.66224242e-01\n",
      "  -3.46548853e-01  1.09707703e-01  9.17438204e-01  8.79477918e-01\n",
      "  -4.29087664e-01]\n",
      " [-1.02600697e+00  1.10862498e+00 -5.03647090e-01 -1.13098064e+00\n",
      "   1.01310034e-01  2.41878206e-01 -1.00589926e+00 -7.69786287e-01\n",
      "   7.84877722e-01]\n",
      " [ 1.17652103e+00 -9.98889605e-01  4.92403873e-01  7.57445949e-01\n",
      "  -1.90403004e-01 -2.88943975e-01  8.36229921e-01  8.72296119e-01\n",
      "  -5.82361191e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.45850684  1.90250021 -2.20697739  0.29003152  0.51209497 -2.81326147\n",
      "   1.28378266]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:69 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.18740851]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 70 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 9.17940117e-01 -1.12366727e+00  3.18234309e-01  9.97304199e-01\n",
      "  -3.19004280e-01 -1.46754175e-01  1.28324705e+00  2.46108956e-01\n",
      "  -6.13701389e-01]\n",
      " [-8.66897907e-01  8.84822430e-01 -5.28412327e-01 -1.01084623e+00\n",
      "   2.20622227e-01 -1.14400977e-01 -7.19265165e-01 -4.13131650e-01\n",
      "   8.11460277e-01]\n",
      " [ 6.83691510e-01 -5.00382050e-01  1.04484547e-02  5.97528738e-01\n",
      "  -4.52451091e-01  2.75718500e-01  7.19543546e-01  6.01451914e-04\n",
      "  -7.20683068e-01]\n",
      " [ 7.22647643e-01 -8.05038443e-01  3.70851722e-01  6.65671610e-01\n",
      "  -3.46548853e-01  1.09707703e-01  9.17438204e-01  8.79477918e-01\n",
      "  -4.29640296e-01]\n",
      " [-1.02508312e+00  1.10954883e+00 -5.03647090e-01 -1.13005679e+00\n",
      "   1.01310034e-01  2.41878206e-01 -1.00589926e+00 -7.69786287e-01\n",
      "   7.85801571e-01]\n",
      " [ 1.17530106e+00 -1.00010957e+00  4.92403873e-01  7.56225981e-01\n",
      "  -1.90403004e-01 -2.88943975e-01  8.36229921e-01  8.72296119e-01\n",
      "  -5.83581159e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.47277674  1.89472332 -2.21345753  0.28267893  0.50440515 -2.81946185\n",
      "   1.27540229]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:70 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.84376441]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 70 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 9.19729184e-01 -1.12366727e+00  3.20023376e-01  9.97304199e-01\n",
      "  -3.19004280e-01 -1.46754175e-01  1.28503612e+00  2.46108956e-01\n",
      "  -6.13701389e-01]\n",
      " [-8.68990377e-01  8.84822430e-01 -5.30504796e-01 -1.01084623e+00\n",
      "   2.20622227e-01 -1.14400977e-01 -7.21357635e-01 -4.13131650e-01\n",
      "   8.11460277e-01]\n",
      " [ 6.85982560e-01 -5.00382050e-01  1.27395045e-02  5.97528738e-01\n",
      "  -4.52451091e-01  2.75718500e-01  7.21834596e-01  6.01451914e-04\n",
      "  -7.20683068e-01]\n",
      " [ 7.24803845e-01 -8.05038443e-01  3.73007924e-01  6.65671610e-01\n",
      "  -3.46548853e-01  1.09707703e-01  9.19594406e-01  8.79477918e-01\n",
      "  -4.29640296e-01]\n",
      " [-1.02685982e+00  1.10954883e+00 -5.05423785e-01 -1.13005679e+00\n",
      "   1.01310034e-01  2.41878206e-01 -1.00767595e+00 -7.69786287e-01\n",
      "   7.85801571e-01]\n",
      " [ 1.17710269e+00 -1.00010957e+00  4.94205501e-01  7.56225981e-01\n",
      "  -1.90403004e-01 -2.88943975e-01  8.38031549e-01  8.72296119e-01\n",
      "  -5.83581159e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.46247878  1.904254   -2.21234856  0.29096206  0.51348733 -2.8187053\n",
      "   1.2849219 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:70 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.66359366]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 70 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 9.03271999e-01 -1.14012445e+00  3.03566191e-01  9.97304199e-01\n",
      "  -3.19004280e-01 -1.46754175e-01  1.26857893e+00  2.46108956e-01\n",
      "  -6.13701389e-01]\n",
      " [-8.53019570e-01  9.00793238e-01 -5.14533989e-01 -1.01084623e+00\n",
      "   2.20622227e-01 -1.14400977e-01 -7.05386827e-01 -4.13131650e-01\n",
      "   8.11460277e-01]\n",
      " [ 6.72096195e-01 -5.14268415e-01 -1.14686035e-03  5.97528738e-01\n",
      "  -4.52451091e-01  2.75718500e-01  7.07948231e-01  6.01451914e-04\n",
      "  -7.20683068e-01]\n",
      " [ 7.08935232e-01 -8.20907055e-01  3.57139311e-01  6.65671610e-01\n",
      "  -3.46548853e-01  1.09707703e-01  9.03725793e-01  8.79477918e-01\n",
      "  -4.29640296e-01]\n",
      " [-1.01035580e+00  1.12605284e+00 -4.88919769e-01 -1.13005679e+00\n",
      "   1.01310034e-01  2.41878206e-01 -9.91171938e-01 -7.69786287e-01\n",
      "   7.85801571e-01]\n",
      " [ 1.16052762e+00 -1.01668465e+00  4.77630427e-01  7.56225981e-01\n",
      "  -1.90403004e-01 -2.88943975e-01  8.21456475e-01  8.72296119e-01\n",
      "  -5.83581159e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.53654814  1.84482357 -2.22902363  0.2379967   0.45640077 -2.83300326\n",
      "   1.22426301]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:70 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.74283564]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 70 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90875453 -1.14012445  0.30904872  0.9973042  -0.31900428 -0.14675418\n",
      "   1.26857893  0.25159149 -0.61370139]\n",
      " [-0.85841785  0.90079324 -0.51993227 -1.01084623  0.22062223 -0.11440098\n",
      "  -0.70538683 -0.41852993  0.81146028]\n",
      " [ 0.67578794 -0.51426842  0.00254489  0.59752874 -0.45245109  0.2757185\n",
      "   0.70794823  0.0042932  -0.72068307]\n",
      " [ 0.71416355 -0.82090706  0.36236763  0.66567161 -0.34654885  0.1097077\n",
      "   0.90372579  0.88470623 -0.4296403 ]\n",
      " [-1.01508967  1.12605284 -0.49365364 -1.13005679  0.10131003  0.24187821\n",
      "  -0.99117194 -0.77452016  0.78580157]\n",
      " [ 1.16481234 -1.01668465  0.48191515  0.75622598 -0.190403   -0.28894398\n",
      "   0.82145648  0.87658084 -0.58358116]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.51198498  1.86473082 -2.22548117  0.25425384  0.47789257 -2.83070125\n",
      "   1.24698077]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:70 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.57265317]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 70 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92017748 -1.14012445  0.30904872  1.00872715 -0.31900428 -0.14675418\n",
      "   1.26857893  0.25159149 -0.60227844]\n",
      " [-0.86899951  0.90079324 -0.51993227 -1.02142789  0.22062223 -0.11440098\n",
      "  -0.70538683 -0.41852993  0.80087861]\n",
      " [ 0.68248763 -0.51426842  0.00254489  0.60422843 -0.45245109  0.2757185\n",
      "   0.70794823  0.0042932  -0.71398338]\n",
      " [ 0.72415497 -0.82090706  0.36236763  0.67566303 -0.34654885  0.1097077\n",
      "   0.90372579  0.88470623 -0.41964887]\n",
      " [-1.02664664  1.12605284 -0.49365364 -1.14161376  0.10131003  0.24187821\n",
      "  -0.99117194 -0.77452016  0.7742446 ]\n",
      " [ 1.17633045 -1.01668465  0.48191515  0.76774409 -0.190403   -0.28894398\n",
      "   0.82145648  0.87658084 -0.57206305]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.4596945   1.90575511 -2.21200437  0.28744504  0.51560208 -2.82001586\n",
      "   1.28839857]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:70 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.10924696]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 70 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.9203729  -1.14012445  0.30904872  1.00872715 -0.31880886 -0.14655876\n",
      "   1.26857893  0.25159149 -0.60208302]\n",
      " [-0.86905013  0.90079324 -0.51993227 -1.02142789  0.22057161 -0.11445159\n",
      "  -0.70538683 -0.41852993  0.800828  ]\n",
      " [ 0.68276136 -0.51426842  0.00254489  0.60422843 -0.45217736  0.27599223\n",
      "   0.70794823  0.0042932  -0.71370965]\n",
      " [ 0.72406516 -0.82090706  0.36236763  0.67566303 -0.34663867  0.10961789\n",
      "   0.90372579  0.88470623 -0.41973869]\n",
      " [-1.02676704  1.12605284 -0.49365364 -1.14161376  0.10118964  0.24175781\n",
      "  -0.99117194 -0.77452016  0.77412421]\n",
      " [ 1.17616509 -1.01668465  0.48191515  0.76774409 -0.19056836 -0.28910933\n",
      "   0.82145648  0.87658084 -0.57222841]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.46501002  1.90329348 -2.21471276  0.28506299  0.51285444 -2.82279418\n",
      "   1.28557502]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:70 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.01489265]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 70 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92039674 -1.14010061  0.30904872  1.00872715 -0.31878502 -0.14653492\n",
      "   1.26857893  0.25159149 -0.60205918]\n",
      " [-0.86907085  0.90077251 -0.51993227 -1.02142789  0.22055089 -0.11447232\n",
      "  -0.70538683 -0.41852993  0.80080728]\n",
      " [ 0.68277871 -0.51425107  0.00254489  0.60422843 -0.45216001  0.27600958\n",
      "   0.70794823  0.0042932  -0.7136923 ]\n",
      " [ 0.72408307 -0.82088914  0.36236763  0.67566303 -0.34662075  0.1096358\n",
      "   0.90372579  0.88470623 -0.41972077]\n",
      " [-1.02679047  1.12602941 -0.49365364 -1.14161376  0.10116621  0.24173438\n",
      "  -0.99117194 -0.77452016  0.77410077]\n",
      " [ 1.17618519 -1.01666455  0.48191515  0.76774409 -0.19054826 -0.28908924\n",
      "   0.82145648  0.87658084 -0.57220831]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.46511927  1.90326985 -2.21479129  0.28502726  0.51281948 -2.82287845\n",
      "   1.28554328]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:70 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.31875194]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 70 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91575434 -1.14474301  0.30904872  1.00872715 -0.32342742 -0.15117732\n",
      "   1.26393653  0.25159149 -0.60205918]\n",
      " [-0.86453511  0.90530825 -0.51993227 -1.02142789  0.22508663 -0.10993657\n",
      "  -0.70085109 -0.41852993  0.80080728]\n",
      " [ 0.67740562 -0.51962416  0.00254489  0.60422843 -0.4575331   0.27063649\n",
      "   0.70257514  0.0042932  -0.7136923 ]\n",
      " [ 0.71953166 -0.82544055  0.36236763  0.67566303 -0.35117216  0.10508439\n",
      "   0.89917438  0.88470623 -0.41972077]\n",
      " [-1.02238092  1.13043896 -0.49365364 -1.14161376  0.10557575  0.24614393\n",
      "  -0.98676239 -0.77452016  0.77410077]\n",
      " [ 1.17210909 -1.02074065  0.48191515  0.76774409 -0.19462437 -0.29316534\n",
      "   0.81738037  0.87658084 -0.57220831]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.49972762  1.88105526 -2.22731222  0.26189988  0.49071345 -2.83554819\n",
      "   1.26399009]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:70 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.86978663]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 70 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91664859 -1.14474301  0.30904872  1.0096214  -0.32342742 -0.15117732\n",
      "   1.26483078  0.25159149 -0.60205918]\n",
      " [-0.86577675  0.90530825 -0.51993227 -1.02266953  0.22508663 -0.10993657\n",
      "  -0.70209272 -0.41852993  0.80080728]\n",
      " [ 0.67896035 -0.51962416  0.00254489  0.60578316 -0.4575331   0.27063649\n",
      "   0.70412987  0.0042932  -0.7136923 ]\n",
      " [ 0.72093929 -0.82544055  0.36236763  0.67707067 -0.35117216  0.10508439\n",
      "   0.90058201  0.88470623 -0.41972077]\n",
      " [-1.02329574  1.13043896 -0.49365364 -1.14252858  0.10557575  0.24614393\n",
      "  -0.98767721 -0.77452016  0.77410077]\n",
      " [ 1.17325004 -1.02074065  0.48191515  0.76888504 -0.19462437 -0.29316534\n",
      "   0.81852133  0.87658084 -0.57220831]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.49235378  1.88813707 -2.22679611  0.26838244  0.49741192 -2.83524541\n",
      "   1.27092386]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:70 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.70544817]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 70 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92343838 -1.14474301  0.30904872  1.0096214  -0.31663763 -0.14438753\n",
      "   1.27162057  0.25159149 -0.60205918]\n",
      " [-0.87260719  0.90530825 -0.51993227 -1.02266953  0.21825619 -0.11676702\n",
      "  -0.70892316 -0.41852993  0.80080728]\n",
      " [ 0.68548578 -0.51962416  0.00254489  0.60578316 -0.45100767  0.27716192\n",
      "   0.7106553   0.0042932  -0.7136923 ]\n",
      " [ 0.72771789 -0.82544055  0.36236763  0.67707067 -0.34439356  0.11186299\n",
      "   0.90736062  0.88470623 -0.41972077]\n",
      " [-1.03011547  1.13043896 -0.49365364 -1.14252858  0.09875602  0.23932419\n",
      "  -0.99449695 -0.77452016  0.77410077]\n",
      " [ 1.18009727 -1.02074065  0.48191515  0.76888504 -0.18777714 -0.28631811\n",
      "   0.82536855  0.87658084 -0.57220831]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.46175116  1.91404006 -2.22099436  0.29188057  0.52184066 -2.8303532\n",
      "   1.29596193]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:70 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.59508841]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 70 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90819529 -1.14474301  0.30904872  0.9943783  -0.31663763 -0.15963062\n",
      "   1.27162057  0.25159149 -0.61730228]\n",
      " [-0.85725204  0.90530825 -0.51993227 -1.00731438  0.21825619 -0.10141187\n",
      "  -0.70892316 -0.41852993  0.81616242]\n",
      " [ 0.67265515 -0.51962416  0.00254489  0.59295252 -0.45100767  0.26433128\n",
      "   0.7106553   0.0042932  -0.72652293]\n",
      " [ 0.71295952 -0.82544055  0.36236763  0.66231229 -0.34439356  0.09710462\n",
      "   0.90736062  0.88470623 -0.43447915]\n",
      " [-1.0150076   1.13043896 -0.49365364 -1.1274207   0.09875602  0.25443206\n",
      "  -0.99449695 -0.77452016  0.78920865]\n",
      " [ 1.16537861 -1.02074065  0.48191515  0.75416638 -0.18777714 -0.30103677\n",
      "   0.82536855  0.87658084 -0.58692697]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.53344688  1.85911148 -2.23744726  0.24158171  0.46809149 -2.8474748\n",
      "   1.24230002]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:70 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.7156951]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 70 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91445405 -1.13848425  0.30904872  1.00063706 -0.31663763 -0.15337186\n",
      "   1.27787933  0.25159149 -0.61730228]\n",
      " [-0.86361919  0.8989411  -0.51993227 -1.01368153  0.21825619 -0.10777902\n",
      "  -0.71529032 -0.41852993  0.81616242]\n",
      " [ 0.67906257 -0.51321673  0.00254489  0.59935995 -0.45100767  0.27073871\n",
      "   0.71706273  0.0042932  -0.72652293]\n",
      " [ 0.71943463 -0.81896543  0.36236763  0.6687874  -0.34439356  0.10357973\n",
      "   0.91383573  0.88470623 -0.43447915]\n",
      " [-1.02139026  1.1240563  -0.49365364 -1.13380337  0.09875602  0.2480494\n",
      "  -1.00087961 -0.77452016  0.78920865]\n",
      " [ 1.1718194  -1.01429985  0.48191515  0.76060717 -0.18777714 -0.29459598\n",
      "   0.83180935  0.87658084 -0.58692697]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.50452232  1.8841728  -2.23323605  0.26611707  0.49196968 -2.84320009\n",
      "   1.26560828]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:70 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.76591461]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 70 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91879525 -1.13414305  0.30904872  1.00497826 -0.31663763 -0.15337186\n",
      "   1.28222053  0.25159149 -0.61730228]\n",
      " [-0.86828137  0.89427893 -0.51993227 -1.01834371  0.21825619 -0.10777902\n",
      "  -0.71995249 -0.41852993  0.81616242]\n",
      " [ 0.68375405 -0.50852526  0.00254489  0.60405143 -0.45100767  0.27073871\n",
      "   0.72175421  0.0042932  -0.72652293]\n",
      " [ 0.72412628 -0.81427379  0.36236763  0.67347905 -0.34439356  0.10357973\n",
      "   0.91852738  0.88470623 -0.43447915]\n",
      " [-1.02575897  1.11968759 -0.49365364 -1.13817207  0.09875602  0.2480494\n",
      "  -1.00524831 -0.77452016  0.78920865]\n",
      " [ 1.17645127 -1.00966798  0.48191515  0.76523904 -0.18777714 -0.29459598\n",
      "   0.83644122  0.87658084 -0.58692697]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.4835378   1.90277348 -2.22997723  0.28321767  0.50907289 -2.84076836\n",
      "   1.28348593]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:70 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.83465941]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 70 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92092058 -1.13414305  0.31117405  1.00497826 -0.31663763 -0.15124653\n",
      "   1.28434586  0.25159149 -0.61730228]\n",
      " [-0.87052278  0.89427893 -0.52217368 -1.01834371  0.21825619 -0.11002044\n",
      "  -0.7221939  -0.41852993  0.81616242]\n",
      " [ 0.68629231 -0.50852526  0.00508315  0.60405143 -0.45100767  0.27327697\n",
      "   0.72429247  0.0042932  -0.72652293]\n",
      " [ 0.72644876 -0.81427379  0.36469011  0.67347905 -0.34439356  0.10590221\n",
      "   0.92084986  0.88470623 -0.43447915]\n",
      " [-1.02795151  1.11968759 -0.49584618 -1.13817207  0.09875602  0.24585686\n",
      "  -1.00744085 -0.77452016  0.78920865]\n",
      " [ 1.17870503 -1.00966798  0.48416891  0.76523904 -0.18777714 -0.29234222\n",
      "   0.83869498  0.87658084 -0.58692697]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.47212905  1.91319497 -2.22885544  0.29283233  0.51924649 -2.83970645\n",
      "   1.29375687]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:70 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.1827195]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 71 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92033006 -1.13473357  0.31117405  1.00438775 -0.31663763 -0.15124653\n",
      "   1.28434586  0.25159149 -0.61789279]\n",
      " [-0.86991903  0.89488267 -0.52217368 -1.01773996  0.21825619 -0.11002044\n",
      "  -0.7221939  -0.41852993  0.81676617]\n",
      " [ 0.68610386 -0.50871371  0.00508315  0.60386297 -0.45100767  0.27327697\n",
      "   0.72429247  0.0042932  -0.72671139]\n",
      " [ 0.72593607 -0.81478647  0.36469011  0.67296637 -0.34439356  0.10590221\n",
      "   0.92084986  0.88470623 -0.43499184]\n",
      " [-1.02708852  1.12055058 -0.49584618 -1.13730909  0.09875602  0.24585686\n",
      "  -1.00744085 -0.77452016  0.79007163]\n",
      " [ 1.17755533 -1.01081768  0.48416891  0.76408935 -0.18777714 -0.29234222\n",
      "   0.83869498  0.87658084 -0.58807667]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.48577208  1.90577994 -2.23507     0.28582227  0.51191033 -2.84565543\n",
      "   1.2857624 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:71 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.84509723]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 71 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92209437 -1.13473357  0.31293836  1.00438775 -0.31663763 -0.15124653\n",
      "   1.28611017  0.25159149 -0.61789279]\n",
      " [-0.87197939  0.89488267 -0.52423404 -1.01773996  0.21825619 -0.11002044\n",
      "  -0.72425426 -0.41852993  0.81676617]\n",
      " [ 0.68835994 -0.50871371  0.00733923  0.60386297 -0.45100767  0.27327697\n",
      "   0.72654855  0.0042932  -0.72671139]\n",
      " [ 0.72805869 -0.81478647  0.36681272  0.67296637 -0.34439356  0.10590221\n",
      "   0.92297247  0.88470623 -0.43499184]\n",
      " [-1.0288412   1.12055058 -0.49759886 -1.13730909  0.09875602  0.24585686\n",
      "  -1.00919353 -0.77452016  0.79007163]\n",
      " [ 1.17933195 -1.01081768  0.48594553  0.76408935 -0.18777714 -0.29234222\n",
      "   0.8404716   0.87658084 -0.58807667]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.47563308  1.91516101 -2.23397786  0.2939804   0.52085289 -2.84490762\n",
      "   1.29513257]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:71 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.65966607]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 71 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.90566938 -1.15115855  0.29651337  1.00438775 -0.31663763 -0.15124653\n",
      "   1.26968518  0.25159149 -0.61789279]\n",
      " [-0.85605688  0.91080519 -0.50831152 -1.01773996  0.21825619 -0.11002044\n",
      "  -0.70833175 -0.41852993  0.81676617]\n",
      " [ 0.67453814 -0.52253551 -0.00648257  0.60386297 -0.45100767  0.27327697\n",
      "   0.71272675  0.0042932  -0.72671139]\n",
      " [ 0.71223679 -0.83060837  0.35099083  0.67296637 -0.34439356  0.10590221\n",
      "   0.90715058  0.88470623 -0.43499184]\n",
      " [-1.01236464  1.13702714 -0.4811223  -1.13730909  0.09875602  0.24585686\n",
      "  -0.99271697 -0.77452016  0.79007163]\n",
      " [ 1.16276837 -1.02738126  0.46938195  0.76408935 -0.18777714 -0.29234222\n",
      "   0.82390802  0.87658084 -0.58807667]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.54968282  1.85591985 -2.25078383  0.24112941  0.46390359 -2.85937942\n",
      "   1.23464746]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:71 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.74457021]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 71 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91109114 -1.15115855  0.30193513  1.00438775 -0.31663763 -0.15124653\n",
      "   1.26968518  0.25701324 -0.61789279]\n",
      " [-0.86139324  0.91080519 -0.51364788 -1.01773996  0.21825619 -0.11002044\n",
      "  -0.70833175 -0.42386629  0.81676617]\n",
      " [ 0.67819215 -0.52253551 -0.00282856  0.60386297 -0.45100767  0.27327697\n",
      "   0.71272675  0.00794721 -0.72671139]\n",
      " [ 0.71740395 -0.83060837  0.35615798  0.67296637 -0.34439356  0.10590221\n",
      "   0.90715058  0.88987339 -0.43499184]\n",
      " [-1.01704761  1.13702714 -0.48580526 -1.13730909  0.09875602  0.24585686\n",
      "  -0.99271697 -0.77920313  0.79007163]\n",
      " [ 1.16700866 -1.02738126  0.47362223  0.76408935 -0.18777714 -0.29234222\n",
      "   0.82390802  0.88082113 -0.58807667]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.52539331  1.87560841 -2.24728746  0.25720976  0.48516228 -2.85710089\n",
      "   1.25710923]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:71 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.57435932]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 71 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92246885 -1.15115855  0.30193513  1.01576546 -0.31663763 -0.15124653\n",
      "   1.26968518  0.25701324 -0.60651508]\n",
      " [-0.87194387  0.91080519 -0.51364788 -1.0282906   0.21825619 -0.11002044\n",
      "  -0.70833175 -0.42386629  0.80621553]\n",
      " [ 0.68488604 -0.52253551 -0.00282856  0.61055686 -0.45100767  0.27327697\n",
      "   0.71272675  0.00794721 -0.7200175 ]\n",
      " [ 0.72737657 -0.83060837  0.35615798  0.68293899 -0.34439356  0.10590221\n",
      "   0.90715058  0.88987339 -0.42501921]\n",
      " [-1.02855472  1.13702714 -0.48580526 -1.1488162   0.09875602  0.24585686\n",
      "  -0.99271697 -0.77920313  0.77856452]\n",
      " [ 1.17847929 -1.02738126  0.47362223  0.77555998 -0.18777714 -0.29234222\n",
      "   0.82390802  0.88082113 -0.57660603]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.47336498  1.91647294 -2.2339236   0.29026724  0.52273691 -2.84651079\n",
      "   1.29836699]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:71 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.10612876]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 71 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92265895 -1.15115855  0.30193513  1.01576546 -0.31644753 -0.15105643\n",
      "   1.26968518  0.25701324 -0.60632498]\n",
      " [-0.87199735  0.91080519 -0.51364788 -1.0282906   0.21820271 -0.11007391\n",
      "  -0.70833175 -0.42386629  0.80616206]\n",
      " [ 0.68515092 -0.52253551 -0.00282856  0.61055686 -0.4507428   0.27354185\n",
      "   0.71272675  0.00794721 -0.71975263]\n",
      " [ 0.72729628 -0.83060837  0.35615798  0.68293899 -0.34447386  0.10582192\n",
      "   0.90715058  0.88987339 -0.42509951]\n",
      " [-1.02867354  1.13702714 -0.48580526 -1.1488162   0.0986372   0.24573805\n",
      "  -0.99271697 -0.77920313  0.77844571]\n",
      " [ 1.17832663 -1.02738126  0.47362223  0.77555998 -0.1879298  -0.29249488\n",
      "   0.82390802  0.88082113 -0.57675869]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.47839895  1.91414679 -2.23649407  0.28801713  0.52013958 -2.84914677\n",
      "   1.29569696]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:71 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.01396027]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 71 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92267998 -1.15113752  0.30193513  1.01576546 -0.31642649 -0.15103539\n",
      "   1.26968518  0.25701324 -0.60630394]\n",
      " [-0.87201574  0.91078679 -0.51364788 -1.0282906   0.21818432 -0.1100923\n",
      "  -0.70833175 -0.42386629  0.80614367]\n",
      " [ 0.68516638 -0.52252005 -0.00282856  0.61055686 -0.45072734  0.27355731\n",
      "   0.71272675  0.00794721 -0.71973717]\n",
      " [ 0.72731224 -0.83059241  0.35615798  0.68293899 -0.3444579   0.10583788\n",
      "   0.90715058  0.88987339 -0.42508354]\n",
      " [-1.02869423  1.13700644 -0.48580526 -1.1488162   0.09861651  0.24571736\n",
      "  -0.99271697 -0.77920313  0.77842501]\n",
      " [ 1.17834448 -1.02736341  0.47362223  0.77555998 -0.18791195 -0.29247703\n",
      "   0.82390802  0.88082113 -0.57674085]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.47849504  1.91412625 -2.23656342  0.28798598  0.52010911 -2.84922114\n",
      "   1.29566931]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:71 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.31287737]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 71 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91823199 -1.15558551  0.30193513  1.01576546 -0.32087448 -0.15548338\n",
      "   1.26523719  0.25701324 -0.60630394]\n",
      " [-0.86764826  0.91515427 -0.51364788 -1.0282906   0.2225518  -0.10572482\n",
      "  -0.70396427 -0.42386629  0.80614367]\n",
      " [ 0.67995698 -0.52772945 -0.00282856  0.61055686 -0.45593674  0.26834791\n",
      "   0.70751735  0.00794721 -0.71973717]\n",
      " [ 0.7229199  -0.83498474  0.35615798  0.68293899 -0.34885023  0.10144554\n",
      "   0.90275824  0.88987339 -0.42508354]\n",
      " [-1.02446898  1.14123169 -0.48580526 -1.1488162   0.10284176  0.24994261\n",
      "  -0.98849172 -0.77920313  0.77842501]\n",
      " [ 1.17443089 -1.03127701  0.47362223  0.77555998 -0.19182554 -0.29639063\n",
      "   0.81999442  0.88082113 -0.57674085]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.51212703  1.89261417 -2.24877882  0.2655269   0.49866312 -2.86160347\n",
      "   1.27477826]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:71 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.8723118]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 71 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91908874 -1.15558551  0.30193513  1.0166222  -0.32087448 -0.15548338\n",
      "   1.26609393  0.25701324 -0.60630394]\n",
      " [-0.86883824  0.91515427 -0.51364788 -1.02948058  0.2225518  -0.10572482\n",
      "  -0.70515425 -0.42386629  0.80614367]\n",
      " [ 0.68145101 -0.52772945 -0.00282856  0.61205089 -0.45593674  0.26834791\n",
      "   0.70901138  0.00794721 -0.71973717]\n",
      " [ 0.72427    -0.83498474  0.35615798  0.68428909 -0.34885023  0.10144554\n",
      "   0.90410834  0.88987339 -0.42508354]\n",
      " [-1.02534538  1.14123169 -0.48580526 -1.1496926   0.10284176  0.24994261\n",
      "  -0.98936812 -0.77920313  0.77842501]\n",
      " [ 1.17552394 -1.03127701  0.47362223  0.77665304 -0.19182554 -0.29639063\n",
      "   0.82108747  0.88082113 -0.57674085]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.50501582  1.89944665 -2.24828712  0.27178896  0.50513138 -2.86131454\n",
      "   1.28147014]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:71 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.70767301]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 71 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92579603 -1.15558551  0.30193513  1.0166222  -0.31416719 -0.14877609\n",
      "   1.27280123  0.25701324 -0.60630394]\n",
      " [-0.87558908  0.91515427 -0.51364788 -1.02948058  0.21580096 -0.11247566\n",
      "  -0.71190509 -0.42386629  0.80614367]\n",
      " [ 0.6879113  -0.52772945 -0.00282856  0.61205089 -0.44947645  0.2748082\n",
      "   0.71547167  0.00794721 -0.71973717]\n",
      " [ 0.7309724  -0.83498474  0.35615798  0.68428909 -0.34214783  0.10814794\n",
      "   0.91081074  0.88987339 -0.42508354]\n",
      " [-1.03208225  1.14123169 -0.48580526 -1.1496926   0.09610488  0.24320573\n",
      "  -0.996105   -0.77920313  0.77842501]\n",
      " [ 1.18229019 -1.03127701  0.47362223  0.77665304 -0.18505929 -0.28962437\n",
      "   0.82785373  0.88082113 -0.57674085]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.4747787   1.92504793 -2.24257529  0.29504238  0.5292953  -2.85649163\n",
      "   1.306229  ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:71 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.59537196]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 71 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91054566 -1.15558551  0.30193513  1.00137183 -0.31416719 -0.16402646\n",
      "   1.27280123  0.25701324 -0.62155431]\n",
      " [-0.86022934  0.91515427 -0.51364788 -1.01412084  0.21580096 -0.09711592\n",
      "  -0.71190509 -0.42386629  0.82150341]\n",
      " [ 0.67507453 -0.52772945 -0.00282856  0.59921412 -0.44947645  0.26197143\n",
      "   0.71547167  0.00794721 -0.73257394]\n",
      " [ 0.71620183 -0.83498474  0.35615798  0.66951851 -0.34214783  0.09337737\n",
      "   0.91081074  0.88987339 -0.43985412]\n",
      " [-1.01696582  1.14123169 -0.48580526 -1.13457617  0.09610488  0.25832217\n",
      "  -0.996105   -0.77920313  0.79354145]\n",
      " [ 1.16755473 -1.03127701  0.47362223  0.76191757 -0.18505929 -0.30435984\n",
      "   0.82785373  0.88082113 -0.59147631]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.5464925   1.87009618 -2.25903024  0.2447264   0.4755138  -2.87360549\n",
      "   1.25252491]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:71 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.71640925]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 71 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91678502 -1.14934615  0.30193513  1.00761119 -0.31416719 -0.1577871\n",
      "   1.27904059  0.25701324 -0.62155431]\n",
      " [-0.86657212  0.9088115  -0.51364788 -1.02046362  0.21580096 -0.1034587\n",
      "  -0.71824787 -0.42386629  0.82150341]\n",
      " [ 0.68145392 -0.52135006 -0.00282856  0.60559351 -0.44947645  0.26835082\n",
      "   0.72185106  0.00794721 -0.73257394]\n",
      " [ 0.72265085 -0.82853572  0.35615798  0.67596753 -0.34214783  0.09982639\n",
      "   0.91725976  0.88987339 -0.43985412]\n",
      " [-1.02332613  1.13487138 -0.48580526 -1.14093648  0.09610488  0.25196186\n",
      "  -1.00246531 -0.77920313  0.79354145]\n",
      " [ 1.17396875 -1.02486298  0.47362223  0.76833159 -0.18505929 -0.29794581\n",
      "   0.83426775  0.88082113 -0.59147631]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.51768435  1.8950409  -2.25483105  0.26917425  0.49929849 -2.86933359\n",
      "   1.27573284]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:71 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.7674724]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 71 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92107783 -1.14505334  0.30193513  1.011904   -0.31416719 -0.1577871\n",
      "   1.2833334   0.25701324 -0.62155431]\n",
      " [-0.8711805   0.90420311 -0.51364788 -1.025072    0.21580096 -0.1034587\n",
      "  -0.72285626 -0.42386629  0.82150341]\n",
      " [ 0.68609362 -0.51671035 -0.00282856  0.61023322 -0.44947645  0.26835082\n",
      "   0.72649077  0.00794721 -0.73257394]\n",
      " [ 0.72729052 -0.82389605  0.35615798  0.6806072  -0.34214783  0.09982639\n",
      "   0.92189943  0.88987339 -0.43985412]\n",
      " [-1.02764585  1.13055167 -0.48580526 -1.14525619  0.09610488  0.25196186\n",
      "  -1.00678502 -0.77920313  0.79354145]\n",
      " [ 1.1785474  -1.02028434  0.47362223  0.77291024 -0.18505929 -0.29794581\n",
      "   0.8388464   0.88082113 -0.59147631]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.49693609  1.91343134 -2.25161659  0.28609882  0.51622242 -2.86692889\n",
      "   1.29341384]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:71 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.83524756]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 71 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 9.23196455e-01 -1.14505334e+00  3.04053752e-01  1.01190400e+00\n",
      "  -3.14167191e-01 -1.55668475e-01  1.28545202e+00  2.57013244e-01\n",
      "  -6.21554312e-01]\n",
      " [-8.73411326e-01  9.04203110e-01 -5.15878706e-01 -1.02507200e+00\n",
      "   2.15800961e-01 -1.05689518e-01 -7.25087078e-01 -4.23866290e-01\n",
      "   8.21503412e-01]\n",
      " [ 6.88615793e-01 -5.16710353e-01 -3.06386668e-04  6.10233219e-01\n",
      "  -4.49476448e-01  2.70872990e-01  7.29012940e-01  7.94721001e-03\n",
      "  -7.32573937e-01]\n",
      " [ 7.29600567e-01 -8.23896051e-01  3.58468034e-01  6.80607200e-01\n",
      "  -3.42147831e-01  1.02136442e-01  9.24209480e-01  8.89873390e-01\n",
      "  -4.39854120e-01]\n",
      " [-1.02983113e+00  1.13055167e+00 -4.87990550e-01 -1.14525619e+00\n",
      "   9.61048849e-02  2.49776570e-01 -1.00897031e+00 -7.79203125e-01\n",
      "   7.93541448e-01]\n",
      " [ 1.18079224e+00 -1.02028434e+00  4.75867080e-01  7.72910240e-01\n",
      "  -1.85059292e-01 -2.95700968e-01  8.41091242e-01  8.80821128e-01\n",
      "  -5.91476311e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.48560037  1.9237788  -2.25049714  0.2956509   0.52632713 -2.86586578\n",
      "   1.30361174]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:71 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.17812118]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 72 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 9.22651135e-01 -1.14559866e+00  3.04053752e-01  1.01135868e+00\n",
      "  -3.14167191e-01 -1.55668475e-01  1.28545202e+00  2.57013244e-01\n",
      "  -6.22099632e-01]\n",
      " [-8.72852345e-01  9.04762091e-01 -5.15878706e-01 -1.02451302e+00\n",
      "   2.15800961e-01 -1.05689518e-01 -7.25087078e-01 -4.23866290e-01\n",
      "   8.22062394e-01]\n",
      " [ 6.88454337e-01 -5.16871809e-01 -3.06386668e-04  6.10071763e-01\n",
      "  -4.49476448e-01  2.70872990e-01  7.29012940e-01  7.94721001e-03\n",
      "  -7.32735393e-01]\n",
      " [ 7.29125742e-01 -8.24370877e-01  3.58468034e-01  6.80132375e-01\n",
      "  -3.42147831e-01  1.02136442e-01  9.24209480e-01  8.89873390e-01\n",
      "  -4.40328946e-01]\n",
      " [-1.02902577e+00  1.13135703e+00 -4.87990550e-01 -1.14445083e+00\n",
      "   9.61048849e-02  2.49776570e-01 -1.00897031e+00 -7.79203125e-01\n",
      "   7.94346809e-01]\n",
      " [ 1.17970964e+00 -1.02136694e+00  4.75867080e-01  7.71827641e-01\n",
      "  -1.85059292e-01 -2.95700968e-01  8.41091242e-01  8.80821128e-01\n",
      "  -5.92558910e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.49863831  1.91671193 -2.25645434  0.28897041  0.51933164 -2.8715709\n",
      "   1.29598895]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:72 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.8463838]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 72 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.9243918  -1.14559866  0.30579442  1.01135868 -0.31416719 -0.15566848\n",
      "   1.28719269  0.25701324 -0.62209963]\n",
      " [-0.874882    0.90476209 -0.51790836 -1.02451302  0.21580096 -0.10568952\n",
      "  -0.72711673 -0.42386629  0.82206239]\n",
      " [ 0.69067684 -0.51687181  0.00191611  0.61007176 -0.44947645  0.27087299\n",
      "   0.73123544  0.00794721 -0.73273539]\n",
      " [ 0.73121624 -0.82437088  0.36055853  0.68013237 -0.34214783  0.10213644\n",
      "   0.92629998  0.88987339 -0.44032895]\n",
      " [-1.03075554  1.13135703 -0.48972032 -1.14445083  0.09610488  0.24977657\n",
      "  -1.01070008 -0.77920313  0.79434681]\n",
      " [ 1.18146248 -1.02136694  0.47761991  0.77182764 -0.18505929 -0.29570097\n",
      "   0.84284407  0.88082113 -0.59255891]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.48865185  1.9259493  -2.25537819  0.29700844  0.52814    -2.87083134\n",
      "   1.30521552]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:72 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.65561311]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 72 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.9080054  -1.16198507  0.28940801  1.01135868 -0.31416719 -0.15566848\n",
      "   1.27080628  0.25701324 -0.62209963]\n",
      " [-0.8590129   0.92063119 -0.50203926 -1.02451302  0.21580096 -0.10568952\n",
      "  -0.71124763 -0.42386629  0.82206239]\n",
      " [ 0.67692275 -0.5306259  -0.01183798  0.61007176 -0.44947645  0.27087299\n",
      "   0.71748135  0.00794721 -0.73273539]\n",
      " [ 0.71544607 -0.84014104  0.34478836  0.68013237 -0.34214783  0.10213644\n",
      "   0.91052981  0.88987339 -0.44032895]\n",
      " [-1.01431308  1.14779949 -0.47327786 -1.14445083  0.09610488  0.24977657\n",
      "  -0.99425762 -0.77920313  0.79434681]\n",
      " [ 1.16491664 -1.03791277  0.46107408  0.77182764 -0.18505929 -0.29570097\n",
      "   0.82629824  0.88082113 -0.59255891]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.56266551  1.866911   -2.27231192  0.24428281  0.47134171 -2.88547456\n",
      "   1.24491977]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:72 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.74626182]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 72 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91336796 -1.16198507  0.29477058  1.01135868 -0.31416719 -0.15566848\n",
      "   1.27080628  0.26237581 -0.62209963]\n",
      " [-0.86428916  0.92063119 -0.50731552 -1.02451302  0.21580096 -0.10568952\n",
      "  -0.71124763 -0.42914255  0.82206239]\n",
      " [ 0.68053958 -0.5306259  -0.00822114  0.61007176 -0.44947645  0.27087299\n",
      "   0.71748135  0.01156405 -0.73273539]\n",
      " [ 0.72055396 -0.84014104  0.34989625  0.68013237 -0.34214783  0.10213644\n",
      "   0.91052981  0.89498128 -0.44032895]\n",
      " [-1.01894681  1.14779949 -0.47791159 -1.14445083  0.09610488  0.24977657\n",
      "  -0.99425762 -0.78383686  0.79434681]\n",
      " [ 1.16911409 -1.03791277  0.46527152  0.77182764 -0.18505929 -0.29570097\n",
      "   0.82629824  0.88501857 -0.59255891]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Theta two: \n",
      "[[-1.5386422   1.88638618 -2.26885985  0.26019061  0.49237312 -2.88321852\n",
      "   1.26713215]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:72 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.57604105]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 72 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92470027 -1.16198507  0.29477058  1.02269099 -0.31416719 -0.15566848\n",
      "   1.27080628  0.26237581 -0.61076733]\n",
      " [-0.87480802  0.92063119 -0.50731552 -1.03503188  0.21580096 -0.10568952\n",
      "  -0.71124763 -0.42914255  0.81154353]\n",
      " [ 0.6872259  -0.5306259  -0.00822114  0.61675808 -0.44947645  0.27087299\n",
      "   0.71748135  0.01156405 -0.72604907]\n",
      " [ 0.73050652 -0.84014104  0.34989625  0.69008494 -0.34214783  0.10213644\n",
      "   0.91052981  0.89498128 -0.43037638]\n",
      " [-1.03040414  1.14779949 -0.47791159 -1.15590816  0.09610488  0.24977657\n",
      "  -0.99425762 -0.78383686  0.78288949]\n",
      " [ 1.18053715 -1.03791277  0.46527152  0.7832507  -0.18505929 -0.29570097\n",
      "   0.82629824  0.88501857 -0.58113585]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.48687305  1.92709107 -2.25560572  0.29311377  0.52981222 -2.87272112\n",
      "   1.30822988]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:72 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.10312043]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 72 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92488501 -1.16198507  0.29477058  1.02269099 -0.31398246 -0.15548374\n",
      "   1.27080628  0.26237581 -0.61058259]\n",
      " [-0.87486383  0.92063119 -0.50731552 -1.03503188  0.21574514 -0.10574534\n",
      "  -0.71124763 -0.42914255  0.81148772]\n",
      " [ 0.68748207 -0.5306259  -0.00822114  0.61675808 -0.44922028  0.27112916\n",
      "   0.71748135  0.01156405 -0.72579291]\n",
      " [ 0.73043492 -0.84014104  0.34989625  0.69008494 -0.34221944  0.10206484\n",
      "   0.91052981  0.89498128 -0.43044799]\n",
      " [-1.03052112  1.14779949 -0.47791159 -1.15590816  0.0959879   0.24965958\n",
      "  -0.99425762 -0.78383686  0.7827725 ]\n",
      " [ 1.18039621 -1.03791277  0.46527152  0.7832507  -0.18520023 -0.29584191\n",
      "   0.82629824  0.88501857 -0.58127679]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.49164168  1.92489224 -2.25804588  0.29098765  0.52735626 -2.87522261\n",
      "   1.30570429]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:72 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.01309392]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 72 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92490358 -1.1619665   0.29477058  1.02269099 -0.31396388 -0.15546517\n",
      "   1.27080628  0.26237581 -0.61056402]\n",
      " [-0.87488017  0.92061486 -0.50731552 -1.03503188  0.21572881 -0.10576167\n",
      "  -0.71124763 -0.42914255  0.81147138]\n",
      " [ 0.68749585 -0.53061211 -0.00822114  0.61675808 -0.44920649  0.27114294\n",
      "   0.71748135  0.01156405 -0.72577912]\n",
      " [ 0.73044914 -0.84012682  0.34989625  0.69008494 -0.34220521  0.10207906\n",
      "   0.91052981  0.89498128 -0.43043376]\n",
      " [-1.03053941  1.14778121 -0.47791159 -1.15590816  0.09596961  0.2496413\n",
      "  -0.99425762 -0.78383686  0.78275421]\n",
      " [ 1.18041206 -1.03789691  0.46527152  0.7832507  -0.18518438 -0.29582605\n",
      "   0.82629824  0.88501857 -0.58126093]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.49172628  1.92487436 -2.25810718  0.29096045  0.52732967 -2.87528831\n",
      "   1.30568018]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:72 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.30714467]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 72 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92064138 -1.1662287   0.29477058  1.02269099 -0.31822608 -0.15972737\n",
      "   1.26654409  0.26237581 -0.61056402]\n",
      " [-0.87067414  0.92482088 -0.50731552 -1.03503188  0.21993483 -0.10155565\n",
      "  -0.70704161 -0.42914255  0.81147138]\n",
      " [ 0.68244429 -0.53566368 -0.00822114  0.61675808 -0.45425806  0.26609138\n",
      "   0.71242978  0.01156405 -0.72577912]\n",
      " [ 0.72620988 -0.84436608  0.34989625  0.69008494 -0.34644447  0.0978398\n",
      "   0.90629055  0.89498128 -0.43043376]\n",
      " [-1.02649031  1.1518303  -0.47791159 -1.15590816  0.10001871  0.25369039\n",
      "  -0.99020853 -0.78383686  0.78275421]\n",
      " [ 1.17665456 -1.04165442  0.46527152  0.7832507  -0.18894188 -0.29958356\n",
      "   0.82254074  0.88501857 -0.58126093]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.52440752  1.9040417  -2.27002206  0.26915006  0.50652412 -2.88738655\n",
      "   1.28543125]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:72 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.87476769]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 72 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92146245 -1.1662287   0.29477058  1.02351207 -0.31822608 -0.15972737\n",
      "   1.26736516  0.26237581 -0.61056402]\n",
      " [-0.87181498  0.92482088 -0.50731552 -1.03617272  0.21993483 -0.10155565\n",
      "  -0.70818244 -0.42914255  0.81147138]\n",
      " [ 0.68388029 -0.53566368 -0.00822114  0.61819408 -0.45425806  0.26609138\n",
      "   0.71386578  0.01156405 -0.72577912]\n",
      " [ 0.72750515 -0.84436608  0.34989625  0.69138021 -0.34644447  0.0978398\n",
      "   0.90758582  0.89498128 -0.43043376]\n",
      " [-1.0273302   1.1518303  -0.47791159 -1.15674804  0.10001871  0.25369039\n",
      "  -0.99104841 -0.78383686  0.78275421]\n",
      " [ 1.17770208 -1.04165442  0.46527152  0.78429823 -0.18894188 -0.29958356\n",
      "   0.82358826  0.88501857 -0.58126093]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.51754797  1.91063513 -2.2695534   0.27520027  0.51277132 -2.88711073\n",
      "   1.29189102]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:72 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.70990245]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 72 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92808716 -1.1662287   0.29477058  1.02351207 -0.31160137 -0.15310266\n",
      "   1.27398987  0.26237581 -0.61056402]\n",
      " [-0.87848604  0.92482088 -0.50731552 -1.03617272  0.21326377 -0.1082267\n",
      "  -0.7148535  -0.42914255  0.81147138]\n",
      " [ 0.69027487 -0.53566368 -0.00822114  0.61819408 -0.44786347  0.27248596\n",
      "   0.72026036  0.01156405 -0.72577912]\n",
      " [ 0.73413101 -0.84436608  0.34989625  0.69138021 -0.33981861  0.10446566\n",
      "   0.91421168  0.89498128 -0.43043376]\n",
      " [-1.03398414  1.1518303  -0.47791159 -1.15674804  0.09336476  0.24703645\n",
      "  -0.99770236 -0.78383686  0.78275421]\n",
      " [ 1.18438721 -1.04165442  0.46527152  0.78429823 -0.18225676 -0.29289843\n",
      "   0.83027339  0.88501857 -0.58126093]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.48767649  1.93593477 -2.26393099  0.29820802  0.53666943 -2.8823571\n",
      "   1.31636982]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:72 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.59560298]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 72 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91283087 -1.1662287   0.29477058  1.00825577 -0.31160137 -0.16835895\n",
      "   1.27398987  0.26237581 -0.62582032]\n",
      " [-0.86312309  0.92482088 -0.50731552 -1.02080977  0.21326377 -0.09286376\n",
      "  -0.7148535  -0.42914255  0.82683433]\n",
      " [ 0.67743407 -0.53566368 -0.00822114  0.60535328 -0.44786347  0.25964516\n",
      "   0.72026036  0.01156405 -0.73861992]\n",
      " [ 0.71935004 -0.84436608  0.34989625  0.67659924 -0.33981861  0.08968469\n",
      "   0.91421168  0.89498128 -0.44521474]\n",
      " [-1.01886053  1.1518303  -0.47791159 -1.14162442  0.09336476  0.26216006\n",
      "  -0.99770236 -0.78383686  0.79787783]\n",
      " [ 1.16963677 -1.04165442  0.46527152  0.76954779 -0.18225676 -0.30764888\n",
      "   0.83027339  0.88501857 -0.59601138]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.55940498  1.88096413 -2.28038916  0.2478796   0.48286055 -2.89946407\n",
      "   1.26262847]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:72 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.71710543]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 72 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91905125 -1.16000832  0.29477058  1.01447615 -0.31160137 -0.16213858\n",
      "   1.28021025  0.26237581 -0.62582032]\n",
      " [-0.86944215  0.91850182 -0.50731552 -1.02712883  0.21326377 -0.09918282\n",
      "  -0.72117256 -0.42914255  0.82683433]\n",
      " [ 0.68378607 -0.52931167 -0.00822114  0.61170528 -0.44786347  0.26599717\n",
      "   0.72661237  0.01156405 -0.73861992]\n",
      " [ 0.72577365 -0.83794247  0.34989625  0.68302284 -0.33981861  0.09610829\n",
      "   0.92063529  0.89498128 -0.44521474]\n",
      " [-1.02519902  1.14549181 -0.47791159 -1.14796291  0.09336476  0.25582157\n",
      "  -1.00404084 -0.78383686  0.79787783]\n",
      " [ 1.17602463 -1.03526655  0.46527152  0.77593565 -0.18225676 -0.30126101\n",
      "   0.83666125  0.88501857 -0.59601138]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.53071024  1.90579523 -2.27620154  0.27224235  0.5065539  -2.89519485\n",
      "   1.28573814]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:72 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.76901319]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 72 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.9232963  -1.15576327  0.29477058  1.0187212  -0.31160137 -0.16213858\n",
      "   1.2844553   0.26237581 -0.62582032]\n",
      " [-0.87399752  0.91394646 -0.50731552 -1.0316842   0.21326377 -0.09918282\n",
      "  -0.72572793 -0.42914255  0.82683433]\n",
      " [ 0.68837462 -0.52472312 -0.00822114  0.61629383 -0.44786347  0.26599717\n",
      "   0.73120092  0.01156405 -0.73861992]\n",
      " [ 0.73036197 -0.83335415  0.34989625  0.68761117 -0.33981861  0.09610829\n",
      "   0.92522361  0.89498128 -0.44521474]\n",
      " [-1.0294704   1.14122043 -0.47791159 -1.1522343   0.09336476  0.25582157\n",
      "  -1.00831223 -0.78383686  0.79787783]\n",
      " [ 1.18055087 -1.03074032  0.46527152  0.78046189 -0.18225676 -0.30126101\n",
      "   0.84118749  0.88501857 -0.59601138]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.51019493  1.92397844 -2.2730306   0.28899304  0.52330058 -2.89281685\n",
      "   1.30322489]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:72 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.83579725]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 72 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92540906 -1.15576327  0.29688335  1.0187212  -0.31160137 -0.16002581\n",
      "   1.28656807  0.26237581 -0.62582032]\n",
      " [-0.87621875  0.91394646 -0.50953675 -1.0316842   0.21326377 -0.10140405\n",
      "  -0.72794916 -0.42914255  0.82683433]\n",
      " [ 0.69088181 -0.52472312 -0.00571396  0.61629383 -0.44786347  0.26850435\n",
      "   0.7337081   0.01156405 -0.73861992]\n",
      " [ 0.73266067 -0.83335415  0.35219495  0.68761117 -0.33981861  0.098407\n",
      "   0.92752231  0.89498128 -0.44521474]\n",
      " [-1.03164932  1.14122043 -0.48009051 -1.1522343   0.09336476  0.25364265\n",
      "  -1.01049115 -0.78383686  0.79787783]\n",
      " [ 1.18278782 -1.03074032  0.46750847  0.78046189 -0.18225676 -0.29902406\n",
      "   0.84342444  0.88501857 -0.59601138]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.49892732  1.93425638 -2.27191293  0.29848661  0.53334067 -2.89175205\n",
      "   1.31335405]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:72 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.17361265]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 73 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.9249063  -1.15626604  0.29688335  1.01821843 -0.31160137 -0.16002581\n",
      "   1.28656807  0.26237581 -0.62632309]\n",
      " [-0.87570202  0.91446319 -0.50953675 -1.03116747  0.21326377 -0.10140405\n",
      "  -0.72794916 -0.42914255  0.82735105]\n",
      " [ 0.6907454  -0.52485953 -0.00571396  0.61615742 -0.44786347  0.26850435\n",
      "   0.7337081   0.01156405 -0.73875633]\n",
      " [ 0.73222168 -0.83379314  0.35219495  0.68717217 -0.33981861  0.098407\n",
      "   0.92752231  0.89498128 -0.44565373]\n",
      " [-1.03089847  1.14197128 -0.48009051 -1.15148345  0.09336476  0.25364265\n",
      "  -1.01049115 -0.78383686  0.79862868]\n",
      " [ 1.18176922 -1.03175891  0.46750847  0.77944329 -0.18225676 -0.29902406\n",
      "   0.84342444  0.88501857 -0.59702997]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.51138153  1.92752429 -2.2776209   0.29212305  0.5266731  -2.89722078\n",
      "   1.30608901]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:73 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.84762695]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 73 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92662436 -1.15626604  0.29860142  1.01821843 -0.31160137 -0.16002581\n",
      "   1.28828614  0.26237581 -0.62632309]\n",
      " [-0.87770228  0.91446319 -0.51153701 -1.03116747  0.21326377 -0.10140405\n",
      "  -0.72994942 -0.42914255  0.82735105]\n",
      " [ 0.69293561 -0.52485953 -0.00352374  0.61615742 -0.44786347  0.26850435\n",
      "   0.73589832  0.01156405 -0.73875633]\n",
      " [ 0.73428142 -0.83379314  0.3542547   0.68717217 -0.33981861  0.098407\n",
      "   0.92958205  0.89498128 -0.44565373]\n",
      " [-1.03260636  1.14197128 -0.4817984  -1.15148345  0.09336476  0.25364265\n",
      "  -1.01219904 -0.78383686  0.79862868]\n",
      " [ 1.18349939 -1.03175891  0.46923865  0.77944329 -0.18225676 -0.29902406\n",
      "   0.84515461  0.88501857 -0.59702997]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.50154162  1.93662351 -2.27655995  0.30004556  0.53535234 -2.89648903\n",
      "   1.31517743]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:73 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.65143932]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 73 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91028312 -1.17260728  0.28226017  1.01821843 -0.31160137 -0.16002581\n",
      "   1.27194489  0.26237581 -0.62632309]\n",
      " [-0.86189188  0.93027359 -0.49572661 -1.03116747  0.21326377 -0.10140405\n",
      "  -0.71413902 -0.42914255  0.82735105]\n",
      " [ 0.67925246 -0.53854269 -0.01720689  0.61615742 -0.44786347  0.26850435\n",
      "   0.72221516  0.01156405 -0.73875633]\n",
      " [ 0.71856815 -0.84950641  0.33854143  0.68717217 -0.33981861  0.098407\n",
      "   0.91386878  0.89498128 -0.44565373]\n",
      " [-1.01620486  1.15837278 -0.4653969  -1.15148345  0.09336476  0.25364265\n",
      "  -0.99579754 -0.78383686  0.79862868]\n",
      " [ 1.16697788 -1.04828042  0.45271714  0.77944329 -0.18225676 -0.29902406\n",
      "   0.8286331   0.88501857 -0.59702997]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.57550153  1.87780233 -2.29361774  0.24745695  0.47871943 -2.91130069\n",
      "   1.2550873 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:73 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.74791427]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 73 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91558796 -1.17260728  0.28756501  1.01821843 -0.31160137 -0.16002581\n",
      "   1.27194489  0.26768065 -0.62632309]\n",
      " [-0.8671097   0.93027359 -0.50094443 -1.03116747  0.21326377 -0.10140405\n",
      "  -0.71413902 -0.43436037  0.82735105]\n",
      " [ 0.68283264 -0.53854269 -0.01362671  0.61615742 -0.44786347  0.26850435\n",
      "   0.72221516  0.01514423 -0.73875633]\n",
      " [ 0.7236185  -0.84950641  0.34359178  0.68717217 -0.33981861  0.098407\n",
      "   0.91386878  0.90003163 -0.44565373]\n",
      " [-1.02079087  1.15837278 -0.4699829  -1.15148345  0.09336476  0.25364265\n",
      "  -0.99579754 -0.78842286  0.79862868]\n",
      " [ 1.17113393 -1.04828042  0.45687318  0.77944329 -0.18225676 -0.29902406\n",
      "   0.8286331   0.88917462 -0.59702997]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.55173759  1.89706904 -2.29020832  0.26319607  0.4995289  -2.90906626\n",
      "   1.27705639]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:73 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.5776982]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 73 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92687476 -1.17260728  0.28756501  1.02950523 -0.31160137 -0.16002581\n",
      "   1.27194489  0.26768065 -0.61503628]\n",
      " [-0.87759612  0.93027359 -0.50094443 -1.04165388  0.21326377 -0.10140405\n",
      "  -0.71413902 -0.43436037  0.81686464]\n",
      " [ 0.68950971 -0.53854269 -0.01362671  0.62283448 -0.44786347  0.26850435\n",
      "   0.72221516  0.01514423 -0.73207927]\n",
      " [ 0.73354985 -0.84950641  0.34359178  0.69710352 -0.33981861  0.098407\n",
      "   0.91386878  0.90003163 -0.43572238]\n",
      " [-1.03219852  1.15837278 -0.4699829  -1.1628911   0.09336476  0.25364265\n",
      "  -0.99579754 -0.78842286  0.78722102]\n",
      " [ 1.18250937 -1.04828042  0.45687318  0.79081874 -0.18225676 -0.29902406\n",
      "   0.8286331   0.88917462 -0.58565453]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.50022459  1.9376145  -2.27706081  0.29598443  0.536832   -2.89865901\n",
      "   1.31799423]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:73 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.10021584]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 73 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92705411 -1.17260728  0.28756501  1.02950523 -0.31142202 -0.15984646\n",
      "   1.27194489  0.26768065 -0.61485694]\n",
      " [-0.87765383  0.93027359 -0.50094443 -1.04165388  0.21320606 -0.10146177\n",
      "  -0.71413902 -0.43436037  0.81680693]\n",
      " [ 0.68975734 -0.53854269 -0.01362671  0.62283448 -0.44761585  0.26875198\n",
      "   0.72221516  0.01514423 -0.73183164]\n",
      " [ 0.73348617 -0.84950641  0.34359178  0.69710352 -0.33988229  0.09834332\n",
      "   0.91386878  0.90003163 -0.43578606]\n",
      " [-1.03231347  1.15837278 -0.4699829  -1.1628911   0.09324981  0.2535277\n",
      "  -0.99579754 -0.78842286  0.78710607]\n",
      " [ 1.18237926 -1.04828042  0.45687318  0.79081874 -0.18238687 -0.29915418\n",
      "   0.8286331   0.88917462 -0.58578464]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.50474295  1.93553543 -2.27937773  0.29397492  0.5345091  -2.90103334\n",
      "   1.31560464]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:73 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.01228829]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 73 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92707052 -1.17259087  0.28756501  1.02950523 -0.31140561 -0.15983005\n",
      "   1.27194489  0.26768065 -0.61484052]\n",
      " [-0.87766834  0.93025908 -0.50094443 -1.04165388  0.21319155 -0.10147628\n",
      "  -0.71413902 -0.43436037  0.81679242]\n",
      " [ 0.68976963 -0.53853039 -0.01362671  0.62283448 -0.44760355  0.26876428\n",
      "   0.72221516  0.01514423 -0.73181934]\n",
      " [ 0.73349886 -0.84949372  0.34359178  0.69710352 -0.3398696   0.09835601\n",
      "   0.91386878  0.90003163 -0.43577337]\n",
      " [-1.03232965  1.15835661 -0.4699829  -1.1628911   0.09323363  0.25351153\n",
      "  -0.99579754 -0.78842286  0.7870899 ]\n",
      " [ 1.18239336 -1.04826632  0.45687318  0.79081874 -0.18237278 -0.29914008\n",
      "   0.8286331   0.88917462 -0.58577054]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.50481752  1.93551985 -2.27943197  0.29395115  0.53448588 -2.90109144\n",
      "   1.31558359]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:73 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.30155353]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 73 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92298575 -1.17667565  0.28756501  1.02950523 -0.31549038 -0.16391482\n",
      "   1.26786012  0.26768065 -0.61484052]\n",
      " [-0.87361713  0.93431029 -0.50094443 -1.04165388  0.21724276 -0.09742507\n",
      "  -0.71008781 -0.43436037  0.81679242]\n",
      " [ 0.68487014 -0.54342988 -0.01362671  0.62283448 -0.45250304  0.26386478\n",
      "   0.71731567  0.01514423 -0.73181934]\n",
      " [ 0.72940677 -0.85358582  0.34359178  0.69710352 -0.34396169  0.09426391\n",
      "   0.90977669  0.90003163 -0.43577337]\n",
      " [-1.0284488   1.16223745 -0.4699829  -1.1628911   0.09711448  0.25739237\n",
      "  -0.99191669 -0.78842286  0.7870899 ]\n",
      " [ 1.17878563 -1.05187405  0.45687318  0.79081874 -0.1859805  -0.30274781\n",
      "   0.82502537  0.88917462 -0.58577054]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.53657398  1.91534375 -2.29105176  0.27276982  0.51430116 -2.91290944\n",
      "   1.29595676]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:73 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.87715758]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 73 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92377288 -1.17667565  0.28756501  1.03029237 -0.31549038 -0.16391482\n",
      "   1.26864725  0.26768065 -0.61484052]\n",
      " [-0.87471116  0.93431029 -0.50094443 -1.04274791  0.21724276 -0.09742507\n",
      "  -0.71118184 -0.43436037  0.81679242]\n",
      " [ 0.68625063 -0.54342988 -0.01362671  0.62421497 -0.45250304  0.26386478\n",
      "   0.71869616  0.01514423 -0.73181934]\n",
      " [ 0.73064977 -0.85358582  0.34359178  0.69834652 -0.34396169  0.09426391\n",
      "   0.91101969  0.90003163 -0.43577337]\n",
      " [-1.02925394  1.16223745 -0.4699829  -1.16369624  0.09711448  0.25739237\n",
      "  -0.99272183 -0.78842286  0.7870899 ]\n",
      " [ 1.17978986 -1.05187405  0.45687318  0.79182297 -0.1859805  -0.30274781\n",
      "   0.8260296   0.88917462 -0.58577054]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.52995571  1.92170783 -2.29060487  0.27861639  0.52033595 -2.91264603\n",
      "   1.30219372]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:73 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.71213597]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 73 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93031496 -1.17667565  0.28756501  1.03029237 -0.30894831 -0.15737275\n",
      "   1.27518933  0.26768065 -0.61484052]\n",
      " [-0.88130229  0.93431029 -0.50094443 -1.04274791  0.21065163 -0.1040162\n",
      "  -0.71777297 -0.43436037  0.81679242]\n",
      " [ 0.69257897 -0.54342988 -0.01362671  0.62421497 -0.4461747   0.27019312\n",
      "   0.7250245   0.01514423 -0.73181934]\n",
      " [ 0.73719879 -0.85358582  0.34359178  0.69834652 -0.33741267  0.10081294\n",
      "   0.91756871  0.90003163 -0.43577337]\n",
      " [-1.03582493  1.16223745 -0.4699829  -1.16369624  0.09054349  0.25082138\n",
      "  -0.99929281 -0.78842286  0.7870899 ]\n",
      " [ 1.18639374 -1.05187405  0.45687318  0.79182297 -0.17937663 -0.29614393\n",
      "   0.83263348  0.88917462 -0.58577054]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.50044988  1.94670596 -2.28507135  0.30137758  0.54396737 -2.90796163\n",
      "   1.32639173]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:73 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.59578042]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 73 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91505409 -1.17667565  0.28756501  1.0150315  -0.30894831 -0.17263361\n",
      "   1.27518933  0.26768065 -0.63010139]\n",
      " [-0.86593755  0.93431029 -0.50094443 -1.02738317  0.21065163 -0.08865146\n",
      "  -0.71777297 -0.43436037  0.83215716]\n",
      " [ 0.67973623 -0.54342988 -0.01362671  0.61137223 -0.4461747   0.25735038\n",
      "   0.7250245   0.01514423 -0.74466208]\n",
      " [ 0.7224092  -0.85358582  0.34359178  0.68355693 -0.33741267  0.08602335\n",
      "   0.91756871  0.90003163 -0.45056295]\n",
      " [-1.02069552  1.16223745 -0.4699829  -1.14856683  0.09054349  0.26595079\n",
      "  -0.99929281 -0.78842286  0.8022193 ]\n",
      " [ 1.17163013 -1.05187405  0.45687318  0.77705936 -0.17937663 -0.31090754\n",
      "   0.83263348  0.88917462 -0.60053415]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.57218962  1.89172077 -2.30153388  0.25104138  0.49013604 -2.92506258\n",
      "   1.27261802]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:73 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.71778591]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 73 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92125584 -1.1704739   0.28756501  1.02123325 -0.30894831 -0.16643186\n",
      "   1.28139108  0.26768065 -0.63010139]\n",
      " [-0.87223346  0.92801438 -0.50094443 -1.03367908  0.21065163 -0.09494736\n",
      "  -0.72406887 -0.43436037  0.83215716]\n",
      " [ 0.68606142 -0.53710469 -0.01362671  0.61769742 -0.4461747   0.26367557\n",
      "   0.73134969  0.01514423 -0.74466208]\n",
      " [ 0.72880798 -0.84718704  0.34359178  0.68995571 -0.33741267  0.09242213\n",
      "   0.92396749  0.90003163 -0.45056295]\n",
      " [-1.02701263  1.15592034 -0.4699829  -1.15488394  0.09054349  0.25963367\n",
      "  -1.00560993 -0.78842286  0.8022193 ]\n",
      " [ 1.17799236 -1.04551182  0.45687318  0.78342159 -0.17937663 -0.3045453\n",
      "   0.83899572  0.88917462 -0.60053415]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.54360567  1.91644092 -2.29735745  0.27532115  0.51373995 -2.92079596\n",
      "   1.29563123]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:73 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.77053918]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 73 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92545369 -1.16627605  0.28756501  1.0254311  -0.30894831 -0.16643186\n",
      "   1.28558892  0.26768065 -0.63010139]\n",
      " [-0.87673649  0.92351135 -0.50094443 -1.0381821   0.21065163 -0.09494736\n",
      "  -0.7285719  -0.43436037  0.83215716]\n",
      " [ 0.69059935 -0.53256676 -0.01362671  0.62223535 -0.4461747   0.26367557\n",
      "   0.73588762  0.01514423 -0.74466208]\n",
      " [ 0.73334554 -0.84264948  0.34359178  0.69449326 -0.33741267  0.09242213\n",
      "   0.92850504  0.90003163 -0.45056295]\n",
      " [-1.03123628  1.15169669 -0.4699829  -1.15910759  0.09054349  0.25963367\n",
      "  -1.00983357 -0.78842286  0.8022193 ]\n",
      " [ 1.18246693 -1.04103725  0.45687318  0.78789615 -0.17937663 -0.3045453\n",
      "   0.84347028  0.88917462 -0.60053415]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.52332035  1.93441962 -2.29422928  0.29189986  0.5303112  -2.91844436\n",
      "   1.31292588]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:73 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.83631063]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 73 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92756139 -1.16627605  0.28967271  1.0254311  -0.30894831 -0.16432416\n",
      "   1.28769662  0.26768065 -0.63010139]\n",
      " [-0.87894906  0.92351135 -0.50315701 -1.0381821   0.21065163 -0.09715994\n",
      "  -0.73078448 -0.43436037  0.83215716]\n",
      " [ 0.69309259 -0.53256676 -0.01113347  0.62223535 -0.4461747   0.26616881\n",
      "   0.73838086  0.01514423 -0.74466208]\n",
      " [ 0.73563389 -0.84264948  0.34588013  0.69449326 -0.33741267  0.09471049\n",
      "   0.9307934   0.90003163 -0.45056295]\n",
      " [-1.03340966  1.15169669 -0.47215628 -1.15910759  0.09054349  0.2574603\n",
      "  -1.01200695 -0.78842286  0.8022193 ]\n",
      " [ 1.18469693 -1.04103725  0.45910318  0.78789615 -0.17937663 -0.3023153\n",
      "   0.84570028  0.88917462 -0.60053415]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.51211621  1.94463227 -2.29311287  0.30133874  0.54029068 -2.91737742\n",
      "   1.32299034]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:73 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.1691931]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 74 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92709863 -1.1667388   0.28967271  1.02496835 -0.30894831 -0.16432416\n",
      "   1.28769662  0.26768065 -0.63056414]\n",
      " [-0.87847217  0.92398824 -0.50315701 -1.03770521  0.21065163 -0.09715994\n",
      "  -0.73078448 -0.43436037  0.83263405]\n",
      " [ 0.69297937 -0.53267998 -0.01113347  0.62212213 -0.4461747   0.26616881\n",
      "   0.73838086  0.01514423 -0.7447753 ]\n",
      " [ 0.73522876 -0.84305461  0.34588013  0.69408814 -0.33741267  0.09471049\n",
      "   0.9307934   0.90003163 -0.45096808]\n",
      " [-1.03271033  1.15239602 -0.47215628 -1.15840826  0.09054349  0.2574603\n",
      "  -1.01200695 -0.78842286  0.80291863]\n",
      " [ 1.18373932 -1.04199486  0.45910318  0.78693854 -0.17937663 -0.3023153\n",
      "   0.84570028  0.88917462 -0.60149176]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.52400768  1.9382219  -2.29857963  0.29527975  0.53393855 -2.92261717\n",
      "   1.31606941]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:74 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.84882942]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 74 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92879506 -1.1667388   0.29136914  1.02496835 -0.30894831 -0.16432416\n",
      "   1.28939305  0.26768065 -0.63056414]\n",
      " [-0.88044426  0.92398824 -0.50512909 -1.03770521  0.21065163 -0.09715994\n",
      "  -0.73275656 -0.43436037  0.83263405]\n",
      " [ 0.69513851 -0.53267998 -0.00897432  0.62212213 -0.4461747   0.26616881\n",
      "   0.74054     0.01514423 -0.7447753 ]\n",
      " [ 0.73725902 -0.84305461  0.34791039  0.69408814 -0.33741267  0.09471049\n",
      "   0.93282366  0.90003163 -0.45096808]\n",
      " [-1.03439728  1.15239602 -0.47384323 -1.15840826  0.09054349  0.2574603\n",
      "  -1.0136939  -0.78842286  0.80291863]\n",
      " [ 1.18544787 -1.04199486  0.46081173  0.78693854 -0.17937663 -0.3023153\n",
      "   0.84740883  0.88917462 -0.60149176]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.51430872  1.94718814 -2.29753315  0.30309104  0.54249343 -2.92189282\n",
      "   1.3250248 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:74 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.64714978]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 74 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91250572 -1.18302815  0.27507979  1.02496835 -0.30894831 -0.16432416\n",
      "   1.2731037   0.26768065 -0.63056414]\n",
      " [-0.86469797  0.93973453 -0.4893828  -1.03770521  0.21065163 -0.09715994\n",
      "  -0.71701027 -0.43436037  0.83263405]\n",
      " [ 0.6815296  -0.5462889  -0.02258324  0.62212213 -0.4461747   0.26616881\n",
      "   0.72693109  0.01514423 -0.7447753 ]\n",
      " [ 0.72160795 -0.85870568  0.33225932  0.69408814 -0.33741267  0.09471049\n",
      "   0.91717259  0.90003163 -0.45096808]\n",
      " [-1.01804377  1.16874953 -0.45748973 -1.15840826  0.09054349  0.2574603\n",
      "  -0.9973404  -0.78842286  0.80291863]\n",
      " [ 1.16895753 -1.0584852   0.4443214   0.78693854 -0.17937663 -0.3023153\n",
      "   0.8309185   0.88917462 -0.60149176]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.58819606  1.88859895 -2.31471073  0.25065172  0.48604081 -2.93686937\n",
      "   1.26515715]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:74 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.74953125]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 74 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91775416 -1.18302815  0.28032824  1.02496835 -0.30894831 -0.16432416\n",
      "   1.2731037   0.2729291  -0.63056414]\n",
      " [-0.86985887  0.93973453 -0.4945437  -1.03770521  0.21065163 -0.09715994\n",
      "  -0.71701027 -0.43952127  0.83263405]\n",
      " [ 0.68507362 -0.5462889  -0.01903922  0.62212213 -0.4461747   0.26616881\n",
      "   0.72693109  0.01868825 -0.7447753 ]\n",
      " [ 0.72660234 -0.85870568  0.3372537   0.69408814 -0.33741267  0.09471049\n",
      "   0.91717259  0.90502601 -0.45096808]\n",
      " [-1.02258341  1.16874953 -0.46202937 -1.15840826  0.09054349  0.2574603\n",
      "  -0.9973404  -0.7929625   0.80291863]\n",
      " [ 1.17307348 -1.0584852   0.44843734  0.78693854 -0.17937663 -0.3023153\n",
      "   0.8309185   0.89329056 -0.60149176]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.56468529  1.90766166 -2.31134248  0.26622569  0.50663321 -2.93465577\n",
      "   1.28688847]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:74 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.57933068]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 74 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92899542 -1.18302815  0.28032824  1.03620961 -0.30894831 -0.16432416\n",
      "   1.2731037   0.2729291  -0.61932288]\n",
      " [-0.88031224  0.93973453 -0.4945437  -1.04815858  0.21065163 -0.09715994\n",
      "  -0.71701027 -0.43952127  0.82218068]\n",
      " [ 0.69173983 -0.5462889  -0.01903922  0.62878834 -0.4461747   0.26616881\n",
      "   0.72693109  0.01868825 -0.73810909]\n",
      " [ 0.73651142 -0.85870568  0.3372537   0.70399722 -0.33741267  0.09471049\n",
      "   0.91717259  0.90502601 -0.441059  ]\n",
      " [-1.03394157  1.16874953 -0.46202937 -1.16976642  0.09054349  0.2574603\n",
      "  -0.9973404  -0.7929625   0.79156047]\n",
      " [ 1.18440133 -1.0584852   0.44843734  0.7982664  -0.17937663 -0.3023153\n",
      "   0.8309185   0.89329056 -0.59016391]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.51342533  1.94804805 -2.29829853  0.29887892  0.54379997 -2.92433619\n",
      "   1.32766674]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:74 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.09740923]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 74 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92916939 -1.18302815  0.28032824  1.03620961 -0.30877434 -0.16415019\n",
      "   1.2731037   0.2729291  -0.61914891]\n",
      " [-0.88037146  0.93973453 -0.4945437  -1.04815858  0.21059241 -0.09721916\n",
      "  -0.71701027 -0.43952127  0.82212146]\n",
      " [ 0.69197909 -0.5462889  -0.01903922  0.62878834 -0.44593544  0.26640808\n",
      "   0.72693109  0.01868825 -0.73786983]\n",
      " [ 0.73645499 -0.85870568  0.3372537   0.70399722 -0.3374691   0.09465406\n",
      "   0.91717259  0.90502601 -0.44111543]\n",
      " [-1.03405433  1.16874953 -0.46202937 -1.16976642  0.09043073  0.25734754\n",
      "  -0.9973404  -0.7929625   0.79144771]\n",
      " [ 1.18428123 -1.0584852   0.44843734  0.7982664  -0.17949673 -0.30243541\n",
      "   0.8309185   0.89329056 -0.59028401]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.51770748  1.94608173 -2.30049885  0.29697916  0.54160244 -2.92659023\n",
      "   1.32540532]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:74 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.01153857]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 74 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92918391 -1.18301363  0.28032824  1.03620961 -0.30875982 -0.16413568\n",
      "   1.2731037   0.2729291  -0.6191344 ]\n",
      " [-0.88038436  0.93972163 -0.4945437  -1.04815858  0.21057951 -0.09723206\n",
      "  -0.71701027 -0.43952127  0.82210856]\n",
      " [ 0.69199007 -0.54627792 -0.01903922  0.62878834 -0.44592446  0.26641906\n",
      "   0.72693109  0.01868825 -0.73785885]\n",
      " [ 0.73646631 -0.85869436  0.3372537   0.70399722 -0.33745778  0.09466538\n",
      "   0.91717259  0.90502601 -0.44110411]\n",
      " [-1.03406865  1.16873521 -0.46202937 -1.16976642  0.09041641  0.25733322\n",
      "  -0.9973404  -0.7929625   0.79143339]\n",
      " [ 1.18429377 -1.05847265  0.44843734  0.7982664  -0.17948418 -0.30242286\n",
      "   0.8309185   0.89329056 -0.59027147]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.51777328  1.94606814 -2.30054689  0.29695836  0.54158213 -2.92664164\n",
      "   1.32538692]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:74 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.29610337]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 74 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92526846 -1.18692909  0.28032824  1.03620961 -0.31267528 -0.16805113\n",
      "   1.26918825  0.2729291  -0.6191344 ]\n",
      " [-0.87648148  0.9436245  -0.4945437  -1.04815858  0.21448238 -0.09332919\n",
      "  -0.7131074  -0.43952127  0.82210856]\n",
      " [ 0.687237   -0.55103099 -0.01903922  0.62878834 -0.45067753  0.26166599\n",
      "   0.72217802  0.01868825 -0.73785885]\n",
      " [ 0.73251559 -0.86264507  0.3372537   0.70399722 -0.34140849  0.09071466\n",
      "   0.91322187  0.90502601 -0.44110411]\n",
      " [-1.03034839  1.17245547 -0.46202937 -1.16976642  0.09413667  0.26105348\n",
      "  -0.99362013 -0.7929625   0.79143339]\n",
      " [ 1.18082965 -1.06193678  0.44843734  0.7982664  -0.18294831 -0.30588698\n",
      "   0.82745438  0.89329056 -0.59027147]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.54863112  1.92652602 -2.31187733  0.27638653  0.52199866 -2.93818371\n",
      "   1.30636223]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:74 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.87948458]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 74 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92602327 -1.18692909  0.28032824  1.03696442 -0.31267528 -0.16805113\n",
      "   1.26994306  0.2729291  -0.6191344 ]\n",
      " [-0.87753092  0.9436245  -0.4945437  -1.04920802  0.21448238 -0.09332919\n",
      "  -0.71415683 -0.43952127  0.82210856]\n",
      " [ 0.68856437 -0.55103099 -0.01903922  0.63011571 -0.45067753  0.26166599\n",
      "   0.72350539  0.01868825 -0.73785885]\n",
      " [ 0.73370872 -0.86264507  0.3372537   0.70519034 -0.34140849  0.09071466\n",
      "   0.914415    0.90502601 -0.44110411]\n",
      " [-1.03112044  1.17245547 -0.46202937 -1.17053847  0.09413667  0.26105348\n",
      "  -0.99439218 -0.7929625   0.79143339]\n",
      " [ 1.18179268 -1.06193678  0.44843734  0.79922942 -0.18294831 -0.30588698\n",
      "   0.8284174   0.89329056 -0.59027147]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.54224432  1.93266994 -2.31145103  0.28203725  0.52782926 -2.93793206\n",
      "   1.31238516]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:74 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.71437308]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 74 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93248269 -1.18692909  0.28032824  1.03696442 -0.30621586 -0.16159171\n",
      "   1.27640248  0.2729291  -0.6191344 ]\n",
      " [-0.88404199  0.9436245  -0.4945437  -1.04920802  0.20797131 -0.09984027\n",
      "  -0.72066791 -0.43952127  0.82210856]\n",
      " [ 0.69482595 -0.55103099 -0.01903922  0.63011571 -0.44441595  0.26792757\n",
      "   0.72976697  0.01868825 -0.73785885]\n",
      " [ 0.74018063 -0.86264507  0.3372537   0.70519034 -0.33493657  0.09718658\n",
      "   0.92088692  0.90502601 -0.44110411]\n",
      " [-1.03760846  1.17245547 -0.46202937 -1.17053847  0.08764865  0.25456545\n",
      "  -1.00088021 -0.7929625   0.79143339]\n",
      " [ 1.18831522 -1.06193678  0.44843734  0.79922942 -0.17642576 -0.29936444\n",
      "   0.83493994  0.89329056 -0.59027147]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.51310407  1.95736674 -2.30600584  0.30455105  0.55119321 -2.93331679\n",
      "   1.33630176]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:74 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.59590327]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 74 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91721863 -1.18692909  0.28032824  1.02170036 -0.30621586 -0.17685577\n",
      "   1.27640248  0.2729291  -0.63439846]\n",
      " [-0.86867687  0.9436245  -0.4945437  -1.03384289  0.20797131 -0.08447514\n",
      "  -0.72066791 -0.43952127  0.83747369]\n",
      " [ 0.68198337 -0.55103099 -0.01903922  0.61727313 -0.44441595  0.25508499\n",
      "   0.72976697  0.01868825 -0.75070143]\n",
      " [ 0.72538421 -0.86264507  0.3372537   0.69039392 -0.33493657  0.08239016\n",
      "   0.92088692  0.90502601 -0.45590053]\n",
      " [-1.02247467  1.17245547 -0.46202937 -1.15540468  0.08764865  0.26969925\n",
      "  -1.00088021 -0.7929625   0.80656719]\n",
      " [ 1.17354023 -1.06193678  0.44843734  0.78445444 -0.17642576 -0.31413943\n",
      "   0.83493994  0.89329056 -0.60504646]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.58485158  1.90237139 -2.32247386  0.25421176  0.49734438 -2.95041258\n",
      "   1.28250059]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:74 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.71845297]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 74 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92340202 -1.18074569  0.28032824  1.02788376 -0.30621586 -0.17067238\n",
      "   1.28258588  0.2729291  -0.63439846]\n",
      " [-0.8749501   0.93735127 -0.4945437  -1.04011613  0.20797131 -0.09074837\n",
      "  -0.72694115 -0.43952127  0.83747369]\n",
      " [ 0.68828222 -0.54473213 -0.01903922  0.62357198 -0.44441595  0.26138384\n",
      "   0.73606582  0.01868825 -0.75070143]\n",
      " [ 0.73175867 -0.85627061  0.3372537   0.69676838 -0.33493657  0.08876462\n",
      "   0.92726137  0.90502601 -0.45590053]\n",
      " [-1.02877078  1.16615937 -0.46202937 -1.16170078  0.08764865  0.26340314\n",
      "  -1.00717632 -0.7929625   0.80656719]\n",
      " [ 1.17987729 -1.05559971  0.44843734  0.7907915  -0.17642576 -0.30780237\n",
      "   0.841277    0.89329056 -0.60504646]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.55637616  1.92698291 -2.31830832  0.27841034  0.52086046 -2.94614854\n",
      "   1.30541891]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:74 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.77205258]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 74 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92755315 -1.17659457  0.28032824  1.03203488 -0.30621586 -0.17067238\n",
      "   1.286737    0.2729291  -0.63439846]\n",
      " [-0.8794014   0.93289997 -0.4945437  -1.04456742  0.20797131 -0.09074837\n",
      "  -0.73139244 -0.43952127  0.83747369]\n",
      " [ 0.69277001 -0.54024434 -0.01903922  0.62805977 -0.44441595  0.26138384\n",
      "   0.74055361  0.01868825 -0.75070143]\n",
      " [ 0.73624596 -0.85178333  0.3372537   0.70125566 -0.33493657  0.08876462\n",
      "   0.93174866  0.90502601 -0.45590053]\n",
      " [-1.03294719  1.16198295 -0.46202937 -1.1658772   0.08764865  0.26340314\n",
      "  -1.01135273 -0.7929625   0.80656719]\n",
      " [ 1.18430083 -1.05117618  0.44843734  0.79521504 -0.17642576 -0.30780237\n",
      "   0.84570054  0.89329056 -0.60504646]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.53631822  1.94475952 -2.31522224  0.29481873  0.53725787 -2.9438231\n",
      "   1.32252334]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:74 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.83678979]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 74 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92965652 -1.17659457  0.28243161  1.03203488 -0.30621586 -0.16856901\n",
      "   1.28884037  0.2729291  -0.63439846]\n",
      " [-0.8816062   0.93289997 -0.4967485  -1.04456742  0.20797131 -0.09295317\n",
      "  -0.73359724 -0.43952127  0.83747369]\n",
      " [ 0.69525029 -0.54024434 -0.01655894  0.62805977 -0.44441595  0.26386412\n",
      "   0.74303389  0.01868825 -0.75070143]\n",
      " [ 0.73852491 -0.85178333  0.33953265  0.70125566 -0.33493657  0.09104357\n",
      "   0.93402761  0.90502601 -0.45590053]\n",
      " [-1.03511579  1.16198295 -0.46419797 -1.1658772   0.08764865  0.26123454\n",
      "  -1.01352133 -0.7929625   0.80656719]\n",
      " [ 1.18652476 -1.05117618  0.45066127  0.79521504 -0.17642576 -0.30557844\n",
      "   0.84792447  0.89329056 -0.60504646]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.5251732   1.95491086 -2.3141066   0.30420652  0.54718051 -2.9427536\n",
      "   1.33252691]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:74 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.16486182]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 75 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92923136 -1.17701973  0.28243161  1.03160972 -0.30621586 -0.16856901\n",
      "   1.28884037  0.2729291  -0.63482362]\n",
      " [-0.88116681  0.93333935 -0.4967485  -1.04412804  0.20797131 -0.09295317\n",
      "  -0.73359724 -0.43952127  0.83791307]\n",
      " [ 0.69515848 -0.54033615 -0.01655894  0.62796797 -0.44441595  0.26386412\n",
      "   0.74303389  0.01868825 -0.75079324]\n",
      " [ 0.73815173 -0.8521565   0.33953265  0.70088249 -0.33493657  0.09104357\n",
      "   0.93402761  0.90502601 -0.4562737 ]\n",
      " [-1.03446511  1.16263364 -0.46419797 -1.16522651  0.08764865  0.26123454\n",
      "  -1.01352133 -0.7929625   0.80721787]\n",
      " [ 1.1856252  -1.05207573  0.45066127  0.79431548 -0.17642576 -0.30557844\n",
      "   0.84792447  0.89329056 -0.60594601]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.53652248  1.94880945 -2.31934008  0.29844005  0.54113161 -2.94777169\n",
      "   1.32593674]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:75 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.8499939]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 75 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93090702 -1.17701973  0.28410727  1.03160972 -0.30621586 -0.16856901\n",
      "   1.29051603  0.2729291  -0.63482362]\n",
      " [-0.88311184  0.93333935 -0.49869353 -1.04412804  0.20797131 -0.09295317\n",
      "  -0.73554227 -0.43952127  0.83791307]\n",
      " [ 0.69728768 -0.54033615 -0.01442974  0.62796797 -0.44441595  0.26386412\n",
      "   0.74516309  0.01868825 -0.75079324]\n",
      " [ 0.74015368 -0.8521565   0.3415346   0.70088249 -0.33493657  0.09104357\n",
      "   0.93602955  0.90502601 -0.4562737 ]\n",
      " [-1.03613198  1.16263364 -0.46586484 -1.16522651  0.08764865  0.26123454\n",
      "  -1.0151882  -0.7929625   0.80721787]\n",
      " [ 1.18731309 -1.05207573  0.45234915  0.79431548 -0.17642576 -0.30557844\n",
      "   0.84961235  0.89329056 -0.60594601]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.52695927  1.95764755 -2.31830742  0.30614412  0.54956656 -2.94705436\n",
      "   1.33476384]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:75 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.64275007]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 75 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91467645 -1.1932503   0.2678767   1.03160972 -0.30621586 -0.16856901\n",
      "   1.27428546  0.2729291  -0.63482362]\n",
      " [-0.86743518  0.94901601 -0.48301686 -1.04412804  0.20797131 -0.09295317\n",
      "  -0.7198656  -0.43952127  0.83791307]\n",
      " [ 0.68375635 -0.55386748 -0.02796108  0.62796797 -0.44441595  0.26386412\n",
      "   0.73163175  0.01868825 -0.75079324]\n",
      " [ 0.72457021 -0.86773997  0.32595113  0.70088249 -0.33493657  0.09104357\n",
      "   0.92044608  0.90502601 -0.4562737 ]\n",
      " [-1.01983366  1.17893196 -0.44956652 -1.16522651  0.08764865  0.26123454\n",
      "  -0.99888988 -0.7929625   0.80721787]\n",
      " [ 1.17086102 -1.0685278   0.43589709  0.79431548 -0.17642576 -0.30557844\n",
      "   0.83316028  0.89329056 -0.60594601]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.60075418  1.89930572 -2.33559995  0.25386691  0.49330962 -2.96219171\n",
      "   1.27513604]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:75 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.75111634]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 75 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91986971 -1.1932503   0.27306996  1.03160972 -0.30621586 -0.16856901\n",
      "   1.27428546  0.27812236 -0.63482362]\n",
      " [-0.87254053  0.94901601 -0.48812222 -1.04412804  0.20797131 -0.09295317\n",
      "  -0.7198656  -0.44462662  0.83791307]\n",
      " [ 0.68726465 -0.55386748 -0.02445277  0.62796797 -0.44441595  0.26386412\n",
      "   0.73163175  0.02219655 -0.75079324]\n",
      " [ 0.72951005 -0.86773997  0.33089097  0.70088249 -0.33493657  0.09104357\n",
      "   0.92044608  0.90996585 -0.4562737 ]\n",
      " [-1.02432815  1.17893196 -0.45406101 -1.16522651  0.08764865  0.26123454\n",
      "  -0.99888988 -0.79745699  0.80721787]\n",
      " [ 1.17493803 -1.0685278   0.43997409  0.79431548 -0.17642576 -0.30557844\n",
      "   0.83316028  0.89736757 -0.60594601]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.57749096  1.9181685  -2.33227154  0.26927894  0.51368938 -2.95999826\n",
      "   1.29663464]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:75 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.58093842]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 75 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93106544 -1.1932503   0.27306996  1.04280545 -0.30621586 -0.16856901\n",
      "   1.27428546  0.27812236 -0.62362788]\n",
      " [-0.88296034  0.94901601 -0.48812222 -1.05454785  0.20797131 -0.09295317\n",
      "  -0.7198656  -0.44462662  0.82749326]\n",
      " [ 0.69391849 -0.55386748 -0.02445277  0.6346218  -0.44441595  0.26386412\n",
      "   0.73163175  0.02219655 -0.7441394 ]\n",
      " [ 0.73939591 -0.86773997  0.33089097  0.71076835 -0.33493657  0.09104357\n",
      "   0.92044608  0.90996585 -0.44638784]\n",
      " [-1.03563703  1.17893196 -0.45406101 -1.1765354   0.08764865  0.26123454\n",
      "  -0.99888988 -0.79745699  0.79590899]\n",
      " [ 1.18621837 -1.0685278   0.43997409  0.80559583 -0.17642576 -0.30557844\n",
      "   0.83316028  0.89736757 -0.59466566]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.5264809   1.95839629 -2.31932816  0.30179679  0.5507196  -2.94976391\n",
      "   1.33725378]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:75 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.09469522]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 75 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93123407 -1.1932503   0.27306996  1.04280545 -0.30604723 -0.16840038\n",
      "   1.27428546  0.27812236 -0.62345925]\n",
      " [-0.88302071  0.94901601 -0.48812222 -1.05454785  0.20791093 -0.09301355\n",
      "  -0.7198656  -0.44462662  0.82743288]\n",
      " [ 0.69414957 -0.55386748 -0.02445277  0.6346218  -0.44418486  0.2640952\n",
      "   0.73163175  0.02219655 -0.74390831]\n",
      " [ 0.7393461  -0.86773997  0.33089097  0.71076835 -0.33498638  0.09099376\n",
      "   0.92044608  0.90996585 -0.44643765]\n",
      " [-1.03574747  1.17893196 -0.45406101 -1.1765354   0.08753821  0.2611241\n",
      "  -0.99888988 -0.79745699  0.79579855]\n",
      " [ 1.18610754 -1.0685278   0.43997409  0.80559583 -0.17653659 -0.30568927\n",
      "   0.83316028  0.89736757 -0.59477649]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.53053992  1.9565362  -2.32141808  0.30000042  0.54864026 -2.95190408\n",
      "   1.33511322]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:75 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.01084034]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 75 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93124692 -1.19323745  0.27306996  1.04280545 -0.30603438 -0.16838753\n",
      "   1.27428546  0.27812236 -0.6234464 ]\n",
      " [-0.88303219  0.94900454 -0.48812222 -1.05454785  0.20789945 -0.09302502\n",
      "  -0.7198656  -0.44462662  0.82742141]\n",
      " [ 0.69415938 -0.55385768 -0.02445277  0.6346218  -0.44417506  0.26410501\n",
      "   0.73163175  0.02219655 -0.74389851]\n",
      " [ 0.73935621 -0.86772986  0.33089097  0.71076835 -0.33497628  0.09100387\n",
      "   0.92044608  0.90996585 -0.44642755]\n",
      " [-1.03576015  1.17891928 -0.45406101 -1.1765354   0.08752553  0.26111142\n",
      "  -0.99888988 -0.79745699  0.79578587]\n",
      " [ 1.18611871 -1.06851663  0.43997409  0.80559583 -0.17652543 -0.30567811\n",
      "   0.83316028  0.89736757 -0.59476533]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Theta two: \n",
      "[[-1.53059804  1.95652433 -2.32146067  0.2999822   0.54862248 -2.95194963\n",
      "   1.33509712]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:75 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.29079339]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 75 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92749294 -1.19699143  0.27306996  1.04280545 -0.30978836 -0.17214151\n",
      "   1.27053148  0.27812236 -0.6234464 ]\n",
      " [-0.87927135  0.95276538 -0.48812222 -1.05454785  0.2116603  -0.08926418\n",
      "  -0.71610476 -0.44462662  0.82742141]\n",
      " [ 0.68954721 -0.55846985 -0.02445277  0.6346218  -0.44878723  0.25949284\n",
      "   0.72701958  0.02219655 -0.74389851]\n",
      " [ 0.7355412  -0.87154486  0.33089097  0.71076835 -0.33879128  0.08718886\n",
      "   0.91663108  0.90996585 -0.44642755]\n",
      " [-1.03219305  1.18248638 -0.45406101 -1.1765354   0.09109262  0.26467852\n",
      "  -0.99532278 -0.79745699  0.79578587]\n",
      " [ 1.18279216 -1.07184318  0.43997409  0.80559583 -0.17985197 -0.30900465\n",
      "   0.82983373  0.89736757 -0.59476533]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.56058358  1.93759394 -2.33250781  0.2800004   0.5296208  -2.96322044\n",
      "   1.31665474]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:75 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.88175161]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 75 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92821695 -1.19699143  0.27306996  1.04352947 -0.30978836 -0.17214151\n",
      "   1.2712555   0.27812236 -0.6234464 ]\n",
      " [-0.88027826  0.95276538 -0.48812222 -1.05555476  0.2116603  -0.08926418\n",
      "  -0.71711168 -0.44462662  0.82742141]\n",
      " [ 0.69082372 -0.55846985 -0.02445277  0.63589832 -0.44878723  0.25949284\n",
      "   0.7282961   0.02219655 -0.74389851]\n",
      " [ 0.73668672 -0.87154486  0.33089097  0.71191386 -0.33879128  0.08718886\n",
      "   0.91777659  0.90996585 -0.44642755]\n",
      " [-1.03293358  1.18248638 -0.45406101 -1.17727592  0.09109262  0.26467852\n",
      "  -0.99606331 -0.79745699  0.79578587]\n",
      " [ 1.18371594 -1.07184318  0.43997409  0.80651961 -0.17985197 -0.30900465\n",
      "   0.83075751  0.89736757 -0.59476533]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.55441895  1.94352642 -2.33210099  0.28546267  0.53525501 -2.96297994\n",
      "   1.322472  ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:75 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.71661332]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 75 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93459373 -1.19699143  0.27306996  1.04352947 -0.30341158 -0.16576473\n",
      "   1.27763227  0.27812236 -0.6234464 ]\n",
      " [-0.88670919  0.95276538 -0.48812222 -1.05555476  0.20522936 -0.09569511\n",
      "  -0.72354261 -0.44462662  0.82742141]\n",
      " [ 0.69701806 -0.55846985 -0.02445277  0.63589832 -0.44259289  0.26568717\n",
      "   0.73449043  0.02219655 -0.74389851]\n",
      " [ 0.7430813  -0.87154486  0.33089097  0.71191386 -0.33239669  0.09358345\n",
      "   0.92417118  0.90996585 -0.44642755]\n",
      " [-1.03933867  1.18248638 -0.45406101 -1.17727592  0.08468753  0.25827343\n",
      "  -1.0024684  -0.79745699  0.79578587]\n",
      " [ 1.19015709 -1.07184318  0.43997409  0.80651961 -0.17341082 -0.3025635\n",
      "   0.83719867  0.89736757 -0.59476533]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.52564405  1.96792212 -2.32674354  0.30772834  0.55835082 -2.95843366\n",
      "   1.34610667]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:75 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.59597057]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 75 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91932787 -1.19699143  0.27306996  1.0282636  -0.30341158 -0.1810306\n",
      "   1.27763227  0.27812236 -0.63871227]\n",
      " [-0.8713451   0.95276538 -0.48812222 -1.04019067  0.20522936 -0.08033102\n",
      "  -0.72354261 -0.44462662  0.8427855 ]\n",
      " [ 0.68417772 -0.55846985 -0.02445277  0.62305798 -0.44259289  0.25284683\n",
      "   0.73449043  0.02219655 -0.75673885]\n",
      " [ 0.72827981 -0.87154486  0.33089097  0.69711237 -0.33239669  0.07878195\n",
      "   0.92417118  0.90996585 -0.46122904]\n",
      " [-1.0242019   1.18248638 -0.45406101 -1.16213915  0.08468753  0.27341019\n",
      "  -1.0024684  -0.79745699  0.81092263]\n",
      " [ 1.17537251 -1.07184318  0.43997409  0.79173502 -0.17341082 -0.31734809\n",
      "   0.83719867  0.89736757 -0.60954992]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.59739583  1.91292105 -2.34321817  0.25739066  0.50448942 -2.97552517\n",
      "   1.29228295]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:75 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.7191089]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 75 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92549311 -1.19082619  0.27306996  1.03442885 -0.30341158 -0.17486536\n",
      "   1.28379751  0.27812236 -0.63871227]\n",
      " [-0.87759606  0.94651442 -0.48812222 -1.04644163  0.20522936 -0.08658198\n",
      "  -0.72979357 -0.44462662  0.8427855 ]\n",
      " [ 0.69045064 -0.55219693 -0.02445277  0.6293309  -0.44259289  0.25911975\n",
      "   0.74076335  0.02219655 -0.75673885]\n",
      " [ 0.73463037 -0.8651943   0.33089097  0.70346293 -0.33239669  0.08513251\n",
      "   0.93052174  0.90996585 -0.46122904]\n",
      " [-1.03047729  1.17621099 -0.45406101 -1.16841454  0.08468753  0.2671348\n",
      "  -1.00874379 -0.79745699  0.81092263]\n",
      " [ 1.18168477 -1.06553092  0.43997409  0.79804728 -0.17341082 -0.31103583\n",
      "   0.84351093  0.89736757 -0.60954992]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.56902705  1.93742595 -2.3390633   0.28150955  0.52791902 -2.97126374\n",
      "   1.31510766]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:75 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.77355553]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 75 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92959792 -1.18672138  0.27306996  1.03853366 -0.30341158 -0.17486536\n",
      "   1.28790233  0.27812236 -0.63871227]\n",
      " [-0.88199616  0.94211432 -0.48812222 -1.05084173  0.20522936 -0.08658198\n",
      "  -0.73419367 -0.44462662  0.8427855 ]\n",
      " [ 0.69488869 -0.54775888 -0.02445277  0.63376895 -0.44259289  0.25911975\n",
      "   0.7452014   0.02219655 -0.75673885]\n",
      " [ 0.73906783 -0.86075685  0.33089097  0.70790039 -0.33239669  0.08513251\n",
      "   0.9349592   0.90996585 -0.46122904]\n",
      " [-1.03460691  1.17208137 -0.45406101 -1.17254416  0.08468753  0.2671348\n",
      "  -1.0128734  -0.79745699  0.81092263]\n",
      " [ 1.18605785 -1.06115784  0.43997409  0.80242036 -0.17341082 -0.31103583\n",
      "   0.84788401  0.89736757 -0.60954992]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.54919421  1.9550026  -2.33601872  0.297749    0.54414395 -2.96896427\n",
      "   1.33202354]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:75 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.83723678]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 75 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93169764 -1.18672138  0.27516967  1.03853366 -0.30341158 -0.17276564\n",
      "   1.29000204  0.27812236 -0.63871227]\n",
      " [-0.88419398  0.94211432 -0.49032004 -1.05084173  0.20522936 -0.0887798\n",
      "  -0.73639149 -0.44462662  0.8427855 ]\n",
      " [ 0.69735691 -0.54775888 -0.02198455  0.63376895 -0.44259289  0.26158798\n",
      "   0.74766963  0.02219655 -0.75673885]\n",
      " [ 0.74133823 -0.86075685  0.33316138  0.70790039 -0.33239669  0.08740292\n",
      "   0.93722961  0.90996585 -0.46122904]\n",
      " [-1.03677144  1.17208137 -0.45622554 -1.17254416  0.08468753  0.26497027\n",
      "  -1.01503793 -0.79745699  0.81092263]\n",
      " [ 1.18827651 -1.06115784  0.44219276  0.80242036 -0.17341082 -0.30881716\n",
      "   0.85010268  0.89736757 -0.60954992]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.53810422  1.96509637 -2.33490338  0.30708909  0.55401328 -2.96789183\n",
      "   1.34196977]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:75 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.16061818]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 76 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93130774 -1.18711127  0.27516967  1.03814377 -0.30341158 -0.17276564\n",
      "   1.29000204  0.27812236 -0.63910216]\n",
      " [-0.88378987  0.94251843 -0.49032004 -1.05043762  0.20522936 -0.0887798\n",
      "  -0.73639149 -0.44462662  0.84318961]\n",
      " [ 0.69728485 -0.54783095 -0.02198455  0.63369688 -0.44259289  0.26158798\n",
      "   0.74766963  0.02219655 -0.75681091]\n",
      " [ 0.74099518 -0.86109991  0.33316138  0.70755733 -0.33239669  0.08740292\n",
      "   0.93722961  0.90996585 -0.4615721 ]\n",
      " [-1.03616663  1.17268618 -0.45622554 -1.17193935  0.08468753  0.26497027\n",
      "  -1.01503793 -0.79745699  0.81152744]\n",
      " [ 1.18743216 -1.06200219  0.44219276  0.80157601 -0.17341082 -0.30881716\n",
      "   0.85010268  0.89736757 -0.61039427]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.54893149  1.95929147 -2.33991139  0.30160338  0.54825566 -2.97269548\n",
      "   1.3356973 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:76 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.85112298]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 76 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93296345 -1.18711127  0.27682538  1.03814377 -0.30341158 -0.17276564\n",
      "   1.29165775  0.27812236 -0.63910216]\n",
      " [-0.88570888  0.94251843 -0.49223904 -1.05043762  0.20522936 -0.0887798\n",
      "  -0.73831049 -0.44462662  0.84318961]\n",
      " [ 0.69938516 -0.54783095 -0.01988424  0.63369688 -0.44259289  0.26158798\n",
      "   0.74976994  0.02219655 -0.75681091]\n",
      " [ 0.74296989 -0.86109991  0.33513609  0.70755733 -0.33239669  0.08740292\n",
      "   0.93920432  0.90996585 -0.4615721 ]\n",
      " [-1.03781422  1.17268618 -0.45787313 -1.17193935  0.08468753  0.26497027\n",
      "  -1.01668552 -0.79745699  0.81152744]\n",
      " [ 1.18910025 -1.06200219  0.44386084  0.80157601 -0.17341082 -0.30881716\n",
      "   0.85177076  0.89736757 -0.61039427]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.53949919  1.96800593 -2.33889194  0.30920396  0.55657483 -2.97198486\n",
      "   1.34440056]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:76 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.63824625]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 76 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.9167986  -1.20327612  0.26066053  1.03814377 -0.30341158 -0.17276564\n",
      "   1.2754929   0.27812236 -0.63910216]\n",
      " [-0.87010742  0.95811989 -0.47663758 -1.05043762  0.20522936 -0.0887798\n",
      "  -0.72270904 -0.44462662  0.84318961]\n",
      " [ 0.68593478 -0.56128133 -0.03333462  0.63369688 -0.44259289  0.26158798\n",
      "   0.73631955  0.02219655 -0.75681091]\n",
      " [ 0.72745949 -0.8766103   0.31962569  0.70755733 -0.33239669  0.08740292\n",
      "   0.92369392  0.90996585 -0.4615721 ]\n",
      " [-1.02157836  1.18892204 -0.44163727 -1.17193935  0.08468753  0.26497027\n",
      "  -1.00044966 -0.79745699  0.81152744]\n",
      " [ 1.17269374 -1.0784087   0.42745434  0.80157601 -0.17341082 -0.30881716\n",
      "   0.83536425  0.89736757 -0.61039427]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.61318088  1.90992724 -2.35629406  0.25710212  0.50052934 -2.98727835\n",
      "   1.28503036]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:76 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.75267291]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 76 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92193777 -1.20327612  0.2657997   1.03814377 -0.30341158 -0.17276564\n",
      "   1.2754929   0.28326153 -0.63910216]\n",
      " [-0.87515846  0.95811989 -0.48168862 -1.05043762  0.20522936 -0.0887798\n",
      "  -0.72270904 -0.44967766  0.84318961]\n",
      " [ 0.68940777 -0.56128133 -0.02986163  0.63369688 -0.44259289  0.26158798\n",
      "   0.73631955  0.02566955 -0.75681091]\n",
      " [ 0.73234607 -0.8766103   0.32451228  0.70755733 -0.33239669  0.08740292\n",
      "   0.92369392  0.91485244 -0.4615721 ]\n",
      " [-1.02602878  1.18892204 -0.44608769 -1.17193935  0.08468753  0.26497027\n",
      "  -1.00044966 -0.80190741  0.81152744]\n",
      " [ 1.17673283 -1.0784087   0.43149343  0.80157601 -0.17341082 -0.30881716\n",
      "   0.83536425  0.90140667 -0.61039427]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.59016012  1.92859376 -2.35300428  0.27235514  0.52070044 -2.98510444\n",
      "   1.3063008 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:76 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.58252138]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 76 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93308805 -1.20327612  0.2657997   1.04929404 -0.30341158 -0.17276564\n",
      "   1.2754929   0.28326153 -0.62795188]\n",
      " [-0.88554425  0.95811989 -0.48168862 -1.06082341  0.20522936 -0.0887798\n",
      "  -0.72270904 -0.44967766  0.83280382]\n",
      " [ 0.69604779 -0.56128133 -0.02986163  0.6403369  -0.44259289  0.26158798\n",
      "   0.73631955  0.02566955 -0.75017089]\n",
      " [ 0.74220784 -0.8766103   0.32451228  0.71741909 -0.33239669  0.08740292\n",
      "   0.92369392  0.91485244 -0.45171033]\n",
      " [-1.03728865  1.18892204 -0.44608769 -1.18319923  0.08468753  0.26497027\n",
      "  -1.00044966 -0.80190741  0.80026757]\n",
      " [ 1.18796581 -1.0784087   0.43149343  0.81280899 -0.17341082 -0.30881716\n",
      "   0.83536425  0.90140667 -0.59916129]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.53939677  1.96866354 -2.34015857  0.30473747  0.55759408 -2.97495295\n",
      "   1.34676138]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:76 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.09206873]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 76 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93325139 -1.20327612  0.2657997   1.04929404 -0.30324823 -0.17260229\n",
      "   1.2754929   0.28326153 -0.62778854]\n",
      " [-0.88560548  0.95811989 -0.48168862 -1.06082341  0.20516814 -0.08884103\n",
      "  -0.72270904 -0.44967766  0.83274259]\n",
      " [ 0.69627089 -0.56128133 -0.02986163  0.6403369  -0.44236979  0.26181108\n",
      "   0.73631955  0.02566955 -0.74994779]\n",
      " [ 0.74216409 -0.8766103   0.32451228  0.71741909 -0.33244045  0.08735917\n",
      "   0.92369392  0.91485244 -0.45175409]\n",
      " [-1.03739667  1.18892204 -0.44608769 -1.18319923  0.08457952  0.26486226\n",
      "  -1.00044966 -0.80190741  0.80015955]\n",
      " [ 1.18786358 -1.0784087   0.43149343  0.81280899 -0.17351306 -0.3089194\n",
      "   0.83536425  0.90140667 -0.59926353]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.54324487  1.96690363 -2.34214389  0.30303857  0.55562626 -2.97698525\n",
      "   1.34473489]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:76 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.01018958]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 76 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93326277 -1.20326474  0.2657997   1.04929404 -0.30323686 -0.17259091\n",
      "   1.2754929   0.28326153 -0.62777716]\n",
      " [-0.8856157   0.95810967 -0.48168862 -1.06082341  0.20515792 -0.08885125\n",
      "  -0.72270904 -0.44967766  0.83273237]\n",
      " [ 0.69627965 -0.56127257 -0.02986163  0.6403369  -0.44236103  0.26181984\n",
      "   0.73631955  0.02566955 -0.74993903]\n",
      " [ 0.74217311 -0.87660128  0.32451228  0.71741909 -0.33243142  0.0873682\n",
      "   0.92369392  0.91485244 -0.45174506]\n",
      " [-1.03740791  1.18891079 -0.44608769 -1.18319923  0.08456827  0.26485101\n",
      "  -1.00044966 -0.80190741  0.8001483 ]\n",
      " [ 1.18787352 -1.07839875  0.43149343  0.81280899 -0.17350311 -0.30890945\n",
      "   0.83536425  0.90140667 -0.59925358]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.54329626  1.96689325 -2.34218168  0.30302259  0.55561067 -2.97702564\n",
      "   1.34472079]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:76 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.28562258]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 76 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92966269 -1.20686482  0.2657997   1.04929404 -0.30683694 -0.17619099\n",
      "   1.27189282  0.28326153 -0.62777716]\n",
      " [-0.88199076  0.96173461 -0.48168862 -1.06082341  0.20878286 -0.08522631\n",
      "  -0.7190841  -0.44967766  0.83273237]\n",
      " [ 0.69180298 -0.56574924 -0.02986163  0.6403369  -0.4468377   0.25734317\n",
      "   0.73184289  0.02566955 -0.74993903]\n",
      " [ 0.73848828 -0.88028611  0.32451228  0.71741909 -0.33611625  0.08368337\n",
      "   0.92000909  0.91485244 -0.45174506]\n",
      " [-1.0339868   1.1923319  -0.44608769 -1.18319923  0.08798938  0.26827212\n",
      "  -0.99702855 -0.80190741  0.8001483 ]\n",
      " [ 1.18467866 -1.08159362  0.43149343  0.81280899 -0.17669798 -0.31210432\n",
      "   0.83216939  0.90140667 -0.59925358]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.57243581  1.94855269 -2.35295182  0.28361153  0.53717147 -2.98803024\n",
      "   1.32684106]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:76 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.88396139]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 76 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93035734 -1.20686482  0.2657997   1.04998869 -0.30683694 -0.17619099\n",
      "   1.27258747  0.28326153 -0.62777716]\n",
      " [-0.88295711  0.96173461 -0.48168862 -1.06178976  0.20878286 -0.08522631\n",
      "  -0.72005045 -0.44967766  0.83273237]\n",
      " [ 0.69303079 -0.56574924 -0.02986163  0.64156471 -0.4468377   0.25734317\n",
      "   0.7330707   0.02566955 -0.74993903]\n",
      " [ 0.73958832 -0.88028611  0.32451228  0.71851913 -0.33611625  0.08368337\n",
      "   0.92110913  0.91485244 -0.45174506]\n",
      " [-1.03469727  1.1923319  -0.44608769 -1.18390969  0.08798938  0.26827212\n",
      "  -0.99773902 -0.80190741  0.8001483 ]\n",
      " [ 1.18556504 -1.08159362  0.43149343  0.81369537 -0.17669798 -0.31210432\n",
      "   0.83305577  0.90140667 -0.59925358]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.56648455  1.95428198 -2.35256345  0.28889239  0.5426167  -2.98780032\n",
      "   1.33246058]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:76 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.71885616]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 76 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93665151 -1.20686482  0.2657997   1.04998869 -0.30054276 -0.16989682\n",
      "   1.27888164  0.28326153 -0.62777716]\n",
      " [-0.88930783  0.96173461 -0.48168862 -1.06178976  0.20243213 -0.09157704\n",
      "  -0.72640118 -0.44967766  0.83273237]\n",
      " [ 0.69915743 -0.56574924 -0.02986163  0.64156471 -0.44071106  0.26346981\n",
      "   0.73919733  0.02566955 -0.74993903]\n",
      " [ 0.74590538 -0.88028611  0.32451228  0.71851913 -0.32979919  0.09000043\n",
      "   0.92742619  0.91485244 -0.45174506]\n",
      " [-1.04101948  1.1923319  -0.44608769 -1.18390969  0.08166717  0.26194991\n",
      "  -1.00406123 -0.80190741  0.8001483 ]\n",
      " [ 1.19192478 -1.08159362  0.43149343  0.81369537 -0.17033823 -0.30574457\n",
      "   0.83941551  0.90140667 -0.59925358]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.53807469  1.9783769  -2.34729312  0.31090926  0.5654438  -2.98332285\n",
      "   1.35581291]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:76 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.59598137]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 76 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92138525 -1.20686482  0.2657997   1.03472243 -0.30054276 -0.18516308\n",
      "   1.27888164  0.28326153 -0.64304342]\n",
      " [-0.87394621  0.96173461 -0.48168862 -1.04642813  0.20243213 -0.07621541\n",
      "  -0.72640118 -0.44967766  0.848094  ]\n",
      " [ 0.68632143 -0.56574924 -0.02986163  0.62872871 -0.44071106  0.2506338\n",
      "   0.73919733  0.02566955 -0.76277504]\n",
      " [ 0.73110057 -0.88028611  0.32451228  0.70371432 -0.32979919  0.07519562\n",
      "   0.92742619  0.91485244 -0.46654987]\n",
      " [-1.02588117  1.1923319  -0.44608769 -1.16877138  0.08166717  0.27708823\n",
      "  -1.00406123 -0.80190741  0.81528662]\n",
      " [ 1.17713235 -1.08159362  0.43149343  0.79890294 -0.17033823 -0.320537\n",
      "   0.83941551  0.90140667 -0.61404601]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.60982715  1.92337462 -2.36377547  0.26057789  0.51157479 -3.00041093\n",
      "   1.30197155]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:76 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.71975592]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 76 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92753247 -1.20071761  0.2657997   1.04086964 -0.30054276 -0.17901587\n",
      "   1.28502886  0.28326153 -0.64304342]\n",
      " [-0.88017521  0.9555056  -0.48168862 -1.05265713  0.20243213 -0.08244441\n",
      "  -0.73263018 -0.44967766  0.848094  ]\n",
      " [ 0.69256872 -0.55950194 -0.02986163  0.63497601 -0.44071106  0.2568811\n",
      "   0.74544463  0.02566955 -0.76277504]\n",
      " [ 0.73742758 -0.8739591   0.32451228  0.71004133 -0.32979919  0.08152262\n",
      "   0.93375319  0.91485244 -0.46654987]\n",
      " [-1.03213605  1.18607702 -0.44608769 -1.17502626  0.08166717  0.27083335\n",
      "  -1.01031611 -0.80190741  0.81528662]\n",
      " [ 1.18342011 -1.07530586  0.43149343  0.8051907  -0.17033823 -0.31424924\n",
      "   0.84570327  0.90140667 -0.61404601]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.5815635   1.94777458 -2.35963109  0.28461829  0.53491898 -2.99615222\n",
      "   1.32470369]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:76 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.77505011]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 76 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93159131 -1.19665877  0.2657997   1.04492848 -0.30054276 -0.17901587\n",
      "   1.2890877   0.28326153 -0.64304342]\n",
      " [-0.88452456  0.95115625 -0.48168862 -1.05700649  0.20243213 -0.08244441\n",
      "  -0.73697953 -0.44967766  0.848094  ]\n",
      " [ 0.69695737 -0.55511329 -0.02986163  0.63936466 -0.44071106  0.2568811\n",
      "   0.74983328  0.02566955 -0.76277504]\n",
      " [ 0.74181557 -0.8695711   0.32451228  0.71442932 -0.32979919  0.08152262\n",
      "   0.93814119  0.91485244 -0.46654987]\n",
      " [-1.03621923  1.18199384 -0.44608769 -1.17910945  0.08166717  0.27083335\n",
      "  -1.01439929 -0.80190741  0.81528662]\n",
      " [ 1.18774322 -1.07098274  0.43149343  0.80951381 -0.17033823 -0.31424924\n",
      "   0.85002638  0.90140667 -0.61404601]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.56195378  1.96515313 -2.35662748  0.30068997  0.55097258 -2.99387857\n",
      "   1.34143245]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:76 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.83765363]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 76 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.933688   -1.19665877  0.26789639  1.04492848 -0.30054276 -0.17691917\n",
      "   1.29118439  0.28326153 -0.64304342]\n",
      " [-0.88671616  0.95115625 -0.48388022 -1.05700649  0.20243213 -0.08463601\n",
      "  -0.73917113 -0.44967766  0.848094  ]\n",
      " [ 0.6994144  -0.55511329 -0.0274046   0.63936466 -0.44071106  0.25933812\n",
      "   0.75229031  0.02566955 -0.76277504]\n",
      " [ 0.74407825 -0.8695711   0.32677495  0.71442932 -0.32979919  0.0837853\n",
      "   0.94040387  0.91485244 -0.46654987]\n",
      " [-1.03838035  1.18199384 -0.4482488  -1.17910945  0.08166717  0.26867223\n",
      "  -1.01656041 -0.80190741  0.81528662]\n",
      " [ 1.18995738 -1.07098274  0.43370758  0.80951381 -0.17033823 -0.31203509\n",
      "   0.85224053  0.90140667 -0.61404601]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.55091504  1.9751928  -2.35551204  0.30998551  0.56079192 -2.99280283\n",
      "   1.35132467]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:76 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.15646161]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 77 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93333115 -1.19701561  0.26789639  1.04457164 -0.30054276 -0.17691917\n",
      "   1.29118439  0.28326153 -0.64340027]\n",
      " [-0.88634518  0.95152723 -0.48388022 -1.05663551  0.20243213 -0.08463601\n",
      "  -0.73917113 -0.44967766  0.84846498]\n",
      " [ 0.69936048 -0.55516721 -0.0274046   0.63931074 -0.44071106  0.25933812\n",
      "   0.75229031  0.02566955 -0.76282896]\n",
      " [ 0.74376352 -0.86988584  0.32677495  0.71411459 -0.32979919  0.0837853\n",
      "   0.94040387  0.91485244 -0.4668646 ]\n",
      " [-1.03781877  1.18255542 -0.4482488  -1.17854787  0.08166717  0.26867223\n",
      "  -1.01656041 -0.80190741  0.8158482 ]\n",
      " [ 1.18916546 -1.07177466  0.43370758  0.80872189 -0.17033823 -0.31203509\n",
      "   0.85224053  0.90140667 -0.61483793]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.56124005  1.96967229 -2.36030227  0.30476909  0.55531389 -2.99739921\n",
      "   1.34535713]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:77 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.85221921]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 77 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93496764 -1.19701561  0.26953288  1.04457164 -0.30054276 -0.17691917\n",
      "   1.29282088  0.28326153 -0.64340027]\n",
      " [-0.88823911  0.95152723 -0.48577415 -1.05663551  0.20243213 -0.08463601\n",
      "  -0.74106506 -0.44967766  0.84846498]\n",
      " [ 0.70143288 -0.55516721 -0.0253322   0.63931074 -0.44071106  0.25933812\n",
      "   0.75436271  0.02566955 -0.76282896]\n",
      " [ 0.745712   -0.86988584  0.32872343  0.71411459 -0.32979919  0.0837853\n",
      "   0.94235235  0.91485244 -0.4668646 ]\n",
      " [-1.03944779  1.18255542 -0.44987782 -1.17854787  0.08166717  0.26867223\n",
      "  -1.01818943 -0.80190741  0.8158482 ]\n",
      " [ 1.19081454 -1.07177466  0.43535666  0.80872189 -0.17033823 -0.31203509\n",
      "   0.85388962  0.90140667 -0.61483793]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.55193417  1.97826728 -2.3592955   0.31226965  0.56352113 -2.99669499\n",
      "   1.35394067]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:77 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.63364489]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 77 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.91887551 -1.21310775  0.25344074  1.04457164 -0.30054276 -0.17691917\n",
      "   1.27672875  0.28326153 -0.64340027]\n",
      " [-0.87271846  0.96704788 -0.4702535  -1.05663551  0.20243213 -0.08463601\n",
      "  -0.72554441 -0.44967766  0.84846498]\n",
      " [ 0.68806682 -0.56853327 -0.03869826  0.63931074 -0.44071106  0.25933812\n",
      "   0.74099665  0.02566955 -0.76282896]\n",
      " [ 0.73028017 -0.88531766  0.3132916   0.71411459 -0.32979919  0.0837853\n",
      "   0.92692052  0.91485244 -0.4668646 ]\n",
      " [-1.02328171  1.19872149 -0.43371175 -1.17854787  0.08166717  0.26867223\n",
      "  -1.00202335 -0.80190741  0.8158482 ]\n",
      " [ 1.17446103 -1.08812817  0.41900315  0.80872189 -0.17033823 -0.31203509\n",
      "   0.83753611  0.90140667 -0.61483793]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.62548103  1.9204678  -2.37680131  0.26035686  0.50770314 -3.01213942\n",
      "   1.2948461 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:77 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.7542042]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 77 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92396156 -1.21310775  0.25852679  1.04457164 -0.30054276 -0.17691917\n",
      "   1.27672875  0.28834758 -0.64340027]\n",
      " [-0.8777163   0.96704788 -0.47525134 -1.05663551  0.20243213 -0.08463601\n",
      "  -0.72554441 -0.4546755   0.84846498]\n",
      " [ 0.69150488 -0.56853327 -0.0352602   0.63931074 -0.44071106  0.25933812\n",
      "   0.74099665  0.02910761 -0.76282896]\n",
      " [ 0.73511464 -0.88531766  0.31812608  0.71411459 -0.32979919  0.0837853\n",
      "   0.92692052  0.91968691 -0.4668646 ]\n",
      " [-1.02768901  1.19872149 -0.43811905 -1.17854787  0.08166717  0.26867223\n",
      "  -1.00202335 -0.80631471  0.8158482 ]\n",
      " [ 1.17846311 -1.08812817  0.42300523  0.80872189 -0.17033823 -0.31203509\n",
      "   0.83753611  0.90540875 -0.61483793]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.60269819  1.93894139 -2.37354911  0.27545346  0.52766916 -3.00998452\n",
      "   1.31589247]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:77 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.58407947]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 77 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.9350665  -1.21310775  0.25852679  1.05567658 -0.30054276 -0.17691917\n",
      "   1.27672875  0.28834758 -0.63229533]\n",
      " [-0.88806769  0.96704788 -0.47525134 -1.0669869   0.20243213 -0.08463601\n",
      "  -0.72554441 -0.4546755   0.83811359]\n",
      " [ 0.69812971 -0.56853327 -0.0352602   0.64593557 -0.44071106  0.25933812\n",
      "   0.74099665  0.02910761 -0.75620412]\n",
      " [ 0.74495154 -0.88531766  0.31812608  0.72395149 -0.32979919  0.0837853\n",
      "   0.92692052  0.91968691 -0.45702771]\n",
      " [-1.03890018  1.19872149 -0.43811905 -1.18975903  0.08166717  0.26867223\n",
      "  -1.00202335 -0.80631471  0.80463703]\n",
      " [ 1.18964892 -1.08812817  0.42300523  0.8199077  -0.17033823 -0.31203509\n",
      "   0.83753611  0.90540875 -0.60365212]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.55217827  1.97885386 -2.3607982   0.30770025  0.5644263  -2.99991359\n",
      "   1.3561952 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:77 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.08952502]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 77 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93522463 -1.21310775  0.25852679  1.05567658 -0.30038463 -0.17676104\n",
      "   1.27672875  0.28834758 -0.63213719]\n",
      " [-0.8881295   0.96704788 -0.47525134 -1.0669869   0.20237032 -0.08469782\n",
      "  -0.72554441 -0.4546755   0.83805178]\n",
      " [ 0.69834502 -0.56853327 -0.0352602   0.64593557 -0.44049575  0.25955344\n",
      "   0.74099665  0.02910761 -0.75598881]\n",
      " [ 0.74491332 -0.88531766  0.31812608  0.72395149 -0.3298374   0.08374709\n",
      "   0.92692052  0.91968691 -0.45706592]\n",
      " [-1.0390057   1.19872149 -0.43811905 -1.18975903  0.08156165  0.26856671\n",
      "  -1.00202335 -0.80631471  0.80453151]\n",
      " [ 1.18955465 -1.08812817  0.42300523  0.8199077  -0.1704325  -0.31212936\n",
      "   0.83753611  0.90540875 -0.60374638]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.55582688  1.9771885  -2.36268436  0.30609332  0.56256378 -3.00184365\n",
      "   1.35427646]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:77 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.00958265]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 77 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93523472 -1.21309766  0.25852679  1.05567658 -0.30037454 -0.17675095\n",
      "   1.27672875  0.28834758 -0.6321271 ]\n",
      " [-0.8881386   0.96703878 -0.47525134 -1.0669869   0.20236122 -0.08470692\n",
      "  -0.72554441 -0.4546755   0.83804268]\n",
      " [ 0.69835285 -0.56852544 -0.0352602   0.64593557 -0.44048792  0.25956127\n",
      "   0.74099665  0.02910761 -0.75598098]\n",
      " [ 0.74492139 -0.8853096   0.31812608  0.72395149 -0.32982933  0.08375516\n",
      "   0.92692052  0.91968691 -0.45705785]\n",
      " [-1.03901568  1.19871152 -0.43811905 -1.18975903  0.08155167  0.26855673\n",
      "  -1.00202335 -0.80631471  0.80452153]\n",
      " [ 1.18956352 -1.08811931  0.42300523  0.8199077  -0.17042363 -0.31212049\n",
      "   0.83753611  0.90540875 -0.60373752]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.55587235  1.97717941 -2.36271791  0.30607929  0.5625501  -3.00187949\n",
      "   1.35426409]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:77 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.28058973]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 77 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93178122 -1.21655115  0.25852679  1.05567658 -0.30382803 -0.18020444\n",
      "   1.27327525  0.28834758 -0.6321271 ]\n",
      " [-0.88464363  0.97053375 -0.47525134 -1.0669869   0.20585618 -0.08121195\n",
      "  -0.72204944 -0.4546755   0.83804268]\n",
      " [ 0.69400643 -0.57287186 -0.0352602   0.64593557 -0.44483434  0.25521485\n",
      "   0.73665023  0.02910761 -0.75598098]\n",
      " [ 0.74136133 -0.88886965  0.31812608  0.72395149 -0.33338939  0.0801951\n",
      "   0.92336046  0.91968691 -0.45705785]\n",
      " [-1.03573363  1.20199356 -0.43811905 -1.18975903  0.08483372  0.27183878\n",
      "  -0.99874131 -0.80631471  0.80452153]\n",
      " [ 1.18649461 -1.09118822  0.42300523  0.8199077  -0.17349254 -0.3151894\n",
      "   0.83446719  0.90540875 -0.60373752]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.58419215  1.95940719 -2.37321755  0.28721989  0.54465428 -3.0126232\n",
      "   1.3369276 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:77 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.88611644]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 77 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93244785 -1.21655115  0.25852679  1.0563432  -0.30382803 -0.18020444\n",
      "   1.27394188  0.28834758 -0.6321271 ]\n",
      " [-0.88557126  0.97053375 -0.47525134 -1.06791453  0.20585618 -0.08121195\n",
      "  -0.72297707 -0.4546755   0.83804268]\n",
      " [ 0.69518757 -0.57287186 -0.0352602   0.64711671 -0.44483434  0.25521485\n",
      "   0.73783137  0.02910761 -0.75598098]\n",
      " [ 0.74241792 -0.88886965  0.31812608  0.72500807 -0.33338939  0.0801951\n",
      "   0.92441704  0.91968691 -0.45705785]\n",
      " [-1.03641543  1.20199356 -0.43811905 -1.19044083  0.08483372  0.27183878\n",
      "  -0.9994231  -0.80631471  0.80452153]\n",
      " [ 1.18734532 -1.09118822  0.42300523  0.82075841 -0.17349254 -0.3151894\n",
      "   0.8353179   0.90540875 -0.60373752]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.57844592  1.96494114 -2.37284666  0.29232603  0.54991759 -3.01240333\n",
      "   1.3423569 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:77 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.72110111]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 77 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.9386595  -1.21655115  0.25852679  1.0563432  -0.29761638 -0.17399279\n",
      "   1.28015353  0.28834758 -0.6321271 ]\n",
      " [-0.89184175  0.97053375 -0.47525134 -1.06791453  0.19958569 -0.08748244\n",
      "  -0.72924756 -0.4546755   0.83804268]\n",
      " [ 0.70124609 -0.57287186 -0.0352602   0.64711671 -0.43877583  0.26127336\n",
      "   0.74388988  0.02910761 -0.75598098]\n",
      " [ 0.7486573  -0.88886965  0.31812608  0.72500807 -0.32715001  0.08643447\n",
      "   0.93065642  0.91968691 -0.45705785]\n",
      " [-1.04265484  1.20199356 -0.43811905 -1.19044083  0.07859431  0.26559937\n",
      "  -1.00566251 -0.80631471  0.80452153]\n",
      " [ 1.19362366 -1.09118822  0.42300523  0.82075841 -0.1672142  -0.30891106\n",
      "   0.84159625  0.90540875 -0.60373752]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.55040065  1.98873565 -2.36766282  0.31409351  0.57247552 -3.00799445\n",
      "   1.36542661]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:77 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.59593472]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 77 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92339426 -1.21655115  0.25852679  1.04107796 -0.29761638 -0.18925804\n",
      "   1.28015353  0.28834758 -0.64739235]\n",
      " [-0.87648404  0.97053375 -0.47525134 -1.05255681  0.19958569 -0.07212473\n",
      "  -0.72924756 -0.4546755   0.8534004 ]\n",
      " [ 0.6884165  -0.57287186 -0.0352602   0.63428712 -0.43877583  0.24844377\n",
      "   0.74388988  0.02910761 -0.76881057]\n",
      " [ 0.73385092 -0.88886965  0.31812608  0.7102017  -0.32715001  0.0716281\n",
      "   0.93065642  0.91968691 -0.47186423]\n",
      " [-1.02751642  1.20199356 -0.43811905 -1.1753024   0.07859431  0.28073779\n",
      "  -1.00566251 -0.80631471  0.81965995]\n",
      " [ 1.17882514 -1.09118822  0.42300523  0.8059599  -0.1672142  -0.32370958\n",
      "   0.84159625  0.90540875 -0.61853603]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.62215015  1.9337367  -2.38415396  0.26377319  0.51860386 -3.02507995\n",
      "   1.31157252]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:77 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.72039623]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 77 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92952351 -1.21042191  0.25852679  1.04720721 -0.29761638 -0.18312879\n",
      "   1.28628278  0.28834758 -0.64739235]\n",
      " [-0.88269131  0.96432648 -0.47525134 -1.05876408  0.19958569 -0.078332\n",
      "  -0.73545483 -0.4546755   0.8534004 ]\n",
      " [ 0.69463842 -0.56664994 -0.0352602   0.64050904 -0.43877583  0.25466569\n",
      "   0.7501118   0.02910761 -0.76881057]\n",
      " [ 0.74015463 -0.88256594  0.31812608  0.71650541 -0.32715001  0.07793181\n",
      "   0.93696013  0.91968691 -0.47186423]\n",
      " [-1.03375092  1.19575906 -0.43811905 -1.18153691  0.07859431  0.27450329\n",
      "  -1.01189702 -0.80631471  0.81965995]\n",
      " [ 1.18508862 -1.08492474  0.42300523  0.81222338 -0.1672142  -0.3174461\n",
      "   0.84785973  0.90540875 -0.61853603]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.59399049  1.9580331  -2.38001997  0.28773599  0.54186345 -3.02082413\n",
      "   1.33421289]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:77 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.77653835]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 77 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93353665 -1.20640876  0.25852679  1.05122035 -0.29761638 -0.18312879\n",
      "   1.29029592  0.28834758 -0.64739235]\n",
      " [-0.8869903   0.96002748 -0.47525134 -1.06306308  0.19958569 -0.078332\n",
      "  -0.73975383 -0.4546755   0.8534004 ]\n",
      " [ 0.69897794 -0.56231042 -0.0352602   0.64484857 -0.43877583  0.25466569\n",
      "   0.75445132  0.02910761 -0.76881057]\n",
      " [ 0.74449348 -0.87822709  0.31812608  0.72084426 -0.32715001  0.07793181\n",
      "   0.94129898  0.91968691 -0.47186423]\n",
      " [-1.03778797  1.19172201 -0.43811905 -1.18557395  0.07859431  0.27450329\n",
      "  -1.01593407 -0.80631471  0.81965995]\n",
      " [ 1.18936218 -1.08065118  0.42300523  0.81649694 -0.1672142  -0.3174461\n",
      "   0.85213329  0.90540875 -0.61853603]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.57460222  1.97521516 -2.37705688  0.30364084  0.55774665 -3.0185762\n",
      "   1.35075574]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:77 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.83804233]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 77 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.9356309  -1.20640876  0.26062104  1.05122035 -0.29761638 -0.18103454\n",
      "   1.29239017  0.28834758 -0.64739235]\n",
      " [-0.88917637  0.96002748 -0.4774374  -1.06306308  0.19958569 -0.08051806\n",
      "  -0.74193989 -0.4546755   0.8534004 ]\n",
      " [ 0.70142456 -0.56231042 -0.03281359  0.64484857 -0.43877583  0.2571123\n",
      "   0.75689794  0.02910761 -0.76881057]\n",
      " [ 0.74674917 -0.87822709  0.32038177  0.72084426 -0.32715001  0.0801875\n",
      "   0.94355467  0.91968691 -0.47186423]\n",
      " [-1.03994626  1.19172201 -0.44027734 -1.18557395  0.07859431  0.272345\n",
      "  -1.01809235 -0.80631471  0.81965995]\n",
      " [ 1.19157251 -1.08065118  0.42521556  0.81649694 -0.1672142  -0.31523578\n",
      "   0.85434361  0.90540875 -0.61853603]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.56361118  1.98520397 -2.37594095  0.31289479  0.56751908 -3.01749683\n",
      "   1.36059705]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:77 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.15239159]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 78 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93530497 -1.20673469  0.26062104  1.05089442 -0.29761638 -0.18103454\n",
      "   1.29239017  0.28834758 -0.64771828]\n",
      " [-0.88883646  0.96036739 -0.4774374  -1.06272317  0.19958569 -0.08051806\n",
      "  -0.74193989 -0.4546755   0.85374031]\n",
      " [ 0.70138728 -0.5623477  -0.03281359  0.64481129 -0.43877583  0.2571123\n",
      "   0.75689794  0.02910761 -0.76884785]\n",
      " [ 0.74646105 -0.87851521  0.32038177  0.72055614 -0.32715001  0.0801875\n",
      "   0.94355467  0.91968691 -0.47215235]\n",
      " [-1.03942536  1.19224291 -0.44027734 -1.18505306  0.07859431  0.272345\n",
      "  -1.01809235 -0.80631471  0.82018085]\n",
      " [ 1.19083034 -1.08139334  0.42521556  0.81575477 -0.1672142  -0.31523578\n",
      "   0.85434361  0.90540875 -0.6192782 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.57345327  1.97995603 -2.38052099  0.30793646  0.56230926 -3.02189299\n",
      "   1.35492198]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:78 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.85328502]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 78 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.9369229  -1.20673469  0.26223897  1.05089442 -0.29761638 -0.18103454\n",
      "   1.2940081   0.28834758 -0.64771828]\n",
      " [-0.89070619  0.96036739 -0.47930713 -1.06272317  0.19958569 -0.08051806\n",
      "  -0.74380962 -0.4546755   0.85374031]\n",
      " [ 0.70343267 -0.5623477  -0.0307682   0.64481129 -0.43877583  0.2571123\n",
      "   0.75894333  0.02910761 -0.76884785]\n",
      " [ 0.7483842  -0.87851521  0.32230492  0.72055614 -0.32715001  0.0801875\n",
      "   0.94547783  0.91968691 -0.47215235]\n",
      " [-1.04103646  1.19224291 -0.44188844 -1.18505306  0.07859431  0.272345\n",
      "  -1.01970346 -0.80631471  0.82018085]\n",
      " [ 1.19246114 -1.08139334  0.42684636  0.81575477 -0.1672142  -0.31523578\n",
      "   0.85597441  0.90540875 -0.6192782 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.56426967  1.98843542 -2.3795264   0.31534023  0.57040813 -3.02119491\n",
      "   1.36338962]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:78 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.62895298]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 78 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92091046 -1.22274713  0.24622653  1.05089442 -0.29761638 -0.18103454\n",
      "   1.27799566  0.28834758 -0.64771828]\n",
      " [-0.87527193  0.97580164 -0.46387288 -1.06272317  0.19958569 -0.08051806\n",
      "  -0.72837537 -0.4546755   0.85374031]\n",
      " [ 0.69015429 -0.57562608 -0.04404658  0.64481129 -0.43877583  0.2571123\n",
      "   0.74566495  0.02910761 -0.76884785]\n",
      " [ 0.73303643 -0.89386299  0.30695715  0.72055614 -0.32715001  0.0801875\n",
      "   0.93013005  0.91968691 -0.47215235]\n",
      " [-1.0249475   1.20833187 -0.42579948 -1.18505306  0.07859431  0.272345\n",
      "  -1.0036145  -0.80631471  0.82018085]\n",
      " [ 1.17616816 -1.09768632  0.41055338  0.81575477 -0.1672142  -0.31523578\n",
      "   0.83968144  0.90540875 -0.6192782 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.6376594   1.9309314  -2.39712955  0.26363043  0.51483389 -3.03678453\n",
      "   1.30458887]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:78 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.75571324]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 78 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92594427 -1.22274713  0.25126035  1.05089442 -0.29761638 -0.18103454\n",
      "   1.27799566  0.29338139 -0.64771828]\n",
      " [-0.88021756  0.97580164 -0.4688185  -1.06272317  0.19958569 -0.08051806\n",
      "  -0.72837537 -0.45962113  0.85374031]\n",
      " [ 0.69355775 -0.57562608 -0.04064312  0.64481129 -0.43877583  0.2571123\n",
      "   0.74566495  0.03251108 -0.76884785]\n",
      " [ 0.73781983 -0.89386299  0.31174054  0.72055614 -0.32715001  0.0801875\n",
      "   0.93013005  0.9244703  -0.47215235]\n",
      " [-1.02931252  1.20833187 -0.43016449 -1.18505306  0.07859431  0.272345\n",
      "  -1.0036145  -0.81067973  0.82018085]\n",
      " [ 1.18013401 -1.09768632  0.41451923  0.81575477 -0.1672142  -0.31523578\n",
      "   0.83968144  0.90937459 -0.6192782 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.61511042  1.94921504 -2.39391395  0.27857296  0.53459803 -3.0346482\n",
      "   1.32541484]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:78 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.58561262]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 78 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93700405 -1.22274713  0.25126035  1.0619542  -0.29761638 -0.18103454\n",
      "   1.27799566  0.29338139 -0.6366585 ]\n",
      " [-0.89053422  0.97580164 -0.4688185  -1.07303983  0.19958569 -0.08051806\n",
      "  -0.72837537 -0.45962113  0.84342365]\n",
      " [ 0.7001661  -0.57562608 -0.04064312  0.65141964 -0.43877583  0.2571123\n",
      "   0.74566495  0.03251108 -0.7622395 ]\n",
      " [ 0.74763114 -0.89386299  0.31174054  0.73036745 -0.32715001  0.0801875\n",
      "   0.93013005  0.9244703  -0.46234103]\n",
      " [-1.04047532  1.20833187 -0.43016449 -1.19621586  0.07859431  0.272345\n",
      "  -1.0036145  -0.81067973  0.80901805]\n",
      " [ 1.19127289 -1.09768632  0.41451923  0.82689365 -0.1672142  -0.31523578\n",
      "   0.83968144  0.90937459 -0.60813933]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.56483063  1.98897102 -2.38125507  0.31068429  0.57121888 -3.02465557\n",
      "   1.36556054]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:78 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.08705967]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 78 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93715705 -1.22274713  0.25126035  1.0619542  -0.29746338 -0.18088154\n",
      "   1.27799566  0.29338139 -0.6365055 ]\n",
      " [-0.89059638  0.97580164 -0.4688185  -1.07303983  0.19952353 -0.08058022\n",
      "  -0.72837537 -0.45962113  0.84336149]\n",
      " [ 0.70037382 -0.57562608 -0.04064312  0.65141964 -0.43856811  0.25732002\n",
      "   0.74566495  0.03251108 -0.76203178]\n",
      " [ 0.747598   -0.89386299  0.31174054  0.73036745 -0.32718315  0.08015436\n",
      "   0.93013005  0.9244703  -0.46237417]\n",
      " [-1.0405783   1.20833187 -0.43016449 -1.19621586  0.07849133  0.27224202\n",
      "  -1.0036145  -0.81067973  0.80891507]\n",
      " [ 1.19118602 -1.09768632  0.41451923  0.82689365 -0.16730107 -0.31532264\n",
      "   0.83968144  0.90937459 -0.60822619]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.56829039  1.98739496 -2.38304716  0.30916419  0.56945585 -3.02648867\n",
      "   1.36374365]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:78 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.00901618]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 78 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.937166   -1.22273818  0.25126035  1.0619542  -0.29745443 -0.18087259\n",
      "   1.27799566  0.29338139 -0.63649655]\n",
      " [-0.89060449  0.97579353 -0.4688185  -1.07303983  0.19951542 -0.08058833\n",
      "  -0.72837537 -0.45962113  0.84335338]\n",
      " [ 0.70038082 -0.57561908 -0.04064312  0.65141964 -0.43856111  0.25732702\n",
      "   0.74566495  0.03251108 -0.76202478]\n",
      " [ 0.74760522 -0.89385577  0.31174054  0.73036745 -0.32717594  0.08016157\n",
      "   0.93013005  0.9244703  -0.46236695]\n",
      " [-1.04058716  1.20832301 -0.43016449 -1.19621586  0.07848247  0.27223316\n",
      "  -1.0036145  -0.81067973  0.80890621]\n",
      " [ 1.19119393 -1.09767841  0.41451923  0.82689365 -0.16729316 -0.31531473\n",
      "   0.83968144  0.90937459 -0.60821828]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.56833067  1.98738699 -2.38307698  0.30915186  0.56944384 -3.02652051\n",
      "   1.36373279]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:78 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.27569343]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 78 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93385206 -1.22605212  0.25126035  1.0619542  -0.30076837 -0.18418653\n",
      "   1.27468172  0.29338139 -0.63649655]\n",
      " [-0.88723374  0.97916428 -0.4688185  -1.07303983  0.20288617 -0.07721759\n",
      "  -0.72500462 -0.45962113  0.84335338]\n",
      " [ 0.69615954 -0.57984036 -0.04064312  0.65141964 -0.44278239  0.25310574\n",
      "   0.74144366  0.03251108 -0.76202478]\n",
      " [ 0.74416469 -0.8972963   0.31174054  0.73036745 -0.33061647  0.07672104\n",
      "   0.92668952  0.9244703  -0.46236695]\n",
      " [-1.0374375   1.21147267 -0.43016449 -1.19621586  0.08163213  0.27538282\n",
      "  -1.00046484 -0.81067973  0.80890621]\n",
      " [ 1.1882454  -1.10062695  0.41451923  0.82689365 -0.17024169 -0.31826327\n",
      "   0.8367329   0.90937459 -0.60821828]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.59585681  1.97016206 -2.39331276  0.29082531  0.55207257 -3.03700889\n",
      "   1.3469204 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:78 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.88821912]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 78 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93449194 -1.22605212  0.25126035  1.06259408 -0.30076837 -0.18418653\n",
      "   1.2753216   0.29338139 -0.63649655]\n",
      " [-0.88812439  0.97916428 -0.4688185  -1.07393048  0.20288617 -0.07721759\n",
      "  -0.72589527 -0.45962113  0.84335338]\n",
      " [ 0.69729594 -0.57984036 -0.04064312  0.65255604 -0.44278239  0.25310574\n",
      "   0.74258007  0.03251108 -0.76202478]\n",
      " [ 0.74517973 -0.8972963   0.31174054  0.73138249 -0.33061647  0.07672104\n",
      "   0.92770456  0.9244703  -0.46236695]\n",
      " [-1.03809193  1.21147267 -0.43016449 -1.19687029  0.08163213  0.27538282\n",
      "  -1.00111926 -0.81067973  0.80890621]\n",
      " [ 1.18906207 -1.10062695  0.41451923  0.82771032 -0.17024169 -0.31826327\n",
      "   0.83754957  0.90937459 -0.60821828]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.59030767  1.97550811 -2.39295846  0.2957631   0.55716068 -3.03679857\n",
      "   1.35216664]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:78 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.7233476]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 78 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94062117 -1.22605212  0.25126035  1.06259408 -0.29463914 -0.1780573\n",
      "   1.28145083  0.29338139 -0.63649655]\n",
      " [-0.89431465  0.97916428 -0.4688185  -1.07393048  0.19669591 -0.08340785\n",
      "  -0.73208553 -0.45962113  0.84335338]\n",
      " [ 0.70328594 -0.57984036 -0.04064312  0.65255604 -0.4367924   0.25909574\n",
      "   0.74857006  0.03251108 -0.76202478]\n",
      " [ 0.7513413  -0.8972963   0.31174054  0.73138249 -0.3244549   0.08288262\n",
      "   0.93386614  0.9244703  -0.46236695]\n",
      " [-1.04424866  1.21147267 -0.43016449 -1.19687029  0.0754754   0.26922609\n",
      "  -1.007276   -0.81067973  0.80890621]\n",
      " [ 1.19525906 -1.10062695  0.41451923  0.82771032 -0.16404471 -0.31206628\n",
      "   0.84374656  0.90937459 -0.60821828]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.56262641  1.99900268 -2.38786042  0.3172807   0.57944908 -3.03245801\n",
      "   1.37495356]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:78 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.59582969]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 78 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92535838 -1.22605212  0.25126035  1.04733129 -0.29463914 -0.19332009\n",
      "   1.28145083  0.29338139 -0.65175934]\n",
      " [-0.87896231  0.97916428 -0.4688185  -1.05857814  0.19669591 -0.0680555\n",
      "  -0.73208553 -0.45962113  0.85870572]\n",
      " [ 0.69046486 -0.57984036 -0.04064312  0.63973497 -0.4367924   0.24627466\n",
      "   0.74857006  0.03251108 -0.77484585]\n",
      " [ 0.73653511 -0.8972963   0.31174054  0.71657631 -0.3244549   0.06807643\n",
      "   0.93386614  0.9244703  -0.47717314]\n",
      " [-1.02911157  1.21147267 -0.43016449 -1.18173321  0.0754754   0.28436317\n",
      "  -1.007276   -0.81067973  0.82404329]\n",
      " [ 1.18045619 -1.10062695  0.41451923  0.81290746 -0.16404471 -0.32686914\n",
      "   0.84374656  0.90937459 -0.62302114]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.63436927  1.94401167 -2.40436143  0.2669762   0.52557976 -3.04954178\n",
      "   1.32109169]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:78 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.72103198]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 78 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93146964 -1.21994086  0.25126035  1.05344255 -0.29463914 -0.18720883\n",
      "   1.28756209  0.29338139 -0.65175934]\n",
      " [-0.885148    0.97297859 -0.4688185  -1.06476382  0.19669591 -0.07424119\n",
      "  -0.73827122 -0.45962113  0.85870572]\n",
      " [ 0.69666156 -0.57364366 -0.04064312  0.64593166 -0.4367924   0.25247136\n",
      "   0.75476676  0.03251108 -0.77484585]\n",
      " [ 0.74281571 -0.89101571  0.31174054  0.7228569  -0.3244549   0.07435702\n",
      "   0.94014673  0.9244703  -0.47717314]\n",
      " [-1.03532576  1.20525848 -0.43016449 -1.18794739  0.0754754   0.27814898\n",
      "  -1.01349018 -0.81067973  0.82404329]\n",
      " [ 1.18669555 -1.0943876   0.41451923  0.81914681 -0.16404471 -0.32062979\n",
      "   0.84998591  0.90937459 -0.62302114]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.60631278  1.96820559 -2.40023779  0.290862    0.5487553  -3.04528906\n",
      "   1.34364083]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:78 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.77802219]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 78 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.9354373  -1.21597321  0.25126035  1.0574102  -0.29463914 -0.18720883\n",
      "   1.29152975  0.29338139 -0.65175934]\n",
      " [-0.88939696  0.96872962 -0.4688185  -1.06901279  0.19669591 -0.07424119\n",
      "  -0.74252018 -0.45962113  0.85870572]\n",
      " [ 0.70095218 -0.56935304 -0.04064312  0.65022228 -0.4367924   0.25247136\n",
      "   0.75905738  0.03251108 -0.77484585]\n",
      " [ 0.74710566 -0.88672575  0.31174054  0.72714686 -0.3244549   0.07435702\n",
      "   0.94443669  0.9244703  -0.47717314]\n",
      " [-1.0393169   1.20126735 -0.43016449 -1.19193853  0.0754754   0.27814898\n",
      "  -1.01748132 -0.81067973  0.82404329]\n",
      " [ 1.1909199  -1.09016324  0.41451923  0.82337116 -0.16404471 -0.32062979\n",
      "   0.85421027  0.90937459 -0.62302114]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.58714459  1.9851925  -2.39731484  0.30660072  0.56446885 -3.04306681\n",
      "   1.35999877]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:78 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.8384048]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 78 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93752962 -1.21597321  0.25335267  1.0574102  -0.29463914 -0.18511651\n",
      "   1.29362207  0.29338139 -0.65175934]\n",
      " [-0.89157812  0.96872962 -0.47099967 -1.06901279  0.19669591 -0.07642235\n",
      "  -0.74470134 -0.45962113  0.85870572]\n",
      " [ 0.70338911 -0.56935304 -0.03820618  0.65022228 -0.4367924   0.25490829\n",
      "   0.76149431  0.03251108 -0.77484585]\n",
      " [ 0.74935505 -0.88672575  0.31398993  0.72714686 -0.3244549   0.07660641\n",
      "   0.94668608  0.9244703  -0.47717314]\n",
      " [-1.04147291  1.20126735 -0.4323205  -1.19193853  0.0754754   0.27599298\n",
      "  -1.01963733 -0.81067973  0.82404329]\n",
      " [ 1.19312702 -1.09016324  0.41672635  0.82337116 -0.16404471 -0.31842267\n",
      "   0.85641739  0.90937459 -0.62302114]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.57619795  1.99513348 -2.39619806  0.31581582  0.57419724 -3.04198351\n",
      "   1.36979205]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:78 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.14840761]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 79 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93723259 -1.21627024  0.25335267  1.05711317 -0.29463914 -0.18511651\n",
      "   1.29362207  0.29338139 -0.65205637]\n",
      " [-0.89126732  0.96904043 -0.47099967 -1.06870198  0.19669591 -0.07642235\n",
      "  -0.74470134 -0.45962113  0.85901652]\n",
      " [ 0.70336705 -0.56937511 -0.03820618  0.65020022 -0.4367924   0.25490829\n",
      "   0.76149431  0.03251108 -0.77486792]\n",
      " [ 0.74909189 -0.88698892  0.31398993  0.72688369 -0.3244549   0.07660641\n",
      "   0.94668608  0.9244703  -0.47743631]\n",
      " [-1.04099025  1.20175    -0.4323205  -1.19145588  0.0754754   0.27599298\n",
      "  -1.01963733 -0.81067973  0.82452595]\n",
      " [ 1.192432   -1.09085826  0.41672635  0.82267614 -0.16404471 -0.31842267\n",
      "   0.85641739  0.90937459 -0.62371616]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.58557604  1.9901466  -2.40057537  0.31110471  0.56924448 -3.0461864\n",
      "   1.36439729]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:79 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.85432278]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 79 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93883257 -1.21627024  0.25495265  1.05711317 -0.29463914 -0.18511651\n",
      "   1.29522205  0.29338139 -0.65205637]\n",
      " [-0.89311364  0.96904043 -0.47284599 -1.06870198  0.19669591 -0.07642235\n",
      "  -0.74654766 -0.45962113  0.85901652]\n",
      " [ 0.70538627 -0.56937511 -0.03618695  0.65020022 -0.4367924   0.25490829\n",
      "   0.76351354  0.03251108 -0.77486792]\n",
      " [ 0.75099055 -0.88698892  0.31588859  0.72688369 -0.3244549   0.07660641\n",
      "   0.94858474  0.9244703  -0.47743631]\n",
      " [-1.04258402  1.20175    -0.43391427 -1.19145588  0.0754754   0.27599298\n",
      "  -1.0212311  -0.81067973  0.82452595]\n",
      " [ 1.19404517 -1.09085826  0.41833952  0.82267614 -0.16404471 -0.31842267\n",
      "   0.85803056  0.90937459 -0.62371616]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.57651088  1.99851398 -2.39959252  0.31841467  0.5772383  -3.04549424\n",
      "   1.37275258]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:79 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.62417796]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 79 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92290675 -1.23219606  0.23902683  1.05711317 -0.29463914 -0.18511651\n",
      "   1.27929623  0.29338139 -0.65205637]\n",
      " [-0.87777131  0.98438276 -0.45750366 -1.06870198  0.19669591 -0.07642235\n",
      "  -0.73120533 -0.45962113  0.85901652]\n",
      " [ 0.69219887 -0.58256252 -0.04937436  0.65020022 -0.4367924   0.25490829\n",
      "   0.75032613  0.03251108 -0.77486792]\n",
      " [ 0.73573228 -0.90224719  0.30063032  0.72688369 -0.3244549   0.07660641\n",
      "   0.93332647  0.9244703  -0.47743631]\n",
      " [-1.02657945  1.21775457 -0.4179097  -1.19145588  0.0754754   0.27599298\n",
      "  -1.00522652 -0.81067973  0.82452595]\n",
      " [ 1.1778203  -1.10708313  0.40211465  0.82267614 -0.16404471 -0.31842267\n",
      "   0.84180569  0.90937459 -0.62371616]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.64972066  1.9413217  -2.41728618  0.26692202  0.52192412 -3.06122281\n",
      "   1.31426383]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:79 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.75720286]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 79 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92788911 -1.23219606  0.24400919  1.05711317 -0.29463914 -0.18511651\n",
      "   1.27929623  0.29836375 -0.65205637]\n",
      " [-0.88266561  0.98438276 -0.46239796 -1.06870198  0.19669591 -0.07642235\n",
      "  -0.73120533 -0.46451543  0.85901652]\n",
      " [ 0.69556805 -0.58256252 -0.04600518  0.65020022 -0.4367924   0.25490829\n",
      "   0.75032613  0.03588026 -0.77486792]\n",
      " [ 0.7404655  -0.90224719  0.30536354  0.72688369 -0.3244549   0.07660641\n",
      "   0.93332647  0.92920352 -0.47743631]\n",
      " [-1.03090289  1.21775457 -0.42223314 -1.19145588  0.0754754   0.27599298\n",
      "  -1.00522652 -0.81500317  0.82452595]\n",
      " [ 1.18175059 -1.10708313  0.40604494  0.82267614 -0.16404471 -0.31842267\n",
      "   0.84180569  0.91330488 -0.62371616]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.62740193  1.95941806 -2.41410635  0.28171257  0.54148923 -3.05910466\n",
      "   1.3348727 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:79 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.58712069]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 79 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93890393 -1.23219606  0.24400919  1.06812799 -0.29463914 -0.18511651\n",
      "   1.27929623  0.29836375 -0.64104155]\n",
      " [-0.89294727  0.98438276 -0.46239796 -1.07898365  0.19669591 -0.07642235\n",
      "  -0.73120533 -0.46451543  0.84873486]\n",
      " [ 0.70215868 -0.58256252 -0.04600518  0.65679085 -0.4367924   0.25490829\n",
      "   0.75032613  0.03588026 -0.76827729]\n",
      " [ 0.75025062 -0.90224719  0.30536354  0.73666881 -0.3244549   0.07660641\n",
      "   0.93332647  0.92920352 -0.46765119]\n",
      " [-1.04201773  1.21775457 -0.42223314 -1.20257071  0.0754754   0.27599298\n",
      "  -1.00522652 -0.81500317  0.81341111]\n",
      " [ 1.19284283 -1.10708313  0.40604494  0.83376838 -0.16404471 -0.31842267\n",
      "   0.84180569  0.91330488 -0.61262393]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.5773589   1.99901849 -2.40153675  0.31368861  0.57797414 -3.04918812\n",
      "   1.37486234]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:79 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.08466853]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 79 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.9390519  -1.23219606  0.24400919  1.06812799 -0.29449117 -0.18496854\n",
      "   1.27929623  0.29836375 -0.64089358]\n",
      " [-0.89300957  0.98438276 -0.46239796 -1.07898365  0.19663361 -0.07648465\n",
      "  -0.73120533 -0.46451543  0.84867256]\n",
      " [ 0.70235901 -0.58256252 -0.04600518  0.65679085 -0.43659207  0.25510862\n",
      "   0.75032613  0.03588026 -0.76807696]\n",
      " [ 0.75022213 -0.90224719  0.30536354  0.73666881 -0.32448339  0.07657792\n",
      "   0.93332647  0.92920352 -0.46767968]\n",
      " [-1.04211812  1.21775457 -0.42223314 -1.20257071  0.075375    0.27589258\n",
      "  -1.00522652 -0.81500317  0.81331072]\n",
      " [ 1.19276284 -1.10708313  0.40604494  0.83376838 -0.16412469 -0.31850266\n",
      "   0.84180569  0.91330488 -0.61270391]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.5806398   1.99752683 -2.40323956  0.31225054  0.57630519 -3.05092922\n",
      "   1.37314178]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:79 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.00848712]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 79 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93905984 -1.23218812  0.24400919  1.06812799 -0.29448323 -0.1849606\n",
      "   1.27929623  0.29836375 -0.64088564]\n",
      " [-0.89301681  0.98437553 -0.46239796 -1.07898365  0.19662638 -0.07649189\n",
      "  -0.73120533 -0.46451543  0.84866533]\n",
      " [ 0.70236527 -0.58255625 -0.04600518  0.65679085 -0.43658581  0.25511488\n",
      "   0.75032613  0.03588026 -0.7680707 ]\n",
      " [ 0.75022858 -0.90224074  0.30536354  0.73666881 -0.32447694  0.07658437\n",
      "   0.93332647  0.92920352 -0.46767323]\n",
      " [-1.04212599  1.21774671 -0.42223314 -1.20257071  0.07536713  0.27588471\n",
      "  -1.00522652 -0.81500317  0.81330285]\n",
      " [ 1.1927699  -1.10707607  0.40604494  0.83376838 -0.16411763 -0.3184956\n",
      "   0.84180569  0.91330488 -0.61269686]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.58067551  1.99751984 -2.40326608  0.3122397   0.57629463 -3.05095752\n",
      "   1.37313224]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:79 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.27093206]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 79 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93587868 -1.23536927  0.24400919  1.06812799 -0.29766439 -0.18814175\n",
      "   1.27611508  0.29836375 -0.64088564]\n",
      " [-0.88976473  0.9876276  -0.46239796 -1.07898365  0.19987845 -0.07323981\n",
      "  -0.72795326 -0.46451543  0.84866533]\n",
      " [ 0.69826417 -0.58665735 -0.04600518  0.65679085 -0.44068691  0.25101378\n",
      "   0.74622503  0.03588026 -0.7680707 ]\n",
      " [ 0.74690248 -0.90556684  0.30536354  0.73666881 -0.32780304  0.07325827\n",
      "   0.93000036  0.92920352 -0.46767323]\n",
      " [-1.0391023   1.2207704  -0.42223314 -1.20257071  0.07839082  0.2789084\n",
      "  -1.00220283 -0.81500317  0.81330285]\n",
      " [ 1.18993633 -1.10990964  0.40604494  0.83376838 -0.1669512  -0.32132917\n",
      "   0.83897212  0.91330488 -0.61269686]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.60743382  1.98082163 -2.41324478  0.2944275   0.55942937 -3.06119635\n",
      "   1.35682512]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:79 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.8902716]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 79 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93649301 -1.23536927  0.24400919  1.06874232 -0.29766439 -0.18814175\n",
      "   1.27672941  0.29836375 -0.64088564]\n",
      " [-0.89062005  0.9876276  -0.46239796 -1.07983896  0.19987845 -0.07323981\n",
      "  -0.72880857 -0.46451543  0.84866533]\n",
      " [ 0.69935768 -0.58665735 -0.04600518  0.65788436 -0.44068691  0.25101378\n",
      "   0.74731854  0.03588026 -0.7680707 ]\n",
      " [ 0.74787778 -0.90556684  0.30536354  0.73764412 -0.32780304  0.07325827\n",
      "   0.93097567  0.92920352 -0.46767323]\n",
      " [-1.03973059  1.2207704  -0.42223314 -1.203199    0.07839082  0.2789084\n",
      "  -1.00283112 -0.81500317  0.81330285]\n",
      " [ 1.19072051 -1.10990964  0.40604494  0.83455256 -0.1669512  -0.32132917\n",
      "   0.8397563   0.91330488 -0.61269686]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.60207425  1.98598687 -2.41290623  0.29920303  0.56434869 -3.06099511\n",
      "   1.36189512]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:79 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.72559504]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 79 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94253997 -1.23536927  0.24400919  1.06874232 -0.29161743 -0.1820948\n",
      "   1.28277636  0.29836375 -0.64088564]\n",
      " [-0.8967301   0.9876276  -0.46239796 -1.07983896  0.19376839 -0.07934987\n",
      "  -0.73491863 -0.46451543  0.84866533]\n",
      " [ 0.70527879 -0.58665735 -0.04600518  0.65788436 -0.4347658   0.25693489\n",
      "   0.75323965  0.03588026 -0.7680707 ]\n",
      " [ 0.75396147 -0.90556684  0.30536354  0.73764412 -0.32171935  0.07934196\n",
      "   0.93705936  0.92920352 -0.46767323]\n",
      " [-1.04580478  1.2207704  -0.42223314 -1.203199    0.07231663  0.27283421\n",
      "  -1.00890531 -0.81500317  0.81330285]\n",
      " [ 1.19683622 -1.10990964  0.40604494  0.83455256 -0.16083549 -0.31521345\n",
      "   0.84587201  0.91330488 -0.61269686]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.57475629  2.00918204 -2.40789329  0.32047033  0.58636732 -3.05672258\n",
      "   1.38439921]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:79 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.59566533]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 79 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92728109 -1.23536927  0.24400919  1.05348344 -0.29161743 -0.19735368\n",
      "   1.28277636  0.29836375 -0.65614452]\n",
      " [-0.88138462  0.9876276  -0.46239796 -1.06449347  0.19376839 -0.06400439\n",
      "  -0.73491863 -0.46451543  0.86401081]\n",
      " [ 0.69246833 -0.58665735 -0.04600518  0.6450739  -0.4347658   0.24412442\n",
      "   0.75323965  0.03588026 -0.78088116]\n",
      " [ 0.73915722 -0.90556684  0.30536354  0.72283987 -0.32171935  0.06453771\n",
      "   0.93705936  0.92920352 -0.48247747]\n",
      " [-1.03067051  1.2207704  -0.42223314 -1.18806472  0.07231663  0.28796848\n",
      "  -1.00890531 -0.81500317  0.82843712]\n",
      " [ 1.18203075 -1.10990964  0.40604494  0.81974709 -0.16083549 -0.33001892\n",
      "   0.84587201  0.91330488 -0.62750232]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.64648873  1.95420363 -2.42440519  0.27018644  0.53250536 -3.07380543\n",
      "   1.3305345 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:79 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.72166521]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 79 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93337428 -1.22927609  0.24400919  1.05957663 -0.29161743 -0.19126049\n",
      "   1.28886955  0.29836375 -0.65614452]\n",
      " [-0.8875488   0.98146342 -0.46239796 -1.07065765  0.19376839 -0.07016857\n",
      "  -0.74108281 -0.46451543  0.86401081]\n",
      " [ 0.69863988 -0.58048579 -0.04600518  0.65124546 -0.4347658   0.25029598\n",
      "   0.75941121  0.03588026 -0.78088116]\n",
      " [ 0.74541481 -0.89930926  0.30536354  0.72909746 -0.32171935  0.0707953\n",
      "   0.94331694  0.92920352 -0.48247747]\n",
      " [-1.03686436  1.21457654 -0.42223314 -1.19425858  0.07231663  0.28177463\n",
      "  -1.01509917 -0.81500317  0.82843712]\n",
      " [ 1.18824605 -1.10369434  0.40604494  0.82596239 -0.16083549 -0.32380362\n",
      "   0.85208731  0.91330488 -0.62750232]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.61853494  1.97829586 -2.42029194  0.29399557  0.55559715 -3.0695561\n",
      "   1.35299276]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:79 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.77950344]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 79 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.9372966  -1.22535377  0.24400919  1.06349895 -0.29161743 -0.19126049\n",
      "   1.29279187  0.29836375 -0.65614452]\n",
      " [-0.89174798  0.97726423 -0.46239796 -1.07485684  0.19376839 -0.07016857\n",
      "  -0.745282   -0.46451543  0.86401081]\n",
      " [ 0.70288176 -0.57624392 -0.04600518  0.65548733 -0.4347658   0.25029598\n",
      "   0.76365308  0.03588026 -0.78088116]\n",
      " [ 0.74965607 -0.895068    0.30536354  0.73333871 -0.32171935  0.0707953\n",
      "   0.9475582   0.92920352 -0.48247747]\n",
      " [-1.04080975  1.21063115 -0.42223314 -1.19820397  0.07231663  0.28177463\n",
      "  -1.01904456 -0.81500317  0.82843712]\n",
      " [ 1.19242148 -1.09951891  0.40604494  0.83013781 -0.16083549 -0.32380362\n",
      "   0.85626274  0.91330488 -0.62750232]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.5995857   1.99508876 -2.41740879  0.30956867  0.5711416  -3.0673595\n",
      "   1.3691666 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:79 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.83874293]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 79 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93938748 -1.22535377  0.24610007  1.06349895 -0.29161743 -0.18916961\n",
      "   1.29488275  0.29836375 -0.65614452]\n",
      " [-0.89392483  0.97726423 -0.4645748  -1.07485684  0.19376839 -0.07234541\n",
      "  -0.74745884 -0.46451543  0.86401081]\n",
      " [ 0.70530969 -0.57624392 -0.04357724  0.65548733 -0.4347658   0.25272392\n",
      "   0.76608102  0.03588026 -0.78088116]\n",
      " [ 0.75189978 -0.895068    0.30760725  0.73333871 -0.32171935  0.07303901\n",
      "   0.94980191  0.92920352 -0.48247747]\n",
      " [-1.04296397  1.21063115 -0.42438736 -1.19820397  0.07231663  0.27962041\n",
      "  -1.02119878 -0.81500317  0.82843712]\n",
      " [ 1.19462596 -1.09951891  0.40824942  0.83013781 -0.16083549 -0.32159914\n",
      "   0.85846722  0.91330488 -0.62750232]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.58868043  2.0049847  -2.41629084  0.31874748  0.58082862 -3.066272\n",
      "   1.37891452]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:79 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.14450918]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 80 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.9391174  -1.22562385  0.24610007  1.06322887 -0.29161743 -0.18916961\n",
      "   1.29488275  0.29836375 -0.6564146 ]\n",
      " [-0.89364124  0.97754782 -0.4645748  -1.07457325  0.19376839 -0.07234541\n",
      "  -0.74745884 -0.46451543  0.8642944 ]\n",
      " [ 0.70530149 -0.57625212 -0.04357724  0.65547913 -0.4347658   0.25272392\n",
      "   0.76608102  0.03588026 -0.78088936]\n",
      " [ 0.75165998 -0.8953078   0.30760725  0.73309892 -0.32171935  0.07303901\n",
      "   0.94980191  0.92920352 -0.48271727]\n",
      " [-1.04251723  1.21107789 -0.42438736 -1.19775723  0.07231663  0.27962041\n",
      "  -1.02119878 -0.81500317  0.82888386]\n",
      " [ 1.19397558 -1.10016929  0.40824942  0.82948743 -0.16083549 -0.32159914\n",
      "   0.85846722  0.91330488 -0.6281527 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.597613    2.00024767 -2.42047277  0.314273    0.57612207 -3.0702885\n",
      "   1.37378821]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:80 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.85533475]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 80 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94069997 -1.22562385  0.24768264  1.06322887 -0.29161743 -0.18916961\n",
      "   1.29646532  0.29836375 -0.6564146 ]\n",
      " [-0.89546488  0.97754782 -0.46639844 -1.07457325  0.19376839 -0.07234541\n",
      "  -0.74928249 -0.46451543  0.8642944 ]\n",
      " [ 0.70729532 -0.57625212 -0.04158341  0.65547913 -0.4347658   0.25272392\n",
      "   0.76807485  0.03588026 -0.78088936]\n",
      " [ 0.75353493 -0.8953078   0.3094822   0.73309892 -0.32171935  0.07303901\n",
      "   0.95167686  0.92920352 -0.48271727]\n",
      " [-1.0440942   1.21107789 -0.42596433 -1.19775723  0.07231663  0.27962041\n",
      "  -1.02277575 -0.81500317  0.82888386]\n",
      " [ 1.1955717  -1.10016929  0.40984554  0.82948743 -0.16083549 -0.32159914\n",
      "   0.86006334  0.91330488 -0.6281527 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.58866276  2.00850637 -2.41950125  0.32149189  0.58401391 -3.06960205\n",
      "   1.38203442]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:80 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.61932762]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 80 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92486759 -1.24145623  0.23185026  1.06322887 -0.29161743 -0.18916961\n",
      "   1.28063294  0.29836375 -0.6564146 ]\n",
      " [-0.88021992  0.99279278 -0.45115348 -1.07457325  0.19376839 -0.07234541\n",
      "  -0.73403752 -0.46451543  0.8642944 ]\n",
      " [ 0.69420214 -0.58934531 -0.0546766   0.65547913 -0.4347658   0.25272392\n",
      "   0.75498166  0.03588026 -0.78088936]\n",
      " [ 0.7383715  -0.91047123  0.29431877  0.73309892 -0.32171935  0.07303901\n",
      "   0.93651343  0.92920352 -0.48271727]\n",
      " [-1.02818118  1.22699091 -0.41005131 -1.19775723  0.07231663  0.27962041\n",
      "  -1.00686273 -0.81500317  0.82888386]\n",
      " [ 1.17942251 -1.11631849  0.39369635  0.82948743 -0.16083549 -0.32159914\n",
      "   0.84391415  0.91330488 -0.6281527 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.66166939  1.95164205 -2.4372782   0.27023063  0.52897605 -3.08546284\n",
      "   1.32387576]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:80 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.75867565]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 80 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92979918 -1.24145623  0.23678185  1.06322887 -0.29161743 -0.18916961\n",
      "   1.28063294  0.30329534 -0.6564146 ]\n",
      " [-0.88506366  0.99279278 -0.45599722 -1.07457325  0.19376839 -0.07234541\n",
      "  -0.73403752 -0.46935917  0.8642944 ]\n",
      " [ 0.69753731 -0.58934531 -0.05134142  0.65547913 -0.4347658   0.25272392\n",
      "   0.75498166  0.03921543 -0.78088936]\n",
      " [ 0.74305535 -0.91047123  0.29900263  0.73309892 -0.32171935  0.07303901\n",
      "   0.93651343  0.93388738 -0.48271727]\n",
      " [-1.03246367  1.22699091 -0.4143338  -1.19775723  0.07231663  0.27962041\n",
      "  -1.00686273 -0.81928566  0.82888386]\n",
      " [ 1.1833178  -1.11631849  0.39759165  0.82948743 -0.16083549 -0.32159914\n",
      "   0.84391415  0.91720018 -0.6281527 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.63957772  1.96955352 -2.4341334   0.28487106  0.54834465 -3.08336257\n",
      "   1.34427045]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:80 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.5886035]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 80 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94076932 -1.24145623  0.23678185  1.07419902 -0.29161743 -0.18916961\n",
      "   1.28063294  0.30329534 -0.64544445]\n",
      " [-0.89531014  0.99279278 -0.45599722 -1.08481973  0.19376839 -0.07234541\n",
      "  -0.73403752 -0.46935917  0.85404792]\n",
      " [ 0.70410907 -0.58934531 -0.05134142  0.66205089 -0.4347658   0.25272392\n",
      "   0.75498166  0.03921543 -0.7743176 ]\n",
      " [ 0.75281373 -0.91047123  0.29900263  0.7428573  -0.32171935  0.07303901\n",
      "   0.93651343  0.93388738 -0.47295889]\n",
      " [-1.04353096  1.22699091 -0.4143338  -1.20882451  0.07231663  0.27962041\n",
      "  -1.00686273 -0.81928566  0.81781657]\n",
      " [ 1.19436374 -1.11631849  0.39759165  0.84053337 -0.16083549 -0.32159914\n",
      "   0.84391415  0.91720018 -0.61710677]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.58976801  2.00899945 -2.42165041  0.31671211  0.5846941  -3.07351997\n",
      "   1.38410513]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:80 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.08234775]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 80 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94091235 -1.24145623  0.23678185  1.07419902 -0.29147441 -0.18902659\n",
      "   1.28063294  0.30329534 -0.64530143]\n",
      " [-0.89537239  0.99279278 -0.45599722 -1.08481973  0.19370614 -0.07240766\n",
      "  -0.73403752 -0.46935917  0.85398567]\n",
      " [ 0.70430219 -0.58934531 -0.05134142  0.66205089 -0.43457268  0.25291704\n",
      "   0.75498166  0.03921543 -0.77412448]\n",
      " [ 0.75278949 -0.91047123  0.29900263  0.7428573  -0.3217436   0.07301477\n",
      "   0.93651343  0.93388738 -0.47298313]\n",
      " [-1.04362875  1.22699091 -0.4143338  -1.20882451  0.07221884  0.27952261\n",
      "  -1.00686273 -0.81928566  0.81771878]\n",
      " [ 1.19429015 -1.11631849  0.39759165  0.84053337 -0.16090908 -0.32167273\n",
      "   0.84391415  0.91720018 -0.61718036]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.59287938  2.00758761 -2.42326841  0.3153516   0.58311416 -3.07517371\n",
      "   1.38247574]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:80 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.00799268]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 80 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94091941 -1.24144917  0.23678185  1.07419902 -0.29146735 -0.18901953\n",
      "   1.28063294  0.30329534 -0.64529437]\n",
      " [-0.89537884  0.99278633 -0.45599722 -1.08481973  0.19369968 -0.07241412\n",
      "  -0.73403752 -0.46935917  0.85397922]\n",
      " [ 0.7043078  -0.5893397  -0.05134142  0.66205089 -0.43456707  0.25292265\n",
      "   0.75498166  0.03921543 -0.77411887]\n",
      " [ 0.75279527 -0.91046545  0.29900263  0.7428573  -0.32173782  0.07302054\n",
      "   0.93651343  0.93388738 -0.47297736]\n",
      " [-1.04363575  1.22698391 -0.4143338  -1.20882451  0.07221184  0.27951562\n",
      "  -1.00686273 -0.81928566  0.81771179]\n",
      " [ 1.19429645 -1.11631219  0.39759165  0.84053337 -0.16090278 -0.32166643\n",
      "   0.84391415  0.91720018 -0.61717406]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.59291107  2.00758147 -2.42329202  0.31534205  0.58310487 -3.07519888\n",
      "   1.38246735]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:80 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.26630383]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 80 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93786454 -1.24450404  0.23678185  1.07419902 -0.29452222 -0.1920744\n",
      "   1.27757807  0.30329534 -0.64529437]\n",
      " [-0.89224009  0.99592508 -0.45599722 -1.08481973  0.19683844 -0.06927536\n",
      "  -0.73089877 -0.46935917  0.85397922]\n",
      " [ 0.70032207 -0.59332543 -0.05134142  0.66205089 -0.4385528   0.24893692\n",
      "   0.75099593  0.03921543 -0.77411887]\n",
      " [ 0.74957866 -0.91368206  0.29900263  0.7428573  -0.32495443  0.06980394\n",
      "   0.93329682  0.93388738 -0.47297736]\n",
      " [-1.04073185  1.22988781 -0.4143338  -1.20882451  0.07511574  0.28241952\n",
      "  -1.00395883 -0.81928566  0.81771179]\n",
      " [ 1.19157261 -1.11903603  0.39759165  0.84053337 -0.16362662 -0.32439027\n",
      "   0.84119031  0.91720018 -0.61717406]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.6189271   1.9913899  -2.4330205   0.29802604  0.56672745 -3.08519407\n",
      "   1.36664703]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:80 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.89227588]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 80 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93845445 -1.24450404  0.23678185  1.07478893 -0.29452222 -0.1920744\n",
      "   1.27816798  0.30329534 -0.64529437]\n",
      " [-0.89306162  0.99592508 -0.45599722 -1.08564126  0.19683844 -0.06927536\n",
      "  -0.73172029 -0.46935917  0.85397922]\n",
      " [ 0.70137443 -0.59332543 -0.05134142  0.66310325 -0.4385528   0.24893692\n",
      "   0.75204829  0.03921543 -0.77411887]\n",
      " [ 0.75051594 -0.91368206  0.29900263  0.74379457 -0.32495443  0.06980394\n",
      "   0.9342341   0.93388738 -0.47297736]\n",
      " [-1.04133515  1.22988781 -0.4143338  -1.20942782  0.07511574  0.28241952\n",
      "  -1.00456213 -0.81928566  0.81771179]\n",
      " [ 1.19232574 -1.11903603  0.39759165  0.8412865  -0.16362662 -0.32439027\n",
      "   0.84194344  0.91720018 -0.61717406]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.6137499   1.99638109 -2.4326969   0.30264512  0.5714841  -3.08500147\n",
      "   1.3715473 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:80 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.72784277]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 80 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94441931 -1.24450404  0.23678185  1.07478893 -0.28855737 -0.18610955\n",
      "   1.28413284  0.30329534 -0.64529437]\n",
      " [-0.89909155  0.99592508 -0.45599722 -1.08564126  0.19080851 -0.07530529\n",
      "  -0.73775022 -0.46935917  0.85397922]\n",
      " [ 0.70722634 -0.59332543 -0.05134142  0.66310325 -0.4327009   0.25478882\n",
      "   0.7579002   0.03921543 -0.77411887]\n",
      " [ 0.7565217  -0.91368206  0.29900263  0.74379457 -0.31894866  0.0758097\n",
      "   0.94023986  0.93388738 -0.47297736]\n",
      " [-1.04732699  1.22988781 -0.4143338  -1.20942782  0.0691239   0.27642767\n",
      "  -1.01055397 -0.81928566  0.81771179]\n",
      " [ 1.19836029 -1.11903603  0.39759165  0.8412865  -0.15759207 -0.31835571\n",
      "   0.84797799  0.91720018 -0.61717406]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.5867944   2.01927751 -2.42776833  0.3236618   0.59323285 -3.08079665\n",
      "   1.39376865]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:80 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.59544071]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 80 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92916581 -1.24450404  0.23678185  1.05953543 -0.28855737 -0.20136305\n",
      "   1.28413284  0.30329534 -0.66054787]\n",
      " [-0.88375442  0.99592508 -0.45599722 -1.07030413  0.19080851 -0.05996816\n",
      "  -0.73775022 -0.46935917  0.86931635]\n",
      " [ 0.69442859 -0.59332543 -0.05134142  0.6503055  -0.4327009   0.24199108\n",
      "   0.7579002   0.03921543 -0.78691662]\n",
      " [ 0.74172115 -0.91368206  0.29900263  0.72899403 -0.31894866  0.06100915\n",
      "   0.94023986  0.93388738 -0.48777791]\n",
      " [-1.03219701  1.22988781 -0.4143338  -1.19429783  0.0691239   0.29155766\n",
      "  -1.01055397 -0.81928566  0.83284177]\n",
      " [ 1.18355395 -1.11903603  0.39759165  0.82648015 -0.15759207 -0.33316206\n",
      "   0.84797799  0.91720018 -0.6319804 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.65851258  1.96431642 -2.44429214  0.27340337  0.53938328 -3.09787937\n",
      "   1.3399061 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:80 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.72229788]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 80 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93524077 -1.23842908  0.23678185  1.06561039 -0.28855737 -0.19528809\n",
      "   1.2902078   0.30329534 -0.66054787]\n",
      " [-0.88989709  0.98978241 -0.45599722 -1.0764468   0.19080851 -0.06611084\n",
      "  -0.7438929  -0.46935917  0.86931635]\n",
      " [ 0.70057503 -0.587179   -0.05134142  0.65645194 -0.4327009   0.24813751\n",
      "   0.76404663  0.03921543 -0.78691662]\n",
      " [ 0.74795577 -0.90744744  0.29900263  0.73522864 -0.31894866  0.06724377\n",
      "   0.94647448  0.93388738 -0.48777791]\n",
      " [-1.03837045  1.22371437 -0.4143338  -1.20047127  0.0691239   0.28538422\n",
      "  -1.01672741 -0.81928566  0.83284177]\n",
      " [ 1.18974522 -1.11284476  0.39759165  0.83267143 -0.15759207 -0.32697079\n",
      "   0.85416926  0.91720018 -0.6319804 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.63066132  1.98830749 -2.44018935  0.29713589  0.56239139 -3.09363377\n",
      "   1.36227358]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:80 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.78098382]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 80 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93911785 -1.234552    0.23678185  1.06948748 -0.28855737 -0.19528809\n",
      "   1.29408489  0.30329534 -0.66054787]\n",
      " [-0.8940467   0.9856328  -0.45599722 -1.08059641  0.19080851 -0.06611084\n",
      "  -0.74804251 -0.46935917  0.86931635]\n",
      " [ 0.70476826 -0.58298576 -0.05134142  0.66064518 -0.4327009   0.24813751\n",
      "   0.76823987  0.03921543 -0.78691662]\n",
      " [ 0.75214847 -0.90325474  0.29900263  0.73942134 -0.31894866  0.06724377\n",
      "   0.95066718  0.93388738 -0.48777791]\n",
      " [-1.04227021  1.21981461 -0.4143338  -1.20437103  0.0691239   0.28538422\n",
      "  -1.02062717 -0.81928566  0.83284177]\n",
      " [ 1.19387193 -1.10871805  0.39759165  0.83679814 -0.15759207 -0.32697079\n",
      "   0.85829597  0.91720018 -0.6319804 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.61193018  2.00490728 -2.43734573  0.31254367  0.57776712 -3.09146286\n",
      "   1.37826394]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:80 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.83905854]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 80 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94120772 -1.234552    0.23887172  1.06948748 -0.28855737 -0.19319822\n",
      "   1.29617475  0.30329534 -0.66054787]\n",
      " [-0.89621975  0.9856328  -0.45817027 -1.08059641  0.19080851 -0.06828388\n",
      "  -0.75021556 -0.46935917  0.86931635]\n",
      " [ 0.70718783 -0.58298576 -0.04892186  0.66064518 -0.4327009   0.25055707\n",
      "   0.77065943  0.03921543 -0.78691662]\n",
      " [ 0.75438707 -0.90325474  0.30124123  0.73942134 -0.31894866  0.06948238\n",
      "   0.95290578  0.93388738 -0.48777791]\n",
      " [-1.04442308  1.21981461 -0.41648667 -1.20437103  0.0691239   0.28323135\n",
      "  -1.02278004 -0.81928566  0.83284177]\n",
      " [ 1.19607429 -1.10871805  0.399794    0.83679814 -0.15759207 -0.32476843\n",
      "   0.86049833  0.91720018 -0.6319804 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.60106346  2.01476078 -2.43622633  0.32168855  0.58741523 -3.09037092\n",
      "   1.38796898]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:80 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.14069582]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 81 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94096275 -1.23479696  0.23887172  1.06924251 -0.28855737 -0.19319822\n",
      "   1.29617475  0.30329534 -0.66079284]\n",
      " [-0.89596159  0.98589096 -0.45817027 -1.08033825  0.19080851 -0.06828388\n",
      "  -0.75021556 -0.46935917  0.86957451]\n",
      " [ 0.70719223 -0.58298136 -0.04892186  0.66064958 -0.4327009   0.25055707\n",
      "   0.77065943  0.03921543 -0.78691222]\n",
      " [ 0.75416912 -0.9034727   0.30124123  0.73920339 -0.31894866  0.06948238\n",
      "   0.95290578  0.93388738 -0.48799586]\n",
      " [-1.04401002  1.22022767 -0.41648667 -1.20395797  0.0691239   0.28323135\n",
      "  -1.02278004 -0.81928566  0.83325483]\n",
      " [ 1.19546612 -1.10932622  0.399794    0.83618997 -0.15759207 -0.32476843\n",
      "   0.86049833  0.91720018 -0.63258857]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.60956856  2.01026271 -2.44022007  0.3174404   0.58294434 -3.09420775\n",
      "   1.38309957]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:81 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.85632312]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 81 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.9425284  -1.23479696  0.24043737  1.06924251 -0.28855737 -0.19319822\n",
      "   1.2977404   0.30329534 -0.66079284]\n",
      " [-0.89776321  0.98589096 -0.4599719  -1.08033825  0.19080851 -0.06828388\n",
      "  -0.75201718 -0.46935917  0.86957451]\n",
      " [ 0.70916137 -0.58298136 -0.04695272  0.66064958 -0.4327009   0.25055707\n",
      "   0.77262858  0.03921543 -0.78691222]\n",
      " [ 0.75602104 -0.9034727   0.30309315  0.73920339 -0.31894866  0.06948238\n",
      "   0.95475771  0.93388738 -0.48799586]\n",
      " [-1.04557065  1.22022767 -0.4180473  -1.20395797  0.0691239   0.28323135\n",
      "  -1.02434067 -0.81928566  0.83325483]\n",
      " [ 1.19704572 -1.10932622  0.4013736   0.83618997 -0.15759207 -0.32476843\n",
      "   0.86207793  0.91720018 -0.63258857]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.60073     2.01841578 -2.43925954  0.32457077  0.59073701 -3.09352684\n",
      "   1.39123972]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:81 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.61441008]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 81 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92679611 -1.25052925  0.22470508  1.06924251 -0.28855737 -0.19319822\n",
      "   1.28200811  0.30329534 -0.66079284]\n",
      " [-0.88262091  1.00103326 -0.44482959 -1.08033825  0.19080851 -0.06828388\n",
      "  -0.73687488 -0.46935917  0.86957451]\n",
      " [ 0.69616554 -0.59597719 -0.05994855  0.66064958 -0.4327009   0.25055707\n",
      "   0.75963274  0.03921543 -0.78691222]\n",
      " [ 0.74095768 -0.91853606  0.28802979  0.73920339 -0.31894866  0.06948238\n",
      "   0.93969434  0.93388738 -0.48799586]\n",
      " [-1.02975619  1.23604213 -0.40223284 -1.20395797  0.0691239   0.28323135\n",
      "  -1.00852622 -0.81928566  0.83325483]\n",
      " [ 1.18097968 -1.12539225  0.38530757  0.83618997 -0.15759207 -0.32476843\n",
      "   0.84601189  0.91720018 -0.63258857]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.67351005  1.96189545 -2.45711218  0.27355513  0.5359916  -3.10951268\n",
      "   1.333429  ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:81 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.76013401]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 81 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93167756 -1.25052925  0.22958652  1.06924251 -0.28855737 -0.19319822\n",
      "   1.28200811  0.30817678 -0.66079284]\n",
      " [-0.88741479  1.00103326 -0.44962347 -1.08033825  0.19080851 -0.06828388\n",
      "  -0.73687488 -0.47415305  0.86957451]\n",
      " [ 0.69946696 -0.59597719 -0.05664713  0.66064958 -0.4327009   0.25055707\n",
      "   0.75963274  0.04251685 -0.78691222]\n",
      " [ 0.74559288 -0.91853606  0.29266499  0.73920339 -0.31894866  0.06948238\n",
      "   0.93969434  0.93852258 -0.48799586]\n",
      " [-1.03399826  1.23604213 -0.40647491 -1.20395797  0.0691239   0.28323135\n",
      "  -1.00852622 -0.82352772  0.83325483]\n",
      " [ 1.18484047 -1.12539225  0.38916835  0.83618997 -0.15759207 -0.32476843\n",
      "   0.84601189  0.92106096 -0.63258857]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.65164263  1.97962415 -2.45400176  0.28804708  0.55516591 -3.10743004\n",
      "   1.35361211]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:81 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.59006081]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 81 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94260333 -1.25052925  0.22958652  1.08016829 -0.28855737 -0.19319822\n",
      "   1.28200811  0.30817678 -0.64986706]\n",
      " [-0.89762592  1.00103326 -0.44962347 -1.09054938  0.19080851 -0.06828388\n",
      "  -0.73687488 -0.47415305  0.85936337]\n",
      " [ 0.70601875 -0.59597719 -0.05664713  0.66720137 -0.4327009   0.25055707\n",
      "   0.75963274  0.04251685 -0.78036043]\n",
      " [ 0.75532405 -0.91853606  0.29266499  0.74893456 -0.31894866  0.06948238\n",
      "   0.93969434  0.93852258 -0.47826469]\n",
      " [-1.04501846  1.23604213 -0.40647491 -1.21497818  0.0691239   0.28323135\n",
      "  -1.00852622 -0.82352772  0.82223462]\n",
      " [ 1.19584049 -1.12539225  0.38916835  0.84718998 -0.15759207 -0.32476843\n",
      "   0.84601189  0.92106096 -0.62158855]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.60206273  2.01891676 -2.44160275  0.31975353  0.59138051 -3.09765927\n",
      "   1.39329305]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:81 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.08009374]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 81 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94274152 -1.25052925  0.22958652  1.08016829 -0.28841917 -0.19306003\n",
      "   1.28200811  0.30817678 -0.64972887]\n",
      " [-0.89768797  1.00103326 -0.44962347 -1.09054938  0.19074646 -0.06834593\n",
      "  -0.73687488 -0.47415305  0.85930133]\n",
      " [ 0.70620487 -0.59597719 -0.05664713  0.66720137 -0.43251478  0.25074319\n",
      "   0.75963274  0.04251685 -0.78017431]\n",
      " [ 0.7553037  -0.91853606  0.29266499  0.74893456 -0.31896901  0.06946203\n",
      "   0.93969434  0.93852258 -0.47828504]\n",
      " [-1.04511364  1.23604213 -0.40647491 -1.21497818  0.06902872  0.28313617\n",
      "  -1.00852622 -0.82352772  0.82213944]\n",
      " [ 1.19577284 -1.12539225  0.38916835  0.84718998 -0.15765971 -0.32483607\n",
      "   0.84601189  0.92106096 -0.6216562 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.60501333  2.01758047 -2.44314017  0.3184664   0.58988486 -3.09923002\n",
      "   1.39175001]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:81 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.00753028]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 81 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.9427478  -1.25052298  0.22958652  1.08016829 -0.2884129  -0.19305376\n",
      "   1.28200811  0.30817678 -0.64972259]\n",
      " [-0.89769373  1.0010275  -0.44962347 -1.09054938  0.1907407  -0.06835169\n",
      "  -0.73687488 -0.47415305  0.85929557]\n",
      " [ 0.70620989 -0.59597217 -0.05664713  0.66720137 -0.43250976  0.25074822\n",
      "   0.75963274  0.04251685 -0.78016929]\n",
      " [ 0.75530887 -0.91853089  0.29266499  0.74893456 -0.31896384  0.0694672\n",
      "   0.93969434  0.93852258 -0.47827987]\n",
      " [-1.04511987  1.2360359  -0.40647491 -1.21497818  0.06902249  0.28312994\n",
      "  -1.00852622 -0.82352772  0.82213322]\n",
      " [ 1.19577847 -1.12538662  0.38916835  0.84718998 -0.15765408 -0.32483044\n",
      "   0.84601189  0.92106096 -0.62165057]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.60504147  2.01757508 -2.44316121  0.31845798  0.58987667 -3.09925243\n",
      "   1.39174263]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:81 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.26180673]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 81 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93981298 -1.2534578   0.22958652  1.08016829 -0.29134772 -0.19598858\n",
      "   1.27907329  0.30817678 -0.64972259]\n",
      " [-0.89466314  1.00405809 -0.44962347 -1.09054938  0.19377129 -0.0653211\n",
      "  -0.73384429 -0.47415305  0.85929557]\n",
      " [ 0.70233488 -0.59984718 -0.05664713  0.66720137 -0.43638477  0.2468732\n",
      "   0.75575773  0.04251685 -0.78016929]\n",
      " [ 0.75219699 -0.92164278  0.29266499  0.74893456 -0.32207573  0.06635531\n",
      "   0.93658246  0.93852258 -0.47827987]\n",
      " [-1.04232984  1.23882593 -0.40647491 -1.21497818  0.07181252  0.28591997\n",
      "  -1.00573619 -0.82352772  0.82213322]\n",
      " [ 1.19315929 -1.1280058   0.38916835  0.84718998 -0.16027326 -0.32744962\n",
      "   0.84339271  0.92106096 -0.62165057]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.63034038  2.00187058 -2.45264636  0.3016204   0.57396929 -3.10901002\n",
      "   1.37639103]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:81 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.89423381]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 81 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94037955 -1.2534578   0.22958652  1.08073486 -0.29134772 -0.19598858\n",
      "   1.27963986  0.30817678 -0.64972259]\n",
      " [-0.89545236  1.00405809 -0.44962347 -1.0913386   0.19377129 -0.0653211\n",
      "  -0.73463351 -0.47415305  0.85929557]\n",
      " [ 0.70334776 -0.59984718 -0.05664713  0.66821425 -0.43638477  0.2468732\n",
      "   0.75677062  0.04251685 -0.78016929]\n",
      " [ 0.75309786 -0.92164278  0.29266499  0.74983543 -0.32207573  0.06635531\n",
      "   0.93748333  0.93852258 -0.47827987]\n",
      " [-1.04290926  1.23882593 -0.40647491 -1.2155576   0.07181252  0.28591997\n",
      "  -1.00631561 -0.82352772  0.82213322]\n",
      " [ 1.19388274 -1.1280058   0.38916835  0.84791344 -0.16027326 -0.32744962\n",
      "   0.84411617  0.92106096 -0.62165057]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.62533871  2.00669417 -2.45233696  0.30608857  0.57856912 -3.10882565\n",
      "   1.38112779]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:81 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.73009006]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 81 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94626252 -1.2534578   0.22958652  1.08073486 -0.28546475 -0.19010561\n",
      "   1.28552283  0.30817678 -0.64972259]\n",
      " [-0.90140227  1.00405809 -0.44962347 -1.0913386   0.18782138 -0.07127101\n",
      "  -0.74058342 -0.47415305  0.85929557]\n",
      " [ 0.70913018 -0.59984718 -0.05664713  0.66821425 -0.43060235  0.25265562\n",
      "   0.76255303  0.04251685 -0.78016929]\n",
      " [ 0.7590257  -0.92164278  0.29266499  0.74983543 -0.31614789  0.07228315\n",
      "   0.94341117  0.93852258 -0.47827987]\n",
      " [-1.04881896  1.23882593 -0.40647491 -1.2155576   0.06590282  0.28001027\n",
      "  -1.01222531 -0.82352772  0.82213322]\n",
      " [ 1.19983629 -1.1280058   0.38916835  0.84791344 -0.15431971 -0.32149607\n",
      "   0.85006971  0.92106096 -0.62165057]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.59874468  2.02929259 -2.44749202  0.32685443  0.60004799 -3.10468816\n",
      "   1.40306663]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:81 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.59515489]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 81 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93101589 -1.2534578   0.22958652  1.06548823 -0.28546475 -0.20535224\n",
      "   1.28552283  0.30817678 -0.66496922]\n",
      " [-0.88607502  1.00405809 -0.44962347 -1.07601135  0.18782138 -0.05594376\n",
      "  -0.74058342 -0.47415305  0.87462282]\n",
      " [ 0.69634726 -0.59984718 -0.05664713  0.65543133 -0.43060235  0.2398727\n",
      "   0.76255303  0.04251685 -0.79295221]\n",
      " [ 0.74423062 -0.92164278  0.29266499  0.73504035 -0.31614789  0.05748806\n",
      "   0.94341117  0.93852258 -0.49307496]\n",
      " [-1.03369477  1.23882593 -0.40647491 -1.20043341  0.06590282  0.29513446\n",
      "  -1.01222531 -0.82352772  0.83725741]\n",
      " [ 1.18503081 -1.1280058   0.38916835  0.83310796 -0.15431971 -0.33630156\n",
      "   0.85006971  0.92106096 -0.63645605]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.67044464  1.97435362 -2.4640287   0.27662634  0.54621591 -3.12177152\n",
      "   1.34921124]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:81 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.72293185]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 81 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93707241 -1.24740127  0.22958652  1.07154475 -0.28546475 -0.19929571\n",
      "   1.29157935  0.30817678 -0.66496922]\n",
      " [-0.89219612  0.99793699 -0.44962347 -1.08213245  0.18782138 -0.06206486\n",
      "  -0.74670452 -0.47415305  0.87462282]\n",
      " [ 0.70246853 -0.59372591 -0.05664713  0.6615526  -0.43060235  0.24599396\n",
      "   0.7686743   0.04251685 -0.79295221]\n",
      " [ 0.75044223 -0.91543116  0.29266499  0.74125196 -0.31614789  0.06369968\n",
      "   0.94962279  0.93852258 -0.49307496]\n",
      " [-1.03984764  1.23267306 -0.40647491 -1.20658628  0.06590282  0.28898159\n",
      "  -1.01837819 -0.82352772  0.83725741]\n",
      " [ 1.191198   -1.12183861  0.38916835  0.83927515 -0.15431971 -0.33013436\n",
      "   0.85623691  0.92106096 -0.63645605]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.64269607  1.99824379 -2.45993652  0.30028204  0.56914017 -3.11753005\n",
      "   1.37148785]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:81 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.78246493]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 81 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94090431 -1.24356937  0.22958652  1.07537665 -0.28546475 -0.19929571\n",
      "   1.29541125  0.30817678 -0.66496922]\n",
      " [-0.8962963   0.9938368  -0.44962347 -1.08623264  0.18782138 -0.06206486\n",
      "  -0.75080471 -0.47415305  0.87462282]\n",
      " [ 0.70661319 -0.58958125 -0.05664713  0.66569727 -0.43060235  0.24599396\n",
      "   0.77281896  0.04251685 -0.79295221]\n",
      " [ 0.75458647 -0.91128692  0.29266499  0.7453962  -0.31614789  0.06369968\n",
      "   0.95376702  0.93852258 -0.49307496]\n",
      " [-1.04370183  1.22881887 -0.40647491 -1.21044047  0.06590282  0.28898159\n",
      "  -1.02223237 -0.82352772  0.83725741]\n",
      " [ 1.19527616 -1.11776045  0.38916835  0.84335331 -0.15431971 -0.33013436\n",
      "   0.86031507  0.92106096 -0.63645605]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.62418236  2.01465119 -2.4571322   0.31552465  0.5843474  -3.11538487\n",
      "   1.38729521]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:81 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.8393534]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 81 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94299354 -1.24356937  0.23167575  1.07537665 -0.28546475 -0.19720648\n",
      "   1.29750048  0.30817678 -0.66496922]\n",
      " [-0.89846603  0.9938368  -0.4517932  -1.08623264  0.18782138 -0.06423459\n",
      "  -0.75297444 -0.47415305  0.87462282]\n",
      " [ 0.70902495 -0.58958125 -0.05423537  0.66569727 -0.43060235  0.24840572\n",
      "   0.77523072  0.04251685 -0.79295221]\n",
      " [ 0.75682049 -0.91128692  0.29489902  0.7453962  -0.31614789  0.0659337\n",
      "   0.95600105  0.93852258 -0.49307496]\n",
      " [-1.04585375  1.22881887 -0.40862682 -1.21044047  0.06590282  0.28682967\n",
      "  -1.02438429 -0.82352772  0.83725741]\n",
      " [ 1.19747685 -1.11776045  0.39136905  0.84335331 -0.15431971 -0.32793367\n",
      "   0.86251576  0.92106096 -0.63645605]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.61335163  2.02446463 -2.45601108  0.32463777  0.59395888 -3.11428828\n",
      "   1.39695965]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:81 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.13696703]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 82 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94277193 -1.24379098  0.23167575  1.07515504 -0.28546475 -0.19720648\n",
      "   1.29750048  0.30817678 -0.66519083]\n",
      " [-0.89823158  0.99407126 -0.4517932  -1.08599818  0.18782138 -0.06423459\n",
      "  -0.75297444 -0.47415305  0.87485727]\n",
      " [ 0.70904076 -0.58956544 -0.05423537  0.66571307 -0.43060235  0.24840572\n",
      "   0.77523072  0.04251685 -0.7929364 ]\n",
      " [ 0.75662293 -0.91148449  0.29489902  0.74519863 -0.31614789  0.0659337\n",
      "   0.95600105  0.93852258 -0.49327252]\n",
      " [-1.04547224  1.22920037 -0.40862682 -1.21005897  0.06590282  0.28682967\n",
      "  -1.02438429 -0.82352772  0.83763892]\n",
      " [ 1.19690855 -1.11832875  0.39136905  0.84278501 -0.15431971 -0.32793367\n",
      "   0.86251576  0.92106096 -0.63702435]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.62144686  2.02019496 -2.45982372  0.32060596  0.58971338 -3.11795209\n",
      "   1.39233592]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:82 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.85728995]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 82 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.9443211  -1.24379098  0.23322492  1.07515504 -0.28546475 -0.19720648\n",
      "   1.29904965  0.30817678 -0.66519083]\n",
      " [-0.90001179  0.99407126 -0.45357341 -1.08599818  0.18782138 -0.06423459\n",
      "  -0.75475464 -0.47415305  0.87485727]\n",
      " [ 0.71098586 -0.58956544 -0.05229026  0.66571307 -0.43060235  0.24840572\n",
      "   0.77717583  0.04251685 -0.7929364 ]\n",
      " [ 0.75845246 -0.91148449  0.29672855  0.74519863 -0.31614789  0.0659337\n",
      "   0.95783058  0.93852258 -0.49327252]\n",
      " [-1.04701695  1.22920037 -0.41017153 -1.21005897  0.06590282  0.28682967\n",
      "  -1.025929   -0.82352772  0.83763892]\n",
      " [ 1.19847209 -1.11832875  0.39293258  0.84278501 -0.15431971 -0.32793367\n",
      "   0.86407929  0.92106096 -0.63702435]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.61271701  2.02824521 -2.45887386  0.32765014  0.59740948 -3.11727657\n",
      "   1.40037279]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:82 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.60943371]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 82 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.92869536 -1.25941672  0.21759918  1.07515504 -0.28546475 -0.19720648\n",
      "   1.28342391  0.30817678 -0.66519083]\n",
      " [-0.88497728  1.00910576 -0.4385389  -1.08599818  0.18782138 -0.06423459\n",
      "  -0.73972013 -0.47415305  0.87485727]\n",
      " [ 0.69809041 -0.60246089 -0.06518572  0.66571307 -0.43060235  0.24840572\n",
      "   0.76428038  0.04251685 -0.7929364 ]\n",
      " [ 0.74349421 -0.92644273  0.28177031  0.74519863 -0.31614789  0.0659337\n",
      "   0.94287234  0.93852258 -0.49327252]\n",
      " [-1.03130786  1.24490946 -0.39446244 -1.21005897  0.06590282  0.28682967\n",
      "  -1.01021991 -0.82352772  0.83763892]\n",
      " [ 1.18249658 -1.13430426  0.37695708  0.84278501 -0.15431971 -0.32793367\n",
      "   0.84810379  0.92106096 -0.63702435]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.68524701  1.97208457 -2.47679428  0.27689425  0.54297237 -3.13337988\n",
      "   1.34292745]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:82 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.76158009]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 82 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93352719 -1.25941672  0.22243101  1.07515504 -0.28546475 -0.19720648\n",
      "   1.28342391  0.31300861 -0.66519083]\n",
      " [-0.88972189  1.00910576 -0.44328351 -1.08599818  0.18782138 -0.06423459\n",
      "  -0.73972013 -0.47889766  0.87485727]\n",
      " [ 0.70135831 -0.60246089 -0.06191782  0.66571307 -0.43060235  0.24840572\n",
      "   0.76428038  0.04578475 -0.7929364 ]\n",
      " [ 0.74808139 -0.92644273  0.28635748  0.74519863 -0.31614789  0.0659337\n",
      "   0.94287234  0.94310976 -0.49327252]\n",
      " [-1.03550993  1.24490946 -0.39866451 -1.21005897  0.06590282  0.28682967\n",
      "  -1.01021991 -0.8277298   0.83763892]\n",
      " [ 1.18632325 -1.13430426  0.38078374  0.84278501 -0.15431971 -0.32793367\n",
      "   0.84810379  0.92488763 -0.63702435]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.66360137  1.98963238 -2.47371767  0.29123917  0.56195435 -3.13131467\n",
      "   1.36290129]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:82 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.59149233]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 82 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94440896 -1.25941672  0.22243101  1.08603681 -0.28546475 -0.19720648\n",
      "   1.28342391  0.31300861 -0.65430906]\n",
      " [-0.8998976   1.00910576 -0.44328351 -1.0961739   0.18782138 -0.06423459\n",
      "  -0.73972013 -0.47889766  0.86468156]\n",
      " [ 0.70788909 -0.60246089 -0.06191782  0.67224386 -0.43060235  0.24840572\n",
      "   0.76428038  0.04578475 -0.78640561]\n",
      " [ 0.75778495 -0.92644273  0.28635748  0.7549022  -0.31614789  0.0659337\n",
      "   0.94287234  0.94310976 -0.48356896]\n",
      " [-1.04648357  1.24490946 -0.39866451 -1.2210326   0.06590282  0.28682967\n",
      "  -1.01021991 -0.8277298   0.82666528]\n",
      " [ 1.19727778 -1.13430426  0.38078374  0.85373954 -0.15431971 -0.32793367\n",
      "   0.84810379  0.92488763 -0.62606981]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.61424768  2.02877299 -2.46140007  0.32281154  0.59803486 -3.12161367\n",
      "   1.40242987]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:82 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.07790314]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 82 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94454243 -1.25941672  0.22243101  1.08603681 -0.28533129 -0.19707302\n",
      "   1.28342391  0.31300861 -0.6541756 ]\n",
      " [-0.8999593   1.00910576 -0.44328351 -1.0961739   0.18775969 -0.06429629\n",
      "  -0.73972013 -0.47889766  0.86461986]\n",
      " [ 0.7080684  -0.60246089 -0.06191782  0.67224386 -0.43042305  0.24858503\n",
      "   0.76428038  0.04578475 -0.78622631]\n",
      " [ 0.75776816 -0.92644273  0.28635748  0.7549022  -0.31616468  0.06591691\n",
      "   0.94287234  0.94310976 -0.48358575]\n",
      " [-1.04657613  1.24490946 -0.39866451 -1.2210326   0.06581025  0.28673711\n",
      "  -1.01021991 -0.8277298   0.82657272]\n",
      " [ 1.19721568 -1.13430426  0.38078374  0.85373954 -0.15438182 -0.32799577\n",
      "   0.84810379  0.92488763 -0.62613192]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.61704574  2.02750825 -2.46286087  0.32159386  0.59661904 -3.12310554\n",
      "   1.40096866]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:82 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.00709759]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 82 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94454801 -1.25941114  0.22243101  1.08603681 -0.28532571 -0.19706744\n",
      "   1.28342391  0.31300861 -0.65417002]\n",
      " [-0.89996445  1.00910062 -0.44328351 -1.0961739   0.18775454 -0.06430143\n",
      "  -0.73972013 -0.47889766  0.86461471]\n",
      " [ 0.70807291 -0.60245639 -0.06191782  0.67224386 -0.43041854  0.24858953\n",
      "   0.76428038  0.04578475 -0.7862218 ]\n",
      " [ 0.7577728  -0.92643809  0.28635748  0.7549022  -0.31616004  0.06592155\n",
      "   0.94287234  0.94310976 -0.48358111]\n",
      " [-1.04658168  1.24490392 -0.39866451 -1.2210326   0.06580471  0.28673156\n",
      "  -1.01021991 -0.8277298   0.82656718]\n",
      " [ 1.19722071 -1.13429922  0.38078374  0.85373954 -0.15437678 -0.32799074\n",
      "   0.84810379  0.92488763 -0.62612688]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.61707075  2.02750351 -2.46287962  0.32158644  0.59661182 -3.12312551\n",
      "   1.40096215]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:82 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.25743859]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 82 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94172728 -1.26223187  0.22243101  1.08603681 -0.28814644 -0.19988817\n",
      "   1.28060318  0.31300861 -0.65417002]\n",
      " [-0.89703706  1.01202801 -0.44328351 -1.0961739   0.19068193 -0.06137404\n",
      "  -0.73679274 -0.47889766  0.86461471]\n",
      " [ 0.70430412 -0.60622518 -0.06191782  0.67224386 -0.43418733  0.24482075\n",
      "   0.76051159  0.04578475 -0.7862218 ]\n",
      " [ 0.75476103 -0.92944986  0.28635748  0.7549022  -0.31917181  0.06290978\n",
      "   0.93986057  0.94310976 -0.48358111]\n",
      " [-1.04389984  1.24758576 -0.39866451 -1.2210326   0.06848655  0.2894134\n",
      "  -1.00753807 -0.8277298   0.82656718]\n",
      " [ 1.19470129 -1.13681865  0.38078374  0.85373954 -0.15689621 -0.33051016\n",
      "   0.84558437  0.92488763 -0.62612688]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.64167724  2.01226709 -2.47212838  0.30520991  0.58115709 -3.1326516\n",
      "   1.38606162]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:82 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.89614709]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 82 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94227152 -1.26223187  0.22243101  1.08658106 -0.28814644 -0.19988817\n",
      "   1.28114742  0.31300861 -0.65417002]\n",
      " [-0.89779536  1.01202801 -0.44328351 -1.09693221  0.19068193 -0.06137404\n",
      "  -0.73755105 -0.47889766  0.86461471]\n",
      " [ 0.70527912 -0.60622518 -0.06191782  0.67321886 -0.43418733  0.24482075\n",
      "   0.7614866   0.04578475 -0.7862218 ]\n",
      " [ 0.75562704 -0.92944986  0.28635748  0.75576821 -0.31917181  0.06290978\n",
      "   0.94072658  0.94310976 -0.48358111]\n",
      " [-1.04445641  1.24758576 -0.39866451 -1.22158918  0.06848655  0.2894134\n",
      "  -1.00809465 -0.8277298   0.82656718]\n",
      " [ 1.19539637 -1.13681865  0.38078374  0.85443462 -0.15689621 -0.33051016\n",
      "   0.84627945  0.92488763 -0.62612688]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.63684458  2.01692923 -2.47183249  0.3095325   0.58560569 -3.13247508\n",
      "   1.39064081]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:82 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.73233615]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 82 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94807285 -1.26223187  0.22243101  1.08658106 -0.28234511 -0.19408684\n",
      "   1.28694875  0.31300861 -0.65417002]\n",
      " [-0.9036654   1.01202801 -0.44328351 -1.09693221  0.18481189 -0.06724408\n",
      "  -0.74342109 -0.47889766  0.86461471]\n",
      " [ 0.71099181 -0.60622518 -0.06191782  0.67321886 -0.42847465  0.25053343\n",
      "   0.76719928  0.04578475 -0.7862218 ]\n",
      " [ 0.761477   -0.92944986  0.28635748  0.75576821 -0.31332185  0.06875974\n",
      "   0.94657654  0.94310976 -0.48358111]\n",
      " [-1.05028424  1.24758576 -0.39866451 -1.22158918  0.06265873  0.28358558\n",
      "  -1.01392247 -0.8277298   0.82656718]\n",
      " [ 1.20126911 -1.13681865  0.38078374  0.85443462 -0.15102347 -0.32463742\n",
      "   0.85215219  0.92488763 -0.62612688]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.61061085  2.03923051 -2.46707037  0.33004745  0.60681484 -3.12840452\n",
      "   1.41229752]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:82 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.59480694]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 82 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93283461 -1.26223187  0.22243101  1.07134281 -0.28234511 -0.20932508\n",
      "   1.28694875  0.31300861 -0.66940826]\n",
      " [-0.88834957  1.01202801 -0.44328351 -1.08161638  0.18481189 -0.05192825\n",
      "  -0.74342109 -0.47889766  0.87993054]\n",
      " [ 0.69822584 -0.60622518 -0.06191782  0.66045289 -0.42847465  0.23776746\n",
      "   0.76719928  0.04578475 -0.79898777]\n",
      " [ 0.74668915 -0.92944986  0.28635748  0.74098036 -0.31332185  0.05397189\n",
      "   0.94657654  0.94310976 -0.49836897]\n",
      " [-1.03516735  1.24758576 -0.39866451 -1.20647229  0.06265873  0.29870247\n",
      "  -1.01392247 -0.8277298   0.84168406]\n",
      " [ 1.18646622 -1.13681865  0.38078374  0.83963173 -0.15102347 -0.33944032\n",
      "   0.85215219  0.92488763 -0.64092978]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.68228855  1.98431851 -2.48362087  0.27985463  0.55300539 -3.14548924\n",
      "   1.35845435]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:82 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.72356884]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 82 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93887242 -1.25619405  0.22243101  1.07738063 -0.28234511 -0.20328727\n",
      "   1.29298657  0.31300861 -0.66940826]\n",
      " [-0.89444897  1.00592861 -0.44328351 -1.08771578  0.18481189 -0.05802765\n",
      "  -0.74952049 -0.47889766  0.87993054]\n",
      " [ 0.70432182 -0.60012919 -0.06191782  0.66654888 -0.42847465  0.24386345\n",
      "   0.77329526  0.04578475 -0.79898777]\n",
      " [ 0.75287767 -0.92326135  0.28635748  0.74716888 -0.31332185  0.0601604\n",
      "   0.95276506  0.94310976 -0.49836897]\n",
      " [-1.04129945  1.24145365 -0.39866451 -1.21260439  0.06265873  0.29257036\n",
      "  -1.02005458 -0.8277298   0.84168406]\n",
      " [ 1.19260923 -1.13067563  0.38078374  0.84577475 -0.15102347 -0.3332973\n",
      "   0.8582952   0.92488763 -0.64092978]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.65464309  2.00810783 -2.47953949  0.30343307  0.57584542 -3.14125235\n",
      "   1.38063982]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:82 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.78394822]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 82 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94265914 -1.25240734  0.22243101  1.08116734 -0.28234511 -0.20328727\n",
      "   1.29677328  0.31300861 -0.66940826]\n",
      " [-0.89849983  1.00187775 -0.44328351 -1.09176664  0.18481189 -0.05802765\n",
      "  -0.75357135 -0.47889766  0.87993054]\n",
      " [ 0.70841793 -0.59603308 -0.06191782  0.67064499 -0.42847465  0.24386345\n",
      "   0.77739138  0.04578475 -0.79898777]\n",
      " [ 0.7569735  -0.91916551  0.28635748  0.75126471 -0.31332185  0.0601604\n",
      "   0.95686089  0.94310976 -0.49836897]\n",
      " [-1.04510808  1.23764503 -0.39866451 -1.21641302  0.06265873  0.29257036\n",
      "  -1.0238632  -0.8277298   0.84168406]\n",
      " [ 1.19663895 -1.12664592  0.38078374  0.84980446 -0.15102347 -0.3332973\n",
      "   0.86232492  0.92488763 -0.64092978]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.63634638  2.02432338 -2.47677428  0.31851049  0.59088423 -3.139133\n",
      "   1.39626448]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:82 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.83962919]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 82 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94474807 -1.25240734  0.22451995  1.08116734 -0.28234511 -0.20119833\n",
      "   1.29886222  0.31300861 -0.66940826]\n",
      " [-0.90066668  1.00187775 -0.44545035 -1.09176664  0.18481189 -0.0601945\n",
      "  -0.7557382  -0.47889766  0.87993054]\n",
      " [ 0.71082241 -0.59603308 -0.05951334  0.67064499 -0.42847465  0.24626793\n",
      "   0.77979585  0.04578475 -0.79898777]\n",
      " [ 0.75920341 -0.91916551  0.28858739  0.75126471 -0.31332185  0.06239031\n",
      "   0.9590908   0.94310976 -0.49836897]\n",
      " [-1.04725939  1.23764503 -0.40081582 -1.21641302  0.06265873  0.29041905\n",
      "  -1.02601451 -0.8277298   0.84168406]\n",
      " [ 1.19883838 -1.12664592  0.38298317  0.84980446 -0.15102347 -0.33109787\n",
      "   0.86452435  0.92488763 -0.64092978]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.62554925  2.03409897 -2.47565121  0.32759385  0.60046116 -3.13803157\n",
      "   1.40589043]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:82 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.13332228]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 83 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94454816 -1.25260725  0.22451995  1.08096743 -0.28234511 -0.20119833\n",
      "   1.29886222  0.31300861 -0.66960817]\n",
      " [-0.90045431  1.00209012 -0.44545035 -1.09155427  0.18481189 -0.0601945\n",
      "  -0.7557382  -0.47889766  0.88014291]\n",
      " [ 0.71084851 -0.59600698 -0.05951334  0.67067109 -0.42847465  0.24626793\n",
      "   0.77979585  0.04578475 -0.79896168]\n",
      " [ 0.75902484 -0.91934408  0.28858739  0.75108614 -0.31332185  0.06239031\n",
      "   0.9590908   0.94310976 -0.49854754]\n",
      " [-1.04690741  1.23799701 -0.40081582 -1.21606104  0.06265873  0.29041905\n",
      "  -1.02601451 -0.8277298   0.84203604]\n",
      " [ 1.1983077  -1.1271766   0.38298317  0.84927378 -0.15102347 -0.33109787\n",
      "   0.86452435  0.92488763 -0.64146046]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.63325178  2.03004743 -2.47928967  0.32376869  0.59643107 -3.14152885\n",
      "   1.40150147]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:83 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.85823724]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 83 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94608122 -1.25260725  0.22605301  1.08096743 -0.28234511 -0.20119833\n",
      "   1.30039528  0.31300861 -0.66960817]\n",
      " [-0.90221363  1.00209012 -0.44720968 -1.09155427  0.18481189 -0.0601945\n",
      "  -0.75749752 -0.47889766  0.88014291]\n",
      " [ 0.71277018 -0.59600698 -0.05759167  0.67067109 -0.42847465  0.24626793\n",
      "   0.78171753  0.04578475 -0.79896168]\n",
      " [ 0.76083255 -0.91934408  0.2903951   0.75108614 -0.31332185  0.06239031\n",
      "   0.96089851  0.94310976 -0.49854754]\n",
      " [-1.04843656  1.23799701 -0.40234497 -1.21606104  0.06265873  0.29041905\n",
      "  -1.02754366 -0.8277298   0.84203604]\n",
      " [ 1.19985558 -1.1271766   0.38453105  0.84927378 -0.15102347 -0.33109787\n",
      "   0.86607223  0.92488763 -0.64146046]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.62462792  2.03799744 -2.47835021  0.33072881  0.604033   -3.14085861\n",
      "   1.40943762]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:83 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.6044071]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 83 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93056824 -1.26812022  0.21054004  1.08096743 -0.28234511 -0.20119833\n",
      "   1.28488231  0.31300861 -0.66960817]\n",
      " [-0.88729183  1.01701192 -0.43228788 -1.09155427  0.18481189 -0.0601945\n",
      "  -0.74257572 -0.47889766  0.88014291]\n",
      " [ 0.69997799 -0.60879918 -0.07038386  0.67067109 -0.42847465  0.24626793\n",
      "   0.76892533  0.04578475 -0.79896168]\n",
      " [ 0.74598428 -0.93419235  0.27554683  0.75108614 -0.31332185  0.06239031\n",
      "   0.94605025  0.94310976 -0.49854754]\n",
      " [-1.03283939  1.25359418 -0.3867478  -1.21606104  0.06265873  0.29041905\n",
      "  -1.01194649 -0.8277298   0.84203604]\n",
      " [ 1.18397778 -1.14305439  0.36865326  0.84927378 -0.15102347 -0.33109787\n",
      "   0.85019443  0.92488763 -0.64146046]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.69688453  1.98221174 -2.49633025  0.28024655  0.54991964 -3.15707147\n",
      "   1.3523746 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:83 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.76301582]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 83 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93535095 -1.26812022  0.21532274  1.08096743 -0.28234511 -0.20119833\n",
      "   1.28488231  0.31779131 -0.66960817]\n",
      " [-0.8919877   1.01701192 -0.43698375 -1.09155427  0.18481189 -0.0601945\n",
      "  -0.74257572 -0.48359353  0.88014291]\n",
      " [ 0.70321257 -0.60879918 -0.06714928  0.67067109 -0.42847465  0.24626793\n",
      "   0.76892533  0.04901934 -0.79896168]\n",
      " [ 0.75052397 -0.93419235  0.28008652  0.75108614 -0.31332185  0.06239031\n",
      "   0.94605025  0.94764944 -0.49854754]\n",
      " [-1.03700184  1.25359418 -0.39091025 -1.21606104  0.06265873  0.29041905\n",
      "  -1.01194649 -0.83189224  0.84203604]\n",
      " [ 1.18777064 -1.14305439  0.37244612  0.84927378 -0.15102347 -0.33109787\n",
      "   0.85019443  0.92868049 -0.64146046]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.67545847  1.99958037 -2.49328697  0.29444574  0.56871103 -3.15502354\n",
      "   1.37214122]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:83 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.59289772]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 83 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94618912 -1.26812022  0.21532274  1.0918056  -0.28234511 -0.20119833\n",
      "   1.28488231  0.31779131 -0.65877   ]\n",
      " [-0.90212797  1.01701192 -0.43698375 -1.10169453  0.18481189 -0.0601945\n",
      "  -0.74257572 -0.48359353  0.87000265]\n",
      " [ 0.70972139 -0.60879918 -0.06714928  0.6771799  -0.42847465  0.24626793\n",
      "   0.76892533  0.04901934 -0.79245286]\n",
      " [ 0.7601996  -0.93419235  0.28008652  0.76076177 -0.31332185  0.06239031\n",
      "   0.94605025  0.94764944 -0.4888719 ]\n",
      " [-1.04792944  1.25359418 -0.39091025 -1.22698865  0.06265873  0.29041905\n",
      "  -1.01194649 -0.83189224  0.83110843]\n",
      " [ 1.19868018 -1.14305439  0.37244612  0.86018331 -0.15102347 -0.33109787\n",
      "   0.85019443  0.92868049 -0.63055093]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.62632733  2.03857041 -2.48104822  0.32588467  0.60465833 -3.14539029\n",
      "   1.41151896]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:83 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.07577288]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 83 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94631796 -1.26812022  0.21532274  1.0918056  -0.28221626 -0.20106948\n",
      "   1.28488231  0.31779131 -0.65864116]\n",
      " [-0.90218919  1.01701192 -0.43698375 -1.10169453  0.18475067 -0.06025572\n",
      "  -0.74257572 -0.48359353  0.86994143]\n",
      " [ 0.70989408 -0.60879918 -0.06714928  0.6771799  -0.42830196  0.24644061\n",
      "   0.76892533  0.04901934 -0.79228017]\n",
      " [ 0.76018608 -0.93419235  0.28008652  0.76076177 -0.31333538  0.06237679\n",
      "   0.94605025  0.94764944 -0.48888543]\n",
      " [-1.04801941  1.25359418 -0.39091025 -1.22698865  0.06256876  0.29032909\n",
      "  -1.01194649 -0.83189224  0.83101847]\n",
      " [ 1.19862323 -1.14305439  0.37244612  0.86018331 -0.15108041 -0.33115481\n",
      "   0.85019443  0.92868049 -0.63060787]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.62898057  2.03737347 -2.48243615  0.32473276  0.60331819 -3.14680715\n",
      "   1.41013532]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:83 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.00669245]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 83 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94632293 -1.26811526  0.21532274  1.0918056  -0.28221129 -0.20106452\n",
      "   1.28488231  0.31779131 -0.65863619]\n",
      " [-0.90219379  1.01700732 -0.43698375 -1.10169453  0.18474607 -0.06026032\n",
      "  -0.74257572 -0.48359353  0.86993683]\n",
      " [ 0.70989811 -0.60879514 -0.06714928  0.6771799  -0.42829792  0.24644465\n",
      "   0.76892533  0.04901934 -0.79227614]\n",
      " [ 0.76019023 -0.93418819  0.28008652  0.76076177 -0.31333123  0.06238094\n",
      "   0.94605025  0.94764944 -0.48888128]\n",
      " [-1.04802434  1.25358924 -0.39091025 -1.22698865  0.06256383  0.29032416\n",
      "  -1.01194649 -0.83189224  0.83101353]\n",
      " [ 1.19862773 -1.14304989  0.37244612  0.86018331 -0.15107591 -0.33115031\n",
      "   0.85019443  0.92868049 -0.63060337]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.62900282  2.03736929 -2.48245288  0.32472621  0.60331182 -3.14682496\n",
      "   1.41012959]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:83 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.25319703]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 83 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94361058 -1.27082761  0.21532274  1.0918056  -0.28492365 -0.20377687\n",
      "   1.28216995  0.31779131 -0.65863619]\n",
      " [-0.89936484  1.01983626 -0.43698375 -1.10169453  0.18757502 -0.05743137\n",
      "  -0.73974677 -0.48359353  0.86993683]\n",
      " [ 0.70623123 -0.61246203 -0.06714928  0.6771799  -0.43196481  0.24277777\n",
      "   0.76525844  0.04901934 -0.79227614]\n",
      " [ 0.75727414 -0.93710428  0.28008652  0.76076177 -0.31624732  0.05946485\n",
      "   0.94313416  0.94764944 -0.48888128]\n",
      " [-1.04544527  1.25616832 -0.39091025 -1.22698865  0.0651429   0.29290323\n",
      "  -1.00936741 -0.83189224  0.83101353]\n",
      " [ 1.19620335 -1.14547427  0.37244612  0.86018331 -0.15350029 -0.33357469\n",
      "   0.84777005  0.92868049 -0.63060337]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.65294111  2.0225825  -2.49147213  0.30879382  0.5882928  -3.15612571\n",
      "   1.39566292]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:83 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.89801724]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 83 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94413346 -1.27082761  0.21532274  1.09232848 -0.28492365 -0.20377687\n",
      "   1.28269283  0.31779131 -0.65863619]\n",
      " [-0.90009357  1.01983626 -0.43698375 -1.10242326  0.18757502 -0.05743137\n",
      "  -0.7404755  -0.48359353  0.86993683]\n",
      " [ 0.70716986 -0.61246203 -0.06714928  0.67811854 -0.43196481  0.24277777\n",
      "   0.76619708  0.04901934 -0.79227614]\n",
      " [ 0.75810676 -0.93710428  0.28008652  0.7615944  -0.31624732  0.05946485\n",
      "   0.94396678  0.94764944 -0.48888128]\n",
      " [-1.04597998  1.25616832 -0.39091025 -1.22752337  0.0651429   0.29290323\n",
      "  -1.00990213 -0.83189224  0.83101353]\n",
      " [ 1.19687129 -1.14547427  0.37244612  0.86085124 -0.15350029 -0.33357469\n",
      "   0.84843799  0.92868049 -0.63060337]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.64827121  2.02708909 -2.4911891   0.31297592  0.59259553 -3.15595666\n",
      "   1.40009023]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:83 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.73458019]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 83 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94985344 -1.27082761  0.21532274  1.09232848 -0.27920366 -0.19805688\n",
      "   1.28841282  0.31779131 -0.65863619]\n",
      " [-0.90588392  1.01983626 -0.43698375 -1.10242326  0.18178466 -0.06322173\n",
      "  -0.74626585 -0.48359353  0.86993683]\n",
      " [ 0.71281262 -0.61246203 -0.06714928  0.67811854 -0.42632205  0.24842052\n",
      "   0.77183983  0.04901934 -0.79227614]\n",
      " [ 0.76387893 -0.93710428  0.28008652  0.7615944  -0.31047515  0.06523702\n",
      "   0.94973895  0.94764944 -0.48888128]\n",
      " [-1.05172622  1.25616832 -0.39091025 -1.22752337  0.05939667  0.287157\n",
      "  -1.01564837 -0.83189224  0.83101353]\n",
      " [ 1.20266345 -1.14547427  0.37244612  0.86085124 -0.14770812 -0.32778253\n",
      "   0.85423015  0.92868049 -0.63060337]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.62239647  2.04909423 -2.48650899  0.33323999  0.61353525 -3.15195259\n",
      "   1.42146535]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:83 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.59439594]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 83 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93462512 -1.27082761  0.21532274  1.07710015 -0.27920366 -0.21328521\n",
      "   1.28841282  0.31779131 -0.67386451]\n",
      " [-0.89058109  1.01983626 -0.43698375 -1.08712042  0.18178466 -0.04791889\n",
      "  -0.74626585 -0.48359353  0.88523966]\n",
      " [ 0.70006573 -0.61246203 -0.06714928  0.66537166 -0.42632205  0.23567363\n",
      "   0.77183983  0.04901934 -0.80502302]\n",
      " [ 0.7491001  -0.93710428  0.28008652  0.74681557 -0.31047515  0.05045819\n",
      "   0.94973895  0.94764944 -0.50366011]\n",
      " [-1.03661818  1.25616832 -0.39091025 -1.21241533  0.05939667  0.30226504\n",
      "  -1.01564837 -0.83189224  0.84612157]\n",
      " [ 1.18786489 -1.14547427  0.37244612  0.84605268 -0.14770812 -0.34258109\n",
      "   0.85423015  0.92868049 -0.64540194]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.69404776  1.99421413 -2.5030742   0.28308744  0.55975361 -3.16903937\n",
      "   1.36763948]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:83 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.72421045]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 83 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94064391 -1.26480883  0.21532274  1.08311894 -0.27920366 -0.20726642\n",
      "   1.2944316   0.31779131 -0.67386451]\n",
      " [-0.8966586   1.01375875 -0.43698375 -1.09319794  0.18178466 -0.0539964\n",
      "  -0.75234336 -0.48359353  0.88523966]\n",
      " [ 0.70613627 -0.60639149 -0.06714928  0.67144219 -0.42632205  0.24174417\n",
      "   0.77791037  0.04901934 -0.80502302]\n",
      " [ 0.75526537 -0.93093901  0.28008652  0.75298084 -0.31047515  0.05662346\n",
      "   0.95590422  0.94764944 -0.50366011]\n",
      " [-1.04272925  1.25005724 -0.39091025 -1.2185264   0.05939667  0.29615397\n",
      "  -1.02175944 -0.83189224  0.84612157]\n",
      " [ 1.19398358 -1.13935558  0.37244612  0.85217137 -0.14770812 -0.3364624\n",
      "   0.86034884  0.92868049 -0.64540194]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.66650608  2.01790241 -2.49900385  0.30658795  0.58250885 -3.16480756\n",
      "   1.38973338]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:83 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.78543501]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 83 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.9443854  -1.26106733  0.21532274  1.08686044 -0.27920366 -0.20726642\n",
      "   1.2981731   0.31779131 -0.67386451]\n",
      " [-0.9006602   1.00975716 -0.43698375 -1.09719953  0.18178466 -0.0539964\n",
      "  -0.75634496 -0.48359353  0.88523966]\n",
      " [ 0.71018382 -0.60234394 -0.06714928  0.67548974 -0.42632205  0.24174417\n",
      "   0.78195792  0.04901934 -0.80502302]\n",
      " [ 0.75931282 -0.92689157  0.28008652  0.75702828 -0.31047515  0.05662346\n",
      "   0.95995167  0.94764944 -0.50366011]\n",
      " [-1.04649229  1.2462942  -0.39091025 -1.22228944  0.05939667  0.29615397\n",
      "  -1.02552248 -0.83189224  0.84612157]\n",
      " [ 1.19796491 -1.13537425  0.37244612  0.8561527  -0.14770812 -0.3364624\n",
      "   0.86433018  0.92868049 -0.64540194]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.64842609  2.0339265  -2.49627762  0.32150002  0.59737919 -3.16271416\n",
      "   1.40517554]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:83 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.83988754]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 83 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94647434 -1.26106733  0.21741168  1.08686044 -0.27920366 -0.20517748\n",
      "   1.30026204  0.31779131 -0.67386451]\n",
      " [-0.90282454  1.00975716 -0.43914809 -1.09719953  0.18178466 -0.05616075\n",
      "  -0.75850931 -0.48359353  0.88523966]\n",
      " [ 0.71258149 -0.60234394 -0.0647516   0.67548974 -0.42632205  0.24414185\n",
      "   0.78435559  0.04901934 -0.80502302]\n",
      " [ 0.76153903 -0.92689157  0.28231273  0.75702828 -0.31047515  0.05884967\n",
      "   0.96217788  0.94764944 -0.50366011]\n",
      " [-1.04864329  1.2462942  -0.39306125 -1.22228944  0.05939667  0.29400297\n",
      "  -1.02767348 -0.83189224  0.84612157]\n",
      " [ 1.20016344 -1.13537425  0.37464464  0.8561527  -0.14770812 -0.33426388\n",
      "   0.8665287   0.92868049 -0.64540194]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.63766042  2.04366625 -2.4951524   0.33055547  0.6069235  -3.16160772\n",
      "   1.41476491]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:83 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.12976103]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 84 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94629454 -1.26124713  0.21741168  1.08668064 -0.27920366 -0.20517748\n",
      "   1.30026204  0.31779131 -0.67404431]\n",
      " [-0.9026327   1.009949   -0.43914809 -1.09700769  0.18178466 -0.05616075\n",
      "  -0.75850931 -0.48359353  0.8854315 ]\n",
      " [ 0.71261683 -0.6023086  -0.0647516   0.67552508 -0.42632205  0.24414185\n",
      "   0.78435559  0.04901934 -0.80498768]\n",
      " [ 0.76137813 -0.92705247  0.28231273  0.75686738 -0.31047515  0.05884967\n",
      "   0.96217788  0.94764944 -0.50382101]\n",
      " [-1.04831891  1.24661859 -0.39306125 -1.22196506  0.05939667  0.29400297\n",
      "  -1.02767348 -0.83189224  0.84644596]\n",
      " [ 1.19966821 -1.13586948  0.37464464  0.85565747 -0.14770812 -0.33426388\n",
      "   0.8665287   0.92868049 -0.64589716]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.64498693  2.0398229  -2.49862346  0.32692755  0.60309914 -3.16494486\n",
      "   1.41060014]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:84 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.85916685]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 84 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94781183 -1.26124713  0.21892896  1.08668064 -0.27920366 -0.20517748\n",
      "   1.30177933  0.31779131 -0.67404431]\n",
      " [-0.90437162  1.009949   -0.44088702 -1.09700769  0.18178466 -0.05616075\n",
      "  -0.76024823 -0.48359353  0.8854315 ]\n",
      " [ 0.71451561 -0.6023086  -0.06285282  0.67552508 -0.42632205  0.24414185\n",
      "   0.78625437  0.04901934 -0.80498768]\n",
      " [ 0.76316454 -0.92705247  0.28409914  0.75686738 -0.31047515  0.05884967\n",
      "   0.96396429  0.94764944 -0.50382101]\n",
      " [-1.04983282  1.24661859 -0.39457516 -1.22196506  0.05939667  0.29400297\n",
      "  -1.02918739 -0.83189224  0.84644596]\n",
      " [ 1.20120079 -1.13586948  0.37617723  0.85565747 -0.14770812 -0.33426388\n",
      "   0.86806128  0.92868049 -0.64589716]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.63646658  2.04767506 -2.49769415  0.33380558  0.61060908 -3.1642798\n",
      "   1.41843792]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:84 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.59933895]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 84 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93241754 -1.27664142  0.20353468  1.08668064 -0.27920366 -0.20517748\n",
      "   1.28638504  0.31779131 -0.67404431]\n",
      " [-0.88956721  1.02475341 -0.4260826  -1.09700769  0.18178466 -0.05616075\n",
      "  -0.74544382 -0.48359353  0.8854315 ]\n",
      " [ 0.70182939 -0.61499483 -0.07553905  0.67552508 -0.42632205  0.24414185\n",
      "   0.77356815  0.04901934 -0.80498768]\n",
      " [ 0.74843087 -0.94178614  0.26936547  0.75686738 -0.31047515  0.05884967\n",
      "   0.94923062  0.94764944 -0.50382101]\n",
      " [-1.03435382  1.26209759 -0.37909616 -1.22196506  0.05939667  0.29400297\n",
      "  -1.01370839 -0.83189224  0.84644596]\n",
      " [ 1.18542765 -1.15164261  0.36040409  0.85565747 -0.14770812 -0.33426388\n",
      "   0.85228815  0.92868049 -0.64589716]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.70842674  1.99227897 -2.51572544  0.28361052  0.55683444 -3.18059396\n",
      "   1.36177352]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:84 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.76444288]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 84 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93715153 -1.27664142  0.20826867  1.08668064 -0.27920366 -0.20517748\n",
      "   1.28638504  0.32252531 -0.67404431]\n",
      " [-0.8942148   1.02475341 -0.4307302  -1.09700769  0.18178466 -0.05616075\n",
      "  -0.74544382 -0.48824112  0.8854315 ]\n",
      " [ 0.70503086 -0.61499483 -0.07233758  0.67552508 -0.42632205  0.24414185\n",
      "   0.77356815  0.05222081 -0.80498768]\n",
      " [ 0.75292355 -0.94178614  0.27385815  0.75686738 -0.31047515  0.05884967\n",
      "   0.94923062  0.95214212 -0.50382101]\n",
      " [-1.03847693  1.26209759 -0.38321928 -1.22196506  0.05939667  0.29400297\n",
      "  -1.01370839 -0.83601536  0.84644596]\n",
      " [ 1.18918696 -1.15164261  0.3641634   0.85565747 -0.14770812 -0.33426388\n",
      "   0.85228815  0.9324398  -0.64589716]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.68721836  2.00946993 -2.51271505  0.29766513  0.57543676 -3.17856321\n",
      "   1.38133477]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:84 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.59427661]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 84 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94794656 -1.27664142  0.20826867  1.09747566 -0.27920366 -0.20517748\n",
      "   1.28638504  0.32252531 -0.66324929]\n",
      " [-0.90431966  1.02475341 -0.4307302  -1.10711254  0.18178466 -0.05616075\n",
      "  -0.74544382 -0.48824112  0.87532665]\n",
      " [ 0.7115168  -0.61499483 -0.07233758  0.68201102 -0.42632205  0.24414185\n",
      "   0.77356815  0.05222081 -0.79850174]\n",
      " [ 0.76257099 -0.94178614  0.27385815  0.76651483 -0.31047515  0.05884967\n",
      "   0.94923062  0.95214212 -0.49417356]\n",
      " [-1.0493591   1.26209759 -0.38321928 -1.23284723  0.05939667  0.29400297\n",
      "  -1.01370839 -0.83601536  0.83556379]\n",
      " [ 1.20005202 -1.15164261  0.3641634   0.86652253 -0.14770812 -0.33426388\n",
      "   0.85228815  0.9324398  -0.63503211]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.63830599  2.048311   -2.50055268  0.32897135  0.61125191 -3.16899573\n",
      "   1.42056331]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:84 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.07370009]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 84 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.9480709  -1.27664142  0.20826867  1.09747566 -0.27907932 -0.20505314\n",
      "   1.28638504  0.32252531 -0.66312494]\n",
      " [-0.90438029  1.02475341 -0.4307302  -1.10711254  0.18172403 -0.05622138\n",
      "  -0.74544382 -0.48824112  0.87526602]\n",
      " [ 0.71168306 -0.61499483 -0.07233758  0.68201102 -0.4261558   0.2443081\n",
      "   0.77356815  0.05222081 -0.79833548]\n",
      " [ 0.76256045 -0.94178614  0.27385815  0.76651483 -0.3104857   0.05883912\n",
      "   0.94923062  0.95214212 -0.49418411]\n",
      " [-1.04944648  1.26209759 -0.38321928 -1.23284723  0.0593093   0.29391559\n",
      "  -1.01370839 -0.83601536  0.83547642]\n",
      " [ 1.19999988 -1.15164261  0.3641634   0.86652253 -0.14776026 -0.33431602\n",
      "   0.85228815  0.9324398  -0.63508425]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.64082168  2.04717832 -2.50187125  0.32788177  0.60998351 -3.17034124\n",
      "   1.41925326]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:84 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.00631289]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 84 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94807533 -1.27663699  0.20826867  1.09747566 -0.27907489 -0.20504872\n",
      "   1.28638504  0.32252531 -0.66312052]\n",
      " [-0.9043844   1.0247493  -0.4307302  -1.10711254  0.18171992 -0.0562255\n",
      "  -0.74544382 -0.48824112  0.8752619 ]\n",
      " [ 0.71168667 -0.61499121 -0.07233758  0.68201102 -0.42615218  0.24431172\n",
      "   0.77356815  0.05222081 -0.79833186]\n",
      " [ 0.76256417 -0.94178241  0.27385815  0.76651483 -0.31048197  0.05884285\n",
      "   0.94923062  0.95214212 -0.49418039]\n",
      " [-1.04945088  1.26209319 -0.38321928 -1.23284723  0.0593049   0.29391119\n",
      "  -1.01370839 -0.83601536  0.83547202]\n",
      " [ 1.2000039  -1.15163859  0.3641634   0.86652253 -0.14775624 -0.33431199\n",
      "   0.85228815  0.9324398  -0.63508022]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.64084148  2.04717464 -2.50188618  0.32787598  0.60997789 -3.17035712\n",
      "   1.4192482 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:84 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.2490795]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 84 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.9454659  -1.27924642  0.20826867  1.09747566 -0.28168432 -0.20765814\n",
      "   1.28377562  0.32252531 -0.66312052]\n",
      " [-0.90164934  1.02748436 -0.4307302  -1.10711254  0.18445498 -0.05349043\n",
      "  -0.74270875 -0.48824112  0.8752619 ]\n",
      " [ 0.70811753 -0.61856036 -0.07233758  0.68201102 -0.42972133  0.24074257\n",
      "   0.769999    0.05222081 -0.79833186]\n",
      " [ 0.7597395  -0.94460709  0.27385815  0.76651483 -0.31330665  0.05601817\n",
      "   0.94640594  0.95214212 -0.49418039]\n",
      " [-1.04696937  1.2645747  -0.38321928 -1.23284723  0.06178641  0.2963927\n",
      "  -1.01122689 -0.83601536  0.83547202]\n",
      " [ 1.19767003 -1.15397246  0.3641634   0.86652253 -0.15009011 -0.33664586\n",
      "   0.84995427  0.9324398  -0.63508022]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.66413526  2.03281964 -2.51068281  0.31237126  0.59537811 -3.17943872\n",
      "   1.40519868]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:84 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.8998457]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 84 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94596833 -1.27924642  0.20826867  1.09797809 -0.28168432 -0.20765814\n",
      "   1.28427804  0.32252531 -0.66312052]\n",
      " [-0.90234974  1.02748436 -0.4307302  -1.10781294  0.18445498 -0.05349043\n",
      "  -0.74340915 -0.48824112  0.8752619 ]\n",
      " [ 0.70902125 -0.61856036 -0.07233758  0.68291474 -0.42972133  0.24074257\n",
      "   0.77090272  0.05222081 -0.79833186]\n",
      " [ 0.76054013 -0.94460709  0.27385815  0.76731546 -0.31330665  0.05601817\n",
      "   0.94720657  0.95214212 -0.49418039]\n",
      " [-1.04748317  1.2645747  -0.38321928 -1.23336102  0.06178641  0.2963927\n",
      "  -1.01174068 -0.83601536  0.83547202]\n",
      " [ 1.19831198 -1.15397246  0.3641634   0.86716448 -0.15009011 -0.33664586\n",
      "   0.85059623  0.9324398  -0.63508022]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.65962214  2.03717631 -2.510412    0.31641778  0.59954013 -3.17927679\n",
      "   1.40947957]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:84 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.73682125]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 84 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.95160731 -1.27924642  0.20826867  1.09797809 -0.27604534 -0.20201916\n",
      "   1.28991702  0.32252531 -0.66312052]\n",
      " [-0.90806065  1.02748436 -0.4307302  -1.10781294  0.17874407 -0.05920134\n",
      "  -0.74912006 -0.48824112  0.8752619 ]\n",
      " [ 0.71459392 -0.61856036 -0.07233758  0.68291474 -0.42414865  0.24631525\n",
      "   0.77647539  0.05222081 -0.79833186]\n",
      " [ 0.76623464 -0.94460709  0.27385815  0.76731546 -0.30761213  0.06171269\n",
      "   0.95290109  0.95214212 -0.49418039]\n",
      " [-1.05314814  1.2645747  -0.38321928 -1.23336102  0.05612143  0.29072772\n",
      "  -1.01740566 -0.83601536  0.83547202]\n",
      " [ 1.20402386 -1.15397246  0.3641634   0.86716448 -0.14437824 -0.33093399\n",
      "   0.8563081   0.9324398  -0.63508022]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.63410489  2.05888643 -2.50581305  0.33643114  0.62021085 -3.17533874\n",
      "   1.43057378]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:84 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.593921]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 84 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93639047 -1.27924642  0.20826867  1.08276125 -0.27604534 -0.217236\n",
      "   1.28991702  0.32252531 -0.67833736]\n",
      " [-0.89277241  1.02748436 -0.4307302  -1.09252471  0.17874407 -0.0439131\n",
      "  -0.74912006 -0.48824112  0.89055014]\n",
      " [ 0.70186826 -0.61856036 -0.07233758  0.67018909 -0.42414865  0.23358959\n",
      "   0.77647539  0.05222081 -0.81105752]\n",
      " [ 0.75146663 -0.94460709  0.27385815  0.75254745 -0.30761213  0.04694468\n",
      "   0.95290109  0.95214212 -0.5089484 ]\n",
      " [-1.03805051  1.2645747  -0.38321928 -1.21826339  0.05612143  0.30582535\n",
      "  -1.01740566 -0.83601536  0.85056965]\n",
      " [ 1.18923136 -1.15397246  0.3641634   0.85237198 -0.14437824 -0.34572648\n",
      "   0.8563081   0.9324398  -0.64987272]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.70572548  2.00404323 -2.52239382  0.2863239   0.56646226 -3.19242821\n",
      "   1.37677036]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:84 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.72485812]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 84 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94238985 -1.27324703  0.20826867  1.08876063 -0.27604534 -0.21123662\n",
      "   1.29591641  0.32252531 -0.67833736]\n",
      " [-0.89882779  1.02142898 -0.4307302  -1.09858009  0.17874407 -0.04996848\n",
      "  -0.75517544 -0.48824112  0.89055014]\n",
      " [ 0.70791313 -0.61251549 -0.07233758  0.67623396 -0.42414865  0.23963446\n",
      "   0.78252026  0.05222081 -0.81105752]\n",
      " [ 0.75760845 -0.93846527  0.27385815  0.75868926 -0.30761213  0.05308649\n",
      "   0.95904291  0.95214212 -0.5089484 ]\n",
      " [-1.04414024  1.25848497 -0.38321928 -1.22435312  0.05612143  0.29973563\n",
      "  -1.02349539 -0.83601536  0.85056965]\n",
      " [ 1.19532553 -1.1478783   0.3641634   0.85846615 -0.14437824 -0.33963232\n",
      "   0.86240227  0.9324398  -0.64987272]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.67828849  2.0276301  -2.51833478  0.30974562  0.58913196 -3.18820203\n",
      "   1.39877208]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:84 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.78692645]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 84 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94608606 -1.26955082  0.20826867  1.09245684 -0.27604534 -0.21123662\n",
      "   1.29961262  0.32252531 -0.67833736]\n",
      " [-0.90278015  1.01747662 -0.4307302  -1.10253244  0.17874407 -0.04996848\n",
      "  -0.7591278  -0.48824112  0.89055014]\n",
      " [ 0.71191207 -0.60851655 -0.07233758  0.68023289 -0.42414865  0.23963446\n",
      "   0.7865192   0.05222081 -0.81105752]\n",
      " [ 0.76160749 -0.93446623  0.27385815  0.7626883  -0.30761213  0.05308649\n",
      "   0.96304195  0.95214212 -0.5089484 ]\n",
      " [-1.04785763  1.25476757 -0.38321928 -1.22807052  0.05612143  0.29973563\n",
      "  -1.02721278 -0.83601536  0.85056965]\n",
      " [ 1.1992585  -1.14394532  0.3641634   0.86239912 -0.14437824 -0.33963232\n",
      "   0.86633524  0.9324398  -0.64987272]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.66042513  2.04346299 -2.51564741  0.32449206  0.60383369 -3.1861347\n",
      "   1.41403183]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:84 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.84013]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 84 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94817526 -1.26955082  0.21035787  1.09245684 -0.27604534 -0.20914742\n",
      "   1.30170182  0.32252531 -0.67833736]\n",
      " [-0.90494234  1.01747662 -0.43289238 -1.10253244  0.17874407 -0.05213067\n",
      "  -0.76128999 -0.48824112  0.89055014]\n",
      " [ 0.71430338 -0.60851655 -0.06994627  0.68023289 -0.42414865  0.24202577\n",
      "   0.7889105   0.05222081 -0.81105752]\n",
      " [ 0.76383039 -0.93446623  0.27608105  0.7626883  -0.30761213  0.05530939\n",
      "   0.96526485  0.95214212 -0.5089484 ]\n",
      " [-1.05000859  1.25476757 -0.38537024 -1.22807052  0.05612143  0.29758467\n",
      "  -1.02936374 -0.83601536  0.85056965]\n",
      " [ 1.20145643 -1.14394532  0.36636133  0.86239912 -0.14437824 -0.33743439\n",
      "   0.86853317  0.9324398  -0.64987272]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.64968893  2.05316874 -2.51451985  0.33352126  0.61334713 -3.18502313\n",
      "   1.42358639]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:84 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.12628267]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 85 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94801408 -1.269712    0.21035787  1.09229566 -0.27604534 -0.20914742\n",
      "   1.30170182  0.32252531 -0.67849854]\n",
      " [-0.90476956  1.0176494  -0.43289238 -1.10235967  0.17874407 -0.05213067\n",
      "  -0.76128999 -0.48824112  0.89072292]\n",
      " [ 0.71434698 -0.60847295 -0.06994627  0.68027649 -0.42414865  0.24202577\n",
      "   0.7889105   0.05222081 -0.81101391]\n",
      " [ 0.76368589 -0.93461072  0.27608105  0.76254381 -0.30761213  0.05530939\n",
      "   0.96526485  0.95214212 -0.50909289]\n",
      " [-1.04970997  1.2550662  -0.38537024 -1.22777189  0.05612143  0.29758467\n",
      "  -1.02936374 -0.83601536  0.85086828]\n",
      " [ 1.20099459 -1.14440717  0.36636133  0.86193728 -0.14437824 -0.33743439\n",
      "   0.86853317  0.9324398  -0.65033457]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.65665565  2.04952397 -2.51783015  0.3300815   0.60971911 -3.18820638\n",
      "   1.41963555]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:85 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.86008055]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 85 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94951589 -1.269712    0.21185968  1.09229566 -0.27604534 -0.20914742\n",
      "   1.30320362  0.32252531 -0.67849854]\n",
      " [-0.90648852  1.0176494  -0.43461135 -1.10235967  0.17874407 -0.05213067\n",
      "  -0.76300895 -0.48824112  0.89072292]\n",
      " [ 0.71622337 -0.60847295 -0.06806988  0.68027649 -0.42414865  0.24202577\n",
      "   0.79078689  0.05222081 -0.81101391]\n",
      " [ 0.76545146 -0.93461072  0.27784661  0.76254381 -0.30761213  0.05530939\n",
      "   0.96703041  0.95214212 -0.50909289]\n",
      " [-1.05120891  1.2550662  -0.38686918 -1.22777189  0.05612143  0.29758467\n",
      "  -1.03086269 -0.83601536  0.85086828]\n",
      " [ 1.20251218 -1.14440717  0.36787892  0.86193728 -0.14437824 -0.33743439\n",
      "   0.87005077  0.9324398  -0.65033457]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.64823656  2.05728044 -2.5169108   0.33687922  0.61713906 -3.18754642\n",
      "   1.4273771 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:85 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.59423802]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 85 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93424588 -1.28498201  0.19658967  1.09229566 -0.27604534 -0.20914742\n",
      "   1.28793361  0.32252531 -0.67849854]\n",
      " [-0.89180589  1.03233203 -0.41992871 -1.10235967  0.17874407 -0.05213067\n",
      "  -0.74832632 -0.48824112  0.89072292]\n",
      " [ 0.70364564 -0.62105067 -0.0806476   0.68027649 -0.42414865  0.24202577\n",
      "   0.77820917  0.05222081 -0.81101391]\n",
      " [ 0.75083674 -0.94922544  0.2632319   0.76254381 -0.30761213  0.05530939\n",
      "   0.95241569  0.95214212 -0.50909289]\n",
      " [-1.03585399  1.27042112 -0.37151426 -1.22777189  0.05612143  0.29758467\n",
      "  -1.01550777 -0.83601536  0.85086828]\n",
      " [ 1.18685036 -1.16006898  0.35221711  0.86193728 -0.14437824 -0.33743439\n",
      "   0.85438895  0.9324398  -0.65033457]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.71987766  2.00228798 -2.53498481  0.28698451  0.56371753 -3.20395339\n",
      "   1.37112689]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:85 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.76586274]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 85 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93893154 -1.28498201  0.20127533  1.09229566 -0.27604534 -0.20914742\n",
      "   1.28793361  0.32721097 -0.67849854]\n",
      " [-0.89640561  1.03233203 -0.42452843 -1.10235967  0.17874407 -0.05213067\n",
      "  -0.74832632 -0.49284084  0.89072292]\n",
      " [ 0.70681418 -0.62105067 -0.07747907  0.68027649 -0.42414865  0.24202577\n",
      "   0.77820917  0.05538934 -0.81101391]\n",
      " [ 0.75528283 -0.94922544  0.26767799  0.76254381 -0.30761213  0.05530939\n",
      "   0.95241569  0.95658821 -0.50909289]\n",
      " [-1.03993802  1.27042112 -0.37559829 -1.22777189  0.05612143  0.29758467\n",
      "  -1.01550777 -0.84009938  0.85086828]\n",
      " [ 1.19057631 -1.16006898  0.35594306  0.86193728 -0.14437824 -0.33743439\n",
      "   0.85438895  0.93616574 -0.65033457]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.69888526  2.01930264 -2.53200696  0.30089555  0.58213213 -3.20193976\n",
      "   1.3904844 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:85 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.59562857]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 85 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94968392 -1.28498201  0.20127533  1.10304804 -0.27604534 -0.20914742\n",
      "   1.28793361  0.32721097 -0.66774616]\n",
      " [-0.90647514  1.03233203 -0.42452843 -1.11242919  0.17874407 -0.05213067\n",
      "  -0.74832632 -0.49284084  0.88065339]\n",
      " [ 0.71327641 -0.62105067 -0.07747907  0.68673873 -0.42414865  0.24202577\n",
      "   0.77820917  0.05538934 -0.80455168]\n",
      " [ 0.76490191 -0.94922544  0.26767799  0.77216289 -0.30761213  0.05530939\n",
      "   0.95241569  0.95658821 -0.49947381]\n",
      " [-1.05077538  1.27042112 -0.37559829 -1.23860924  0.05612143  0.29758467\n",
      "  -1.01550777 -0.84009938  0.84003092]\n",
      " [ 1.20139746 -1.16006898  0.35594306  0.87275842 -0.14437824 -0.33743439\n",
      "   0.85438895  0.93616574 -0.63951342]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.65018779  2.05799645 -2.51991849  0.33206993  0.61781629 -3.1924361\n",
      "   1.42956556]]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:85 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.07168213]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 85 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94980387 -1.28498201  0.20127533  1.10304804 -0.27592539 -0.20902747\n",
      "   1.28793361  0.32721097 -0.66762621]\n",
      " [-0.90653508  1.03233203 -0.42452843 -1.11242919  0.17868413 -0.05219062\n",
      "  -0.74832632 -0.49284084  0.88059344]\n",
      " [ 0.71343642 -0.62105067 -0.07747907  0.68673873 -0.42398865  0.24218577\n",
      "   0.77820917  0.05538934 -0.80439168]\n",
      " [ 0.76489409 -0.94922544  0.26767799  0.77216289 -0.30761996  0.05530157\n",
      "   0.95241569  0.95658821 -0.49948164]\n",
      " [-1.05086018  1.27042112 -0.37559829 -1.23860924  0.05603662  0.29749986\n",
      "  -1.01550777 -0.84009938  0.83994611]\n",
      " [ 1.20134979 -1.16006898  0.35594306  0.87275842 -0.1444259  -0.33748206\n",
      "   0.85438895  0.93616574 -0.63956109]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.65257279  2.05692473 -2.52117103  0.33103944  0.61661597 -3.1937137\n",
      "   1.42832534]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:85 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.00595708]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 85 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94980781 -1.28497807  0.20127533  1.10304804 -0.27592145 -0.20902353\n",
      "   1.28793361  0.32721097 -0.66762227]\n",
      " [-0.90653876  1.03232835 -0.42452843 -1.11242919  0.17868044 -0.0521943\n",
      "  -0.74832632 -0.49284084  0.88058976]\n",
      " [ 0.71343966 -0.62104743 -0.07747907  0.68673873 -0.4239854   0.24218902\n",
      "   0.77820917  0.05538934 -0.80438843]\n",
      " [ 0.76489743 -0.9492221   0.26767799  0.77216289 -0.30761661  0.05530491\n",
      "   0.95241569  0.95658821 -0.4994783 ]\n",
      " [-1.05086411  1.2704172  -0.37559829 -1.23860924  0.0560327   0.29749594\n",
      "  -1.01550777 -0.84009938  0.83994219]\n",
      " [ 1.2013534  -1.16006538  0.35594306  0.87275842 -0.1444223  -0.33747845\n",
      "   0.85438895  0.93616574 -0.63955748]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.65259043  2.05692148 -2.52118437  0.33103432  0.616611   -3.19372788\n",
      "   1.42832087]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:85 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.24508327]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 85 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94729613 -1.28748976  0.20127533  1.10304804 -0.27843313 -0.21153521\n",
      "   1.28542192  0.32721097 -0.66762227]\n",
      " [-0.90389322  1.0349739  -0.42452843 -1.11242919  0.18132599 -0.04954875\n",
      "  -0.74568077 -0.49284084  0.88058976]\n",
      " [ 0.70996425 -0.62452284 -0.07747907  0.68673873 -0.42746082  0.23871361\n",
      "   0.77473375  0.05538934 -0.80438843]\n",
      " [ 0.76216007 -0.95195946  0.26767799  0.77216289 -0.31035397  0.05256755\n",
      "   0.94967834  0.95658821 -0.4994783 ]\n",
      " [-1.04847522  1.27280608 -0.37559829 -1.23860924  0.05842159  0.29988482\n",
      "  -1.01311888 -0.84009938  0.83994219]\n",
      " [ 1.19910567 -1.16231311  0.35594306  0.87275842 -0.14667003 -0.33972618\n",
      "   0.85214122  0.93616574 -0.63955748]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.67526277  2.042981   -2.52976516  0.31594131  0.60241448 -3.20259646\n",
      "   1.41467228]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:85 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.90163376]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 85 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94777897 -1.28748976  0.20127533  1.10353089 -0.27843313 -0.21153521\n",
      "   1.28590477  0.32721097 -0.66762227]\n",
      " [-0.90456649  1.0349739  -0.42452843 -1.11310247  0.18132599 -0.04954875\n",
      "  -0.74635404 -0.49284084  0.88058976]\n",
      " [ 0.71083444 -0.62452284 -0.07747907  0.68760892 -0.42746082  0.23871361\n",
      "   0.77560394  0.05538934 -0.80438843]\n",
      " [ 0.76293004 -0.95195946  0.26767799  0.77293286 -0.31035397  0.05256755\n",
      "   0.9504483   0.95658821 -0.4994783 ]\n",
      " [-1.04896898  1.27280608 -0.37559829 -1.23910301  0.05842159  0.29988482\n",
      "  -1.01361264 -0.84009938  0.83994219]\n",
      " [ 1.19972275 -1.16231311  0.35594306  0.8733755  -0.14667003 -0.33972618\n",
      "   0.8527583   0.93616574 -0.63955748]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.6709007   2.04719316 -2.529506    0.31985695  0.60644072 -3.20244133\n",
      "   1.41881197]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:85 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.73905836]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 85 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.95333733 -1.28748976  0.20127533  1.10353089 -0.27287478 -0.20597686\n",
      "   1.29146312  0.32721097 -0.66762227]\n",
      " [-0.91019824  1.0349739  -0.42452843 -1.11310247  0.17569425 -0.0551805\n",
      "  -0.75198579 -0.49284084  0.88058976]\n",
      " [ 0.71633693 -0.62452284 -0.07747907  0.68760892 -0.42195832  0.2442161\n",
      "   0.78110644  0.05538934 -0.80438843]\n",
      " [ 0.76854709 -0.95195946  0.26767799  0.77293286 -0.30473691  0.05818461\n",
      "   0.95606536  0.95658821 -0.4994783 ]\n",
      " [-1.05455308  1.27280608 -0.37559829 -1.23910301  0.05283748  0.29430072\n",
      "  -1.01919675 -0.84009938  0.83994219]\n",
      " [ 1.20535466 -1.16231311  0.35594306  0.8733755  -0.14103811 -0.33409427\n",
      "   0.85839022  0.93616574 -0.63955748]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.64573926  2.06860954 -2.52498731  0.3396199   0.62684307 -3.19856879\n",
      "   1.43962613]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:85 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.59338128]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 85 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93813355 -1.28748976  0.20127533  1.08832711 -0.27287478 -0.22118064\n",
      "   1.29146312  0.32721097 -0.68282605]\n",
      " [-0.89492622  1.0349739  -0.42452843 -1.09783045  0.17569425 -0.03990848\n",
      "  -0.75198579 -0.49284084  0.89586178]\n",
      " [ 0.70363466 -0.62452284 -0.07747907  0.67490665 -0.42195832  0.23151383\n",
      "   0.78110644  0.05538934 -0.8170907 ]\n",
      " [ 0.75379171 -0.95195946  0.26767799  0.75817747 -0.30473691  0.04342922\n",
      "   0.95606536  0.95658821 -0.51423368]\n",
      " [-1.03946744  1.27280608 -0.37559829 -1.22401736  0.05283748  0.30938637\n",
      "  -1.01919675 -0.84009938  0.85502783]\n",
      " [ 1.19056998 -1.16231311  0.35594306  0.85859082 -0.14103811 -0.34887895\n",
      "   0.85839022  0.93616574 -0.65434216]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.71732476  2.01380832 -2.54158445  0.28956307  0.57313282 -3.21566154\n",
      "   1.38585036]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:85 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.72551315]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 85 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94411312 -1.28151019  0.20127533  1.09430668 -0.27287478 -0.21520107\n",
      "   1.29744269  0.32721097 -0.68282605]\n",
      " [-0.90095918  1.02894094 -0.42452843 -1.10386342  0.17569425 -0.04594144\n",
      "  -0.75801875 -0.49284084  0.89586178]\n",
      " [ 0.70965361 -0.6185039  -0.07747907  0.68092559 -0.42195832  0.23753277\n",
      "   0.78712538  0.05538934 -0.8170907 ]\n",
      " [ 0.75990982 -0.94584134  0.26767799  0.76429559 -0.30473691  0.04954734\n",
      "   0.96218347  0.95658821 -0.51423368]\n",
      " [-1.04553547  1.26673805 -0.37559829 -1.23008539  0.05283748  0.30331834\n",
      "  -1.02526478 -0.84009938  0.85502783]\n",
      " [ 1.19663938 -1.15624371  0.35594306  0.86466022 -0.14103811 -0.34280955\n",
      "   0.86445962  0.93616574 -0.65434216]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.68999358  2.03729323 -2.53753703  0.31290497  0.59571609 -3.21144157\n",
      "   1.40775918]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:85 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.78842355]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 85 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94776395 -1.27785936  0.20127533  1.09795751 -0.27287478 -0.21520107\n",
      "   1.30109352  0.32721097 -0.68282605]\n",
      " [-0.9048623   1.02503782 -0.42452843 -1.10776653  0.17569425 -0.04594144\n",
      "  -0.76192187 -0.49284084  0.89586178]\n",
      " [ 0.71360386 -0.61455365 -0.07747907  0.68487585 -0.42195832  0.23753277\n",
      "   0.79107563  0.05538934 -0.8170907 ]\n",
      " [ 0.76386041 -0.94189075  0.26767799  0.76824618 -0.30473691  0.04954734\n",
      "   0.96613406  0.95658821 -0.51423368]\n",
      " [-1.04920713  1.2630664  -0.37559829 -1.23375705  0.05283748  0.30331834\n",
      "  -1.02893643 -0.84009938  0.85502783]\n",
      " [ 1.20052399 -1.1523591   0.35594306  0.86854484 -0.14103811 -0.34280955\n",
      "   0.86834423  0.93616574 -0.65434216]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.67234685  2.05293506 -2.53488844  0.32748538  0.61024896 -3.20940048\n",
      "   1.4228365 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:85 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.84035805]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 85 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94985363 -1.27785936  0.20336501  1.09795751 -0.27287478 -0.2131114\n",
      "   1.3031832   0.32721097 -0.68282605]\n",
      " [-0.90702263  1.02503782 -0.42668876 -1.10776653  0.17569425 -0.04810178\n",
      "  -0.7640822  -0.49284084  0.89586178]\n",
      " [ 0.71598919 -0.61455365 -0.07509374  0.68487585 -0.42195832  0.2399181\n",
      "   0.79346097  0.05538934 -0.8170907 ]\n",
      " [ 0.76608033 -0.94189075  0.2698979   0.76824618 -0.30473691  0.05176726\n",
      "   0.96835398  0.95658821 -0.51423368]\n",
      " [-1.05135827  1.2630664  -0.37774943 -1.23375705  0.05283748  0.3011672\n",
      "  -1.03108757 -0.84009938  0.85502783]\n",
      " [ 1.2027216  -1.1523591   0.35814066  0.86854484 -0.14103811 -0.34061194\n",
      "   0.87054183  0.93616574 -0.65434216]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.66163835  2.0626085  -2.53375839  0.33648986  0.61973311 -3.20828367\n",
      "   1.43235787]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:85 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.12288658]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 86 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94970965 -1.27800333  0.20336501  1.09781353 -0.27287478 -0.2131114\n",
      "   1.3031832   0.32721097 -0.68297003]\n",
      " [-0.90686752  1.02519293 -0.42668876 -1.10761143  0.17569425 -0.04810178\n",
      "  -0.7640822  -0.49284084  0.89601689]\n",
      " [ 0.71604014 -0.6145027  -0.07509374  0.68492679 -0.42195832  0.2399181\n",
      "   0.79346097  0.05538934 -0.81703975]\n",
      " [ 0.76595105 -0.94202003  0.2698979   0.7681169  -0.30473691  0.05176726\n",
      "   0.96835398  0.95658821 -0.51436296]\n",
      " [-1.05108365  1.26334101 -0.37774943 -1.23348244  0.05283748  0.3011672\n",
      "  -1.03108757 -0.84009938  0.85530245]\n",
      " [ 1.20229114 -1.15278955  0.35814066  0.86811438 -0.14103811 -0.34061194\n",
      "   0.87054183  0.93616574 -0.65477262]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.66826105  2.05915299 -2.53691441  0.33322947  0.61629235 -3.21131913\n",
      "   1.42861103]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:86 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.86098]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 86 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.95119623 -1.27800333  0.20485159  1.09781353 -0.27287478 -0.2131114\n",
      "   1.30466978  0.32721097 -0.68297003]\n",
      " [-0.90856691  1.02519293 -0.42838815 -1.10761143  0.17569425 -0.04810178\n",
      "  -0.76578159 -0.49284084  0.89601689]\n",
      " [ 0.71789458 -0.6145027  -0.07323929  0.68492679 -0.42195832  0.2399181\n",
      "   0.79531541  0.05538934 -0.81703975]\n",
      " [ 0.7676962  -0.94202003  0.27164305  0.7681169  -0.30473691  0.05176726\n",
      "   0.97009913  0.95658821 -0.51436296]\n",
      " [-1.05256787  1.26334101 -0.37923364 -1.23348244  0.05283748  0.3011672\n",
      "  -1.03257179 -0.84009938  0.85530245]\n",
      " [ 1.20379401 -1.15278955  0.35964353  0.86811438 -0.14103811 -0.34061194\n",
      "   0.8720447   0.93616574 -0.65477262]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.65994116  2.06681576 -2.53600482  0.3399485   0.62362415 -3.21066422\n",
      "   1.43625833]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:86 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.58911307]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 86 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93605572 -1.29314384  0.18971108  1.09781353 -0.27287478 -0.2131114\n",
      "   1.28952928  0.32721097 -0.68297003]\n",
      " [-0.89401016  1.03974968 -0.4138314  -1.10761143  0.17569425 -0.04810178\n",
      "  -0.75122483 -0.49284084  0.89601689]\n",
      " [ 0.7054277  -0.62696958 -0.08570618  0.68492679 -0.42195832  0.2399181\n",
      "   0.78284853  0.05538934 -0.81703975]\n",
      " [ 0.75320449 -0.95651174  0.25715134  0.7681169  -0.30473691  0.05176726\n",
      "   0.95560742  0.95658821 -0.51436296]\n",
      " [-1.03734257  1.27856631 -0.36400834 -1.23348244  0.05283748  0.3011672\n",
      "  -1.01734649 -0.84009938  0.85530245]\n",
      " [ 1.18824985 -1.16833371  0.34409937  0.86811438 -0.14103811 -0.34061194\n",
      "   0.85650054  0.93616574 -0.65477262]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.73124117  2.0122402  -2.55411297  0.29036677  0.57056941 -3.22715531\n",
      "   1.38043699]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:86 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.76727665]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 86 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94069339 -1.29314384  0.19434875  1.09781353 -0.27287478 -0.2131114\n",
      "   1.28952928  0.33184864 -0.68297003]\n",
      " [-0.89856236  1.03974968 -0.41838361 -1.10761143  0.17569425 -0.04810178\n",
      "  -0.75122483 -0.49739305  0.89601689]\n",
      " [ 0.70856347 -0.62696958 -0.08257041  0.68492679 -0.42195832  0.2399181\n",
      "   0.78284853  0.05852511 -0.81703975]\n",
      " [ 0.75760436 -0.95651174  0.26155121  0.7681169  -0.30473691  0.05176726\n",
      "   0.95560742  0.96098808 -0.51436296]\n",
      " [-1.0413877   1.27856631 -0.36805347 -1.23348244  0.05283748  0.3011672\n",
      "  -1.01734649 -0.84414451  0.85530245]\n",
      " [ 1.19194258 -1.16833371  0.3477921   0.86811438 -0.14103811 -0.34061194\n",
      "   0.85650054  0.93985847 -0.65477262]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.71046326  2.02907979 -2.55116732  0.30413518  0.5887975  -3.22515878\n",
      "   1.39959225]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:86 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.59695317]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 86 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.95140368 -1.29314384  0.19434875  1.10852382 -0.27287478 -0.2131114\n",
      "   1.28952928  0.33184864 -0.67225974]\n",
      " [-0.90859671  1.03974968 -0.41838361 -1.11764578  0.17569425 -0.04810178\n",
      "  -0.75122483 -0.49739305  0.88598254]\n",
      " [ 0.71500121 -0.62696958 -0.08257041  0.69136454 -0.42195832  0.2399181\n",
      "   0.78284853  0.05852511 -0.81060201]\n",
      " [ 0.76719495 -0.95651174  0.26155121  0.7777075  -0.30473691  0.05176726\n",
      "   0.95560742  0.96098808 -0.50477236]\n",
      " [-1.05218091  1.27856631 -0.36805347 -1.24427565  0.05283748  0.3011672\n",
      "  -1.01734649 -0.84414451  0.84450923]\n",
      " [ 1.20272043 -1.16833371  0.3477921   0.87889223 -0.14103811 -0.34061194\n",
      "   0.85650054  0.93985847 -0.64399477]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.66197671  2.06762821 -2.53915034  0.33517871  0.62435201 -3.21571702\n",
      "   1.43852797]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:86 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.06971658]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 86 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.95151934 -1.29314384  0.19434875  1.10852382 -0.27275912 -0.21299573\n",
      "   1.28952928  0.33184864 -0.67214407]\n",
      " [-0.90865588  1.03974968 -0.41838361 -1.11764578  0.17563508 -0.04816095\n",
      "  -0.75122483 -0.49739305  0.88592337]\n",
      " [ 0.71515515 -0.62696958 -0.08257041  0.69136454 -0.42180439  0.24007204\n",
      "   0.78284853  0.05852511 -0.81044807]\n",
      " [ 0.76718961 -0.95651174  0.26155121  0.7777075  -0.30474226  0.05176191\n",
      "   0.95560742  0.96098808 -0.50477771]\n",
      " [-1.05226317  1.27856631 -0.36805347 -1.24427565  0.05275522  0.30108493\n",
      "  -1.01734649 -0.84414451  0.84442697]\n",
      " [ 1.20267693 -1.16833371  0.3477921   0.87889223 -0.14108161 -0.34065544\n",
      "   0.85650054  0.93985847 -0.64403827]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.66423749  2.06661431 -2.54034     0.33420424  0.62321628 -3.21692997\n",
      "   1.43735404]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:86 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.00562336]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 86 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.95152286 -1.29314032  0.19434875  1.10852382 -0.2727556  -0.21299221\n",
      "   1.28952928  0.33184864 -0.67214056]\n",
      " [-0.90865918  1.03974639 -0.41838361 -1.11764578  0.17563178 -0.04816424\n",
      "  -0.75122483 -0.49739305  0.88592007]\n",
      " [ 0.71515806 -0.62696666 -0.08257041  0.69136454 -0.42180147  0.24007496\n",
      "   0.78284853  0.05852511 -0.81044516]\n",
      " [ 0.76719261 -0.95650874  0.26155121  0.7777075  -0.30473926  0.05176491\n",
      "   0.95560742  0.96098808 -0.50477471]\n",
      " [-1.05226667  1.27856281 -0.36805347 -1.24427565  0.05275172  0.30108143\n",
      "  -1.01734649 -0.84414451  0.84442347]\n",
      " [ 1.20268016 -1.16833048  0.3477921   0.87889223 -0.14107838 -0.34065221\n",
      "   0.85650054  0.93985847 -0.64403504]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.66425321  2.06661145 -2.54035193  0.33419971  0.62321188 -3.21694264\n",
      "   1.43735009]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:86 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.24120545]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 86 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94910397 -1.29555922  0.19434875  1.10852382 -0.2751745  -0.21541111\n",
      "   1.28711038  0.33184864 -0.67214056]\n",
      " [-0.90609898  1.04230658 -0.41838361 -1.11764578  0.17819198 -0.04560404\n",
      "  -0.74866464 -0.49739305  0.88592007]\n",
      " [ 0.71177254 -0.63035219 -0.08257041  0.69136454 -0.425187    0.23668943\n",
      "   0.779463    0.05852511 -0.81044516]\n",
      " [ 0.76453865 -0.9591627   0.26155121  0.7777075  -0.30739322  0.04911095\n",
      "   0.95295346  0.96098808 -0.50477471]\n",
      " [-1.04996569  1.28086379 -0.36805347 -1.24427565  0.0550527   0.30338242\n",
      "  -1.0150455  -0.84414451  0.84442347]\n",
      " [ 1.2005144  -1.17049624  0.3477921   0.87889223 -0.14324415 -0.34281798\n",
      "   0.85433478  0.93985847 -0.64403504]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.68632657  2.05306883 -2.54872358  0.31950296  0.60940315 -3.22560432\n",
      "   1.42408672]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:86 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.9033826]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 86 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94956806 -1.29555922  0.19434875  1.10898792 -0.2751745  -0.21541111\n",
      "   1.28757447  0.33184864 -0.67214056]\n",
      " [-0.90674627  1.04230658 -0.41838361 -1.11829307  0.17819198 -0.04560404\n",
      "  -0.74931193 -0.49739305  0.88592007]\n",
      " [ 0.71261053 -0.63035219 -0.08257041  0.69220253 -0.425187    0.23668943\n",
      "   0.780301    0.05852511 -0.81044516]\n",
      " [ 0.76527922 -0.9591627   0.26155121  0.77844807 -0.30739322  0.04911095\n",
      "   0.95369403  0.96098808 -0.50477471]\n",
      " [-1.05044027  1.28086379 -0.36805347 -1.24475023  0.0550527   0.30338242\n",
      "  -1.01552008 -0.84414451  0.84442347]\n",
      " [ 1.20110766 -1.17049624  0.3477921   0.87948549 -0.14324415 -0.34281798\n",
      "   0.85492804  0.93985847 -0.64403504]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.68211007  2.05714169 -2.5484755   0.32329223  0.61329837 -3.22545567\n",
      "   1.42809025]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:86 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.74129048]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 86 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.95504621 -1.29555922  0.19434875  1.10898792 -0.26969635 -0.20993296\n",
      "   1.29305262  0.33184864 -0.67214056]\n",
      " [-0.91229918  1.04230658 -0.41838361 -1.11829307  0.17263907 -0.05115695\n",
      "  -0.75486484 -0.49739305  0.88592007]\n",
      " [ 0.7180428  -0.63035219 -0.08257041  0.69220253 -0.41975473  0.2421217\n",
      "   0.78573327  0.05852511 -0.81044516]\n",
      " [ 0.77081905 -0.9591627   0.26155121  0.77844807 -0.30185339  0.05465079\n",
      "   0.95923386  0.96098808 -0.50477471]\n",
      " [-1.05594392  1.28086379 -0.36805347 -1.24475023  0.04954905  0.29787876\n",
      "  -1.02102373 -0.84414451  0.84442347]\n",
      " [ 1.20665998 -1.17049624  0.3477921   0.87948549 -0.13769183 -0.33726566\n",
      "   0.86048036  0.93985847 -0.64403504]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.65730255  2.07826573 -2.54403617  0.34280524  0.6334331  -3.22164809\n",
      "   1.44862541]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:86 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.59277598]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 86 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.9398571  -1.29555922  0.19434875  1.09379881 -0.26969635 -0.22512207\n",
      "   1.29305262  0.33184864 -0.68732966]\n",
      " [-0.89704505  1.04230658 -0.41838361 -1.10303893  0.17263907 -0.03590282\n",
      "  -0.75486484 -0.49739305  0.9011742 ]\n",
      " [ 0.70536608 -0.63035219 -0.08257041  0.67952581 -0.41975473  0.22944498\n",
      "   0.78573327  0.05852511 -0.82312188]\n",
      " [ 0.75607813 -0.9591627   0.26155121  0.76370714 -0.30185339  0.03990986\n",
      "   0.95923386  0.96098808 -0.51951564]\n",
      " [-1.04087187  1.28086379 -0.36805347 -1.22967818  0.04954905  0.31295081\n",
      "  -1.02102373 -0.84414451  0.85949552]\n",
      " [ 1.19188488 -1.17049624  0.3477921   0.86471039 -0.13769183 -0.35204076\n",
      "   0.86048036  0.93985847 -0.65881014]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.72884843  2.02351167 -2.56065041  0.29280399  0.57976658 -3.23874466\n",
      "   1.39488253]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:86 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.7261767]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 86 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94581641 -1.28959991  0.19434875  1.09975812 -0.26969635 -0.21916276\n",
      "   1.29901193  0.33184864 -0.68732966]\n",
      " [-0.90305527  1.03629636 -0.41838361 -1.10904915  0.17263907 -0.04191304\n",
      "  -0.76087506 -0.49739305  0.9011742 ]\n",
      " [ 0.7113588  -0.62435947 -0.08257041  0.68551853 -0.41975473  0.2354377\n",
      "   0.79172599  0.05852511 -0.82312188]\n",
      " [ 0.76217225 -0.95306858  0.26155121  0.76980126 -0.30185339  0.04600398\n",
      "   0.96532798  0.96098808 -0.51951564]\n",
      " [-1.04691781  1.27481786 -0.36805347 -1.23572412  0.04954905  0.30690487\n",
      "  -1.02706967 -0.84414451  0.85949552]\n",
      " [ 1.19792924 -1.16445188  0.3477921   0.87075475 -0.13769183 -0.3459964\n",
      "   0.86652473  0.93985847 -0.65881014]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.70162435  2.04689391 -2.55661497  0.31606486  0.60226237 -3.23453152\n",
      "   1.41669761]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:86 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.78992717]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 86 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94942174 -1.28599458  0.19434875  1.10336345 -0.26969635 -0.21916276\n",
      "   1.30261726  0.33184864 -0.68732966]\n",
      " [-0.90690912  1.03244252 -0.41838361 -1.112903    0.17263907 -0.04191304\n",
      "  -0.7647289  -0.49739305  0.9011742 ]\n",
      " [ 0.71526028 -0.62045799 -0.08257041  0.68942001 -0.41975473  0.2354377\n",
      "   0.79562747  0.05852511 -0.82312188]\n",
      " [ 0.76607432 -0.9491665   0.26155121  0.77370334 -0.30185339  0.04600398\n",
      "   0.96923006  0.96098808 -0.51951564]\n",
      " [-1.05054362  1.27119205 -0.36805347 -1.23934993  0.04954905  0.30690487\n",
      "  -1.03069548 -0.84414451  0.85949552]\n",
      " [ 1.20176545 -1.16061567  0.3477921   0.87459096 -0.13769183 -0.3459964\n",
      "   0.87036094  0.93985847 -0.65881014]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.68419437  2.06234472 -2.55400509  0.33047877  0.61662605 -3.23251683\n",
      "   1.43159243]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:86 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.84057306]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 86 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.95151208 -1.28599458  0.1964391   1.10336345 -0.26969635 -0.21707242\n",
      "   1.3047076   0.33184864 -0.68732966]\n",
      " [-0.90906785  1.03244252 -0.42054234 -1.112903    0.17263907 -0.04407178\n",
      "  -0.76688764 -0.49739305  0.9011742 ]\n",
      " [ 0.71763999 -0.62045799 -0.0801907   0.68942001 -0.41975473  0.2378174\n",
      "   0.79800717  0.05852511 -0.82312188]\n",
      " [ 0.76829156 -0.9491665   0.26376844  0.77370334 -0.30185339  0.04822121\n",
      "   0.97144729  0.96098808 -0.51951564]\n",
      " [-1.05269512  1.27119205 -0.37020497 -1.23934993  0.04954905  0.30475337\n",
      "  -1.03284698 -0.84414451  0.85949552]\n",
      " [ 1.20396295 -1.16061567  0.3499896   0.87459096 -0.13769183 -0.3437989\n",
      "   0.87255844  0.93985847 -0.65881014]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.67351197  2.07198738 -2.55287242  0.33945991  0.62608237 -3.2313947\n",
      "   1.44108206]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:86 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.11957209]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 87 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.95138398 -1.28612269  0.1964391   1.10323534 -0.26969635 -0.21707242\n",
      "   1.3047076   0.33184864 -0.68745777]\n",
      " [-0.9089291   1.03258127 -0.42054234 -1.11276425  0.17263907 -0.04407178\n",
      "  -0.76688764 -0.49739305  0.90131296]\n",
      " [ 0.71769743 -0.62040055 -0.0801907   0.68947745 -0.41975473  0.2378174\n",
      "   0.79800717  0.05852511 -0.82306443]\n",
      " [ 0.76817635 -0.9492817   0.26376844  0.77358813 -0.30185339  0.04822121\n",
      "   0.97144729  0.96098808 -0.51963084]\n",
      " [-1.05244287  1.27144429 -0.37020497 -1.23909768  0.04954905  0.30475337\n",
      "  -1.03284698 -0.84414451  0.85974776]\n",
      " [ 1.20356199 -1.16101663  0.3499896   0.87419    -0.13769183 -0.3437989\n",
      "   0.87255844  0.93985847 -0.65921111]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.67980592  2.06871216 -2.55588047  0.33637039  0.62282009 -3.23428833\n",
      "   1.43752961]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:87 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.86186675]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 87 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.95285555 -1.28612269  0.19791066  1.10323534 -0.26969635 -0.21707242\n",
      "   1.30617917  0.33184864 -0.68745777]\n",
      " [-0.91060926  1.03258127 -0.4222225  -1.11276425  0.17263907 -0.04407178\n",
      "  -0.7685678  -0.49739305  0.90131296]\n",
      " [ 0.71953034 -0.62040055 -0.07835779  0.68947745 -0.41975473  0.2378174\n",
      "   0.79984009  0.05852511 -0.82306443]\n",
      " [ 0.76990145 -0.9492817   0.26549354  0.77358813 -0.30185339  0.04822121\n",
      "   0.97317239  0.96098808 -0.51963084]\n",
      " [-1.05391255  1.27144429 -0.37167465 -1.23909768  0.04954905  0.30475337\n",
      "  -1.03431666 -0.84414451  0.85974776]\n",
      " [ 1.20505035 -1.16101663  0.35147797  0.87419    -0.13769183 -0.3437989\n",
      "   0.87404681  0.93985847 -0.65921111]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.67158337  2.07628304 -2.5549805   0.34301223  0.63006541 -3.23363844\n",
      "   1.44508447]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:87 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.58397274]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 87 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.93784939 -1.30112885  0.1829045   1.10323534 -0.26969635 -0.21707242\n",
      "   1.29117301  0.33184864 -0.68745777]\n",
      " [-0.89618216  1.04700837 -0.4077954  -1.11276425  0.17263907 -0.04407178\n",
      "  -0.7541407  -0.49739305  0.90131296]\n",
      " [ 0.70717643 -0.63275447 -0.09071171  0.68947745 -0.41975473  0.2378174\n",
      "   0.78748617  0.05852511 -0.82306443]\n",
      " [ 0.7555365  -0.96364666  0.25112858  0.77358813 -0.30185339  0.04822121\n",
      "   0.95880743  0.96098808 -0.51963084]\n",
      " [-1.038822    1.28653484 -0.3565841  -1.23909768  0.04954905  0.30475337\n",
      "  -1.01922611 -0.84414451  0.85974776]\n",
      " [ 1.18962983 -1.17643716  0.33605744  0.87419    -0.13769183 -0.3437989\n",
      "   0.85862628  0.93985847 -0.65921111]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.74252105  2.02213676 -2.57311415  0.29375551  0.57739036 -3.25020483\n",
      "   1.38970575]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:87 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.76868565]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 87 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94243937 -1.30112885  0.18749449  1.10323534 -0.26969635 -0.21707242\n",
      "   1.29117301  0.33643862 -0.68745777]\n",
      " [-0.90068717  1.04700837 -0.41230041 -1.11276425  0.17263907 -0.04407178\n",
      "  -0.7541407  -0.50189806  0.90131296]\n",
      " [ 0.71027959 -0.63275447 -0.08760854  0.68947745 -0.41975473  0.2378174\n",
      "   0.78748617  0.06162828 -0.82306443]\n",
      " [ 0.75989047 -0.96364666  0.25548256  0.77358813 -0.30185339  0.04822121\n",
      "   0.95880743  0.96534206 -0.51963084]\n",
      " [-1.04282838  1.28653484 -0.36059048 -1.23909768  0.04954905  0.30475337\n",
      "  -1.01922611 -0.84815089  0.85974776]\n",
      " [ 1.19328944 -1.17643716  0.33971705  0.87419    -0.13769183 -0.3437989\n",
      "   0.85862628  0.94351808 -0.65921111]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.72195627  2.03880243 -2.57020044  0.30738213  0.59543305 -3.24822538\n",
      "   1.40866012]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:87 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.59824995]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 87 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.95310815 -1.30112885  0.18749449  1.11390413 -0.26969635 -0.21707242\n",
      "   1.29117301  0.33643862 -0.67678898]\n",
      " [-0.91068654  1.04700837 -0.41230041 -1.12276362  0.17263907 -0.04407178\n",
      "  -0.7541407  -0.50189806  0.89131359]\n",
      " [ 0.71669213 -0.63275447 -0.08760854  0.69588999 -0.41975473  0.2378174\n",
      "   0.78748617  0.06162828 -0.8166519 ]\n",
      " [ 0.76945254 -0.96364666  0.25548256  0.7831502  -0.30185339  0.04822121\n",
      "   0.95880743  0.96534206 -0.51006877]\n",
      " [-1.05357815  1.28653484 -0.36059048 -1.24984745  0.04954905  0.30475337\n",
      "  -1.01922611 -0.84815089  0.84899799]\n",
      " [ 1.20402465 -1.17643716  0.33971705  0.88492521 -0.13769183 -0.3437989\n",
      "   0.85862628  0.94351808 -0.6484759 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.67367657  2.07720747 -2.55825255  0.33829591  0.63085936 -3.23884365\n",
      "   1.4474525 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:87 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.06780119]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 87 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.95321964 -1.30112885  0.18749449  1.11390413 -0.26958486 -0.21696093\n",
      "   1.29117301  0.33643862 -0.67667749]\n",
      " [-0.91074485  1.04700837 -0.41230041 -1.12276362  0.17258075 -0.04413009\n",
      "  -0.7541407  -0.50189806  0.89125527]\n",
      " [ 0.71684017 -0.63275447 -0.08760854  0.69588999 -0.41960669  0.23796544\n",
      "   0.78748617  0.06162828 -0.81650386]\n",
      " [ 0.76944946 -0.96364666  0.25548256  0.7831502  -0.30185647  0.04821813\n",
      "   0.95880743  0.96534206 -0.51007186]\n",
      " [-1.05365791  1.28653484 -0.36059048 -1.24984745  0.0494693   0.30467362\n",
      "  -1.01922611 -0.84815089  0.84891824]\n",
      " [ 1.20398503 -1.17643716  0.33971705  0.88492521 -0.13773144 -0.34383851\n",
      "   0.85862628  0.94351808 -0.64851551]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.67581923  2.07624845 -2.55938231  0.33737459  0.62978495 -3.23999503\n",
      "   1.44634152]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:87 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.0053102]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 87 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.95322278 -1.30112571  0.18749449  1.11390413 -0.26958172 -0.21695779\n",
      "   1.29117301  0.33643862 -0.67667435]\n",
      " [-0.91074781  1.04700542 -0.41230041 -1.12276362  0.1725778  -0.04413305\n",
      "  -0.7541407  -0.50189806  0.89125232]\n",
      " [ 0.71684279 -0.63275185 -0.08760854  0.69588999 -0.41960407  0.23796806\n",
      "   0.78748617  0.06162828 -0.81650124]\n",
      " [ 0.76945215 -0.96364396  0.25548256  0.7831502  -0.30185377  0.04822082\n",
      "   0.95880743  0.96534206 -0.51006916]\n",
      " [-1.05366103  1.28653172 -0.36059048 -1.24984745  0.04946617  0.30467049\n",
      "  -1.01922611 -0.84815089  0.84891511]\n",
      " [ 1.20398793 -1.17643427  0.33971705  0.88492521 -0.13772855 -0.34383562\n",
      "   0.85862628  0.94351808 -0.64851262]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.67583326  2.07624592 -2.55939298  0.33737058  0.62978106 -3.24000636\n",
      "   1.44633802]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:87 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.237443]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 87 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.95089197 -1.30345652  0.18749449  1.11390413 -0.27191253 -0.2192886\n",
      "   1.2888422   0.33643862 -0.67667435]\n",
      " [-0.90826898  1.04948425 -0.41230041 -1.12276362  0.17505662 -0.04165422\n",
      "  -0.75166187 -0.50189806  0.89125232]\n",
      " [ 0.71354347 -0.63605116 -0.08760854  0.69588999 -0.42290338  0.23466875\n",
      "   0.78418685  0.06162828 -0.81650124]\n",
      " [ 0.76687783 -0.96621828  0.25548256  0.7831502  -0.30442809  0.04564651\n",
      "   0.95623312  0.96534206 -0.51006916]\n",
      " [-1.05144346  1.28874929 -0.36059048 -1.24984745  0.05168375  0.30688806\n",
      "  -1.01700854 -0.84815089  0.84891511]\n",
      " [ 1.20190013 -1.17852207  0.33971705  0.88492521 -0.13981635 -0.34592342\n",
      "   0.85653848  0.94351808 -0.64851262]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.69732943  2.06308514 -2.56756208  0.32305512  0.61634516 -3.24846718\n",
      "   1.43344471]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:87 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.90509331]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 87 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.9513381  -1.30345652  0.18749449  1.11435026 -0.27191253 -0.2192886\n",
      "   1.28928833  0.33643862 -0.67667435]\n",
      " [-0.90889138  1.04948425 -0.41230041 -1.12338602  0.17505662 -0.04165422\n",
      "  -0.75228427 -0.50189806  0.89125232]\n",
      " [ 0.71435055 -0.63605116 -0.08760854  0.69669706 -0.42290338  0.23466875\n",
      "   0.78499392  0.06162828 -0.81650124]\n",
      " [ 0.76759022 -0.96621828  0.25548256  0.78386259 -0.30442809  0.04564651\n",
      "   0.95694551  0.96534206 -0.51006916]\n",
      " [-1.05189966  1.28874929 -0.36059048 -1.25030365  0.05168375  0.30688806\n",
      "  -1.01746474 -0.84815089  0.84891511]\n",
      " [ 1.20247057 -1.17852207  0.33971705  0.88549566 -0.13981635 -0.34592342\n",
      "   0.85710893  0.94351808 -0.64851262]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.69325321  2.06702368 -2.56732457  0.32672239  0.62011395 -3.24832472\n",
      "   1.4373169 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:87 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.74351653]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 87 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.95673652 -1.30345652  0.18749449  1.11435026 -0.26651412 -0.21389018\n",
      "   1.29468675  0.33643862 -0.67667435]\n",
      " [-0.91436583  1.04948425 -0.41230041 -1.12338602  0.16958217 -0.04712867\n",
      "  -0.75775872 -0.50189806  0.89125232]\n",
      " [ 0.7197126  -0.63605116 -0.08760854  0.69669706 -0.41754133  0.2400308\n",
      "   0.79035598  0.06162828 -0.81650124]\n",
      " [ 0.77305312 -0.96621828  0.25548256  0.78386259 -0.29896519  0.05110941\n",
      "   0.96240841  0.96534206 -0.51006916]\n",
      " [-1.05732333  1.28874929 -0.36059048 -1.25030365  0.04626007  0.30146439\n",
      "  -1.02288841 -0.84815089  0.84891511]\n",
      " [ 1.20794372 -1.17852207  0.33971705  0.88549566 -0.1343432  -0.34045027\n",
      "   0.86258207  0.94351808 -0.64851262]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.66879755  2.08785697 -2.56296364  0.34598608  0.639982   -3.24458152\n",
      "   1.45757427]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:87 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.59210436]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 87 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94156372 -1.30345652  0.18749449  1.09917746 -0.26651412 -0.22906298\n",
      "   1.29468675  0.33643862 -0.69184715]\n",
      " [-0.89913127  1.04948425 -0.41230041 -1.10815145  0.16958217 -0.03189411\n",
      "  -0.75775872 -0.50189806  0.90648688]\n",
      " [ 0.7070636  -0.63605116 -0.08760854  0.68404806 -0.41754133  0.2273818\n",
      "   0.79035598  0.06162828 -0.82915024]\n",
      " [ 0.7583285  -0.96621828  0.25548256  0.76913796 -0.29896519  0.03638478\n",
      "   0.96240841  0.96534206 -0.52479379]\n",
      " [-1.04226651  1.28874929 -0.36059048 -1.23524683  0.04626007  0.31652122\n",
      "  -1.02288841 -0.84815089  0.86397194]\n",
      " [ 1.19317995 -1.17852207  0.33971705  0.87073189 -0.1343432  -0.35521404\n",
      "   0.86258207  0.94351808 -0.66327639]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.74029912  2.03315529 -2.57959567  0.29604564  0.58636462 -3.26168239\n",
      "   1.40386958]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:87 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.72684976]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 87 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94750228 -1.29751795  0.18749449  1.10511603 -0.26651412 -0.22312442\n",
      "   1.30062531  0.33643862 -0.69184715]\n",
      " [-0.90511838  1.04349713 -0.41230041 -1.11413857  0.16958217 -0.03788122\n",
      "  -0.76374584 -0.50189806  0.90648688]\n",
      " [ 0.71302976 -0.630085   -0.08760854  0.69001422 -0.41754133  0.23334797\n",
      "   0.79632214  0.06162828 -0.82915024]\n",
      " [ 0.7643983  -0.96014848  0.25548256  0.77520776 -0.29896519  0.04245459\n",
      "   0.96847821  0.96534206 -0.52479379]\n",
      " [-1.04828993  1.28272587 -0.36059048 -1.24127025  0.04626007  0.31049779\n",
      "  -1.02891183 -0.84815089  0.86397194]\n",
      " [ 1.19919898 -1.17250304  0.33971705  0.87675092 -0.1343432  -0.34919501\n",
      "   0.8686011   0.94351808 -0.66327639]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.71318361  2.05643403 -2.57557257  0.31922412  0.60877178 -3.25747672\n",
      "   1.42558998]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:87 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.79143801]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 87 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.95106199 -1.29395825  0.18749449  1.10867573 -0.26651412 -0.22312442\n",
      "   1.30418502  0.33643862 -0.69184715]\n",
      " [-0.90892291  1.0396926  -0.41230041 -1.1179431   0.16958217 -0.03788122\n",
      "  -0.76755037 -0.50189806  0.90648688]\n",
      " [ 0.71688236 -0.6262324  -0.08760854  0.69386682 -0.41754133  0.23334797\n",
      "   0.80017474  0.06162828 -0.82915024]\n",
      " [ 0.76825178 -0.956295    0.25548256  0.77906124 -0.29896519  0.04245459\n",
      "   0.97233169  0.96534206 -0.52479379]\n",
      " [-1.05186976  1.27914604 -0.36059048 -1.24485008  0.04626007  0.31049779\n",
      "  -1.03249166 -0.84815089  0.86397194]\n",
      " [ 1.20298673 -1.16871529  0.33971705  0.88053867 -0.1343432  -0.34919501\n",
      "   0.87238885  0.94351808 -0.66327639]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.69597058  2.0716938  -2.57300136  0.33347102  0.62296591 -3.2554886\n",
      "   1.44030217]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:87 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.84077636]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 87 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.95315315 -1.29395825  0.18958564  1.10867573 -0.26651412 -0.22103326\n",
      "   1.30627618  0.33643862 -0.69184715]\n",
      " [-0.91108028  1.0396926  -0.41445778 -1.1179431   0.16958217 -0.04003859\n",
      "  -0.76970774 -0.50189806  0.90648688]\n",
      " [ 0.71925676 -0.6262324  -0.08523414  0.69386682 -0.41754133  0.23572236\n",
      "   0.80254914  0.06162828 -0.82915024]\n",
      " [ 0.77046658 -0.956295    0.25769736  0.77906124 -0.29896519  0.04466939\n",
      "   0.97454649  0.96534206 -0.52479379]\n",
      " [-1.05402178  1.27914604 -0.36274251 -1.24485008  0.04626007  0.30834577\n",
      "  -1.03464368 -0.84815089  0.86397194]\n",
      " [ 1.20518432 -1.16871529  0.34191464  0.88053867 -0.1343432  -0.34699742\n",
      "   0.87458644  0.94351808 -0.66327639]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.68531283  2.08130707 -2.57186596  0.34243007  0.6323957  -3.25436109\n",
      "   1.44976139]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:87 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.11633845]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 88 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.95303965 -1.29407174  0.18958564  1.10856224 -0.26651412 -0.22103326\n",
      "   1.30627618  0.33643862 -0.69196064]\n",
      " [-0.91095664  1.03981624 -0.41445778 -1.11781946  0.16958217 -0.04003859\n",
      "  -0.76970774 -0.50189806  0.90661052]\n",
      " [ 0.71931991 -0.62616925 -0.08523414  0.69392997 -0.41754133  0.23572236\n",
      "   0.80254914  0.06162828 -0.82908709]\n",
      " [ 0.77036439 -0.95639719  0.25769736  0.77895904 -0.29896519  0.04466939\n",
      "   0.97454649  0.96534206 -0.52489599]\n",
      " [-1.05379034  1.27937748 -0.36274251 -1.24461863  0.04626007  0.30834577\n",
      "  -1.03464368 -0.84815089  0.86420339]\n",
      " [ 1.20481103 -1.16908857  0.34191464  0.88016538 -0.1343432  -0.34699742\n",
      "   0.87458644  0.94351808 -0.66364968]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.69129285  2.07820346 -2.57473219  0.33950323  0.62930341 -3.25711871\n",
      "   1.44639407]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:88 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.86274224]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 88 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.95449639 -1.29407174  0.19104238  1.10856224 -0.26651412 -0.22103326\n",
      "   1.30773291  0.33643862 -0.69196064]\n",
      " [-0.91261788  1.03981624 -0.41611902 -1.11781946  0.16958217 -0.04003859\n",
      "  -0.77136898 -0.50189806  0.90661052]\n",
      " [ 0.72113167 -0.62616925 -0.08342238  0.69392997 -0.41754133  0.23572236\n",
      "   0.80436089  0.06162828 -0.82908709]\n",
      " [ 0.77206976 -0.95639719  0.25940274  0.77895904 -0.29896519  0.04466939\n",
      "   0.97625187  0.96534206 -0.52489599]\n",
      " [-1.05524564  1.27937748 -0.36419781 -1.24461863  0.04626007  0.30834577\n",
      "  -1.03609899 -0.84815089  0.86420339]\n",
      " [ 1.20628509 -1.16908857  0.34338869  0.88016538 -0.1343432  -0.34699742\n",
      "   0.8760605   0.94351808 -0.66364968]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.68316595  2.08568411 -2.57384171  0.34606922  0.63646379 -3.25647382\n",
      "   1.45385813]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:88 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57882556]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 88 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.939629   -1.30893913  0.17617499  1.10856224 -0.26651412 -0.22103326\n",
      "   1.29286553  0.33643862 -0.69196064]\n",
      " [-0.89832386  1.05411027 -0.401825   -1.11781946  0.16958217 -0.04003859\n",
      "  -0.75707495 -0.50189806  0.90661052]\n",
      " [ 0.70889261 -0.63840831 -0.09566144  0.69392997 -0.41754133  0.23572236\n",
      "   0.79212183  0.06162828 -0.82908709]\n",
      " [ 0.75783498 -0.97063198  0.24516796  0.77895904 -0.29896519  0.04466939\n",
      "   0.96201709  0.96534206 -0.52489599]\n",
      " [-1.04029454  1.29432858 -0.34924671 -1.24461863  0.04626007  0.30834577\n",
      "  -1.02114789 -0.84815089  0.86420339]\n",
      " [ 1.19099375 -1.18437991  0.32809736  0.88016538 -0.1343432  -0.34699742\n",
      "   0.86076916  0.94351808 -0.66364968]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.75372089  2.03197859 -2.59199226  0.29714885  0.58418048 -3.27310662\n",
      "   1.39893477]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:88 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.77009056]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 88 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94417158 -1.30893913  0.18071756  1.10856224 -0.26651412 -0.22103326\n",
      "   1.29286553  0.34098119 -0.69196064]\n",
      " [-0.90278196  1.05411027 -0.4062831  -1.11781946  0.16958217 -0.04003859\n",
      "  -0.75707495 -0.50635616  0.90661052]\n",
      " [ 0.71196333 -0.63840831 -0.09259072  0.69392997 -0.41754133  0.23572236\n",
      "   0.79212183  0.06469901 -0.82908709]\n",
      " [ 0.76214335 -0.97063198  0.24947633  0.77895904 -0.29896519  0.04466939\n",
      "   0.96201709  0.96965043 -0.52489599]\n",
      " [-1.0442623   1.29432858 -0.35321447 -1.24461863  0.04626007  0.30834577\n",
      "  -1.02114789 -0.85211865  0.86420339]\n",
      " [ 1.19462031 -1.18437991  0.33172391  0.88016538 -0.1343432  -0.34699742\n",
      "   0.86076916  0.94714463 -0.66364968]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.73336803  2.04847139 -2.58911025  0.31063446  0.60203876 -3.27114427\n",
      "   1.41768949]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:88 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.5995185]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 88 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.95479949 -1.30893913  0.18071756  1.11919015 -0.26651412 -0.22103326\n",
      "   1.29286553  0.34098119 -0.68133273]\n",
      " [-0.9127466   1.05411027 -0.4062831  -1.1277841   0.16958217 -0.04003859\n",
      "  -0.75707495 -0.50635616  0.89664588]\n",
      " [ 0.71835    -0.63840831 -0.09259072  0.70031664 -0.41754133  0.23572236\n",
      "   0.79212183  0.06469901 -0.82270042]\n",
      " [ 0.7716769  -0.97063198  0.24947633  0.78849259 -0.29896519  0.04466939\n",
      "   0.96201709  0.96965043 -0.51536244]\n",
      " [-1.05496937  1.29432858 -0.35321447 -1.25532571  0.04626007  0.30834577\n",
      "  -1.02114789 -0.85211865  0.85349631]\n",
      " [ 1.20531358 -1.18437991  0.33172391  0.89085865 -0.1343432  -0.34699742\n",
      "   0.86076916  0.94714463 -0.65295641]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.68529101  2.08673519 -2.57722909  0.34141972  0.63733849 -3.26182072\n",
      "   1.45634077]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:88 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.06593395]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 88 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.95490692 -1.30893913  0.18071756  1.11919015 -0.26640669 -0.22092583\n",
      "   1.29286553  0.34098119 -0.6812253 ]\n",
      " [-0.912804    1.05411027 -0.4062831  -1.1277841   0.16952478 -0.04009599\n",
      "  -0.75707495 -0.50635616  0.89658848]\n",
      " [ 0.71849232 -0.63840831 -0.09259072  0.70031664 -0.41739901  0.23586468\n",
      "   0.79212183  0.06469901 -0.8225581 ]\n",
      " [ 0.77167587 -0.97063198  0.24947633  0.78849259 -0.29896621  0.04466837\n",
      "   0.96201709  0.96965043 -0.51536347]\n",
      " [-1.05504664  1.29432858 -0.35321447 -1.25532571  0.0461828   0.3082685\n",
      "  -1.02114789 -0.85211865  0.85341904]\n",
      " [ 1.20527757 -1.18437991  0.33172391  0.89085865 -0.1343792  -0.34703342\n",
      "   0.86076916  0.94714463 -0.65299241]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.68732134  2.08582827 -2.57830177  0.34054883  0.6363223  -3.26291346\n",
      "   1.45528958]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:88 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.00501618]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 88 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.95490972 -1.30893632  0.18071756  1.11919015 -0.26640389 -0.22092303\n",
      "   1.29286553  0.34098119 -0.6812225 ]\n",
      " [-0.91280664  1.05410762 -0.4062831  -1.1277841   0.16952213 -0.04009863\n",
      "  -0.75707495 -0.50635616  0.89658584]\n",
      " [ 0.71849468 -0.63840596 -0.09259072  0.70031664 -0.41739666  0.23586704\n",
      "   0.79212183  0.06469901 -0.82255575]\n",
      " [ 0.77167829 -0.97062956  0.24947633  0.78849259 -0.29896379  0.04467078\n",
      "   0.96201709  0.96965043 -0.51536105]\n",
      " [-1.05504944  1.29432579 -0.35321447 -1.25532571  0.04618001  0.3082657\n",
      "  -1.02114789 -0.85211865  0.85341625]\n",
      " [ 1.20528017 -1.18437731  0.33172391  0.89085865 -0.13437661 -0.34703083\n",
      "   0.86076916  0.94714463 -0.65298981]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.68733386  2.08582603 -2.57831132  0.34054528  0.63631886 -3.26292359\n",
      "   1.45528648]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:88 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.23379274]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 88 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.95266253 -1.31118351  0.18071756  1.11919015 -0.26865107 -0.22317021\n",
      "   1.29061834  0.34098119 -0.6812225 ]\n",
      " [-0.91040541  1.05650886 -0.4062831  -1.1277841   0.17192337 -0.0376974\n",
      "  -0.75467372 -0.50635616  0.89658584]\n",
      " [ 0.71527806 -0.64162258 -0.09259072  0.70031664 -0.42061328  0.23265042\n",
      "   0.78890521  0.06469901 -0.82255575]\n",
      " [ 0.76918004 -0.97312781  0.24947633  0.78849259 -0.30146204  0.04217253\n",
      "   0.95951884  0.96965043 -0.51536105]\n",
      " [-1.05291102  1.29646421 -0.35321447 -1.25532571  0.04831843  0.31040412\n",
      "  -1.01900947 -0.85211865  0.85341625]\n",
      " [ 1.20326651 -1.18639097  0.33172391  0.89085865 -0.13639027 -0.34904449\n",
      "   0.85875551  0.94714463 -0.65298981]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.70827394  2.07303167 -2.58628433  0.3265967   0.62324139 -3.27118951\n",
      "   1.44274859]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:88 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.90676688]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 88 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.95309145 -1.31118351  0.18071756  1.11961907 -0.26865107 -0.22317021\n",
      "   1.29104726  0.34098119 -0.6812225 ]\n",
      " [-0.91100395  1.05650886 -0.4062831  -1.12838265  0.17192337 -0.0376974\n",
      "  -0.75527226 -0.50635616  0.89658584]\n",
      " [ 0.71605543 -0.64162258 -0.09259072  0.70109401 -0.42061328  0.23265042\n",
      "   0.78968259  0.06469901 -0.82255575]\n",
      " [ 0.7698654  -0.97312781  0.24947633  0.78917795 -0.30146204  0.04217253\n",
      "   0.9602042   0.96965043 -0.51536105]\n",
      " [-1.05334961  1.29646421 -0.35321447 -1.2557643   0.04831843  0.31040412\n",
      "  -1.01944806 -0.85211865  0.85341625]\n",
      " [ 1.2038151  -1.18639097  0.33172391  0.89140724 -0.13639027 -0.34904449\n",
      "   0.85930409  0.94714463 -0.65298981]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.70433294  2.07684069 -2.58605689  0.33014617  0.62688814 -3.27105296\n",
      "   1.44649411]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:88 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.74573538]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 88 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.95841065 -1.31118351  0.18071756  1.11961907 -0.26333187 -0.21785101\n",
      "   1.29636646  0.34098119 -0.6812225 ]\n",
      " [-0.91640037  1.05650886 -0.4062831  -1.12838265  0.16652695 -0.04309382\n",
      "  -0.76066868 -0.50635616  0.89658584]\n",
      " [ 0.72134733 -0.64162258 -0.09259072  0.70109401 -0.41532138  0.23794232\n",
      "   0.79497449  0.06469901 -0.82255575]\n",
      " [ 0.77525172 -0.97312781  0.24947633  0.78917795 -0.29607573  0.04755885\n",
      "   0.96559051  0.96965043 -0.51536105]\n",
      " [-1.05869381  1.29646421 -0.35321447 -1.2557643   0.04297422  0.30505992\n",
      "  -1.02479226 -0.85211865  0.85341625]\n",
      " [ 1.20920954 -1.18639097  0.33172391  0.89140724 -0.13099583 -0.34365004\n",
      "   0.86469853  0.94714463 -0.65298981]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.68022686  2.09738497 -2.58177339  0.34916132  0.64649063 -3.2673735\n",
      "   1.46647507]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:88 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.59136579]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 88 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94325582 -1.31118351  0.18071756  1.10446424 -0.26333187 -0.23300584\n",
      "   1.29636646  0.34098119 -0.69637733]\n",
      " [-0.9011871   1.05650886 -0.4062831  -1.11316937  0.16652695 -0.02788054\n",
      "  -0.76066868 -0.50635616  0.91179911]\n",
      " [ 0.70872824 -0.64162258 -0.09259072  0.68847492 -0.41532138  0.22532323\n",
      "   0.79497449  0.06469901 -0.83517484]\n",
      " [ 0.76054526 -0.97312781  0.24947633  0.77447149 -0.29607573  0.03285239\n",
      "   0.96559051  0.96965043 -0.53006751]\n",
      " [-1.04365386  1.29646421 -0.35321447 -1.24072435  0.04297422  0.32009986\n",
      "  -1.02479226 -0.85211865  0.8684562 ]\n",
      " [ 1.19445889 -1.18639097  0.33172391  0.87665659 -0.13099583 -0.35840069\n",
      "   0.86469853  0.94714463 -0.66774046]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.75167931  2.04274099 -2.59842382  0.29928697  0.59292789 -3.28447909\n",
      "   1.41281395]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:88 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.72753316]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 88 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94917314 -1.30526619  0.18071756  1.11038156 -0.26333187 -0.22708852\n",
      "   1.30228378  0.34098119 -0.69637733]\n",
      " [-0.90715071  1.05054525 -0.4062831  -1.11913299  0.16652695 -0.03384415\n",
      "  -0.7666323  -0.50635616  0.91179911]\n",
      " [ 0.71466748 -0.63568333 -0.09259072  0.69441417 -0.41532138  0.23126247\n",
      "   0.80091374  0.06469901 -0.83517484]\n",
      " [ 0.76659038 -0.96708269  0.24947633  0.78051661 -0.29607573  0.03889751\n",
      "   0.97163564  0.96965043 -0.53006751]\n",
      " [-1.04965432  1.29046375 -0.35321447 -1.2467248   0.04297422  0.31409941\n",
      "  -1.03079272 -0.85211865  0.8684562 ]\n",
      " [ 1.20045225 -1.18039761  0.33172391  0.88264995 -0.13099583 -0.35240733\n",
      "   0.8706919   0.94714463 -0.66774046]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.72467394  2.06591528 -2.59441346  0.32238162  0.61524516 -3.28028155\n",
      "   1.43443865]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:88 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.79295661]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 88 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.95268708 -1.30175226  0.18071756  1.11389549 -0.26333187 -0.22708852\n",
      "   1.30579771  0.34098119 -0.69637733]\n",
      " [-0.91090586  1.04679009 -0.4062831  -1.12288814  0.16652695 -0.03384415\n",
      "  -0.77038745 -0.50635616  0.91179911]\n",
      " [ 0.71847109 -0.63187972 -0.09259072  0.69821777 -0.41532138  0.23126247\n",
      "   0.80471734  0.06469901 -0.83517484]\n",
      " [ 0.77039517 -0.9632779   0.24947633  0.7843214  -0.29607573  0.03889751\n",
      "   0.97544043  0.96965043 -0.53006751]\n",
      " [-1.05318804  1.28693004 -0.35321447 -1.25025852  0.04297422  0.31409941\n",
      "  -1.03432643 -0.85211865  0.8684562 ]\n",
      " [ 1.20419148 -1.17665838  0.33172391  0.88638918 -0.13099583 -0.35240733\n",
      "   0.87443112  0.94714463 -0.66774046]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.70767812  2.08098395 -2.59188088  0.33646091  0.62926931 -3.27832019\n",
      "   1.44896803]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:88 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.84096916]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 88 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.95477917 -1.30175226  0.18280966  1.11389549 -0.26333187 -0.22499643\n",
      "   1.30788981  0.34098119 -0.69637733]\n",
      " [-0.91306207  1.04679009 -0.4084393  -1.12288814  0.16652695 -0.03600036\n",
      "  -0.77254365 -0.50635616  0.91179911]\n",
      " [ 0.72084046 -0.63187972 -0.09022135  0.69821777 -0.41532138  0.23363185\n",
      "   0.80708671  0.06469901 -0.83517484]\n",
      " [ 0.77260777 -0.9632779   0.25168893  0.7843214  -0.29607573  0.04111011\n",
      "   0.97765303  0.96965043 -0.53006751]\n",
      " [-1.0553407   1.28693004 -0.35536713 -1.25025852  0.04297422  0.31194674\n",
      "  -1.0364791  -0.85211865  0.8684562 ]\n",
      " [ 1.20638931 -1.17665838  0.33392174  0.88638918 -0.13099583 -0.3502095\n",
      "   0.87662895  0.94714463 -0.66774046]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.69704372  2.09056907 -2.59074266  0.34539899  0.63867376 -3.27718726\n",
      "   1.45839805]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:88 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.11318489]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 89 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.95467911 -1.30185231  0.18280966  1.11379544 -0.26333187 -0.22499643\n",
      "   1.30788981  0.34098119 -0.69647739]\n",
      " [-0.91295237  1.04689979 -0.4084393  -1.12277844  0.16652695 -0.03600036\n",
      "  -0.77254365 -0.50635616  0.91190881]\n",
      " [ 0.72090858 -0.6318116  -0.09022135  0.6982859  -0.41532138  0.23363185\n",
      "   0.80708671  0.06469901 -0.83510672]\n",
      " [ 0.77251757 -0.9633681   0.25168893  0.7842312  -0.29607573  0.04111011\n",
      "   0.97765303  0.96965043 -0.53015771]\n",
      " [-1.05512858  1.28714216 -0.35536713 -1.2500464   0.04297422  0.31194674\n",
      "  -1.0364791  -0.85211865  0.86866831]\n",
      " [ 1.20604197 -1.17700572  0.33392174  0.88604184 -0.13099583 -0.3502095\n",
      "   0.87662895  0.94714463 -0.6680878 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.70272414  2.08762872 -2.59347306  0.34262693  0.63574329 -3.27981454\n",
      "   1.45520692]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:89 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.86360781]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 89 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.95612117 -1.30185231  0.18425172  1.11379544 -0.26333187 -0.22499643\n",
      "   1.30933187  0.34098119 -0.69647739]\n",
      " [-0.91459496  1.04689979 -0.41008189 -1.12277844  0.16652695 -0.03600036\n",
      "  -0.77418624 -0.50635616  0.91190881]\n",
      " [ 0.72269952 -0.6318116  -0.08843041  0.6982859  -0.41532138  0.23363185\n",
      "   0.80887765  0.06469901 -0.83510672]\n",
      " [ 0.77420353 -0.9633681   0.25337489  0.7842312  -0.29607573  0.04111011\n",
      "   0.97933899  0.96965043 -0.53015771]\n",
      " [-1.05656964  1.28714216 -0.35680819 -1.2500464   0.04297422  0.31194674\n",
      "  -1.03792016 -0.85211865  0.86866831]\n",
      " [ 1.20750186 -1.17700572  0.33538163  0.88604184 -0.13099583 -0.3502095\n",
      "   0.87808884  0.94714463 -0.6680878 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.69469136  2.09502066 -2.59259196  0.34911829  0.6428201  -3.27917465\n",
      "   1.46258168]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:89 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57367979]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 89 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94139655 -1.31657694  0.16952709  1.11379544 -0.26333187 -0.22499643\n",
      "   1.29460724  0.34098119 -0.69647739]\n",
      " [-0.90043709  1.06105766 -0.39592402 -1.12277844  0.16652695 -0.03600036\n",
      "  -0.76002837 -0.50635616  0.91190881]\n",
      " [ 0.71057698 -0.64393414 -0.10055295  0.6982859  -0.41532138  0.23363185\n",
      "   0.79675511  0.06469901 -0.83510672]\n",
      " [ 0.76010198 -0.97746965  0.23927334  0.7842312  -0.29607573  0.04111011\n",
      "   0.96523744  0.96965043 -0.53015771]\n",
      " [-1.04176224  1.30194956 -0.34200079 -1.2500464   0.04297422  0.31194674\n",
      "  -1.02311276 -0.85211865  0.86866831]\n",
      " [ 1.19234486 -1.19216272  0.32022463  0.88604184 -0.13099583 -0.3502095\n",
      "   0.86293184  0.94714463 -0.6680878 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.76484417  2.04176638 -2.61075089  0.30054492  0.5909397  -3.29586492\n",
      "   1.40812531]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:89 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.77149204]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 89 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94589197 -1.31657694  0.17402252  1.11379544 -0.26333187 -0.22499643\n",
      "   1.29460724  0.34547662 -0.69647739]\n",
      " [-0.90484854  1.06105766 -0.40033547 -1.12277844  0.16652695 -0.03600036\n",
      "  -0.76002837 -0.51076761  0.91190881]\n",
      " [ 0.71361543 -0.64393414 -0.0975145   0.6982859  -0.41532138  0.23363185\n",
      "   0.79675511  0.06773745 -0.83510672]\n",
      " [ 0.76436501 -0.97746965  0.24353638  0.7842312  -0.29607573  0.04111011\n",
      "   0.96523744  0.97391347 -0.53015771]\n",
      " [-1.04569147  1.30194956 -0.34593002 -1.2500464   0.04297422  0.31194674\n",
      "  -1.02311276 -0.85604787  0.86866831]\n",
      " [ 1.19593839 -1.19216272  0.32381816  0.88604184 -0.13099583 -0.3502095\n",
      "   0.86293184  0.95073817 -0.6680878 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.74470209  2.05808732 -2.60790038  0.31389025  0.60861449 -3.29391971\n",
      "   1.42668157]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:89 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.6007584]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 89 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.95647969 -1.31657694  0.17402252  1.12438315 -0.26333187 -0.22499643\n",
      "   1.29460724  0.34547662 -0.68588967]\n",
      " [-0.91477876  1.06105766 -0.40033547 -1.13270866  0.16652695 -0.03600036\n",
      "  -0.76002837 -0.51076761  0.90197859]\n",
      " [ 0.71997563 -0.64393414 -0.0975145   0.7046461  -0.41532138  0.23363185\n",
      "   0.79675511  0.06773745 -0.82874652]\n",
      " [ 0.77387011 -0.97746965  0.24353638  0.7937363  -0.29607573  0.04111011\n",
      "   0.96523744  0.97391347 -0.52065261]\n",
      " [-1.05635662  1.30194956 -0.34593002 -1.26071155  0.04297422  0.31194674\n",
      "  -1.02311276 -0.85604787  0.85800317]\n",
      " [ 1.20659045 -1.19216272  0.32381816  0.8966939  -0.13099583 -0.3502095\n",
      "   0.86293184  0.95073817 -0.65743574]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.6968235   2.09621214 -2.59608363  0.34454833  0.64378938 -3.28465253\n",
      "   1.46519412]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:89 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.06411299]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 89 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.95658316 -1.31657694  0.17402252  1.12438315 -0.2632284  -0.22489296\n",
      "   1.29460724  0.34547662 -0.6857862 ]\n",
      " [-0.91483517  1.06105766 -0.40033547 -1.13270866  0.16647054 -0.03605677\n",
      "  -0.76002837 -0.51076761  0.90192217]\n",
      " [ 0.7201124  -0.64393414 -0.0975145   0.7046461  -0.41518461  0.23376861\n",
      "   0.79675511  0.06773745 -0.82860975]\n",
      " [ 0.77387095 -0.97746965  0.24353638  0.7937363  -0.29607489  0.04111095\n",
      "   0.96523744  0.97391347 -0.52065177]\n",
      " [-1.05643144  1.30194956 -0.34593002 -1.26071155  0.04289939  0.31187192\n",
      "  -1.02311276 -0.85604787  0.85792834]\n",
      " [ 1.20655781 -1.19216272  0.32381816  0.8966939  -0.13102846 -0.35024214\n",
      "   0.86293184  0.95073817 -0.65746838]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.69874697  2.0953547  -2.59710191  0.34372529  0.64282849 -3.2856894\n",
      "   1.46419972]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:89 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.00473999]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 89 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.95658566 -1.31657443  0.17402252  1.12438315 -0.2632259  -0.22489045\n",
      "   1.29460724  0.34547662 -0.6857837 ]\n",
      " [-0.91483754  1.06105529 -0.40033547 -1.13270866  0.16646816 -0.03605914\n",
      "  -0.76002837 -0.51076761  0.9019198 ]\n",
      " [ 0.72011451 -0.64393202 -0.0975145   0.7046461  -0.41518249  0.23377073\n",
      "   0.79675511  0.06773745 -0.82860763]\n",
      " [ 0.77387313 -0.97746748  0.24353638  0.7937363  -0.29607271  0.04111313\n",
      "   0.96523744  0.97391347 -0.5206496 ]\n",
      " [-1.05643394  1.30194706 -0.34593002 -1.26071155  0.0428969   0.31186942\n",
      "  -1.02311276 -0.85604787  0.85792584]\n",
      " [ 1.20656014 -1.1921604   0.32381816  0.8966939  -0.13102614 -0.35023981\n",
      "   0.86293184  0.95073817 -0.65746605]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.69875815  2.09535271 -2.59711046  0.34372214  0.64282544 -3.28569847\n",
      "   1.46419698]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:89 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.23025139]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 89 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.95441786 -1.31874223  0.17402252  1.12438315 -0.26539369 -0.22705825\n",
      "   1.29243944  0.34547662 -0.6857837 ]\n",
      " [-0.91251029  1.06338255 -0.40033547 -1.13270866  0.16879542 -0.03373189\n",
      "  -0.75770112 -0.51076761  0.9019198 ]\n",
      " [ 0.71697723 -0.64706931 -0.0975145   0.7046461  -0.41831978  0.23063344\n",
      "   0.79361783  0.06773745 -0.82860763]\n",
      " [ 0.77144752 -0.97989308  0.24353638  0.7937363  -0.29849832  0.03868752\n",
      "   0.96281184  0.97391347 -0.5206496 ]\n",
      " [-1.05437063  1.30401037 -0.34593002 -1.26071155  0.0449602   0.31393273\n",
      "  -1.02104945 -0.85604787  0.85792584]\n",
      " [ 1.20461698 -1.19410356  0.32381816  0.8966939  -0.1329693  -0.35218297\n",
      "   0.86098868  0.95073817 -0.65746605]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.71916253  2.08290997 -2.6048937   0.33012657  0.63009252 -3.29377532\n",
      "   1.45200042]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:89 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.90840424]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 89 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.95483029 -1.31874223  0.17402252  1.12479558 -0.26539369 -0.22705825\n",
      "   1.29285187  0.34547662 -0.6857837 ]\n",
      " [-0.91308598  1.06338255 -0.40033547 -1.13328435  0.16879542 -0.03373189\n",
      "  -0.75827681 -0.51076761  0.9019198 ]\n",
      " [ 0.71772607 -0.64706931 -0.0975145   0.70539494 -0.41831978  0.23063344\n",
      "   0.79436667  0.06773745 -0.82860763]\n",
      " [ 0.77210697 -0.97989308  0.24353638  0.79439575 -0.29849832  0.03868752\n",
      "   0.96347128  0.97391347 -0.5206496 ]\n",
      " [-1.05479235  1.30401037 -0.34593002 -1.26113326  0.0449602   0.31393273\n",
      "  -1.02147117 -0.85604787  0.85792584]\n",
      " [ 1.20514461 -1.19410356  0.32381816  0.89722154 -0.1329693  -0.35218297\n",
      "   0.86151631  0.95073817 -0.65746605]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.71535187  2.08659409 -2.60467585  0.33356229  0.6336215  -3.29364442\n",
      "   1.45562376]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:89 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.74794587]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 89 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.96007084 -1.31874223  0.17402252  1.12479558 -0.26015314 -0.2218177\n",
      "   1.29809242  0.34547662 -0.6857837 ]\n",
      " [-0.91840485  1.06338255 -0.40033547 -1.13328435  0.16347655 -0.03905076\n",
      "  -0.76359567 -0.51076761  0.9019198 ]\n",
      " [ 0.72294794 -0.64706931 -0.0975145   0.70539494 -0.41309791  0.23585532\n",
      "   0.79958854  0.06773745 -0.82860763]\n",
      " [ 0.7774171  -0.97989308  0.24353638  0.79439575 -0.29318819  0.04399765\n",
      "   0.96878141  0.97391347 -0.5206496 ]\n",
      " [-1.06005765  1.30401037 -0.34593002 -1.26113326  0.0396949   0.30866742\n",
      "  -1.02673647 -0.85604787  0.85792584]\n",
      " [ 1.21046087 -1.19410356  0.32381816  0.89722154 -0.12765304 -0.34686672\n",
      "   0.86683257  0.95073817 -0.65746605]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.69159289  2.10685127 -2.60046876  0.35232984  0.6529597  -3.29002806\n",
      "   1.47532988]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:89 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.59055971]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 89 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94493567 -1.31874223  0.17402252  1.10966041 -0.26015314 -0.23695287\n",
      "   1.29809242  0.34547662 -0.70091887]\n",
      " [-0.90321461  1.06338255 -0.40033547 -1.11809411  0.16347655 -0.02386052\n",
      "  -0.76359567 -0.51076761  0.91711004]\n",
      " [ 0.71036094 -0.64706931 -0.0975145   0.69280794 -0.41309791  0.22326831\n",
      "   0.79958854  0.06773745 -0.84119464]\n",
      " [ 0.76273068 -0.97989308  0.24353638  0.77970933 -0.29318819  0.02931123\n",
      "   0.96878141  0.97391347 -0.53533602]\n",
      " [-1.04503626  1.30401037 -0.34593002 -1.24611187  0.0396949   0.32368881\n",
      "  -1.02673647 -0.85604787  0.87294724]\n",
      " [ 1.19572513 -1.19410356  0.32381816  0.88248579 -0.12765304 -0.36160246\n",
      "   0.86683257  0.95073817 -0.67220179]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.76299125  2.05227039 -2.61713813  0.30252694  0.59945718 -3.3071387\n",
      "   1.42171777]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:89 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.7282276]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 89 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.95083122 -1.31284668  0.17402252  1.11555596 -0.26015314 -0.23105732\n",
      "   1.30398798  0.34547662 -0.70091887]\n",
      " [-0.90915431  1.05744284 -0.40033547 -1.12403381  0.16347655 -0.02980022\n",
      "  -0.76953537 -0.51076761  0.91711004]\n",
      " [ 0.7162729  -0.64115735 -0.0975145   0.6987199  -0.41309791  0.22918027\n",
      "   0.8055005   0.06773745 -0.84119464]\n",
      " [ 0.76875074 -0.97387302  0.24353638  0.78572939 -0.29318819  0.0353313\n",
      "   0.97480147  0.97391347 -0.53533602]\n",
      " [-1.05101328  1.29803335 -0.34593002 -1.25208888  0.0396949   0.3177118\n",
      "  -1.03271349 -0.85604787  0.87294724]\n",
      " [ 1.20169248 -1.1881362   0.32381816  0.88845315 -0.12765304 -0.35563511\n",
      "   0.87279993  0.95073817 -0.67220179]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.73609771  2.07533919 -2.61314093  0.32553618  0.6216832  -3.30294999\n",
      "   1.44324568]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:89 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.79448338]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 89 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.95429923 -1.30937867  0.17402252  1.11902397 -0.26015314 -0.23105732\n",
      "   1.30745599  0.34547662 -0.70091887]\n",
      " [-0.91286002  1.05373714 -0.40033547 -1.12773952  0.16347655 -0.02980022\n",
      "  -0.77324108 -0.51076761  0.91711004]\n",
      " [ 0.72002739 -0.63740286 -0.0975145   0.70247439 -0.41309791  0.22918027\n",
      "   0.80925499  0.06773745 -0.84119464]\n",
      " [ 0.77250675 -0.97011702  0.24353638  0.78948539 -0.29318819  0.0353313\n",
      "   0.97855747  0.97391347 -0.53533602]\n",
      " [-1.05450073  1.2945459  -0.34593002 -1.25557633  0.0396949   0.3177118\n",
      "  -1.03620094 -0.85604787  0.87294724]\n",
      " [ 1.20538309 -1.18444559  0.32381816  0.89214376 -0.12765304 -0.35563511\n",
      "   0.87649054  0.95073817 -0.67220179]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.71931938  2.09021664 -2.61064695  0.33944727  0.63553696 -3.30101557\n",
      "   1.45759207]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:89 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.84115262]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 89 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.95639236 -1.30937867  0.17611564  1.11902397 -0.26015314 -0.2289642\n",
      "   1.30954911  0.34547662 -0.70091887]\n",
      " [-0.91501522  1.05373714 -0.40249067 -1.12773952  0.16347655 -0.03195542\n",
      "  -0.77539628 -0.51076761  0.91711004]\n",
      " [ 0.72239198 -0.63740286 -0.09514991  0.70247439 -0.41309791  0.23154486\n",
      "   0.81161958  0.06773745 -0.84119464]\n",
      " [ 0.77471734 -0.97011702  0.24574697  0.78948539 -0.29318819  0.03754189\n",
      "   0.98076807  0.97391347 -0.53533602]\n",
      " [-1.05665412  1.2945459  -0.34808341 -1.25557633  0.0396949   0.3155584\n",
      "  -1.03835434 -0.85604787  0.87294724]\n",
      " [ 1.20758128 -1.18444559  0.32601635  0.89214376 -0.12765304 -0.35343692\n",
      "   0.87868872  0.95073817 -0.67220179]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.7087072   2.09977474 -2.60950583  0.34836538  0.64491712 -3.29987719\n",
      "   1.46699397]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:89 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.11011059]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 90 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.95630463 -1.3094664   0.17611564  1.11893624 -0.26015314 -0.2289642\n",
      "   1.30954911  0.34547662 -0.7010066 ]\n",
      " [-0.91491836  1.05383399 -0.40249067 -1.12764267  0.16347655 -0.03195542\n",
      "  -0.77539628 -0.51076761  0.91720689]\n",
      " [ 0.7224644  -0.63733045 -0.09514991  0.7025468  -0.41309791  0.23154486\n",
      "   0.81161958  0.06773745 -0.84112222]\n",
      " [ 0.77463817 -0.97019618  0.24574697  0.78940623 -0.29318819  0.03754189\n",
      "   0.98076807  0.97391347 -0.53541518]\n",
      " [-1.05645994  1.29474008 -0.34808341 -1.25538215  0.0396949   0.3155584\n",
      "  -1.03835434 -0.85604787  0.87314142]\n",
      " [ 1.20725824 -1.18476863  0.32601635  0.89182073 -0.12765304 -0.35343692\n",
      "   0.87868872  0.95073817 -0.67252483]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.71410186  2.09698962 -2.61210622  0.3457405   0.64214058 -3.30237966\n",
      "   1.46397042]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:90 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.86446467]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 90 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.95773213 -1.3094664   0.17754314  1.11893624 -0.26015314 -0.2289642\n",
      "   1.31097662  0.34547662 -0.7010066 ]\n",
      " [-0.91654254  1.05383399 -0.40411485 -1.12764267  0.16347655 -0.03195542\n",
      "  -0.77702046 -0.51076761  0.91720689]\n",
      " [ 0.72423481 -0.63733045 -0.09337949  0.7025468  -0.41309791  0.23154486\n",
      "   0.81339     0.06773745 -0.84112222]\n",
      " [ 0.77630497 -0.97019618  0.24741377  0.78940623 -0.29318819  0.03754189\n",
      "   0.98243487  0.97391347 -0.53541518]\n",
      " [-1.05788687  1.29474008 -0.34951034 -1.25538215  0.0396949   0.3155584\n",
      "  -1.03978127 -0.85604787  0.87314142]\n",
      " [ 1.2087041  -1.18476863  0.3274622   0.89182073 -0.12765304 -0.35343692\n",
      "   0.88013458  0.95073817 -0.67252483]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.70616183  2.10429423 -2.61123442  0.35215835  0.6491351  -3.30174478\n",
      "   1.47125726]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:90 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.56854345]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 90 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94315383 -1.3240447   0.16296484  1.11893624 -0.26015314 -0.2289642\n",
      "   1.29639831  0.34547662 -0.7010066 ]\n",
      " [-0.90252353  1.06785301 -0.39009583 -1.12764267  0.16347655 -0.03195542\n",
      "  -0.76300145 -0.51076761  0.91720689]\n",
      " [ 0.71223023 -0.64933503 -0.10538407  0.7025468  -0.41309791  0.23154486\n",
      "   0.80138542  0.06773745 -0.84112222]\n",
      " [ 0.76233936 -0.98416179  0.23344816  0.78940623 -0.29318819  0.03754189\n",
      "   0.96846926  0.97391347 -0.53541518]\n",
      " [-1.04322696  1.3094     -0.33485043 -1.25538215  0.0396949   0.3155584\n",
      "  -1.02512136 -0.85604787  0.87314142]\n",
      " [ 1.19368611 -1.19978661  0.31244422  0.89182073 -0.12765304 -0.35343692\n",
      "   0.86511659  0.95073817 -0.67252483]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.77589419  2.05150062 -2.62939336  0.30394178  0.59766778 -3.31848361\n",
      "   1.41727837]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:90 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.77289058]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 90 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94760234 -1.3240447   0.16741336  1.11893624 -0.26015314 -0.2289642\n",
      "   1.29639831  0.34992514 -0.7010066 ]\n",
      " [-0.90688857  1.06785301 -0.39446087 -1.12764267  0.16347655 -0.03195542\n",
      "  -0.76300145 -0.51513265  0.91720689]\n",
      " [ 0.71523655 -0.64933503 -0.10237775  0.7025468  -0.41309791  0.23154486\n",
      "   0.80138542  0.07074378 -0.84112222]\n",
      " [ 0.76655731 -0.98416179  0.2376661   0.78940623 -0.29318819  0.03754189\n",
      "   0.96846926  0.97813142 -0.53541518]\n",
      " [-1.04711772  1.3094     -0.3387412  -1.25538215  0.0396949   0.3155584\n",
      "  -1.02512136 -0.85993864  0.87314142]\n",
      " [ 1.19724664 -1.19978661  0.31600474  0.89182073 -0.12765304 -0.35343692\n",
      "   0.86511659  0.95429869 -0.67252483]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.75596185  2.06765068 -2.62657414  0.31714753  0.61515999 -3.3165556\n",
      "   1.4356373 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:90 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.60196928]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 90 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.95815057 -1.3240447   0.16741336  1.12948447 -0.26015314 -0.2289642\n",
      "   1.29639831  0.34992514 -0.69045837]\n",
      " [-0.91678472  1.06785301 -0.39446087 -1.13753882  0.16347655 -0.03195542\n",
      "  -0.76300145 -0.51513265  0.90731074]\n",
      " [ 0.72156975 -0.64933503 -0.10237775  0.70887999 -0.41309791  0.23154486\n",
      "   0.80138542  0.07074378 -0.83478903]\n",
      " [ 0.77603409 -0.98416179  0.2376661   0.79888302 -0.29318819  0.03754189\n",
      "   0.96846926  0.97813142 -0.5259384 ]\n",
      " [-1.05774176  1.3094     -0.3387412  -1.26600618  0.0396949   0.3155584\n",
      "  -1.02512136 -0.85993864  0.86251739]\n",
      " [ 1.20785826 -1.19978661  0.31600474  0.90243235 -0.12765304 -0.35343692\n",
      "   0.86511659  0.95429869 -0.6619132 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.70827732  2.10563891 -2.61481953  0.34767988  0.6502119  -3.30734301\n",
      "   1.47401361]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:90 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.06233663]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 90 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.9582502  -1.3240447   0.16741336  1.12948447 -0.26005352 -0.22886458\n",
      "   1.29639831  0.34992514 -0.69035875]\n",
      " [-0.9168401   1.06785301 -0.39446087 -1.13753882  0.16342117 -0.0320108\n",
      "  -0.76300145 -0.51513265  0.90725536]\n",
      " [ 0.72170113 -0.64933503 -0.10237775  0.70887999 -0.41296653  0.23167624\n",
      "   0.80138542  0.07074378 -0.83465765]\n",
      " [ 0.77603662 -0.98416179  0.2376661   0.79888302 -0.29318566  0.03754442\n",
      "   0.96846926  0.97813142 -0.52593587]\n",
      " [-1.05781418  1.3094     -0.3387412  -1.26600618  0.03962247  0.31548598\n",
      "  -1.02512136 -0.85993864  0.86244497]\n",
      " [ 1.20782876 -1.19978661  0.31600474  0.90243235 -0.12768255 -0.35346643\n",
      "   0.86511659  0.95429869 -0.66194271]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.71009913  2.10482844 -2.61578595  0.34690226  0.64930352 -3.30832665\n",
      "   1.47307317]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:90 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.00448043]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 90 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.95825243 -1.32404247  0.16741336  1.12948447 -0.26005128 -0.22886234\n",
      "   1.29639831  0.34992514 -0.69035651]\n",
      " [-0.91684223  1.06785088 -0.39446087 -1.13753882  0.16341904 -0.03201292\n",
      "  -0.76300145 -0.51513265  0.90725324]\n",
      " [ 0.72170303 -0.64933313 -0.10237775  0.70887999 -0.41296462  0.23167815\n",
      "   0.80138542  0.07074378 -0.83465575]\n",
      " [ 0.77603857 -0.98415984  0.2376661   0.79888302 -0.29318371  0.03754637\n",
      "   0.96846926  0.97813142 -0.52593392]\n",
      " [-1.05781641  1.30939776 -0.3387412  -1.26600618  0.03962024  0.31548375\n",
      "  -1.02512136 -0.85993864  0.86244273]\n",
      " [ 1.20783084 -1.19978452  0.31600474  0.90243235 -0.12768046 -0.35346434\n",
      "   0.86511659  0.95429869 -0.66194062]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.71010912  2.10482669 -2.61579361  0.34689947  0.64930082 -3.30833477\n",
      "   1.47307074]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:90 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.22681556]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 90 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.95616001 -1.32613489  0.16741336  1.12948447 -0.2621437  -0.23095476\n",
      "   1.29430589  0.34992514 -0.69035651]\n",
      " [-0.91458554  1.07010757 -0.39446087 -1.13753882  0.16567573 -0.02975624\n",
      "  -0.76074476 -0.51513265  0.90725324]\n",
      " [ 0.71864188 -0.65239428 -0.10237775  0.70887999 -0.41602577  0.228617\n",
      "   0.79832427  0.07074378 -0.83465575]\n",
      " [ 0.77368237 -0.98651604  0.2376661   0.79888302 -0.29553991  0.03519017\n",
      "   0.96611305  0.97813142 -0.52593392]\n",
      " [-1.05582439  1.31138979 -0.3387412  -1.26600618  0.04161227  0.31747578\n",
      "  -1.02312933 -0.85993864  0.86244273]\n",
      " [ 1.2059547  -1.20166066  0.31600474  0.90243235 -0.1295566  -0.35534048\n",
      "   0.86324045  0.95429869 -0.66194062]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.72999748  2.09272139 -2.62339322  0.33364356  0.63689912 -3.31622828\n",
      "   1.461202  ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:90 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.91000625]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 90 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.95655663 -1.32613489  0.16741336  1.12988109 -0.2621437  -0.23095476\n",
      "   1.29470251  0.34992514 -0.69035651]\n",
      " [-0.91513932  1.07010757 -0.39446087 -1.1380926   0.16567573 -0.02975624\n",
      "  -0.76129854 -0.51513265  0.90725324]\n",
      " [ 0.71936332 -0.65239428 -0.10237775  0.70960144 -0.41602577  0.228617\n",
      "   0.79904571  0.07074378 -0.83465575]\n",
      " [ 0.77431696 -0.98651604  0.2376661   0.79951761 -0.29553991  0.03519017\n",
      "   0.96674764  0.97813142 -0.52593392]\n",
      " [-1.05622992  1.31138979 -0.3387412  -1.26641172  0.04161227  0.31747578\n",
      "  -1.02353487 -0.85993864  0.86244273]\n",
      " [ 1.20646226 -1.20166066  0.31600474  0.90293991 -0.1295566  -0.35534048\n",
      "   0.86374801  0.95429869 -0.66194062]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.72631246  2.09628506 -2.62318453  0.33696946  0.64031443 -3.31610277\n",
      "   1.46470747]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:90 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.75014682]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 90 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.96171915 -1.32613489  0.16741336  1.12988109 -0.25698118 -0.22579224\n",
      "   1.29986503  0.34992514 -0.69035651]\n",
      " [-0.92038116  1.07010757 -0.39446087 -1.1380926   0.16043389 -0.03499808\n",
      "  -0.76654038 -0.51513265  0.90725324]\n",
      " [ 0.72451534 -0.65239428 -0.10237775  0.70960144 -0.41087375  0.23376902\n",
      "   0.80419773  0.07074378 -0.83465575]\n",
      " [ 0.77955135 -0.98651604  0.2376661   0.79951761 -0.29030552  0.04042456\n",
      "   0.97198203  0.97813142 -0.52593392]\n",
      " [-1.06141694  1.31138979 -0.3387412  -1.26641172  0.03642525  0.31228876\n",
      "  -1.02872189 -0.85993864  0.86244273]\n",
      " [ 1.2117009  -1.20166066  0.31600474  0.90293991 -0.12431797 -0.35010185\n",
      "   0.86898665  0.95429869 -0.66194062]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.7028979   2.11625719 -2.6190528   0.35549052  0.65938982 -3.31254881\n",
      "   1.48414052]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:90 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.5896857]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 90 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94660535 -1.32613489  0.16741336  1.11476728 -0.25698118 -0.24090604\n",
      "   1.29986503  0.34992514 -0.70547032]\n",
      " [-0.90521573  1.07010757 -0.39446087 -1.12292717  0.16043389 -0.01983265\n",
      "  -0.76654038 -0.51513265  0.92241866]\n",
      " [ 0.71196262 -0.65239428 -0.10237775  0.69704872 -0.41087375  0.2212163\n",
      "   0.80419773  0.07074378 -0.84720847]\n",
      " [ 0.76488687 -0.98651604  0.2376661   0.78485313 -0.29030552  0.02576008\n",
      "   0.97198203  0.97813142 -0.5405984 ]\n",
      " [-1.0464158   1.31138979 -0.3387412  -1.25141058  0.03642525  0.3272899\n",
      "  -1.02872189 -0.85993864  0.87744387]\n",
      " [ 1.19698185 -1.20166066  0.31600474  0.88822086 -0.12431797 -0.36482089\n",
      "   0.86898665  0.95429869 -0.67665967]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.77423704  2.06174489 -2.63574159  0.30576447  0.60595313 -3.32966477\n",
      "   1.4305829 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:90 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.72893364]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 90 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.95247859 -1.32026164  0.16741336  1.12064053 -0.25698118 -0.2350328\n",
      "   1.30573828  0.34992514 -0.70547032]\n",
      " [-0.91113109  1.06419221 -0.39446087 -1.12884253  0.16043389 -0.02574801\n",
      "  -0.77245573 -0.51513265  0.92241866]\n",
      " [ 0.71784689 -0.64651001 -0.10237775  0.70293299 -0.41087375  0.22710057\n",
      "   0.810082    0.07074378 -0.84720847]\n",
      " [ 0.77088148 -0.98052143  0.2376661   0.79084774 -0.29030552  0.03175469\n",
      "   0.97797664  0.97813142 -0.5405984 ]\n",
      " [-1.0523689   1.30543669 -0.3387412  -1.25736368  0.03642525  0.3213368\n",
      "  -1.03467499 -0.85993864  0.87744387]\n",
      " [ 1.20292284 -1.19571967  0.31600474  0.89416185 -0.12431797 -0.3588799\n",
      "   0.87492764  0.95429869 -0.67665967]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.74745712  2.08470709 -2.63175797  0.32868667  0.62808652 -3.32548559\n",
      "   1.45201289]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:90 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.79601856]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 90 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.95590054 -1.3168397   0.16741336  1.12406247 -0.25698118 -0.2350328\n",
      "   1.30916022  0.34992514 -0.70547032]\n",
      " [-0.91478728  1.06053602 -0.39446087 -1.13249872  0.16043389 -0.02574801\n",
      "  -0.77611193 -0.51513265  0.92241866]\n",
      " [ 0.72155215 -0.64280475 -0.10237775  0.70663825 -0.41087375  0.22710057\n",
      "   0.81378726  0.07074378 -0.84720847]\n",
      " [ 0.77458859 -0.97681432  0.2376661   0.79455485 -0.29030552  0.03175469\n",
      "   0.98168375  0.97813142 -0.5405984 ]\n",
      " [-1.05580994  1.30199565 -0.3387412  -1.26080472  0.03642525  0.3213368\n",
      "  -1.03811603 -0.85993864  0.87744387]\n",
      " [ 1.20656475 -1.19207776  0.31600474  0.89780377 -0.12431797 -0.3588799\n",
      "   0.87856955  0.95429869 -0.67665967]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.73089658  2.0993932  -2.62930254  0.34242894  0.64176944 -3.32357829\n",
      "   1.4661761 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:90 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.8413278]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 90 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.95799476 -1.3168397   0.16950757  1.12406247 -0.25698118 -0.23293858\n",
      "   1.31125444  0.34992514 -0.70547032]\n",
      " [-0.91694162  1.06053602 -0.39661521 -1.13249872  0.16043389 -0.02790234\n",
      "  -0.77826626 -0.51513265  0.92241866]\n",
      " [ 0.72391219 -0.64280475 -0.10001771  0.70663825 -0.41087375  0.2294606\n",
      "   0.8161473   0.07074378 -0.84720847]\n",
      " [ 0.77679733 -0.97681432  0.23987485  0.79455485 -0.29030552  0.03396343\n",
      "   0.9838925   0.97813142 -0.5405984 ]\n",
      " [-1.05796413  1.30199565 -0.34089538 -1.26080472  0.03642525  0.31918262\n",
      "  -1.04027021 -0.85993864  0.87744387]\n",
      " [ 1.20876339 -1.19207776  0.31820337  0.89780377 -0.12431797 -0.35668127\n",
      "   0.88076818  0.95429869 -0.67665967]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.72030558  2.10892531 -2.62815848  0.35132799  0.65112625 -3.32243446\n",
      "   1.47555086]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:90 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.10711465]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 91 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.95791831 -1.31691614  0.16950757  1.12398603 -0.25698118 -0.23293858\n",
      "   1.31125444  0.34992514 -0.70554676]\n",
      " [-0.91685657  1.06062106 -0.39661521 -1.13241368  0.16043389 -0.02790234\n",
      "  -0.77826626 -0.51513265  0.92250371]\n",
      " [ 0.72398827 -0.64272867 -0.10001771  0.70671432 -0.41087375  0.2294606\n",
      "   0.8161473   0.07074378 -0.84713239]\n",
      " [ 0.77672831 -0.97688334  0.23987485  0.79448582 -0.29030552  0.03396343\n",
      "   0.9838925   0.97813142 -0.54066742]\n",
      " [-1.05778657  1.30217321 -0.34089538 -1.26062716  0.03642525  0.31918262\n",
      "  -1.04027021 -0.85993864  0.87762143]\n",
      " [ 1.20846309 -1.19237805  0.31820337  0.89750348 -0.12431797 -0.35668127\n",
      "   0.88076818  0.95429869 -0.67695996]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.72542786  2.10628768 -2.63063452  0.34884297  0.64849606 -3.32481747\n",
      "   1.47268659]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:91 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.86531395]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 91 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.95933137 -1.31691614  0.17092063  1.12398603 -0.25698118 -0.23293858\n",
      "   1.31266749  0.34992514 -0.70554676]\n",
      " [-0.91846255  1.06062106 -0.39822119 -1.13241368  0.16043389 -0.02790234\n",
      "  -0.77987225 -0.51513265  0.92250371]\n",
      " [ 0.72573845 -0.64272867 -0.09826753  0.70671432 -0.41087375  0.2294606\n",
      "   0.81789748  0.07074378 -0.84713239]\n",
      " [ 0.77837619 -0.97688334  0.24152273  0.79448582 -0.29030552  0.03396343\n",
      "   0.98554037  0.97813142 -0.54066742]\n",
      " [-1.05919946  1.30217321 -0.34230827 -1.26062716  0.03642525  0.31918262\n",
      "  -1.0416831  -0.85993864  0.87762143]\n",
      " [ 1.20989501 -1.19237805  0.31963529  0.89750348 -0.12431797 -0.35668127\n",
      "   0.8822001   0.95429869 -0.67695996]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.71757932  2.11350622 -2.62977194  0.3551883   0.65540944 -3.32418762\n",
      "   1.47988678]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:91 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.56342419]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 91 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94490248 -1.33134503  0.15649174  1.12398603 -0.25698118 -0.23293858\n",
      "   1.2982386   0.34992514 -0.70554676]\n",
      " [-0.90458473  1.07449889 -0.38434337 -1.13241368  0.16043389 -0.02790234\n",
      "  -0.76599442 -0.51513265  0.92250371]\n",
      " [ 0.713853   -0.65461412 -0.11015298  0.70671432 -0.41087375  0.2294606\n",
      "   0.80601203  0.07074378 -0.84713239]\n",
      " [ 0.76454887 -0.99071067  0.2276954   0.79448582 -0.29030552  0.03396343\n",
      "   0.97171305  0.97813142 -0.54066742]\n",
      " [-1.04469035  1.31668232 -0.32779917 -1.26062716  0.03642525  0.31918262\n",
      "  -1.027174   -0.85993864  0.87762143]\n",
      " [ 1.19502026 -1.20725281  0.30476054  0.89750348 -0.12431797 -0.35668127\n",
      "   0.86732535  0.95429869 -0.67695996]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.78687412  2.06118167 -2.64792268  0.30733755  0.6043644  -3.34096622\n",
      "   1.42639469]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:91 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.77428651]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 91 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94930433 -1.33134503  0.16089359  1.12398603 -0.25698118 -0.23293858\n",
      "   1.2982386   0.35432699 -0.70554676]\n",
      " [-0.90890359  1.07449889 -0.38866223 -1.13241368  0.16043389 -0.02790234\n",
      "  -0.76599442 -0.51945151  0.92250371]\n",
      " [ 0.71682736 -0.65461412 -0.10717862  0.70671432 -0.41087375  0.2294606\n",
      "   0.80601203  0.07371814 -0.84713239]\n",
      " [ 0.76872195 -0.99071067  0.23186848  0.79448582 -0.29030552  0.03396343\n",
      "   0.97171305  0.9823045  -0.54066742]\n",
      " [-1.04854272  1.31668232 -0.33165153 -1.26062716  0.03642525  0.31918262\n",
      "  -1.027174   -0.86379101  0.87762143]\n",
      " [ 1.19854777 -1.20725281  0.30828805  0.89750348 -0.12431797 -0.35668127\n",
      "   0.86732535  0.9578262  -0.67695996]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.76715049  2.07716178 -2.6451346   0.3204044   0.62167487 -3.33905547\n",
      "   1.44455738]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:91 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.60315087]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 91 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.95981382 -1.33134503  0.16089359  1.13449551 -0.25698118 -0.23293858\n",
      "   1.2982386   0.35432699 -0.69503728]\n",
      " [-0.91876607  1.07449889 -0.38866223 -1.14227616  0.16043389 -0.02790234\n",
      "  -0.76599442 -0.51945151  0.91264123]\n",
      " [ 0.72313306 -0.65461412 -0.10717862  0.71302002 -0.41087375  0.2294606\n",
      "   0.80601203  0.07371814 -0.84082669]\n",
      " [ 0.7781706  -0.99071067  0.23186848  0.80393448 -0.29030552  0.03396343\n",
      "   0.97171305  0.9823045  -0.53121876]\n",
      " [-1.05912647  1.31668232 -0.33165153 -1.27121091  0.03642525  0.31918262\n",
      "  -1.027174   -0.86379101  0.86703768]\n",
      " [ 1.20911977 -1.20725281  0.30828805  0.90807547 -0.12431797 -0.35668127\n",
      "   0.86732535  0.9578262  -0.66638797]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.71965561  2.11501593 -2.63343987  0.35081256  0.6566058  -3.32989573\n",
      "   1.48280006]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:91 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.06060334]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 91 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.9599097  -1.33134503  0.16089359  1.13449551 -0.2568853  -0.2328427\n",
      "   1.2982386   0.35432699 -0.6949414 ]\n",
      " [-0.91882037  1.07449889 -0.38866223 -1.14227616  0.16037959 -0.02795664\n",
      "  -0.76599442 -0.51945151  0.91258693]\n",
      " [ 0.72325921 -0.65461412 -0.10717862  0.71302002 -0.4107476   0.22958676\n",
      "   0.80601203  0.07371814 -0.84070054]\n",
      " [ 0.77817465 -0.99071067  0.23186848  0.80393448 -0.29030147  0.03396748\n",
      "   0.97171305  0.9823045  -0.53121471]\n",
      " [-1.05919652  1.31668232 -0.33165153 -1.27121091  0.0363552   0.31911256\n",
      "  -1.027174   -0.86379101  0.86696763]\n",
      " [ 1.20909317 -1.20725281  0.30828805  0.90807547 -0.12434457 -0.35670787\n",
      "   0.86732535  0.9578262  -0.66641457]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.7213807   2.11425008 -2.63435686  0.35007806  0.6557473  -3.33082864\n",
      "   1.4819109 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:91 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.00423639]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 91 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.9599117  -1.33134303  0.16089359  1.13449551 -0.2568833  -0.2328407\n",
      "   1.2982386   0.35432699 -0.6949394 ]\n",
      " [-0.91882227  1.07449698 -0.38866223 -1.14227616  0.16037769 -0.02795855\n",
      "  -0.76599442 -0.51945151  0.91258502]\n",
      " [ 0.72326092 -0.65461241 -0.10717862  0.71302002 -0.41074589  0.22958847\n",
      "   0.80601203  0.07371814 -0.84069883]\n",
      " [ 0.77817641 -0.99070891  0.23186848  0.80393448 -0.29029972  0.03396924\n",
      "   0.97171305  0.9823045  -0.53121295]\n",
      " [-1.05919852  1.31668032 -0.33165153 -1.27121091  0.0363532   0.31911057\n",
      "  -1.027174   -0.86379101  0.86696563]\n",
      " [ 1.20909504 -1.20725093  0.30828805  0.90807547 -0.12434269 -0.35670599\n",
      "   0.86732535  0.9578262  -0.66641269]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.72138964  2.11424852 -2.63436373  0.35007557  0.6557449  -3.33083591\n",
      "   1.48190875]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:91 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.22348176]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 91 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.95789086 -1.33336386  0.16089359  1.13449551 -0.25890413 -0.23486154\n",
      "   1.29621777  0.35432699 -0.6949394 ]\n",
      " [-0.91663291  1.07668635 -0.38866223 -1.14227616  0.16256706 -0.02576918\n",
      "  -0.76380505 -0.51945151  0.91258502]\n",
      " [ 0.72027285 -0.65760048 -0.10717862  0.71302002 -0.41373395  0.2266004\n",
      "   0.80302396  0.07371814 -0.84069883]\n",
      " [ 0.77588652 -0.9929988   0.23186848  0.80393448 -0.29258961  0.03167934\n",
      "   0.96942316  0.9823045  -0.53121295]\n",
      " [-1.05727415  1.31860469 -0.33165153 -1.27121091  0.03827757  0.32103493\n",
      "  -1.02524963 -0.86379101  0.86696563]\n",
      " [ 1.20728262 -1.20906335  0.30828805  0.90807547 -0.12615511 -0.35851842\n",
      "   0.86551293  0.9578262  -0.66641269]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.74078089  2.1024671  -2.64178569  0.33714654  0.64366165 -3.33855167\n",
      "   1.47035485]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:91 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.91157369]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 91 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.95827233 -1.33336386  0.16089359  1.13487698 -0.25890413 -0.23486154\n",
      "   1.29659924  0.35432699 -0.6949394 ]\n",
      " [-0.91716568  1.07668635 -0.38866223 -1.14280893  0.16256706 -0.02576918\n",
      "  -0.76433783 -0.51945151  0.91258502]\n",
      " [ 0.72096797 -0.65760048 -0.10717862  0.71371514 -0.41373395  0.2266004\n",
      "   0.80371908  0.07371814 -0.84069883]\n",
      " [ 0.77649727 -0.9929988   0.23186848  0.80454523 -0.29258961  0.03167934\n",
      "   0.97003391  0.9823045  -0.53121295]\n",
      " [-1.05766419  1.31860469 -0.33165153 -1.27160095  0.03827757  0.32103493\n",
      "  -1.02563966 -0.86379101  0.86696563]\n",
      " [ 1.20777093 -1.20906335  0.30828805  0.90856377 -0.12615511 -0.35851842\n",
      "   0.86600124  0.9578262  -0.66641269]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.737217    2.10591462 -2.64158572  0.34036639  0.64696724 -3.3384313\n",
      "   1.47374662]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:91 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.75233705]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 91 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.96335748 -1.33336386  0.16089359  1.13487698 -0.25381898 -0.22977639\n",
      "   1.30168439  0.35432699 -0.6949394 ]\n",
      " [-0.92233108  1.07668635 -0.38866223 -1.14280893  0.15740166 -0.03093457\n",
      "  -0.76950322 -0.51945151  0.91258502]\n",
      " [ 0.72605038 -0.65760048 -0.10717862  0.71371514 -0.40865154  0.23168281\n",
      "   0.80880149  0.07371814 -0.84069883]\n",
      " [ 0.78165642 -0.9929988   0.23186848  0.80454523 -0.28743046  0.0368385\n",
      "   0.97519306  0.9823045  -0.53121295]\n",
      " [-1.06277358  1.31860469 -0.33165153 -1.27160095  0.03316818  0.31592554\n",
      "  -1.03074905 -0.86379101  0.86696563]\n",
      " [ 1.21293256 -1.20906335  0.30828805  0.90856377 -0.12099348 -0.35335679\n",
      "   0.87116286  0.9578262  -0.66641269]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.71414397  2.12560394 -2.63752828  0.35864224  0.66578145 -3.33493902\n",
      "   1.49290852]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:91 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.58874347]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 91 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94826678 -1.33336386  0.16089359  1.11978628 -0.25381898 -0.24486709\n",
      "   1.30168439  0.35432699 -0.7100301 ]\n",
      " [-0.90719227  1.07668635 -0.38866223 -1.12767013  0.15740166 -0.01579577\n",
      "  -0.76950322 -0.51945151  0.92772383]\n",
      " [ 0.71353414 -0.65760048 -0.10717862  0.70119889 -0.40865154  0.21916657\n",
      "   0.80880149  0.07371814 -0.85321507]\n",
      " [ 0.76701579 -0.9929988   0.23186848  0.7899046  -0.28743046  0.02219787\n",
      "   0.97519306  0.9823045  -0.54585359]\n",
      " [-1.04779441  1.31860469 -0.33165153 -1.25662179  0.03316818  0.33090471\n",
      "  -1.03074905 -0.86379101  0.88194479]\n",
      " [ 1.19823202 -1.20906335  0.30828805  0.89386324 -0.12099348 -0.36805732\n",
      "   0.87116286  0.9578262  -0.68111323]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.78541861  2.07116577 -2.65423686  0.3089985   0.6124163  -3.35206049\n",
      "   1.43941095]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:91 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.72965166]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 91 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.95411717 -1.32751347  0.16089359  1.12563667 -0.25381898 -0.23901669\n",
      "   1.30753478  0.35432699 -0.7100301 ]\n",
      " [-0.91308283  1.07079579 -0.38866223 -1.13356069  0.15740166 -0.02168633\n",
      "  -0.77539379 -0.51945151  0.92772383]\n",
      " [ 0.71939031 -0.6517443  -0.10717862  0.70705507 -0.40865154  0.22502274\n",
      "   0.81465767  0.07371814 -0.85321507]\n",
      " [ 0.77298453 -0.98703006  0.23186848  0.79587334 -0.28743046  0.02816661\n",
      "   0.9811618   0.9823045  -0.54585359]\n",
      " [-1.05372309  1.31267601 -0.33165153 -1.26255046  0.03316818  0.32497603\n",
      "  -1.03667773 -0.86379101  0.88194479]\n",
      " [ 1.20414628 -1.2031491   0.30828805  0.8997775  -0.12099348 -0.36214307\n",
      "   0.87707712  0.9578262  -0.68111323]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.75875414  2.0940202  -2.65026727  0.33183197  0.6344556  -3.34789155\n",
      "   1.46074186]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:91 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.79756227]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 91 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.95749291 -1.32413773  0.16089359  1.12901241 -0.25381898 -0.23901669\n",
      "   1.31091052  0.35432699 -0.7100301 ]\n",
      " [-0.91668945  1.06718917 -0.38866223 -1.13716731  0.15740166 -0.02168633\n",
      "  -0.7790004  -0.51945151  0.92772383]\n",
      " [ 0.72304623 -0.64808838 -0.10717862  0.71071098 -0.40865154  0.22502274\n",
      "   0.81831358  0.07371814 -0.85321507]\n",
      " [ 0.77664265 -0.98337194  0.23186848  0.79953146 -0.28743046  0.02816661\n",
      "   0.98481992  0.9823045  -0.54585359]\n",
      " [-1.05711758  1.30928152 -0.33165153 -1.26594495  0.03316818  0.32497603\n",
      "  -1.04007222 -0.86379101  0.88194479]\n",
      " [ 1.2077394  -1.19955597  0.30828805  0.90337062 -0.12099348 -0.36214307\n",
      "   0.88067024  0.9578262  -0.68111323]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.74241167  2.10851485 -2.64785037  0.34540481  0.64796725 -3.34601154\n",
      "   1.47472171]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:91 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.84149567]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 91 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.95958826 -1.32413773  0.16298894  1.12901241 -0.25381898 -0.23692134\n",
      "   1.31300587  0.35432699 -0.7100301 ]\n",
      " [-0.91884303  1.06718917 -0.39081581 -1.13716731  0.15740166 -0.02383991\n",
      "  -0.78115398 -0.51945151  0.92772383]\n",
      " [ 0.7254019  -0.64808838 -0.10482295  0.71071098 -0.40865154  0.22737841\n",
      "   0.82066925  0.07371814 -0.85321507]\n",
      " [ 0.77884968 -0.98337194  0.23407552  0.79953146 -0.28743046  0.03037364\n",
      "   0.98702696  0.9823045  -0.54585359]\n",
      " [-1.0592726   1.30928152 -0.33380656 -1.26594495  0.03316818  0.32282101\n",
      "  -1.04222724 -0.86379101  0.88194479]\n",
      " [ 1.20993854 -1.19955597  0.31048719  0.90337062 -0.12099348 -0.35994392\n",
      "   0.88286938  0.9578262  -0.68111323]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Theta two: \n",
      "[[-1.73184096  2.11802186 -2.64670333  0.35428559  0.65730157 -3.34486227\n",
      "   1.48407021]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:91 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.10419615]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 92 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.95952214 -1.32420386  0.16298894  1.12894629 -0.25381898 -0.23692134\n",
      "   1.31300587  0.35432699 -0.71009622]\n",
      " [-0.91876882  1.06726338 -0.39081581 -1.1370931   0.15740166 -0.02383991\n",
      "  -0.78115398 -0.51945151  0.92779803]\n",
      " [ 0.72548107 -0.64800922 -0.10482295  0.71079015 -0.40865154  0.22737841\n",
      "   0.82066925  0.07371814 -0.8531359 ]\n",
      " [ 0.77878996 -0.98343166  0.23407552  0.79947174 -0.28743046  0.03037364\n",
      "   0.98702696  0.9823045  -0.54591331]\n",
      " [-1.05911044  1.30944369 -0.33380656 -1.26578279  0.03316818  0.32282101\n",
      "  -1.04222724 -0.86379101  0.88210695]\n",
      " [ 1.20965952 -1.199835    0.31048719  0.90309159 -0.12099348 -0.35994392\n",
      "   0.88286938  0.9578262  -0.68139225]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.73670376  2.11552431 -2.64906047  0.35193342  0.65481042 -3.34713102\n",
      "   1.48135726]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:92 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.86615666]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 92 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.96092082 -1.32420386  0.16438763  1.12894629 -0.25381898 -0.23692134\n",
      "   1.31440455  0.35432699 -0.71009622]\n",
      " [-0.9203568   1.06726338 -0.39240378 -1.1370931   0.15740166 -0.02383991\n",
      "  -0.78274195 -0.51945151  0.92779803]\n",
      " [ 0.72721126 -0.64800922 -0.10309276  0.71079015 -0.40865154  0.22737841\n",
      "   0.82239945  0.07371814 -0.8531359 ]\n",
      " [ 0.78041912 -0.98343166  0.23570468  0.79947174 -0.28743046  0.03037364\n",
      "   0.98865612  0.9823045  -0.54591331]\n",
      " [-1.06050934  1.30944369 -0.33520546 -1.26578279  0.03316818  0.32282101\n",
      "  -1.04362614 -0.86379101  0.88210695]\n",
      " [ 1.21107757 -1.199835    0.31190525  0.90309159 -0.12099348 -0.35994392\n",
      "   0.88428744  0.9578262  -0.68139225]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.72894558  2.12265792 -2.64820707  0.35820715  0.66164372 -3.34650624\n",
      "   1.48847196]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:92 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.55832928]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 92 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.946644   -1.33848068  0.15011081  1.12894629 -0.25381898 -0.23692134\n",
      "   1.30012773  0.35432699 -0.71009622]\n",
      " [-0.90662213  1.08099805 -0.37866911 -1.1370931   0.15740166 -0.02383991\n",
      "  -0.76900729 -0.51945151  0.92779803]\n",
      " [ 0.71544589 -0.65977458 -0.11485812  0.71079015 -0.40865154  0.22737841\n",
      "   0.81063408  0.07371814 -0.8531359 ]\n",
      " [ 0.76673207 -0.99711871  0.22201763  0.79947174 -0.28743046  0.03037364\n",
      "   0.97496907  0.9823045  -0.54591331]\n",
      " [-1.04615389  1.32379914 -0.32085001 -1.26578279  0.03316818  0.32282101\n",
      "  -1.02927069 -0.86379101  0.88210695]\n",
      " [ 1.19634979 -1.21456278  0.29717747  0.90309159 -0.12099348 -0.35994392\n",
      "   0.86955966  0.9578262  -0.68139225]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.79778694  2.07080973 -2.66634165  0.31073033  0.61102913 -3.36331593\n",
      "   1.43547479]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:92 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.77568002]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 92 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.95099942 -1.33848068  0.15446623  1.12894629 -0.25381898 -0.23692134\n",
      "   1.30012773  0.35868241 -0.71009622]\n",
      " [-0.91089503  1.08099805 -0.38294201 -1.1370931   0.15740166 -0.02383991\n",
      "  -0.76900729 -0.5237244   0.92779803]\n",
      " [ 0.71838846 -0.65977458 -0.11191556  0.71079015 -0.40865154  0.22737841\n",
      "   0.81063408  0.0766607  -0.8531359 ]\n",
      " [ 0.77086051 -0.99711871  0.22614606  0.79947174 -0.28743046  0.03037364\n",
      "   0.97496907  0.98643293 -0.54591331]\n",
      " [-1.04996791  1.32379914 -0.32466403 -1.26578279  0.03316818  0.32282101\n",
      "  -1.02927069 -0.86760503  0.88210695]\n",
      " [ 1.19984429 -1.21456278  0.30067196  0.90309159 -0.12099348 -0.35994392\n",
      "   0.86955966  0.96132069 -0.68139225]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.77827104  2.08662082 -2.66358455  0.32365897  0.62815871 -3.36142252\n",
      "   1.45344231]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:92 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.60430294]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 92 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.96147093 -1.33848068  0.15446623  1.1394178  -0.25381898 -0.23692134\n",
      "   1.30012773  0.35868241 -0.69962471]\n",
      " [-0.92072427  1.08099805 -0.38294201 -1.14692234  0.15740166 -0.02383991\n",
      "  -0.76900729 -0.5237244   0.91796879]\n",
      " [ 0.72466622 -0.65977458 -0.11191556  0.71706792 -0.40865154  0.22737841\n",
      "   0.81063408  0.0766607  -0.84685814]\n",
      " [ 0.78028128 -0.99711871  0.22614606  0.80889251 -0.28743046  0.03037364\n",
      "   0.97496907  0.98643293 -0.53649254]\n",
      " [-1.06051225  1.32379914 -0.32466403 -1.27632712  0.03316818  0.32282101\n",
      "  -1.02927069 -0.86760503  0.87156262]\n",
      " [ 1.21037747 -1.21456278  0.30067196  0.91362478 -0.12099348 -0.35994392\n",
      "   0.86955966  0.96132069 -0.67085907]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.73096132  2.1243435  -2.65194749  0.35394458  0.66297074 -3.3523139\n",
      "   1.49155408]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:92 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.05891174]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 92 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.96156317 -1.33848068  0.15446623  1.1394178  -0.25372675 -0.2368291\n",
      "   1.30012773  0.35868241 -0.69953247]\n",
      " [-0.92077744  1.08099805 -0.38294201 -1.14692234  0.15734849 -0.02389309\n",
      "  -0.76900729 -0.5237244   0.91791561]\n",
      " [ 0.72478731 -0.65977458 -0.11191556  0.71706792 -0.40853046  0.2274995\n",
      "   0.81063408  0.0766607  -0.84673706]\n",
      " [ 0.78028669 -0.99711871  0.22614606  0.80889251 -0.28742504  0.03037906\n",
      "   0.97496907  0.98643293 -0.53648712]\n",
      " [-1.06057997  1.32379914 -0.32466403 -1.27632712  0.03310045  0.32275329\n",
      "  -1.02927069 -0.86760503  0.8714949 ]\n",
      " [ 1.21035357 -1.21456278  0.30067196  0.91362478 -0.12101738 -0.35996782\n",
      "   0.86955966  0.96132069 -0.67088297]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.73259439  2.12362001 -2.65281735  0.353251    0.66215962 -3.35319847\n",
      "   1.49071364]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:92 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.00400685]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 92 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.96156496 -1.33847889  0.15446623  1.1394178  -0.25372496 -0.23682731\n",
      "   1.30012773  0.35868241 -0.69953068]\n",
      " [-0.92077916  1.08099634 -0.38294201 -1.14692234  0.15734677 -0.0238948\n",
      "  -0.76900729 -0.5237244   0.9179139 ]\n",
      " [ 0.72478885 -0.65977304 -0.11191556  0.71706792 -0.40852892  0.22750104\n",
      "   0.81063408  0.0766607  -0.84673552]\n",
      " [ 0.78028827 -0.99711713  0.22614606  0.80889251 -0.28742346  0.03038064\n",
      "   0.97496907  0.98643293 -0.53648554]\n",
      " [-1.06058176  1.32379735 -0.32466403 -1.27632712  0.03309867  0.3227515\n",
      "  -1.02927069 -0.86760503  0.87149311]\n",
      " [ 1.21035526 -1.21456109  0.30067196  0.91362478 -0.1210157  -0.35996614\n",
      "   0.86955966  0.96132069 -0.67088128]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.73260238  2.12361863 -2.65282351  0.35324879  0.66215748 -3.353205\n",
      "   1.49071173]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:92 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.22024649]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 92 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.95961213 -1.34043173  0.15446623  1.1394178  -0.25567779 -0.23878015\n",
      "   1.2981749   0.35868241 -0.69953068]\n",
      " [-0.91865404  1.08312146 -0.38294201 -1.14692234  0.1594719  -0.02176968\n",
      "  -0.76688216 -0.5237244   0.9179139 ]\n",
      " [ 0.72187097 -0.66269092 -0.11191556  0.71706792 -0.4114468   0.22458316\n",
      "   0.8077162   0.0766607  -0.84673552]\n",
      " [ 0.77806176 -0.99934365  0.22614606  0.80889251 -0.28964997  0.02815412\n",
      "   0.97274256  0.98643293 -0.53648554]\n",
      " [-1.05872163  1.32565748 -0.32466403 -1.27632712  0.0349588   0.32461163\n",
      "  -1.02741056 -0.86760503  0.87149311]\n",
      " [ 1.20860342 -1.21631293  0.30067196  0.91362478 -0.12276754 -0.36171798\n",
      "   0.86780782  0.96132069 -0.67088128]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.75151473  2.11214812 -2.66007363  0.34063437  0.65038044 -3.36074843\n",
      "   1.47946026]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:92 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.91310731]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 92 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.95997907 -1.34043173  0.15446623  1.13978474 -0.25567779 -0.23878015\n",
      "   1.29854184  0.35868241 -0.69953068]\n",
      " [-0.91916668  1.08312146 -0.38294201 -1.14743499  0.1594719  -0.02176968\n",
      "  -0.76739481 -0.5237244   0.9179139 ]\n",
      " [ 0.7225408  -0.66269092 -0.11191556  0.71773775 -0.4114468   0.22458316\n",
      "   0.80838604  0.0766607  -0.84673552]\n",
      " [ 0.77864964 -0.99934365  0.22614606  0.80948039 -0.28964997  0.02815412\n",
      "   0.97333044  0.98643293 -0.53648554]\n",
      " [-1.0590968   1.32565748 -0.32466403 -1.27670229  0.0349588   0.32461163\n",
      "  -1.02778573 -0.86760503  0.87149311]\n",
      " [ 1.20907327 -1.21631293  0.30067196  0.91409463 -0.12276754 -0.36171798\n",
      "   0.86827768  0.96132069 -0.67088128]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.74806759  2.11548363 -2.65988198  0.34375182  0.65358015 -3.36063299\n",
      "   1.48274235]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:92 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.75451537]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 92 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.96498756 -1.34043173  0.15446623  1.13978474 -0.2506693  -0.23377166\n",
      "   1.30355033  0.35868241 -0.69953068]\n",
      " [-0.92425625  1.08312146 -0.38294201 -1.14743499  0.15438233 -0.02685925\n",
      "  -0.77248438 -0.5237244   0.9179139 ]\n",
      " [ 0.7275539  -0.66269092 -0.11191556  0.71773775 -0.40643371  0.22959625\n",
      "   0.81339913  0.0766607  -0.84673552]\n",
      " [ 0.78373411 -0.99934365  0.22614606  0.80948039 -0.2845655   0.0332386\n",
      "   0.97841491  0.98643293 -0.53648554]\n",
      " [-1.06412926  1.32565748 -0.32466403 -1.27670229  0.02992633  0.31957917\n",
      "  -1.0328182  -0.86760503  0.87149311]\n",
      " [ 1.21415855 -1.21631293  0.30067196  0.91409463 -0.11768226 -0.3566327\n",
      "   0.87336296  0.96132069 -0.67088128]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.72533302  2.13489253 -2.65589772  0.36178391  0.67213499 -3.35720162\n",
      "   1.50163521]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:92 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.58773286]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 92 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94992172 -1.34043173  0.15446623  1.12471891 -0.2506693  -0.2488375\n",
      "   1.30355033  0.35868241 -0.71459652]\n",
      " [-0.9091459   1.08312146 -0.38294201 -1.13232464  0.15438233 -0.0117489\n",
      "  -0.77248438 -0.5237244   0.93302425]\n",
      " [ 0.71507631 -0.66269092 -0.11191556  0.70526017 -0.40643371  0.21711866\n",
      "   0.81339913  0.0766607  -0.8592131 ]\n",
      " [ 0.76911926 -0.99934365  0.22614606  0.79486553 -0.2845655   0.01862374\n",
      "   0.97841491  0.98643293 -0.5511004 ]\n",
      " [-1.04917382  1.32565748 -0.32466403 -1.26174685  0.02992633  0.33453461\n",
      "  -1.0328182  -0.86760503  0.88644855]\n",
      " [ 1.19947835 -1.21631293  0.30067196  0.89941443 -0.11768226 -0.3713129\n",
      "   0.87336296  0.96132069 -0.68556149]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.79653772  2.08053411 -2.6726264   0.31222799  0.61884713 -3.37432871\n",
      "   1.4482033 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:92 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.73038197]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 92 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.95574871 -1.33460474  0.15446623  1.1305459  -0.2506693  -0.24301051\n",
      "   1.30937732  0.35868241 -0.71459652]\n",
      " [-0.91501122  1.07725614 -0.38294201 -1.13818996  0.15438233 -0.01761422\n",
      "  -0.7783497  -0.5237244   0.93302425]\n",
      " [ 0.72090398 -0.65686325 -0.11191556  0.71108784 -0.40643371  0.22294634\n",
      "   0.8192268   0.0766607  -0.8592131 ]\n",
      " [ 0.77506171 -0.99340119  0.22614606  0.80080798 -0.2845655   0.02456619\n",
      "   0.98435736  0.98643293 -0.5511004 ]\n",
      " [-1.05507757  1.31975373 -0.32466403 -1.2676506   0.02992633  0.32863086\n",
      "  -1.03872195 -0.86760503  0.88644855]\n",
      " [ 1.2053655  -1.21042578  0.30067196  0.90530158 -0.11768226 -0.36542575\n",
      "   0.87925011  0.96132069 -0.68556149]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.76999057  2.10327958 -2.66867126  0.33497099  0.64079085 -3.37017072\n",
      "   1.46943395]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:92 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.79911451]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 92 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.9590781  -1.33127534  0.15446623  1.13387529 -0.2506693  -0.24301051\n",
      "   1.31270671  0.35868241 -0.71459652]\n",
      " [-0.91856819  1.07369917 -0.38294201 -1.14174693  0.15438233 -0.01761422\n",
      "  -0.78190667 -0.5237244   0.93302425]\n",
      " [ 0.72451045 -0.65325679 -0.11191556  0.7146943  -0.40643371  0.22294634\n",
      "   0.82283327  0.0766607  -0.8592131 ]\n",
      " [ 0.77867075 -0.98979216  0.22614606  0.80441702 -0.2845655   0.02456619\n",
      "   0.9879664   0.98643293 -0.5511004 ]\n",
      " [-1.05842538  1.31640593 -0.32466403 -1.27099841  0.02992633  0.32863086\n",
      "  -1.04206975 -0.86760503  0.88644855]\n",
      " [ 1.20890975 -1.20688153  0.30067196  0.90884583 -0.11768226 -0.36542575\n",
      "   0.88279436  0.96132069 -0.68556149]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.75386645  2.11758266 -2.66629285  0.34837381  0.65413083 -3.36831816\n",
      "   1.4832303 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:92 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.84165714]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 92 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.96117462 -1.33127534  0.15656274  1.13387529 -0.2506693  -0.240914\n",
      "   1.31480322  0.35868241 -0.71459652]\n",
      " [-0.92072111  1.07369917 -0.38509493 -1.14174693  0.15438233 -0.01976713\n",
      "  -0.78405959 -0.5237244   0.93302425]\n",
      " [ 0.72686193 -0.65325679 -0.10956408  0.7146943  -0.40643371  0.22529781\n",
      "   0.82518475  0.0766607  -0.8592131 ]\n",
      " [ 0.78087618 -0.98979216  0.2283515   0.80441702 -0.2845655   0.02677163\n",
      "   0.99017183  0.98643293 -0.5511004 ]\n",
      " [-1.06058125  1.31640593 -0.32681991 -1.27099841  0.02992633  0.32647498\n",
      "  -1.04422563 -0.86760503  0.88644855]\n",
      " [ 1.21110944 -1.20688153  0.30287165  0.90884583 -0.11768226 -0.36322606\n",
      "   0.88499405  0.96132069 -0.68556149]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.74331524  2.12706539 -2.66514281  0.35723704  0.6634434  -3.36716349\n",
      "   1.49255332]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:92 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.10135412]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 93 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.9611179  -1.33133205  0.15656274  1.13381858 -0.2506693  -0.240914\n",
      "   1.31480322  0.35868241 -0.71465324]\n",
      " [-0.92065683  1.07376344 -0.38509493 -1.14168265  0.15438233 -0.01976713\n",
      "  -0.78405959 -0.5237244   0.93308853]\n",
      " [ 0.72694365 -0.65317506 -0.10956408  0.71477603 -0.40643371  0.22529781\n",
      "   0.82518475  0.0766607  -0.85913138]\n",
      " [ 0.78082497 -0.98984337  0.2283515   0.80436581 -0.2845655   0.02677163\n",
      "   0.99017183  0.98643293 -0.55115161]\n",
      " [-1.06043333  1.31655385 -0.32681991 -1.27085048  0.02992633  0.32647498\n",
      "  -1.04422563 -0.86760503  0.88659648]\n",
      " [ 1.21085027 -1.20714069  0.30287165  0.90858666 -0.11768226 -0.36322606\n",
      "   0.88499405  0.96132069 -0.68582066]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.74793098  2.12470079 -2.66738637  0.35501096  0.6610843  -3.36932302\n",
      "   1.48998404]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:93 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.86699372]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 93 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.96250227 -1.33133205  0.15794711  1.13381858 -0.2506693  -0.240914\n",
      "   1.3161876   0.35868241 -0.71465324]\n",
      " [-0.92222696  1.07376344 -0.38666505 -1.14168265  0.15438233 -0.01976713\n",
      "  -0.78562971 -0.5237244   0.93308853]\n",
      " [ 0.72865408 -0.65317506 -0.10785365  0.71477603 -0.40643371  0.22529781\n",
      "   0.82689518  0.0766607  -0.85913138]\n",
      " [ 0.7824356  -0.98984337  0.22996213  0.80436581 -0.2845655   0.02677163\n",
      "   0.99178247  0.98643293 -0.55115161]\n",
      " [-1.0618183   1.31655385 -0.32820488 -1.27085048  0.02992633  0.32647498\n",
      "  -1.0456106  -0.86760503  0.88659648]\n",
      " [ 1.21225452 -1.20714069  0.30427589  0.90858666 -0.11768226 -0.36322606\n",
      "   0.88639829  0.96132069 -0.68582066]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.74026213  2.13175054 -2.6665421   0.36121392  0.66783848 -3.36870335\n",
      "   1.4970143 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:93 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.55326557]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 93 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.94837972 -1.34545461  0.14382456  1.13381858 -0.2506693  -0.240914\n",
      "   1.30206504  0.35868241 -0.71465324]\n",
      " [-0.90863704  1.08735336 -0.37307514 -1.14168265  0.15438233 -0.01976713\n",
      "  -0.7720398  -0.5237244   0.93308853]\n",
      " [ 0.71700951 -0.66481964 -0.11949823  0.71477603 -0.40643371  0.22529781\n",
      "   0.8152506   0.0766607  -0.85913138]\n",
      " [ 0.76889045 -1.00338852  0.21641698  0.80436581 -0.2845655   0.02677163\n",
      "   0.97823731  0.98643293 -0.55115161]\n",
      " [-1.04761888  1.33075327 -0.31400546 -1.27085048  0.02992633  0.32647498\n",
      "  -1.03141118 -0.86760503  0.88659648]\n",
      " [ 1.19767697 -1.22171824  0.28969835  0.90858666 -0.11768226 -0.36322606\n",
      "   0.87182075  0.96132069 -0.68582066]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.80863546  2.08038489 -2.6846528   0.31411829  0.61766148 -3.38553563\n",
      "   1.444519  ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:93 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.7770712]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 93 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.95268895 -1.34545461  0.14813379  1.13381858 -0.2506693  -0.240914\n",
      "   1.30206504  0.36299164 -0.71465324]\n",
      " [-0.9128642   1.08735336 -0.37730229 -1.14168265  0.15438233 -0.01976713\n",
      "  -0.7720398  -0.52795156  0.93308853]\n",
      " [ 0.71992045 -0.66481964 -0.11658728  0.71477603 -0.40643371  0.22529781\n",
      "   0.8152506   0.07957165 -0.85913138]\n",
      " [ 0.77297445 -1.00338852  0.22050098  0.80436581 -0.2845655   0.02677163\n",
      "   0.97823731  0.99051693 -0.55115161]\n",
      " [-1.05139461  1.33075327 -0.31778119 -1.27085048  0.02992633  0.32647498\n",
      "  -1.03141118 -0.87138076  0.88659648]\n",
      " [ 1.20113843 -1.22171824  0.2931598   0.90858666 -0.11768226 -0.36322606\n",
      "   0.87182075  0.96478215 -0.68582066]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.78932631  2.0960279  -2.68192653  0.32690938  0.63461101 -3.38365963\n",
      "   1.46229241]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:93 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.60542537]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 93 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.96312329 -1.34545461  0.14813379  1.14425292 -0.2506693  -0.240914\n",
      "   1.30206504  0.36299164 -0.70421889]\n",
      " [-0.92266068  1.08735336 -0.37730229 -1.15147913  0.15438233 -0.01976713\n",
      "  -0.7720398  -0.52795156  0.92329205]\n",
      " [ 0.72616989 -0.66481964 -0.11658728  0.72102548 -0.40643371  0.22529781\n",
      "   0.8152506   0.07957165 -0.85288193]\n",
      " [ 0.78236761 -1.00338852  0.22050098  0.81375897 -0.2845655   0.02677163\n",
      "   0.97823731  0.99051693 -0.54175845]\n",
      " [-1.0619004   1.33075327 -0.31778119 -1.28135628  0.02992633  0.32647498\n",
      "  -1.03141118 -0.87138076  0.87609068]\n",
      " [ 1.21163366 -1.22171824  0.2931598   0.91908189 -0.11768226 -0.36322606\n",
      "   0.87182075  0.96478215 -0.67532543]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.74219723  2.13362181 -2.67034499  0.35707418  0.66930632 -3.37460047\n",
      "   1.50027608]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:93 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.0572606]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 93 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.96321199 -1.34545461  0.14813379  1.14425292 -0.25058061 -0.2408253\n",
      "   1.30206504  0.36299164 -0.7041302 ]\n",
      " [-0.9227127   1.08735336 -0.37730229 -1.15147913  0.15433031 -0.01981915\n",
      "  -0.7720398  -0.52795156  0.92324003]\n",
      " [ 0.72628607 -0.66481964 -0.11658728  0.72102548 -0.40631753  0.22541399\n",
      "   0.8152506   0.07957165 -0.85276576]\n",
      " [ 0.78237425 -1.00338852  0.22050098  0.81375897 -0.28455886  0.02677827\n",
      "   0.97823731  0.99051693 -0.54175181]\n",
      " [-1.06196584  1.33075327 -0.31778119 -1.28135628  0.0298609   0.32640955\n",
      "  -1.03141118 -0.87138076  0.87602525]\n",
      " [ 1.21161227 -1.22171824  0.2931598   0.91908189 -0.11770364 -0.36324745\n",
      "   0.87182075  0.96478215 -0.67534682]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.74374275  2.13293855 -2.67116993  0.35641943  0.6685402  -3.37543899\n",
      "   1.49948193]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:93 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.00379085]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 93 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.96321359 -1.345453    0.14813379  1.14425292 -0.25057901 -0.2408237\n",
      "   1.30206504  0.36299164 -0.70412859]\n",
      " [-0.92271423  1.08735182 -0.37730229 -1.15147913  0.15432877 -0.01982069\n",
      "  -0.7720398  -0.52795156  0.92323849]\n",
      " [ 0.72628745 -0.66481825 -0.11658728  0.72102548 -0.40631615  0.22541537\n",
      "   0.8152506   0.07957165 -0.85276437]\n",
      " [ 0.78237568 -1.0033871   0.22050098  0.81375897 -0.28455744  0.02677969\n",
      "   0.97823731  0.99051693 -0.54175039]\n",
      " [-1.06196744  1.33075167 -0.31778119 -1.28135628  0.0298593   0.32640794\n",
      "  -1.03141118 -0.87138076  0.87602365]\n",
      " [ 1.21161378 -1.22171672  0.2931598   0.91908189 -0.11770213 -0.36324594\n",
      "   0.87182075  0.96478215 -0.6753453 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.7437499   2.13293732 -2.67117546  0.35641747  0.66853831 -3.37544484\n",
      "   1.49948023]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:93 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.21710617]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 93 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.96132537 -1.34734122  0.14813379  1.14425292 -0.25246722 -0.24271192\n",
      "   1.30017683  0.36299164 -0.70412859]\n",
      " [-0.92065045  1.0894156  -0.37730229 -1.15147913  0.15639255 -0.01775691\n",
      "  -0.76997602 -0.52795156  0.92323849]\n",
      " [ 0.723437   -0.6676687  -0.11658728  0.72102548 -0.4091666   0.22256492\n",
      "   0.81240015  0.07957165 -0.85276437]\n",
      " [ 0.78020976 -1.00555301  0.22050098  0.81375897 -0.28672335  0.02461378\n",
      "   0.9760714   0.99051693 -0.54175039]\n",
      " [-1.06016831  1.3325508  -0.31778119 -1.28135628  0.03165842  0.32820707\n",
      "  -1.02961205 -0.87138076  0.87602365]\n",
      " [ 1.20991955 -1.22341095  0.2931598   0.91908189 -0.11939636 -0.36494017\n",
      "   0.87012652  0.96478215 -0.6753453 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.76220079  2.12176537 -2.67825933  0.34410593  0.65705578 -3.38282123\n",
      "   1.48851932]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:93 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.91460784]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 93 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.96167839 -1.34734122  0.14813379  1.14460594 -0.25246722 -0.24271192\n",
      "   1.30052985  0.36299164 -0.70412859]\n",
      " [-0.9211438   1.0894156  -0.37730229 -1.15197248  0.15639255 -0.01775691\n",
      "  -0.77046936 -0.52795156  0.92323849]\n",
      " [ 0.72408255 -0.6676687  -0.11658728  0.72167103 -0.4091666   0.22256492\n",
      "   0.8130457   0.07957165 -0.85276437]\n",
      " [ 0.78077571 -1.00555301  0.22050098  0.81432491 -0.28672335  0.02461378\n",
      "   0.97663734  0.99051693 -0.54175039]\n",
      " [-1.06052924  1.3325508  -0.31778119 -1.2817172   0.03165842  0.32820707\n",
      "  -1.02997298 -0.87138076  0.87602365]\n",
      " [ 1.21037171 -1.22341095  0.2931598   0.91953405 -0.11939636 -0.36494017\n",
      "   0.87057868  0.96478215 -0.6753453 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.75886621  2.12499285 -2.67807563  0.34712453  0.66015329 -3.38271049\n",
      "   1.49169562]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:93 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.75668063]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 93 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.96661097 -1.34734122  0.14813379  1.14460594 -0.24753465 -0.23777934\n",
      "   1.30546242  0.36299164 -0.70412859]\n",
      " [-0.92615822  1.0894156  -0.37730229 -1.15197248  0.15137813 -0.02277133\n",
      "  -0.77548378 -0.52795156  0.92323849]\n",
      " [ 0.72902668 -0.6676687  -0.11658728  0.72167103 -0.40422247  0.22750905\n",
      "   0.81798983  0.07957165 -0.85276437]\n",
      " [ 0.7857861  -1.00555301  0.22050098  0.81432491 -0.28171295  0.02962417\n",
      "   0.98164774  0.99051693 -0.54175039]\n",
      " [-1.06548553  1.3325508  -0.31778119 -1.2817172   0.02670213  0.32325078\n",
      "  -1.03492926 -0.87138076  0.87602365]\n",
      " [ 1.21538135 -1.22341095  0.2931598   0.91953405 -0.11438672 -0.35993053\n",
      "   0.87558832  0.96478215 -0.6753453 ]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.73646683  2.14412389 -2.67416343  0.36491446  0.67845074 -3.37933925\n",
      "   1.5103217 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:93 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.58665388]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 93 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.95157177 -1.34734122  0.14813379  1.12956675 -0.24753465 -0.25281854\n",
      "   1.30546242  0.36299164 -0.71916779]\n",
      " [-0.91107818  1.0894156  -0.37730229 -1.13689244  0.15137813 -0.00769129\n",
      "  -0.77548378 -0.52795156  0.93831853]\n",
      " [ 0.71658993 -0.6676687  -0.11658728  0.70923428 -0.40422247  0.2150723\n",
      "   0.81798983  0.07957165 -0.86520112]\n",
      " [ 0.77119896 -1.00555301  0.22050098  0.79973776 -0.28171295  0.01503703\n",
      "   0.98164774  0.99051693 -0.55633753]\n",
      " [-1.05055557  1.3325508  -0.31778119 -1.26678724  0.02670213  0.33818074\n",
      "  -1.03492926 -0.87138076  0.8909536 ]\n",
      " [ 1.2007233  -1.22341095  0.2931598   0.904876   -0.11438672 -0.37458858\n",
      "   0.87558832  0.96478215 -0.69000335]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.80759601  2.0898509  -2.6909124   0.31545192  0.62524597 -3.39647197\n",
      "   1.45696112]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:93 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.73112472]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 93 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.95737481 -1.34153818  0.14813379  1.13536979 -0.24753465 -0.2470155\n",
      "   1.31126546  0.36299164 -0.71916779]\n",
      " [-0.9169178   1.08357599 -0.37730229 -1.14273206  0.15137813 -0.01353091\n",
      "  -0.7813234  -0.52795156  0.93831853]\n",
      " [ 0.72238869 -0.66186994 -0.11658728  0.71503304 -0.40422247  0.22087106\n",
      "   0.82378859  0.07957165 -0.86520112]\n",
      " [ 0.77711469 -0.99963727  0.22050098  0.8056535  -0.28171295  0.02095276\n",
      "   0.98756348  0.99051693 -0.55633753]\n",
      " [-1.05643389  1.32667247 -0.31778119 -1.27266557  0.02670213  0.33230241\n",
      "  -1.04080759 -0.87138076  0.8909536 ]\n",
      " [ 1.20658297 -1.21755128  0.2931598   0.91073567 -0.11438672 -0.36872891\n",
      "   0.88144799  0.96478215 -0.69000335]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.78116807  2.11248617 -2.68697217  0.33810268  0.64709262 -3.39232565\n",
      "   1.47809033]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:93 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.80067515]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 93 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.96065775 -1.33825525  0.14813379  1.13865272 -0.24753465 -0.2470155\n",
      "   1.31454839  0.36299164 -0.71916779]\n",
      " [-0.92042508  1.08006871 -0.37730229 -1.14623934  0.15137813 -0.01353091\n",
      "  -0.78483068 -0.52795156  0.93831853]\n",
      " [ 0.72594562 -0.65831302 -0.11658728  0.71858996 -0.40422247  0.22087106\n",
      "   0.82734551  0.07957165 -0.86520112]\n",
      " [ 0.78067457 -0.9960774   0.22050098  0.80921337 -0.28171295  0.02095276\n",
      "   0.99112335  0.99051693 -0.55633753]\n",
      " [-1.0597349   1.32337147 -0.31778119 -1.27596657  0.02670213  0.33230241\n",
      "  -1.04410859 -0.87138076  0.8909536 ]\n",
      " [ 1.21007828 -1.21405597  0.2931598   0.91423098 -0.11438672 -0.36872891\n",
      "   0.8849433   0.96478215 -0.69000335]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.7652625   2.12659762 -2.68463219  0.35133494  0.66026057 -3.39050068\n",
      "   1.49170306]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:93 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.84181304]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 93 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.96275542 -1.33825525  0.15023146  1.13865272 -0.24753465 -0.24491783\n",
      "   1.31664606  0.36299164 -0.71916779]\n",
      " [-0.9225774   1.08006871 -0.37945461 -1.14623934  0.15137813 -0.01568323\n",
      "  -0.786983   -0.52795156  0.93831853]\n",
      " [ 0.72829305 -0.65831302 -0.11423986  0.71858996 -0.40422247  0.22321849\n",
      "   0.82969294  0.07957165 -0.86520112]\n",
      " [ 0.78287849 -0.9960774   0.22270491  0.80921337 -0.28171295  0.02315669\n",
      "   0.99332728  0.99051693 -0.55633753]\n",
      " [-1.06189163  1.32337147 -0.31993792 -1.27596657  0.02670213  0.33014568\n",
      "  -1.04626532 -0.87138076  0.8909536 ]\n",
      " [ 1.21227853 -1.21405597  0.29536006  0.91423098 -0.11438672 -0.36652866\n",
      "   0.88714355  0.96478215 -0.69000335]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.75473011  2.1360568  -2.68347913  0.36018123  0.66955205 -3.38934065\n",
      "   1.50100131]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:93 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.09858752]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 94 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.96270727 -1.3383034   0.15023146  1.13860457 -0.24753465 -0.24491783\n",
      "   1.31664606  0.36299164 -0.71921593]\n",
      " [-0.9225222   1.0801239  -0.37945461 -1.14618414  0.15137813 -0.01568323\n",
      "  -0.786983   -0.52795156  0.93837372]\n",
      " [ 0.72837685 -0.65822922 -0.11423986  0.71867376 -0.40422247  0.22321849\n",
      "   0.82969294  0.07957165 -0.86511732]\n",
      " [ 0.78283506 -0.99612084  0.22270491  0.80916994 -0.28171295  0.02315669\n",
      "   0.99332728  0.99051693 -0.55638097]\n",
      " [-1.06175685  1.32350624 -0.31993792 -1.2758318   0.02670213  0.33014568\n",
      "  -1.04626532 -0.87138076  0.89108838]\n",
      " [ 1.2120379  -1.2142966   0.29536006  0.91399035 -0.11438672 -0.36652866\n",
      "   0.88714355  0.96478215 -0.69024398]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.75911075  2.13381831 -2.68561424  0.35807479  0.66731828 -3.39139585\n",
      "   1.49856837]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:94 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.86782594]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 94 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.96407738 -1.3383034   0.15160157  1.13860457 -0.24753465 -0.24491783\n",
      "   1.31801617  0.36299164 -0.71921593]\n",
      " [-0.92407463  1.0801239  -0.38100704 -1.14618414  0.15137813 -0.01568323\n",
      "  -0.78853542 -0.52795156  0.93837372]\n",
      " [ 0.73006772 -0.65822922 -0.11254898  0.71867376 -0.40422247  0.22321849\n",
      "   0.83138382  0.07957165 -0.86511732]\n",
      " [ 0.78442733 -0.99612084  0.22429717  0.80916994 -0.28171295  0.02315669\n",
      "   0.99491955  0.99051693 -0.55638097]\n",
      " [-1.06312791  1.32350624 -0.32130898 -1.2758318   0.02670213  0.33014568\n",
      "  -1.04763638 -0.87138076  0.89108838]\n",
      " [ 1.21342837 -1.2142966   0.29675052  0.91399035 -0.11438672 -0.36652866\n",
      "   0.88853402  0.96478215 -0.69024398]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.7515303   2.14078517 -2.68477906  0.36420773  0.67399422 -3.39078134\n",
      "   1.50551517]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:94 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.54823944]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 94 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.95011086 -1.35226992  0.13763505  1.13860457 -0.24753465 -0.24491783\n",
      "   1.30404966  0.36299164 -0.71921593]\n",
      " [-0.91063071  1.09356782 -0.36756311 -1.14618414  0.15137813 -0.01568323\n",
      "  -0.7750915  -0.52795156  0.93837372]\n",
      " [ 0.71854441 -0.66975253 -0.12407229  0.71867376 -0.40422247  0.22321849\n",
      "   0.8198605   0.07957165 -0.86511732]\n",
      " [ 0.77102534 -1.00952282  0.21089519  0.80916994 -0.28171295  0.02315669\n",
      "   0.98151756  0.99051693 -0.55638097]\n",
      " [-1.04908645  1.33754771 -0.30726751 -1.2758318   0.02670213  0.33014568\n",
      "  -1.03359491 -0.87138076  0.89108838]\n",
      " [ 1.19900384 -1.22872113  0.282326    0.91399035 -0.11438672 -0.36652866\n",
      "   0.8741095   0.96478215 -0.69024398]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.81942234  2.08990717 -2.7028585   0.31749964  0.62426091 -3.40762795\n",
      "   1.45352746]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:94 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.77846002]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 94 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.95437413 -1.35226992  0.14189832  1.13860457 -0.24753465 -0.24491783\n",
      "   1.30404966  0.3672549  -0.71921593]\n",
      " [-0.91481233  1.09356782 -0.37174474 -1.14618414  0.15137813 -0.01568323\n",
      "  -0.7750915  -0.53213318  0.93837372]\n",
      " [ 0.7214239  -0.66975253 -0.1211928   0.71867376 -0.40422247  0.22321849\n",
      "   0.8198605   0.08245114 -0.86511732]\n",
      " [ 0.77506513 -1.00952282  0.21493497  0.80916994 -0.28171295  0.02315669\n",
      "   0.98151756  0.99455672 -0.55638097]\n",
      " [-1.05282394  1.33754771 -0.311005   -1.2758318   0.02670213  0.33014568\n",
      "  -1.03359491 -0.87511826  0.89108838]\n",
      " [ 1.20243224 -1.22872113  0.28575439  0.91399035 -0.11438672 -0.36652866\n",
      "   0.8741095   0.96821054 -0.69024398]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.80031894  2.10538306 -2.70016292  0.33015389  0.64103126 -3.40576943\n",
      "   1.47110787]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:94 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.60651815]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 94 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.96477212 -1.35226992  0.14189832  1.14900256 -0.24753465 -0.24491783\n",
      "   1.30404966  0.3672549  -0.70881794]\n",
      " [-0.92457655  1.09356782 -0.37174474 -1.15594837  0.15137813 -0.01568323\n",
      "  -0.7750915  -0.53213318  0.92860949]\n",
      " [ 0.72764469 -0.66975253 -0.1211928   0.72489455 -0.40422247  0.22321849\n",
      "   0.8198605   0.08245114 -0.85889653]\n",
      " [ 0.784431   -1.00952282  0.21493497  0.81853582 -0.28171295  0.02315669\n",
      "   0.98151756  0.99455672 -0.54701509]\n",
      " [-1.06329209  1.33754771 -0.311005   -1.28629995  0.02670213  0.33014568\n",
      "  -1.03359491 -0.87511826  0.88062022]\n",
      " [ 1.21289038 -1.22872113  0.28575439  0.9244485  -0.11438672 -0.36652866\n",
      "   0.8741095   0.96821054 -0.67978584]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.75336596  2.14285095 -2.68863478  0.36019965  0.67561212 -3.3967581\n",
      "   1.5089663 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:94 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.0556488]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 94 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.96485737 -1.35226992  0.14189832  1.14900256 -0.24744939 -0.24483257\n",
      "   1.30404966  0.3672549  -0.70873269]\n",
      " [-0.92462738  1.09356782 -0.37174474 -1.15594837  0.1513273  -0.01573406\n",
      "  -0.7750915  -0.53213318  0.92855866]\n",
      " [ 0.7277561  -0.66975253 -0.1211928   0.72489455 -0.40411106  0.2233299\n",
      "   0.8198605   0.08245114 -0.85878512]\n",
      " [ 0.78443873 -1.00952282  0.21493497  0.81853582 -0.28170523  0.02316442\n",
      "   0.98151756  0.99455672 -0.54700737]\n",
      " [-1.06335528  1.33754771 -0.311005   -1.28629995  0.02663894  0.33008249\n",
      "  -1.03359491 -0.87511826  0.88055703]\n",
      " [ 1.21287132 -1.22872113  0.28575439  0.9244485  -0.11440579 -0.36654773\n",
      "   0.8741095   0.96821054 -0.67980491]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.75482818  2.14220589 -2.68941689  0.35958176  0.67488873 -3.39755272\n",
      "   1.50821612]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:94 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.00358751]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 94 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.96485881 -1.35226848  0.14189832  1.14900256 -0.24744796 -0.24483114\n",
      "   1.30404966  0.3672549  -0.70873125]\n",
      " [-0.92462877  1.09356644 -0.37174474 -1.15594837  0.15132592 -0.01573544\n",
      "  -0.7750915  -0.53213318  0.92855728]\n",
      " [ 0.72775735 -0.66975128 -0.1211928   0.72489455 -0.40410981  0.22333114\n",
      "   0.8198605   0.08245114 -0.85878388]\n",
      " [ 0.78444001 -1.00952154  0.21493497  0.81853582 -0.28170394  0.0231657\n",
      "   0.98151756  0.99455672 -0.54700609]\n",
      " [-1.06335672  1.33754627 -0.311005   -1.28629995  0.02663751  0.33008106\n",
      "  -1.03359491 -0.87511826  0.8805556 ]\n",
      " [ 1.21287267 -1.22871977  0.28575439  0.9244485  -0.11440443 -0.36654637\n",
      "   0.8741095   0.96821054 -0.67980355]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.7548346   2.1422048  -2.68942186  0.35958001  0.67488705 -3.39755798\n",
      "   1.50821461]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:94 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.21405721]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 94 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.96303202 -1.35409527  0.14189832  1.14900256 -0.24927475 -0.24665793\n",
      "   1.30222287  0.3672549  -0.70873125]\n",
      " [-0.92262357  1.09557163 -0.37174474 -1.15594837  0.15333111 -0.01373025\n",
      "  -0.77308631 -0.53213318  0.92855728]\n",
      " [ 0.72497172 -0.67253691 -0.1211928   0.72489455 -0.40689544  0.22054551\n",
      "   0.81707487  0.08245114 -0.85878388]\n",
      " [ 0.78233207 -1.01162948  0.21493497  0.81853582 -0.28381189  0.02105775\n",
      "   0.97940962  0.99455672 -0.54700609]\n",
      " [-1.06161555  1.33928744 -0.311005   -1.28629995  0.02837867  0.33182222\n",
      "  -1.03185375 -0.87511826  0.8805556 ]\n",
      " [ 1.21123324 -1.2303592   0.28575439  0.9244485  -0.11604387 -0.3681858\n",
      "   0.87247006  0.96821054 -0.67980355]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.77284074  2.13131962 -2.69634491  0.34756016  0.66368785 -3.40477244\n",
      "   1.49753292]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:94 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.91607595]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 94 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.96337169 -1.35409527  0.14189832  1.14934223 -0.24927475 -0.24665793\n",
      "   1.30256254  0.3672549  -0.70873125]\n",
      " [-0.92309841  1.09557163 -0.37174474 -1.15642321  0.15333111 -0.01373025\n",
      "  -0.77356115 -0.53213318  0.92855728]\n",
      " [ 0.72559394 -0.67253691 -0.1211928   0.72551678 -0.40689544  0.22054551\n",
      "   0.8176971   0.08245114 -0.85878388]\n",
      " [ 0.78287697 -1.01162948  0.21493497  0.81908071 -0.28381189  0.02105775\n",
      "   0.97995452  0.99455672 -0.54700609]\n",
      " [-1.06196282  1.33928744 -0.311005   -1.28664721  0.02837867  0.33182222\n",
      "  -1.03220101 -0.87511826  0.8805556 ]\n",
      " [ 1.21166843 -1.2303592   0.28575439  0.92488369 -0.11604387 -0.3681858\n",
      "   0.87290525  0.96821054 -0.67980355]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.76961466  2.13444293 -2.69616878  0.35048331  0.66668675 -3.4046662\n",
      "   1.50060718]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:94 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.75883169]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 94 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.96822915 -1.35409527  0.14189832  1.14934223 -0.24441729 -0.24180047\n",
      "   1.30742     0.3672549  -0.70873125]\n",
      " [-0.92803841  1.09557163 -0.37174474 -1.15642321  0.14839112 -0.01867024\n",
      "  -0.77850114 -0.53213318  0.92855728]\n",
      " [ 0.73046951 -0.67253691 -0.1211928   0.72551678 -0.40201987  0.22542108\n",
      "   0.82257267  0.08245114 -0.85878388]\n",
      " [ 0.78781394 -1.01162948  0.21493497  0.81908071 -0.27887491  0.02599473\n",
      "   0.98489149  0.99455672 -0.54700609]\n",
      " [-1.06684372  1.33928744 -0.311005   -1.28664721  0.02349777  0.32694132\n",
      "  -1.03708191 -0.87511826  0.8805556 ]\n",
      " [ 1.21660318 -1.2303592   0.28575439  0.92488369 -0.11110912 -0.36325106\n",
      "   0.87784     0.96821054 -0.67980355]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.74754702  2.1532988  -2.69232749  0.36803287  0.68472895 -3.40135425\n",
      "   1.51896892]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:94 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.58550671]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 94 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.9532184  -1.35409527  0.14189832  1.13433148 -0.24441729 -0.25681122\n",
      "   1.30742     0.3672549  -0.723742  ]\n",
      " [-0.91299056  1.09557163 -0.37174474 -1.14137537  0.14839112 -0.0036224\n",
      "  -0.77850114 -0.53213318  0.94360513]\n",
      " [ 0.71807577 -0.67253691 -0.1211928   0.71312304 -0.40201987  0.21302735\n",
      "   0.82257267  0.08245114 -0.87117762]\n",
      " [ 0.77325645 -1.01162948  0.21493497  0.80452322 -0.27887491  0.01143724\n",
      "   0.98489149  0.99455672 -0.56156358]\n",
      " [-1.05194102  1.33928744 -0.311005   -1.27174452  0.02349777  0.34184401\n",
      "  -1.03708191 -0.87511826  0.89545829]\n",
      " [ 1.20196911 -1.2303592   0.28575439  0.91024963 -0.11110912 -0.37788512\n",
      "   0.87784     0.96821054 -0.69443761]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.81859492  2.099117   -2.70909686  0.31866929  0.63161313 -3.41849254\n",
      "   1.46568539]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:94 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.73187997]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 94 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.95899694 -1.34831673  0.14189832  1.14011002 -0.24441729 -0.25103268\n",
      "   1.31319854  0.3672549  -0.723742  ]\n",
      " [-0.91880402  1.08975818 -0.37174474 -1.14718882  0.14839112 -0.00943585\n",
      "  -0.7843146  -0.53213318  0.94360513]\n",
      " [ 0.7238452  -0.66676748 -0.1211928   0.71889247 -0.40201987  0.21879677\n",
      "   0.8283421   0.08245114 -0.87117762]\n",
      " [ 0.77914505 -1.00574089  0.21493497  0.81041182 -0.27887491  0.01732584\n",
      "   0.99078009  0.99455672 -0.56156358]\n",
      " [-1.05779342  1.33343504 -0.311005   -1.27759692  0.02349777  0.33599161\n",
      "  -1.04293431 -0.87511826  0.89545829]\n",
      " [ 1.20780093 -1.22452739  0.28575439  0.91608145 -0.11110912 -0.3720533\n",
      "   0.88367182  0.96821054 -0.69443761]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.7922881   2.12164084 -2.70517198  0.34122605  0.65336121 -3.41435859\n",
      "   1.48671197]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:94 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.80224397]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 94 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.9622333  -1.34508037  0.14189832  1.14334639 -0.24441729 -0.25103268\n",
      "   1.3164349   0.3672549  -0.723742  ]\n",
      " [-0.92226157  1.08630063 -0.37174474 -1.15064638  0.14839112 -0.00943585\n",
      "  -0.78777215 -0.53213318  0.94360513]\n",
      " [ 0.72735252 -0.66326017 -0.1211928   0.72239978 -0.40201987  0.21879677\n",
      "   0.83184941  0.08245114 -0.87117762]\n",
      " [ 0.78265569 -1.00223024  0.21493497  0.81392246 -0.27887491  0.01732584\n",
      "   0.99429073  0.99455672 -0.56156358]\n",
      " [-1.06104752  1.33018094 -0.311005   -1.28085102  0.02349777  0.33599161\n",
      "  -1.04618841 -0.87511826  0.89545829]\n",
      " [ 1.21124724 -1.22108108  0.28575439  0.91952775 -0.11110912 -0.3720533\n",
      "   0.88711812  0.96821054 -0.69443761]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.77660124  2.13556063 -2.70287038  0.35428724  0.6663568  -3.41256135\n",
      "   1.50014102]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:94 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.84196413]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 94 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.96433212 -1.34508037  0.14399713  1.14334639 -0.24441729 -0.24893387\n",
      "   1.31853372  0.3672549  -0.723742  ]\n",
      " [-0.92441334  1.08630063 -0.37389651 -1.15064638  0.14839112 -0.01158762\n",
      "  -0.78992392 -0.53213318  0.94360513]\n",
      " [ 0.72969602 -0.66326017 -0.11884929  0.72239978 -0.40201987  0.22114028\n",
      "   0.83419292  0.08245114 -0.87117762]\n",
      " [ 0.78485818 -1.00223024  0.21713746  0.81392246 -0.27887491  0.01952832\n",
      "   0.99649322  0.99455672 -0.56156358]\n",
      " [-1.06320508  1.33018094 -0.31316257 -1.28085102  0.02349777  0.33383405\n",
      "  -1.04834597 -0.87511826  0.89545829]\n",
      " [ 1.21344805 -1.22108108  0.2879552   0.91952775 -0.11110912 -0.36985249\n",
      "   0.88931893  0.96821054 -0.69443761]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.76608708  2.14499689 -2.7017143   0.36311713  0.67562777 -3.41139602\n",
      "   1.50941513]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:94 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.0958953]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 95 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.96429175 -1.34512073  0.14399713  1.14330602 -0.24441729 -0.24893387\n",
      "   1.31853372  0.3672549  -0.72378237]\n",
      " [-0.92436644  1.08634753 -0.37389651 -1.15059947  0.14839112 -0.01158762\n",
      "  -0.78992392 -0.53213318  0.94365203]\n",
      " [ 0.72978145 -0.66317474 -0.11884929  0.72248521 -0.40201987  0.22114028\n",
      "   0.83419292  0.08245114 -0.87109219]\n",
      " [ 0.78482183 -1.00226659  0.21713746  0.81388611 -0.27887491  0.01952832\n",
      "   0.99649322  0.99455672 -0.56159993]\n",
      " [-1.06308245  1.33030358 -0.31316257 -1.28072838  0.02349777  0.33383405\n",
      "  -1.04834597 -0.87511826  0.89558092]\n",
      " [ 1.21322471 -1.22130442  0.2879552   0.91930441 -0.11110912 -0.36985249\n",
      "   0.88931893  0.96821054 -0.69466095]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.77024411  2.14287799 -2.7037459   0.36112414  0.6735129  -3.41335161\n",
      "   1.50711151]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:95 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.86865404]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 95 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.96564763 -1.34512073  0.14535301  1.14330602 -0.24441729 -0.24893387\n",
      "   1.3198896   0.3672549  -0.72378237]\n",
      " [-0.92590129  1.08634753 -0.37543136 -1.15059947  0.14839112 -0.01158762\n",
      "  -0.79145878 -0.53213318  0.94365203]\n",
      " [ 0.73145296 -0.66317474 -0.11717779  0.72248521 -0.40201987  0.22114028\n",
      "   0.83586442  0.08245114 -0.87109219]\n",
      " [ 0.78639588 -1.00226659  0.21871151  0.81388611 -0.27887491  0.01952832\n",
      "   0.99806727  0.99455672 -0.56159993]\n",
      " [-1.06443962  1.33030358 -0.31451974 -1.28072838  0.02349777  0.33383405\n",
      "  -1.04970314 -0.87511826  0.89558092]\n",
      " [ 1.21460142 -1.22130442  0.28933192  0.91930441 -0.11110912 -0.36985249\n",
      "   0.89069565  0.96821054 -0.69466095]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.7627512   2.14976285 -2.70291981  0.36718776  0.68011141 -3.41274232\n",
      "   1.51397576]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:95 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.54325678]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 95 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.95183849 -1.35892987  0.13154387  1.14330602 -0.24441729 -0.24893387\n",
      "   1.30608046  0.3672549  -0.72378237]\n",
      " [-0.91260425  1.09964457 -0.36213432 -1.15059947  0.14839112 -0.01158762\n",
      "  -0.77816173 -0.53213318  0.94365203]\n",
      " [ 0.72005116 -0.67457654 -0.12857959  0.72248521 -0.40201987  0.22114028\n",
      "   0.82446263  0.08245114 -0.87109219]\n",
      " [ 0.77313799 -1.01552448  0.20545363  0.81388611 -0.27887491  0.01952832\n",
      "   0.98480939  0.99455672 -0.56159993]\n",
      " [-1.05055759  1.34418561 -0.30063771 -1.28072838  0.02349777  0.33383405\n",
      "  -1.03582111 -0.87511826  0.89558092]\n",
      " [ 1.20033223 -1.23557362  0.27506272  0.91930441 -0.11110912 -0.36985249\n",
      "   0.87642645  0.96821054 -0.69466095]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.83015004  2.09937654 -2.72096088  0.32087267  0.63082688 -3.42959523\n",
      "   1.4625002 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:95 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.77984638]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 95 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.95605603 -1.35892987  0.13576141  1.14330602 -0.24441729 -0.24893387\n",
      "   1.30608046  0.37147244 -0.72378237]\n",
      " [-0.91674056  1.09964457 -0.36627063 -1.15059947  0.14839112 -0.01158762\n",
      "  -0.77816173 -0.53626949  0.94365203]\n",
      " [ 0.72289939 -0.67457654 -0.12573135  0.72248521 -0.40201987  0.22114028\n",
      "   0.82446263  0.08529937 -0.87109219]\n",
      " [ 0.77713377 -1.01552448  0.20944941  0.81388611 -0.27887491  0.01952832\n",
      "   0.98480939  0.9985525  -0.56159993]\n",
      " [-1.05425689  1.34418561 -0.30433701 -1.28072838  0.02349777  0.33383405\n",
      "  -1.03582111 -0.87881756  0.89558092]\n",
      " [ 1.20372755 -1.23557362  0.27845804  0.91930441 -0.11110912 -0.36985249\n",
      "   0.87642645  0.97160586 -0.69466095]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.81125139  2.11468627 -2.71829586  0.33339078  0.64741893 -3.42775427\n",
      "   1.47988871]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:95 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.60758139]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 95 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.96641851 -1.35892987  0.13576141  1.1536685  -0.24441729 -0.24893387\n",
      "   1.30608046  0.37147244 -0.7134199 ]\n",
      " [-0.92647307  1.09964457 -0.36627063 -1.16033198  0.14839112 -0.01158762\n",
      "  -0.77816173 -0.53626949  0.93391952]\n",
      " [ 0.72909123 -0.67457654 -0.12573135  0.72867705 -0.40201987  0.22114028\n",
      "   0.82446263  0.08529937 -0.86490035]\n",
      " [ 0.78647272 -1.01552448  0.20944941  0.82322506 -0.27887491  0.01952832\n",
      "   0.98480939  0.9985525  -0.55226097]\n",
      " [-1.06468831  1.34418561 -0.30433701 -1.2911598   0.02349777  0.33383405\n",
      "  -1.03582111 -0.87881756  0.88514951]\n",
      " [ 1.21414948 -1.23557362  0.27845804  0.92972635 -0.11110912 -0.36985249\n",
      "   0.87642645  0.97160586 -0.68423902]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.76446995  2.15203097 -2.70681907  0.36331935  0.68188766 -3.41878918\n",
      "   1.51762484]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:95 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.05407534]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 95 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.96650042 -1.35892987  0.13576141  1.1536685  -0.24433538 -0.24885195\n",
      "   1.30608046  0.37147244 -0.71333798]\n",
      " [-0.92652268  1.09964457 -0.36627063 -1.16033198  0.1483415  -0.01163724\n",
      "  -0.77816173 -0.53626949  0.93386991]\n",
      " [ 0.72919803 -0.67457654 -0.12573135  0.72867705 -0.40191308  0.22124707\n",
      "   0.82446263  0.08529937 -0.86479355]\n",
      " [ 0.78648141 -1.01552448  0.20944941  0.82322506 -0.27886622  0.01953701\n",
      "   0.98480939  0.9985525  -0.55225229]\n",
      " [-1.0647493   1.34418561 -0.30433701 -1.2911598   0.02343678  0.33377306\n",
      "  -1.03582111 -0.87881756  0.88508852]\n",
      " [ 1.21413257 -1.23557362  0.27845804  0.92972635 -0.11112604 -0.36986941\n",
      "   0.87642645  0.97160586 -0.68425593]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.76585295  2.15142217 -2.70756036  0.36273643  0.68120484 -3.419542\n",
      "   1.51691641]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:95 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.00339602]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 95 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.96650171 -1.35892859  0.13576141  1.1536685  -0.24433409 -0.24885067\n",
      "   1.30608046  0.37147244 -0.7133367 ]\n",
      " [-0.92652392  1.09964333 -0.36627063 -1.16033198  0.14834026 -0.01163848\n",
      "  -0.77816173 -0.53626949  0.93386867]\n",
      " [ 0.72919915 -0.67457541 -0.12573135  0.72867705 -0.40191195  0.2212482\n",
      "   0.82446263  0.08529937 -0.86479243]\n",
      " [ 0.78648257 -1.01552332  0.20944941  0.82322506 -0.27886507  0.01953817\n",
      "   0.98480939  0.9985525  -0.55225113]\n",
      " [-1.06475059  1.34418432 -0.30433701 -1.2911598   0.0234355   0.33377177\n",
      "  -1.03582111 -0.87881756  0.88508723]\n",
      " [ 1.21413379 -1.23557239  0.27845804  0.92972635 -0.11112481 -0.36986818\n",
      "   0.87642645  0.97160586 -0.68425471]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.7658587   2.1514212  -2.70756482  0.36273488  0.68120334 -3.41954672\n",
      "   1.51691507]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:95 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.21109605]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 95 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.96473334 -1.36069696  0.13576141  1.1536685  -0.24610246 -0.25061904\n",
      "   1.30431209  0.37147244 -0.7133367 ]\n",
      " [-0.92457473  1.10159253 -0.36627063 -1.16033198  0.15028946 -0.00968928\n",
      "  -0.77621253 -0.53626949  0.93386867]\n",
      " [ 0.72647586 -0.6772987  -0.12573135  0.72867705 -0.40463524  0.21852491\n",
      "   0.82173934  0.08529937 -0.86479243]\n",
      " [ 0.78443011 -1.01757578  0.20944941  0.82322506 -0.28091753  0.01748571\n",
      "   0.98275693  0.9985525  -0.55225113]\n",
      " [-1.06306451  1.34587039 -0.30433701 -1.2911598   0.02512157  0.33545785\n",
      "  -1.03413504 -0.87881756  0.88508723]\n",
      " [ 1.21254648 -1.2371597   0.27845804  0.92972635 -0.11271212 -0.37145549\n",
      "   0.87483915  0.97160586 -0.68425471]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.78343609  2.14081157 -2.71433228  0.35099603  0.67027684 -3.4266042\n",
      "   1.50650181]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:95 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.91751229]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 95 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.96506021 -1.36069696  0.13576141  1.15399537 -0.24610246 -0.25061904\n",
      "   1.30463897  0.37147244 -0.7133367 ]\n",
      " [-0.92503182  1.10159253 -0.36627063 -1.16078907  0.15028946 -0.00968928\n",
      "  -0.77666963 -0.53626949  0.93386867]\n",
      " [ 0.72707568 -0.6772987  -0.12573135  0.72927687 -0.40463524  0.21852491\n",
      "   0.82233916  0.08529937 -0.86479243]\n",
      " [ 0.78495482 -1.01757578  0.20944941  0.82374977 -0.28091753  0.01748571\n",
      "   0.98328164  0.9985525  -0.55225113]\n",
      " [-1.06339868  1.34587039 -0.30433701 -1.29149396  0.02512157  0.33545785\n",
      "  -1.0344692  -0.87881756  0.88508723]\n",
      " [ 1.21296541 -1.2371597   0.27845804  0.93014527 -0.11271212 -0.37145549\n",
      "   0.87525807  0.97160586 -0.68425471]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.78031461  2.14383443 -2.71416337  0.35382704  0.67318057 -3.42650225\n",
      "   1.50947764]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:95 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.76096747]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 95 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.96984338 -1.36069696  0.13576141  1.15399537 -0.24131929 -0.24583586\n",
      "   1.30942214  0.37147244 -0.7133367 ]\n",
      " [-0.92989815  1.10159253 -0.36627063 -1.16078907  0.14542313 -0.01455561\n",
      "  -0.78153596 -0.53626949  0.93386867]\n",
      " [ 0.73188316 -0.6772987  -0.12573135  0.72927687 -0.39982777  0.22333238\n",
      "   0.82714663  0.08529937 -0.86479243]\n",
      " [ 0.78981907 -1.01757578  0.20944941  0.82374977 -0.27605327  0.02234996\n",
      "   0.98814589  0.9985525  -0.55225113]\n",
      " [-1.06820502  1.34587039 -0.30433701 -1.29149396  0.02031523  0.3306515\n",
      "  -1.03927555 -0.87881756  0.88508723]\n",
      " [ 1.21782605 -1.2371597   0.27845804  0.93014527 -0.10785148 -0.36659485\n",
      "   0.88011871  0.97160586 -0.68425471]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.75857508  2.16241798 -2.71039181  0.37113815  0.69096981 -3.42324876\n",
      "   1.52757763]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:95 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.58429171]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 95 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 9.54862891e-01 -1.36069696e+00  1.35761414e-01  1.13901488e+00\n",
      "  -2.41319290e-01 -2.60816355e-01  1.30942214e+00  3.71472444e-01\n",
      "  -7.28317191e-01]\n",
      " [-9.14884401e-01  1.10159253e+00 -3.66270628e-01 -1.14577533e+00\n",
      "   1.45423131e-01  4.58139260e-04 -7.81535959e-01 -5.36269489e-01\n",
      "   9.48882416e-01]\n",
      " [ 7.19534579e-01 -6.77298703e-01 -1.25731353e-01  7.16928295e-01\n",
      "  -3.99827771e-01  2.10983804e-01  8.27146631e-01  8.52993748e-02\n",
      "  -8.77141006e-01]\n",
      " [ 7.75293193e-01 -1.01757578e+00  2.09449408e-01  8.09223891e-01\n",
      "  -2.76053270e-01  7.82408065e-03  9.88145892e-01  9.98552496e-01\n",
      "  -5.66777013e-01]\n",
      " [-1.05333139e+00  1.34587039e+00 -3.04337012e-01 -1.27662033e+00\n",
      "   2.03152261e-02  3.45525132e-01 -1.03927555e+00 -8.78817561e-01\n",
      "   8.99960862e-01]\n",
      " [ 1.20321781e+00 -1.23715970e+00  2.78458041e-01  9.15537037e-01\n",
      "  -1.07851480e-01 -3.81203084e-01  8.80118711e-01  9.71605860e-01\n",
      "  -6.98862941e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.82953582  2.10833315 -2.7271816   0.32187914  0.63794884 -3.44039245\n",
      "   1.47437692]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:95 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.73264768]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 95 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.96061639 -1.35494345  0.13576141  1.14476838 -0.24131929 -0.25506285\n",
      "   1.31517564  0.37147244 -0.72831719]\n",
      " [-0.92067124  1.09580569 -0.36627063 -1.15156217  0.14542313 -0.0053287\n",
      "  -0.7873228  -0.53626949  0.94888242]\n",
      " [ 0.72527427 -0.67155901 -0.12573135  0.72266799 -0.39982777  0.2167235\n",
      "   0.83288633  0.08529937 -0.87714101]\n",
      " [ 0.78115422 -1.01171475  0.20944941  0.81508492 -0.27605327  0.01368511\n",
      "   0.99400692  0.9985525  -0.56677701]\n",
      " [-1.05915737  1.34004441 -0.30433701 -1.28244631  0.02031523  0.33969915\n",
      "  -1.04510153 -0.87881756  0.89996086]\n",
      " [ 1.20902141 -1.2313561   0.27845804  0.92134064 -0.10785148 -0.37539948\n",
      "   0.88592231  0.97160586 -0.69886294]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.803352    2.13074434 -2.7232725   0.34434013  0.65959686 -3.43627157\n",
      "   1.49529971]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:95 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.80382061]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 95 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.96380611 -1.35175374  0.13576141  1.1479581  -0.24131929 -0.25506285\n",
      "   1.31836536  0.37147244 -0.72831719]\n",
      " [-0.92407905  1.09239788 -0.36627063 -1.15496997  0.14542313 -0.0053287\n",
      "  -0.79073061 -0.53626949  0.94888242]\n",
      " [ 0.72873192 -0.66810136 -0.12573135  0.72612564 -0.39982777  0.2167235\n",
      "   0.83634398  0.08529937 -0.87714101]\n",
      " [ 0.78461558 -1.00825339  0.20944941  0.81854628 -0.27605327  0.01368511\n",
      "   0.99746828  0.9985525  -0.56677701]\n",
      " [-1.06236448  1.33683731 -0.30433701 -1.28565342  0.02031523  0.33969915\n",
      "  -1.04830863 -0.87881756  0.89996086]\n",
      " [ 1.21241867 -1.22795884  0.27845804  0.9247379  -0.10785148 -0.37539948\n",
      "   0.88931957  0.97160586 -0.69886294]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.78788394  2.1444725  -2.72100919  0.35722982  0.67241982 -3.43450217\n",
      "   1.50854507]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:95 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.84211108]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 95 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.96590604 -1.35175374  0.13786135  1.1479581  -0.24131929 -0.25296292\n",
      "   1.32046529  0.37147244 -0.72831719]\n",
      " [-0.9262303   1.09239788 -0.36842188 -1.15496997  0.14542313 -0.00747996\n",
      "  -0.79288186 -0.53626949  0.94888242]\n",
      " [ 0.73107161 -0.66810136 -0.12339167  0.72612564 -0.39982777  0.21906319\n",
      "   0.83868366  0.08529937 -0.87714101]\n",
      " [ 0.78681668 -1.00825339  0.21165051  0.81854628 -0.27605327  0.01588621\n",
      "   0.99966938  0.9985525  -0.56677701]\n",
      " [-1.06452285  1.33683731 -0.30649538 -1.28565342  0.02031523  0.33754078\n",
      "  -1.050467   -0.87881756  0.89996086]\n",
      " [ 1.21462002 -1.22795884  0.28065939  0.9247379  -0.10785148 -0.37319814\n",
      "   0.89152092  0.97160586 -0.69886294]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.77738748  2.15388641 -2.71985012  0.36604379  0.68167081 -3.43333161\n",
      "   1.5177956 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:95 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.09327634]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 96 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.96587273 -1.35178705  0.13786135  1.14792478 -0.24131929 -0.25296292\n",
      "   1.32046529  0.37147244 -0.72835051]\n",
      " [-0.92619095  1.09243723 -0.36842188 -1.15493062  0.14542313 -0.00747996\n",
      "  -0.79288186 -0.53626949  0.94892177]\n",
      " [ 0.73115827 -0.6680147  -0.12339167  0.72621229 -0.39982777  0.21906319\n",
      "   0.83868366  0.08529937 -0.87705435]\n",
      " [ 0.78678678 -1.0082833   0.21165051  0.81851638 -0.27605327  0.01588621\n",
      "   0.99966938  0.9985525  -0.56680692]\n",
      " [-1.0644114   1.33694875 -0.30649538 -1.28554197  0.02031523  0.33754078\n",
      "  -1.050467   -0.87881756  0.90007231]\n",
      " [ 1.21441278 -1.22816608  0.28065939  0.92453066 -0.10785148 -0.37319814\n",
      "   0.89152092  0.97160586 -0.69907018]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.78133194  2.15188085 -2.72178299  0.36415833  0.67966867 -3.43519216\n",
      "   1.51561457]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:96 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.86947867]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 96 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.9672144  -1.35178705  0.13920302  1.14792478 -0.24131929 -0.25296292\n",
      "   1.32180697  0.37147244 -0.72835051]\n",
      " [-0.92770835  1.09243723 -0.36993928 -1.15493062  0.14542313 -0.00747996\n",
      "  -0.79439926 -0.53626949  0.94892177]\n",
      " [ 0.73281058 -0.6680147  -0.12173936  0.72621229 -0.39982777  0.21906319\n",
      "   0.84033597  0.08529937 -0.87705435]\n",
      " [ 0.78834274 -1.0082833   0.21320647  0.81851638 -0.27605327  0.01588621\n",
      "   1.00122535  0.9985525  -0.56680692]\n",
      " [-1.06575469  1.33694875 -0.30783867 -1.28554197  0.02031523  0.33754078\n",
      "  -1.05181029 -0.87881756  0.90007231]\n",
      " [ 1.21577575 -1.22816608  0.28202236  0.92453066 -0.10785148 -0.37319814\n",
      "   0.89288389  0.97160586 -0.69907018]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.7739258   2.15868453 -2.72096596  0.37015324  0.6861905  -3.43458813\n",
      "   1.5223971 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:96 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.53832297]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 96 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.95356357 -1.36543788  0.12555219  1.14792478 -0.24131929 -0.25296292\n",
      "   1.30815613  0.37147244 -0.72835051]\n",
      " [-0.91455874  1.10558684 -0.35678967 -1.15493062  0.14542313 -0.00747996\n",
      "  -0.78124965 -0.53626949  0.94892177]\n",
      " [ 0.72153033 -0.67929495 -0.1330196   0.72621229 -0.39982777  0.21906319\n",
      "   0.82905572  0.08529937 -0.87705435]\n",
      " [ 0.77522957 -1.02139647  0.20009329  0.81851638 -0.27605327  0.01588621\n",
      "   0.98811217  0.9985525  -0.56680692]\n",
      " [-1.05203314  1.3506703  -0.29411712 -1.28554197  0.02031523  0.33754078\n",
      "  -1.03808874 -0.87881756  0.90007231]\n",
      " [ 1.20166374 -1.24227809  0.26791035  0.92453066 -0.10785148 -0.37319814\n",
      "   0.87877188  0.97160586 -0.69907018]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.84082087  2.10879291 -2.73896195  0.32423574  0.63735883 -3.45143963\n",
      "   1.47143714]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:96 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.78123011]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 96 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.95773564 -1.36543788  0.12972425  1.14792478 -0.24131929 -0.25296292\n",
      "   1.30815613  0.37564451 -0.72835051]\n",
      " [-0.91864997  1.10558684 -0.36088089 -1.15493062  0.14542313 -0.00747996\n",
      "  -0.78124965 -0.54036071  0.94892177]\n",
      " [ 0.72434749 -0.67929495 -0.13020244  0.72621229 -0.39982777  0.21906319\n",
      "   0.82905572  0.08811654 -0.87705435]\n",
      " [ 0.77918156 -1.02139647  0.20404529  0.81851638 -0.27605327  0.01588621\n",
      "   0.98811217  1.00250449 -0.56680692]\n",
      " [-1.05569432  1.3506703  -0.2977783  -1.28554197  0.02031523  0.33754078\n",
      "  -1.03808874 -0.88247874  0.90007231]\n",
      " [ 1.20502597 -1.24227809  0.27127258  0.92453066 -0.10785148 -0.37319814\n",
      "   0.87877188  0.97496809 -0.69907018]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.82212593  2.12393747 -2.73632734  0.33661845  0.65377349 -3.4496163\n",
      "   1.48863488]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:96 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.60861533]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 96 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.96806343 -1.36543788  0.12972425  1.15825258 -0.24131929 -0.25296292\n",
      "   1.30815613  0.37564451 -0.71802271]\n",
      " [-0.9283513   1.10558684 -0.36088089 -1.16463196  0.14542313 -0.00747996\n",
      "  -0.78124965 -0.54036071  0.93922043]\n",
      " [ 0.73051012 -0.67929495 -0.13020244  0.73237493 -0.39982777  0.21906319\n",
      "   0.82905572  0.08811654 -0.87089172]\n",
      " [ 0.78849397 -1.02139647  0.20404529  0.82782879 -0.27605327  0.01588621\n",
      "   0.98811217  1.00250449 -0.5574945 ]\n",
      " [-1.06608992  1.3506703  -0.2977783  -1.29593757  0.02031523  0.33754078\n",
      "  -1.03808874 -0.88247874  0.88967672]\n",
      " [ 1.2154126  -1.24227809  0.27127258  0.93491728 -0.10785148 -0.37319814\n",
      "   0.87877188  0.97496809 -0.68868356]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.77551149  2.16116184 -2.7248999   0.36643172  0.68813247 -3.44069592\n",
      "   1.52625167]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:96 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.05253932]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 96 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.9681421  -1.36543788  0.12972425  1.15825258 -0.24124062 -0.25288425\n",
      "   1.30815613  0.37564451 -0.71794404]\n",
      " [-0.92839969  1.10558684 -0.36088089 -1.16463196  0.14537475 -0.00752834\n",
      "  -0.78124965 -0.54036071  0.93917205]\n",
      " [ 0.73061245 -0.67929495 -0.13020244  0.73237493 -0.39972544  0.21916551\n",
      "   0.82905572  0.08811654 -0.87078939]\n",
      " [ 0.78850351 -1.02139647  0.20404529  0.82782879 -0.27604374  0.01589574\n",
      "   0.98811217  1.00250449 -0.55748497]\n",
      " [-1.06614875  1.3506703  -0.2977783  -1.29593757  0.02025639  0.33748195\n",
      "  -1.03808874 -0.88247874  0.88961788]\n",
      " [ 1.21539767 -1.24227809  0.27127258  0.93491728 -0.10786641 -0.37321306\n",
      "   0.87877188  0.97496809 -0.68869848]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.77681916  2.16058745 -2.72560229  0.36588198  0.68748816 -3.44140891\n",
      "   1.5255829 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:96 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.00321563]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 96 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.96814325 -1.36543673  0.12972425  1.15825258 -0.24123947 -0.2528831\n",
      "   1.30815613  0.37564451 -0.71794289]\n",
      " [-0.9284008   1.10558572 -0.36088089 -1.16463196  0.14537363 -0.00752945\n",
      "  -0.78124965 -0.54036071  0.93917093]\n",
      " [ 0.73061346 -0.67929394 -0.13020244  0.73237493 -0.39972443  0.21916653\n",
      "   0.82905572  0.08811654 -0.87078838]\n",
      " [ 0.78850455 -1.02139543  0.20404529  0.82782879 -0.2760427   0.01589678\n",
      "   0.98811217  1.00250449 -0.55748393]\n",
      " [-1.0661499   1.35066914 -0.2977783  -1.29593757  0.02025524  0.3374808\n",
      "  -1.03808874 -0.88247874  0.88961673]\n",
      " [ 1.21539877 -1.24227699  0.27127258  0.93491728 -0.10786531 -0.37321196\n",
      "   0.87877188  0.97496809 -0.68869738]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.77682432  2.1605866  -2.7256063   0.36588059  0.68748683 -3.44141315\n",
      "   1.5255817 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:96 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.20821913]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 96 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.96643048 -1.36714951  0.12972425  1.15825258 -0.24295224 -0.25459588\n",
      "   1.30644336  0.37564451 -0.71794289]\n",
      " [-0.92650515  1.10748137 -0.36088089 -1.16463196  0.14726929 -0.0056338\n",
      "  -0.779354   -0.54036071  0.93917093]\n",
      " [ 0.72795017 -0.68195723 -0.13020244  0.73237493 -0.40238772  0.21650324\n",
      "   0.82639243  0.08811654 -0.87078838]\n",
      " [ 0.78650522 -1.02339476  0.20404529  0.82782879 -0.27804202  0.01389746\n",
      "   0.98611284  1.00250449 -0.55748393]\n",
      " [-1.06451622  1.35230282 -0.2977783  -1.29593757  0.02188892  0.33911448\n",
      "  -1.03645506 -0.88247874  0.88961673]\n",
      " [ 1.21386107 -1.24381468  0.27127258  0.93491728 -0.109403   -0.37474966\n",
      "   0.87723418  0.97496809 -0.68869738]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.79398823  2.15024186 -2.73222318  0.35441257  0.67682287 -3.44831843\n",
      "   1.51542657]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:96 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.91891749]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 96 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.96674508 -1.36714951  0.12972425  1.15856718 -0.24295224 -0.25459588\n",
      "   1.30675796  0.37564451 -0.71794289]\n",
      " [-0.92694524  1.10748137 -0.36088089 -1.16507205  0.14726929 -0.0056338\n",
      "  -0.77979409 -0.54036071  0.93917093]\n",
      " [ 0.72852848 -0.68195723 -0.13020244  0.73295323 -0.40238772  0.21650324\n",
      "   0.82697074  0.08811654 -0.87078838]\n",
      " [ 0.78701056 -1.02339476  0.20404529  0.82833414 -0.27804202  0.01389746\n",
      "   0.98661819  1.00250449 -0.55748393]\n",
      " [-1.06483783  1.35230282 -0.2977783  -1.29625917  0.02188892  0.33911448\n",
      "  -1.03677667 -0.88247874  0.88961673]\n",
      " [ 1.21426439 -1.24381468  0.27127258  0.9353206  -0.109403   -0.37474966\n",
      "   0.8776375   0.97496809 -0.68869738]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.79096757  2.15316785 -2.73206118  0.35715463  0.67963477 -3.44822058\n",
      "   1.51830747]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:96 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.76308691]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 96 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.97145484 -1.36714951  0.12972425  1.15856718 -0.23824249 -0.24988612\n",
      "   1.31146772  0.37564451 -0.71794289]\n",
      " [-0.9317387   1.10748137 -0.36088089 -1.16507205  0.14247583 -0.01042726\n",
      "  -0.78458755 -0.54036071  0.93917093]\n",
      " [ 0.73326836 -0.68195723 -0.13020244  0.73295323 -0.39764784  0.22124312\n",
      "   0.83171062  0.08811654 -0.87078838]\n",
      " [ 0.79180284 -1.02339476  0.20404529  0.82833414 -0.27324975  0.01868973\n",
      "   0.99141046  1.00250449 -0.55748393]\n",
      " [-1.06957048  1.35230282 -0.2977783  -1.29625917  0.01715626  0.33438182\n",
      "  -1.04150932 -0.88247874  0.88961673]\n",
      " [ 1.21905176 -1.24381468  0.27127258  0.9353206  -0.10461564 -0.36996229\n",
      "   0.88242487  0.97496809 -0.68869738]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.76955237  2.17148206 -2.72835816  0.37422936  0.69717349 -3.44502467\n",
      "   1.53614845]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:96 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.58300942]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 96 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.95650644 -1.36714951  0.12972425  1.14361878 -0.23824249 -0.26483452\n",
      "   1.31146772  0.37564451 -0.73289129]\n",
      " [-0.91676096  1.10748137 -0.36088089 -1.15009431  0.14247583  0.00455048\n",
      "  -0.78458755 -0.54036071  0.95414867]\n",
      " [ 0.72096708 -0.68195723 -0.13020244  0.72065195 -0.39764784  0.20894184\n",
      "   0.83171062  0.08811654 -0.88308966]\n",
      " [ 0.77731052 -1.02339476  0.20404529  0.81384182 -0.27324975  0.00419742\n",
      "   0.99141046  1.00250449 -0.57197624]\n",
      " [-1.05472772  1.35230282 -0.2977783  -1.28141641  0.01715626  0.34922458\n",
      "  -1.04150932 -0.88247874  0.90445949]\n",
      " [ 1.20447119 -1.24381468  0.27127258  0.92074004 -0.10461564 -0.38454286\n",
      "   0.88242487  0.97496809 -0.70327795]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.84041992  2.11750006 -2.74516826  0.32508056  0.64425329 -3.46217351\n",
      "   1.48303634]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:96 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.73342774]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 96 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 9.62234375e-01 -1.36142157e+00  1.29724255e-01  1.14934672e+00\n",
      "  -2.38242485e-01 -2.59106584e-01  1.31719566e+00  3.75644510e-01\n",
      "  -7.32891294e-01]\n",
      " [-9.22520740e-01  1.10172159e+00 -3.60880895e-01 -1.15585409e+00\n",
      "   1.42475825e-01 -1.20930433e-03 -7.90347330e-01 -5.40360711e-01\n",
      "   9.54148670e-01]\n",
      " [ 7.26676646e-01 -6.76247664e-01 -1.30202440e-01  7.26361517e-01\n",
      "  -3.97647840e-01  2.14651404e-01  8.37420185e-01  8.81165396e-02\n",
      "  -8.83089659e-01]\n",
      " [ 7.83143569e-01 -1.01756172e+00  2.04045288e-01  8.19674866e-01\n",
      "  -2.73249749e-01  1.00304614e-02  9.97243505e-01  1.00250449e+00\n",
      "  -5.71976242e-01]\n",
      " [-1.06052680e+00  1.34650375e+00 -2.97778303e-01 -1.28721549e+00\n",
      "   1.71562641e-02  3.43425504e-01 -1.04730840e+00 -8.82478745e-01\n",
      "   9.04459491e-01]\n",
      " [ 1.21024622e+00 -1.23803966e+00  2.71272580e-01  9.26515062e-01\n",
      "  -1.04615636e-01 -3.78767831e-01  8.88199893e-01  9.74968090e-01\n",
      "  -7.03277946e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.81436095  2.13979738 -2.74127537  0.34744404  0.66579978 -3.45806638\n",
      "   1.50385421]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:96 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.80540468]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 96 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 9.65377382e-01 -1.35827856e+00  1.29724255e-01  1.15248972e+00\n",
      "  -2.38242485e-01 -2.59106584e-01  1.32033867e+00  3.75644510e-01\n",
      "  -7.32891294e-01]\n",
      " [-9.25878800e-01  1.09836353e+00 -3.60880895e-01 -1.15921215e+00\n",
      "   1.42475825e-01 -1.20930433e-03 -7.93705390e-01 -5.40360711e-01\n",
      "   9.54148670e-01]\n",
      " [ 7.30084598e-01 -6.72839713e-01 -1.30202440e-01  7.29769469e-01\n",
      "  -3.97647840e-01  2.14651404e-01  8.40828137e-01  8.81165396e-02\n",
      "  -8.83089659e-01]\n",
      " [ 7.86555619e-01 -1.01414967e+00  2.04045288e-01  8.23086917e-01\n",
      "  -2.73249749e-01  1.00304614e-02  1.00065556e+00  1.00250449e+00\n",
      "  -5.71976242e-01]\n",
      " [-1.06368685e+00  1.34334369e+00 -2.97778303e-01 -1.29037554e+00\n",
      "   1.71562641e-02  3.43425504e-01 -1.05046845e+00 -8.82478745e-01\n",
      "   9.04459491e-01]\n",
      " [ 1.21359440e+00 -1.23469148e+00  2.71272580e-01  9.29863244e-01\n",
      "  -1.04615636e-01 -3.78767831e-01  8.91548076e-01  9.74968090e-01\n",
      "  -7.03277946e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.79911168  2.15333399 -2.73905027  0.36016185  0.67844993 -3.45632492\n",
      "   1.51691596]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:96 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.84225452]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 96 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.96747838 -1.35827856  0.13182526  1.15248972 -0.23824249 -0.25700558\n",
      "   1.32243967  0.37564451 -0.73289129]\n",
      " [-0.92802955  1.09836353 -0.36303165 -1.15921215  0.14247583 -0.00336006\n",
      "  -0.79585614 -0.54036071  0.95414867]\n",
      " [ 0.73242056 -0.67283971 -0.12786648  0.72976947 -0.39764784  0.21698736\n",
      "   0.8431641   0.08811654 -0.88308966]\n",
      " [ 0.78875537 -1.01414967  0.20624504  0.82308692 -0.27324975  0.01223021\n",
      "   1.00285531  1.00250449 -0.57197624]\n",
      " [-1.06584598  1.34334369 -0.29993743 -1.29037554  0.01715626  0.34126638\n",
      "  -1.05262758 -0.88247874  0.90445949]\n",
      " [ 1.21579625 -1.23469148  0.27347442  0.92986324 -0.10461564 -0.37656599\n",
      "   0.89374992  0.97496809 -0.70327795]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.78863251  2.16272605 -2.73788822  0.36896032  0.68768138 -3.45514921\n",
      "   1.52614339]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:96 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.09072951]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 97 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.96745145 -1.3583055   0.13182526  1.15246278 -0.23824249 -0.25700558\n",
      "   1.32243967  0.37564451 -0.73291823]\n",
      " [-0.92799707  1.09839601 -0.36303165 -1.15917967  0.14247583 -0.00336006\n",
      "  -0.79585614 -0.54036071  0.95418115]\n",
      " [ 0.73250807 -0.6727522  -0.12786648  0.72985698 -0.39764784  0.21698736\n",
      "   0.8431641   0.08811654 -0.88300215]\n",
      " [ 0.78873131 -1.01417372  0.20624504  0.82306286 -0.27324975  0.01223021\n",
      "   1.00285531  1.00250449 -0.5720003 ]\n",
      " [-1.06574484  1.34344483 -0.29993743 -1.2902744   0.01715626  0.34126638\n",
      "  -1.05262758 -0.88247874  0.90456063]\n",
      " [ 1.21560401 -1.23488371  0.27347442  0.92967101 -0.10461564 -0.37656599\n",
      "   0.89374992  0.97496809 -0.70347018]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.79237499  2.16082786 -2.73972697  0.36717672  0.68578607 -3.45691911\n",
      "   1.52407853]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:97 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.87030038]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 97 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.96877893 -1.3583055   0.13315274  1.15246278 -0.23824249 -0.25700558\n",
      "   1.32376716  0.37564451 -0.73291823]\n",
      " [-0.92949711  1.09839601 -0.36453169 -1.15917967  0.14247583 -0.00336006\n",
      "  -0.79735618 -0.54036071  0.95418115]\n",
      " [ 0.73414134 -0.6727522  -0.12623321  0.72985698 -0.39764784  0.21698736\n",
      "   0.84479737  0.08811654 -0.88300215]\n",
      " [ 0.79026931 -1.01417372  0.20778303  0.82306286 -0.27324975  0.01223021\n",
      "   1.0043933   1.00250449 -0.5720003 ]\n",
      " [-1.06707424  1.34344483 -0.30126683 -1.2902744   0.01715626  0.34126638\n",
      "  -1.05395698 -0.88247874  0.90456063]\n",
      " [ 1.21695324 -1.23488371  0.27482365  0.92967101 -0.10461564 -0.37656599\n",
      "   0.89509915  0.97496809 -0.70347018]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.7850549   2.16755114 -2.738919    0.37310351  0.69223192 -3.45632042\n",
      "   1.53078013]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:97 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.5334429]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 97 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.95528695 -1.37179748  0.11966076  1.15246278 -0.23824249 -0.25700558\n",
      "   1.31027517  0.37564451 -0.73291823]\n",
      " [-0.91649518  1.11139794 -0.35152976 -1.15917967  0.14247583 -0.00336006\n",
      "  -0.78435425 -0.54036071  0.95418115]\n",
      " [ 0.72298247 -0.68391107 -0.13739208  0.72985698 -0.39764784  0.21698736\n",
      "   0.8336385   0.08811654 -0.88300215]\n",
      " [ 0.77730113 -1.0271419   0.19481486  0.82306286 -0.27324975  0.01223021\n",
      "   0.99142512  1.00250449 -0.5720003 ]\n",
      " [-1.05351382  1.35700525 -0.28770641 -1.2902744   0.01715626  0.34126638\n",
      "  -1.04039656 -0.88247874  0.90456063]\n",
      " [ 1.20299983 -1.24883712  0.26087024  0.92967101 -0.10461564 -0.37656599\n",
      "   0.88114574  0.97496809 -0.70347018]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.85143695  2.11815619 -2.75686353  0.32758732  0.64385624 -3.47316308\n",
      "   1.4803381 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:97 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.78261098]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 97 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.9594138  -1.37179748  0.12378761  1.15246278 -0.23824249 -0.25700558\n",
      "   1.31027517  0.37977136 -0.73291823]\n",
      " [-0.92054155  1.11139794 -0.35557612 -1.15917967  0.14247583 -0.00336006\n",
      "  -0.78435425 -0.54440708  0.95418115]\n",
      " [ 0.72576877 -0.68391107 -0.13460578  0.72985698 -0.39764784  0.21698736\n",
      "   0.8336385   0.09090284 -0.88300215]\n",
      " [ 0.78120956 -1.0271419   0.19872329  0.82306286 -0.27324975  0.01223021\n",
      "   0.99142512  1.00641292 -0.5720003 ]\n",
      " [-1.05713696  1.35700525 -0.29132955 -1.2902744   0.01715626  0.34126638\n",
      "  -1.04039656 -0.88610188  0.90456063]\n",
      " [ 1.20632897 -1.24883712  0.26419938  0.92967101 -0.10461564 -0.37656599\n",
      "   0.88114574  0.97829723 -0.70347018]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.83294464  2.13313662 -2.7542592   0.33983538  0.66009445 -3.47135745\n",
      "   1.49734624]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:97 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.60962035]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 97 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.96970776 -1.37179748  0.12378761  1.16275675 -0.23824249 -0.25700558\n",
      "   1.31027517  0.37977136 -0.72262427]\n",
      " [-0.93021229  1.11139794 -0.35557612 -1.16885041  0.14247583 -0.00336006\n",
      "  -0.78435425 -0.54440708  0.94451041]\n",
      " [ 0.73190198 -0.68391107 -0.13460578  0.73599019 -0.39764784  0.21698736\n",
      "   0.8336385   0.09090284 -0.87686893]\n",
      " [ 0.79049586 -1.0271419   0.19872329  0.83234916 -0.27324975  0.01223021\n",
      "   0.99142512  1.00641292 -0.562714  ]\n",
      " [-1.06749763  1.35700525 -0.29132955 -1.30063508  0.01715626  0.34126638\n",
      "  -1.04039656 -0.88610188  0.89419995]\n",
      " [ 1.21668116 -1.24883712  0.26419938  0.9400232  -0.10461564 -0.37656599\n",
      "   0.88114574  0.97829723 -0.69311799]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.78649271  2.17024352 -2.74287917  0.36953528  0.69434607 -3.46248028\n",
      "   1.53484666]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:97 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.05103995]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 97 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.96978328 -1.37179748  0.12378761  1.16275675 -0.23816697 -0.25693006\n",
      "   1.31027517  0.37977136 -0.72254875]\n",
      " [-0.93025942  1.11139794 -0.35557612 -1.16885041  0.1424287  -0.00340719\n",
      "  -0.78435425 -0.54440708  0.94446328]\n",
      " [ 0.73199998 -0.68391107 -0.13460578  0.73599019 -0.39754984  0.21708537\n",
      "   0.8336385   0.09090284 -0.87677093]\n",
      " [ 0.79050613 -1.0271419   0.19872329  0.83234916 -0.27323948  0.01224048\n",
      "   0.99142512  1.00641292 -0.56270373]\n",
      " [-1.06755435  1.35700525 -0.29132955 -1.30063508  0.01709955  0.34120966\n",
      "  -1.04039656 -0.88610188  0.89414324]\n",
      " [ 1.21666807 -1.24883712  0.26419938  0.9400232  -0.10462873 -0.37657908\n",
      "   0.88114574  0.97829723 -0.69313108]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.78772877  2.16970179 -2.74354451  0.36901699  0.69373831 -3.46315536\n",
      "   1.53421554]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:97 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.00304563]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 97 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.96978431 -1.37179645  0.12378761  1.16275675 -0.23816593 -0.25692903\n",
      "   1.31027517  0.37977136 -0.72254771]\n",
      " [-0.93026042  1.11139694 -0.35557612 -1.16885041  0.14242769 -0.00340819\n",
      "  -0.78435425 -0.54440708  0.94446228]\n",
      " [ 0.7320009  -0.68391016 -0.13460578  0.73599019 -0.39754892  0.21708628\n",
      "   0.8336385   0.09090284 -0.87677002]\n",
      " [ 0.79050706 -1.02714097  0.19872329  0.83234916 -0.27323854  0.01224142\n",
      "   0.99142512  1.00641292 -0.5627028 ]\n",
      " [-1.06755539  1.35700422 -0.29132955 -1.30063508  0.01709851  0.34120863\n",
      "  -1.04039656 -0.88610188  0.8941422 ]\n",
      " [ 1.21666906 -1.24883613  0.26419938  0.9400232  -0.10462774 -0.37657809\n",
      "   0.88114574  0.97829723 -0.69313009]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.78773339  2.16970102 -2.74354812  0.36901576  0.69373712 -3.46315916\n",
      "   1.53421447]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:97 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.20542292]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 97 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.96812447 -1.37345629  0.12378761  1.16275675 -0.23982578 -0.25858887\n",
      "   1.30861533  0.37977136 -0.72254771]\n",
      " [-0.92841601  1.11324136 -0.35557612 -1.16885041  0.14427211 -0.00156377\n",
      "  -0.78250984 -0.54440708  0.94446228]\n",
      " [ 0.72939539 -0.68651567 -0.13460578  0.73599019 -0.40015443  0.21448077\n",
      "   0.83103299  0.09090284 -0.87677002]\n",
      " [ 0.78855865 -1.02908938  0.19872329  0.83234916 -0.27518696  0.01029301\n",
      "   0.98947671  1.00641292 -0.5627028 ]\n",
      " [-1.06597156  1.35858804 -0.29132955 -1.30063508  0.01868233  0.34279245\n",
      "  -1.03881274 -0.88610188  0.8941422 ]\n",
      " [ 1.2151786  -1.25032659  0.26419938  0.9400232  -0.1061182  -0.37806855\n",
      "   0.87965528  0.97829723 -0.69313009]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.8044984   2.15961105 -2.75001925  0.35780887  0.68332606 -3.46991684\n",
      "   1.52430768]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:97 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.92029219]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 97 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.96842731 -1.37345629  0.12378761  1.16305959 -0.23982578 -0.25858887\n",
      "   1.30891817  0.37977136 -0.72254771]\n",
      " [-0.92883978  1.11324136 -0.35557612 -1.16927419  0.14427211 -0.00156377\n",
      "  -0.78293361 -0.54440708  0.94446228]\n",
      " [ 0.72995303 -0.68651567 -0.13460578  0.73654783 -0.40015443  0.21448077\n",
      "   0.83159063  0.09090284 -0.87677002]\n",
      " [ 0.78904541 -1.02908938  0.19872329  0.83283592 -0.27518696  0.01029301\n",
      "   0.98996347  1.00641292 -0.5627028 ]\n",
      " [-1.06628113  1.35858804 -0.29132955 -1.30094464  0.01868233  0.34279245\n",
      "  -1.0391223  -0.88610188  0.8941422 ]\n",
      " [ 1.21556695 -1.25032659  0.26419938  0.94041155 -0.1061182  -0.37806855\n",
      "   0.88004363  0.97829723 -0.69313009]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.80157494  2.16244364 -2.74986383  0.36046507  0.68604934 -3.46982291\n",
      "   1.52709703]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:97 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.76518903]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 97 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.97306456 -1.37345629  0.12378761  1.16305959 -0.23518853 -0.25395162\n",
      "   1.31355542  0.37977136 -0.72254771]\n",
      " [-0.93356122  1.11324136 -0.35557612 -1.16927419  0.13955067 -0.00628521\n",
      "  -0.78765505 -0.54440708  0.94446228]\n",
      " [ 0.73462588 -0.68651567 -0.13460578  0.73654783 -0.39548159  0.21915362\n",
      "   0.83626348  0.09090284 -0.87677002]\n",
      " [ 0.79376649 -1.02908938  0.19872329  0.83283592 -0.27046588  0.01501408\n",
      "   0.99468455  1.00641292 -0.5627028 ]\n",
      " [-1.070941    1.35858804 -0.29132955 -1.30094464  0.01402246  0.33813258\n",
      "  -1.04378218 -0.88610188  0.8941422 ]\n",
      " [ 1.2202819  -1.25032659  0.26419938  0.94041155 -0.10140324 -0.37335359\n",
      "   0.88475859  0.97829723 -0.69313009]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.78048013  2.18049161 -2.74622815  0.37730562  0.70334012 -3.4666837\n",
      "   1.54468184]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:97 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.58166058]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 97 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 9.58150082e-01 -1.37345629e+00  1.23787608e-01  1.14814511e+00\n",
      "  -2.35188526e-01 -2.68866101e-01  1.31355542e+00  3.79771357e-01\n",
      "  -7.37462194e-01]\n",
      " [-9.18621415e-01  1.11324136e+00 -3.55576124e-01 -1.15433439e+00\n",
      "   1.39550675e-01  8.65459534e-03 -7.87655047e-01 -5.44407077e-01\n",
      "   9.59402078e-01]\n",
      " [ 7.22374000e-01 -6.86515667e-01 -1.34605780e-01  7.24295958e-01\n",
      "  -3.95481585e-01  2.06901743e-01  8.36263477e-01  9.09028363e-02\n",
      "  -8.89021893e-01]\n",
      " [ 7.79309700e-01 -1.02908938e+00  1.98723286e-01  8.18379130e-01\n",
      "  -2.70465880e-01  5.57292968e-04  9.94684550e-01  1.00641292e+00\n",
      "  -5.77159586e-01]\n",
      " [-1.05613092e+00  1.35858804e+00 -2.91329546e-01 -1.28613457e+00\n",
      "   1.40224629e-02  3.52942655e-01 -1.04378218e+00 -8.86101878e-01\n",
      "   9.08952275e-01]\n",
      " [ 1.20573085e+00 -1.25032659e+00  2.64199381e-01  9.25860500e-01\n",
      "  -1.01403242e-01 -3.87904642e-01  8.84758586e-01  9.78297226e-01\n",
      "  -7.07681143e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.85124832  2.1266183  -2.76305837  0.32827268  0.65052665 -3.48383733\n",
      "   1.49166419]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:97 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.73421994]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 97 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.96385194 -1.36775443  0.12378761  1.15384697 -0.23518853 -0.26316424\n",
      "   1.31925728  0.37977136 -0.73746219]\n",
      " [-0.9243537   1.10750907 -0.35557612 -1.16006667  0.13955067  0.00292231\n",
      "  -0.79338733 -0.54440708  0.95940208]\n",
      " [ 0.72805305 -0.68083662 -0.13460578  0.72997501 -0.39548159  0.21258079\n",
      "   0.84194253  0.09090284 -0.88902189]\n",
      " [ 0.78511435 -1.02328473  0.19872329  0.82418378 -0.27046588  0.00636194\n",
      "   1.0004892   1.00641292 -0.57715959]\n",
      " [-1.06190263  1.35281634 -0.29132955 -1.29190627  0.01402246  0.34717095\n",
      "  -1.04955388 -0.88610188  0.90895227]\n",
      " [ 1.21147696 -1.24458049  0.26419938  0.9316066  -0.10140324 -0.38215854\n",
      "   0.89050469  0.97829723 -0.70768114]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.82531602  2.14880056 -2.75918211  0.35053692  0.67197017 -3.47974462\n",
      "   1.51237602]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:97 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.80699568]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 97 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.9669482  -1.36465817  0.12378761  1.15694323 -0.23518853 -0.26316424\n",
      "   1.32235354  0.37977136 -0.73746219]\n",
      " [-0.92766204  1.10420073 -0.35557612 -1.16337501  0.13955067  0.00292231\n",
      "  -0.79669567 -0.54440708  0.95940208]\n",
      " [ 0.7314113  -0.67747837 -0.13460578  0.73333325 -0.39548159  0.21258079\n",
      "   0.84530077  0.09090284 -0.88902189]\n",
      " [ 0.78847708 -1.019922    0.19872329  0.82754651 -0.27046588  0.00636194\n",
      "   1.00385193  1.00641292 -0.57715959]\n",
      " [-1.06501559  1.34970338 -0.29132955 -1.29501923  0.01402246  0.34717095\n",
      "  -1.05266684 -0.88610188  0.90895227]\n",
      " [ 1.21477606 -1.24128139  0.26419938  0.9349057  -0.10140324 -0.38215854\n",
      "   0.89380379  0.97829723 -0.70768114]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.81028546  2.1621458  -2.75699508  0.36308257  0.68444739 -3.47803118\n",
      "   1.52525429]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:97 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.84239501]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 97 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 9.69050220e-01 -1.36465817e+00  1.25889627e-01  1.15694323e+00\n",
      "  -2.35188526e-01 -2.61062223e-01  1.32445556e+00  3.79771357e-01\n",
      "  -7.37462194e-01]\n",
      " [-9.29812295e-01  1.10420073e+00 -3.57726380e-01 -1.16337501e+00\n",
      "   1.39550675e-01  7.72051794e-04 -7.98845927e-01 -5.44407077e-01\n",
      "   9.59402078e-01]\n",
      " [ 7.33743605e-01 -6.77478370e-01 -1.32273473e-01  7.33333255e-01\n",
      "  -3.95481585e-01  2.14913101e-01  8.47633081e-01  9.09028363e-02\n",
      "  -8.89021893e-01]\n",
      " [ 7.90675506e-01 -1.01992200e+00  2.00921711e-01  8.27546511e-01\n",
      "  -2.70465880e-01  8.56036778e-03  1.00605036e+00  1.00641292e+00\n",
      "  -5.77159586e-01]\n",
      " [-1.06717541e+00  1.34970338e+00 -2.93489366e-01 -1.29501923e+00\n",
      "   1.40224629e-02  3.45011132e-01 -1.05482666e+00 -8.86101878e-01\n",
      "   9.08952275e-01]\n",
      " [ 1.21697835e+00 -1.24128139e+00  2.66401675e-01  9.34905705e-01\n",
      "  -1.01403242e-01 -3.79956246e-01  8.96006085e-01  9.78297226e-01\n",
      "  -7.07681143e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.7998232   2.17151645 -2.75583009  0.37186588  0.69365968 -3.47685039\n",
      "   1.53445908]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:97 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.08825363]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 98 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 9.69029032e-01 -1.36467936e+00  1.25889627e-01  1.15692204e+00\n",
      "  -2.35188526e-01 -2.61062223e-01  1.32445556e+00  3.79771357e-01\n",
      "  -7.37483382e-01]\n",
      " [-9.29786040e-01  1.10422699e+00 -3.57726380e-01 -1.16334876e+00\n",
      "   1.39550675e-01  7.72051794e-04 -7.98845927e-01 -5.44407077e-01\n",
      "   9.59428334e-01]\n",
      " [ 7.33831642e-01 -6.77390333e-01 -1.32273473e-01  7.33421292e-01\n",
      "  -3.95481585e-01  2.14913101e-01  8.47633081e-01  9.09028363e-02\n",
      "  -8.88933856e-01]\n",
      " [ 7.90656742e-01 -1.01994076e+00  2.00921711e-01  8.27527748e-01\n",
      "  -2.70465880e-01  8.56036778e-03  1.00605036e+00  1.00641292e+00\n",
      "  -5.77178350e-01]\n",
      " [-1.06708375e+00  1.34979504e+00 -2.93489366e-01 -1.29492757e+00\n",
      "   1.40224629e-02  3.45011132e-01 -1.05482666e+00 -8.86101878e-01\n",
      "   9.09043937e-01]\n",
      " [ 1.21680007e+00 -1.24145967e+00  2.66401675e-01  9.34727420e-01\n",
      "  -1.01403242e-01 -3.79956246e-01  8.96006085e-01  9.78297226e-01\n",
      "  -7.07859428e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.80337386  2.16971993 -2.75757916  0.37017874  0.69186559 -3.4785339\n",
      "   1.53250424]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:98 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.87111964]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 98 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 9.70342337e-01 -1.36467936e+00  1.27202932e-01  1.15692204e+00\n",
      "  -2.35188526e-01 -2.61062223e-01  1.32576886e+00  3.79771357e-01\n",
      "  -7.37483382e-01]\n",
      " [-9.31268820e-01  1.10422699e+00 -3.59209160e-01 -1.16334876e+00\n",
      "   1.39550675e-01  7.72051794e-04 -8.00328707e-01 -5.44407077e-01\n",
      "   9.59428334e-01]\n",
      " [ 7.35446023e-01 -6.77390333e-01 -1.30659091e-01  7.33421292e-01\n",
      "  -3.95481585e-01  2.14913101e-01  8.49247462e-01  9.09028363e-02\n",
      "  -8.88933856e-01]\n",
      " [ 7.92176880e-01 -1.01994076e+00  2.02441848e-01  8.27527748e-01\n",
      "  -2.70465880e-01  8.56036778e-03  1.00757049e+00  1.00641292e+00\n",
      "  -5.77178350e-01]\n",
      " [-1.06839926e+00  1.34979504e+00 -2.94804880e-01 -1.29492757e+00\n",
      "   1.40224629e-02  3.45011132e-01 -1.05614218e+00 -8.86101878e-01\n",
      "   9.09043937e-01]\n",
      " [ 1.21813554e+00 -1.24145967e+00  2.67737149e-01  9.34727420e-01\n",
      "  -1.01403242e-01 -3.79956246e-01  8.97341559e-01  9.78297226e-01\n",
      "  -7.07859428e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.79613914  2.17636352 -2.75678024  0.37603794  0.6982361  -3.47794061\n",
      "   1.53912565]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:98 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.5286209]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 98 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 9.57009382e-01 -1.37801232e+00  1.13869977e-01  1.15692204e+00\n",
      "  -2.35188526e-01 -2.61062223e-01  1.31243591e+00  3.79771357e-01\n",
      "  -7.37483382e-01]\n",
      " [-9.18414503e-01  1.11708130e+00 -3.46354843e-01 -1.16334876e+00\n",
      "   1.39550675e-01  7.72051794e-04 -7.87474390e-01 -5.44407077e-01\n",
      "   9.59428334e-01]\n",
      " [ 7.24408170e-01 -6.88428186e-01 -1.41696945e-01  7.33421292e-01\n",
      "  -3.95481585e-01  2.14913101e-01  8.38209609e-01  9.09028363e-02\n",
      "  -8.88933856e-01]\n",
      " [ 7.79353696e-01 -1.03276394e+00  1.89618664e-01  8.27527748e-01\n",
      "  -2.70465880e-01  8.56036778e-03  9.94747309e-01  1.00641292e+00\n",
      "  -5.77178350e-01]\n",
      " [-1.05500023e+00  1.36319406e+00 -2.81405852e-01 -1.29492757e+00\n",
      "   1.40224629e-02  3.45011132e-01 -1.04274315e+00 -8.86101878e-01\n",
      "   9.09043937e-01]\n",
      " [ 1.20434173e+00 -1.25525348e+00  2.53943337e-01  9.34727420e-01\n",
      "  -1.01403242e-01 -3.79956246e-01  8.83547746e-01  9.78297226e-01\n",
      "  -7.07859428e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.86200024  2.12746629 -2.77466733  0.33092596  0.65031858 -3.49476736\n",
      "   1.48920283]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:98 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.78398874]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 98 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 9.61091275e-01 -1.37801232e+00  1.17951870e-01  1.15692204e+00\n",
      "  -2.35188526e-01 -2.61062223e-01  1.31243591e+00  3.83853250e-01\n",
      "  -7.37483382e-01]\n",
      " [-9.22416251e-01  1.11708130e+00 -3.50356591e-01 -1.16334876e+00\n",
      "   1.39550675e-01  7.72051794e-04 -7.87474390e-01 -5.48408825e-01\n",
      "   9.59428334e-01]\n",
      " [ 7.27163807e-01 -6.88428186e-01 -1.38941307e-01  7.33421292e-01\n",
      "  -3.95481585e-01  2.14913101e-01  8.38209609e-01  9.36584736e-02\n",
      "  -8.88933856e-01]\n",
      " [ 7.83218797e-01 -1.03276394e+00  1.93483765e-01  8.27527748e-01\n",
      "  -2.70465880e-01  8.56036778e-03  9.94747309e-01  1.01027802e+00\n",
      "  -5.77178350e-01]\n",
      " [-1.05858540e+00  1.36319406e+00 -2.84991018e-01 -1.29492757e+00\n",
      "   1.40224629e-02  3.45011132e-01 -1.04274315e+00 -8.89687044e-01\n",
      "   9.09043937e-01]\n",
      " [ 1.20763778e+00 -1.25525348e+00  2.57239384e-01  9.34727420e-01\n",
      "  -1.01403242e-01 -3.79956246e-01  8.83547746e-01  9.81593274e-01\n",
      "  -7.07859428e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.84370945  2.14228364 -2.77209313  0.34304017  0.66638135 -3.49297948\n",
      "   1.50602258]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:98 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.61059697]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 98 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 9.71352256e-01 -1.37801232e+00  1.17951870e-01  1.16718302e+00\n",
      "  -2.35188526e-01 -2.61062223e-01  1.31243591e+00  3.83853250e-01\n",
      "  -7.27222402e-01]\n",
      " [-9.32056988e-01  1.11708130e+00 -3.50356591e-01 -1.17298949e+00\n",
      "   1.39550675e-01  7.72051794e-04 -7.87474390e-01 -5.48408825e-01\n",
      "   9.49787597e-01]\n",
      " [ 7.33267421e-01 -6.88428186e-01 -1.38941307e-01  7.39524906e-01\n",
      "  -3.95481585e-01  2.14913101e-01  8.38209609e-01  9.36584736e-02\n",
      "  -8.82830242e-01]\n",
      " [ 7.92479408e-01 -1.03276394e+00  1.93483765e-01  8.36788358e-01\n",
      "  -2.70465880e-01  8.56036778e-03  9.94747309e-01  1.01027802e+00\n",
      "  -5.67917739e-01]\n",
      " [-1.06891207e+00  1.36319406e+00 -2.84991018e-01 -1.30525424e+00\n",
      "   1.40224629e-02  3.45011132e-01 -1.04274315e+00 -8.89687044e-01\n",
      "   8.98717271e-01]\n",
      " [ 1.21795643e+00 -1.25525348e+00  2.57239384e-01  9.45046069e-01\n",
      "  -1.01403242e-01 -3.79956246e-01  8.83547746e-01  9.81593274e-01\n",
      "  -6.97540778e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.7974156   2.17927595 -2.76075864  0.37262861  0.70052802 -3.4841441\n",
      "   1.54340961]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:98 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.0495765]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 98 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 9.71424717e-01 -1.37801232e+00  1.17951870e-01  1.16718302e+00\n",
      "  -2.35116065e-01 -2.60989762e-01  1.31243591e+00  3.83853250e-01\n",
      "  -7.27149941e-01]\n",
      " [-9.32102852e-01  1.11708130e+00 -3.50356591e-01 -1.17298949e+00\n",
      "   1.39504811e-01  7.26187675e-04 -7.87474390e-01 -5.48408825e-01\n",
      "   9.49741733e-01]\n",
      " [ 7.33361239e-01 -6.88428186e-01 -1.38941307e-01  7.39524906e-01\n",
      "  -3.95387767e-01  2.15006919e-01  8.38209609e-01  9.36584736e-02\n",
      "  -8.82736424e-01]\n",
      " [ 7.92490308e-01 -1.03276394e+00  1.93483765e-01  8.36788358e-01\n",
      "  -2.70454980e-01  8.57126828e-03  9.94747309e-01  1.01027802e+00\n",
      "  -5.67906838e-01]\n",
      " [-1.06896672e+00  1.36319406e+00 -2.84991018e-01 -1.30525424e+00\n",
      "   1.39678111e-02  3.44956480e-01 -1.04274315e+00 -8.89687044e-01\n",
      "   8.98662619e-01]\n",
      " [ 1.21794503e+00 -1.25525348e+00  2.57239384e-01  9.45046069e-01\n",
      "  -1.01414642e-01 -3.79967646e-01  8.83547746e-01  9.81593274e-01\n",
      "  -6.97552178e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.79858359  2.17876518 -2.76138869  0.37214015  0.69995493 -3.48478307\n",
      "   1.54281421]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:98 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.00288537]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 98 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 9.71425644e-01 -1.37801139e+00  1.17951870e-01  1.16718302e+00\n",
      "  -2.35115138e-01 -2.60988835e-01  1.31243591e+00  3.83853250e-01\n",
      "  -7.27149014e-01]\n",
      " [-9.32103755e-01  1.11708040e+00 -3.50356591e-01 -1.17298949e+00\n",
      "   1.39503907e-01  7.25284308e-04 -7.87474390e-01 -5.48408825e-01\n",
      "   9.49740829e-01]\n",
      " [ 7.33362063e-01 -6.88427363e-01 -1.38941307e-01  7.39524906e-01\n",
      "  -3.95386943e-01  2.15007743e-01  8.38209609e-01  9.36584736e-02\n",
      "  -8.82735600e-01]\n",
      " [ 7.92491153e-01 -1.03276310e+00  1.93483765e-01  8.36788358e-01\n",
      "  -2.70454135e-01  8.57211290e-03  9.94747309e-01  1.01027802e+00\n",
      "  -5.67905994e-01]\n",
      " [-1.06896765e+00  1.36319314e+00 -2.84991018e-01 -1.30525424e+00\n",
      "   1.39668819e-02  3.44955551e-01 -1.04274315e+00 -8.89687044e-01\n",
      "   8.98661690e-01]\n",
      " [ 1.21794592e+00 -1.25525259e+00  2.57239384e-01  9.45046069e-01\n",
      "  -1.01413752e-01 -3.79966756e-01  8.83547746e-01  9.81593274e-01\n",
      "  -6.97551288e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.79858774  2.1787645  -2.76139194  0.37213905  0.69995387 -3.4847865\n",
      "   1.54281326]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:98 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.20270398]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 98 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.96981623 -1.3796208   0.11795187  1.16718302 -0.23672455 -0.26259824\n",
      "   1.3108265   0.38385325 -0.72714901]\n",
      " [-0.9303084   1.11887576 -0.35035659 -1.17298949  0.14129926  0.00252064\n",
      "  -0.78567903 -0.54840882  0.94974083]\n",
      " [ 0.73081224 -0.69097719 -0.13894131  0.73952491 -0.39793677  0.21245792\n",
      "   0.83565978  0.09365847 -0.8827356 ]\n",
      " [ 0.79059156 -1.03466269  0.19348377  0.83678836 -0.27235372  0.00667252\n",
      "   0.99284772  1.01027802 -0.56790599]\n",
      " [-1.06743129  1.36472949 -0.28499102 -1.30525424  0.01550324  0.3464919\n",
      "  -1.04120679 -0.88968704  0.89866169]\n",
      " [ 1.21650045 -1.25669806  0.25723938  0.94504607 -0.10285921 -0.38141222\n",
      "   0.88210228  0.98159327 -0.69755129]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.81496775  2.16891968 -2.76772196  0.36118409  0.68978651 -3.49140099\n",
      "   1.53314551]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:98 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.92163697]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 98 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.97010779 -1.3796208   0.11795187  1.16747458 -0.23672455 -0.26259824\n",
      "   1.31111806  0.38385325 -0.72714901]\n",
      " [-0.93071653  1.11887576 -0.35035659 -1.17339763  0.14129926  0.00252064\n",
      "  -0.78608717 -0.54840882  0.94974083]\n",
      " [ 0.73135003 -0.69097719 -0.13894131  0.7400627  -0.39793677  0.21245792\n",
      "   0.83619757  0.09365847 -0.8827356 ]\n",
      " [ 0.7910605  -1.03466269  0.19348377  0.8372573  -0.27235372  0.00667252\n",
      "   0.99331666  1.01027802 -0.56790599]\n",
      " [-1.06772931  1.36472949 -0.28499102 -1.30555225  0.01550324  0.3464919\n",
      "  -1.04150481 -0.88968704  0.89866169]\n",
      " [ 1.21687445 -1.25669806  0.25723938  0.94542006 -0.10285921 -0.38141222\n",
      "   0.88247628  0.98159327 -0.69755129]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.81213797  2.1716622  -2.76757282  0.36375741  0.6924243  -3.49131082\n",
      "   1.53584655]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:98 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.76727291]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 98 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.97467347 -1.3796208   0.11795187  1.16747458 -0.23215887 -0.25803257\n",
      "   1.31568373  0.38385325 -0.72714901]\n",
      " [-0.93536681  1.11887576 -0.35035659 -1.17339763  0.13664898 -0.00212964\n",
      "  -0.79073745 -0.54840882  0.94974083]\n",
      " [ 0.73595644 -0.69097719 -0.13894131  0.7400627  -0.39333035  0.21706433\n",
      "   0.84080399  0.09365847 -0.8827356 ]\n",
      " [ 0.7957112  -1.03466269  0.19348377  0.8372573  -0.26770303  0.01132322\n",
      "   0.99796735  1.01027802 -0.56790599]\n",
      " [-1.07231733  1.36472949 -0.28499102 -1.30555225  0.01091521  0.34190388\n",
      "  -1.04609283 -0.88968704  0.89866169]\n",
      " [ 1.22151789 -1.25669806  0.25723938  0.94542006 -0.09821577 -0.37676878\n",
      "   0.88711972  0.98159327 -0.69755129]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.79135949  2.18944716 -2.76400326  0.38036613  0.70946984 -3.48822738\n",
      "   1.5531782 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:98 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.58024612]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 98 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.95979476 -1.3796208   0.11795187  1.15259587 -0.23215887 -0.27291127\n",
      "   1.31568373  0.38385325 -0.74202772]\n",
      " [-0.92046688  1.11887576 -0.35035659 -1.1584977   0.13664898  0.01277029\n",
      "  -0.79073745 -0.54840882  0.96464076]\n",
      " [ 0.72375605 -0.69097719 -0.13894131  0.7278623  -0.39333035  0.20486394\n",
      "   0.84080399  0.09365847 -0.89493599]\n",
      " [ 0.78129189 -1.03466269  0.19348377  0.82283799 -0.26770303 -0.00309609\n",
      "   0.99796735  1.01027802 -0.5823253 ]\n",
      " [-1.05754176  1.36472949 -0.28499102 -1.29077669  0.01091521  0.35667945\n",
      "  -1.04609283 -0.88968704  0.91343726]\n",
      " [ 1.20699819 -1.25669806  0.25723938  0.93090037 -0.09821577 -0.39128848\n",
      "   0.88711972  0.98159327 -0.71207098]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.86202203  2.13568844 -2.78085329  0.33145468  0.65676906 -3.50538535\n",
      "   1.50026084]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:98 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.73502403]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 98 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.96547004 -1.37394552  0.11795187  1.15827115 -0.23215887 -0.26723599\n",
      "   1.32135902  0.38385325 -0.74202772]\n",
      " [-0.92617125  1.11317139 -0.35035659 -1.16420206  0.13664898  0.00706592\n",
      "  -0.79644182 -0.54840882  0.96464076]\n",
      " [ 0.72940421 -0.68532903 -0.13894131  0.73351047 -0.39333035  0.2105121\n",
      "   0.84645215  0.09365847 -0.89493599]\n",
      " [ 0.78706774 -1.02888683  0.19348377  0.82861384 -0.26770303  0.00267977\n",
      "   1.00374321  1.01027802 -0.5823253 ]\n",
      " [-1.06328563  1.35898562 -0.28499102 -1.29652056  0.01091521  0.35093558\n",
      "  -1.0518367  -0.88968704  0.91343726]\n",
      " [ 1.21271503 -1.25098122  0.25723938  0.93661721 -0.09821577 -0.38557163\n",
      "   0.89283656  0.98159327 -0.71207098]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.83621817  2.15775449 -2.77699407  0.353618    0.6781082  -3.50130772\n",
      "   1.52086557]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:98 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.80859305]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 98 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.96851955 -1.37089601  0.11795187  1.16132066 -0.23215887 -0.26723599\n",
      "   1.32440852  0.38385325 -0.74202772]\n",
      " [-0.92942991  1.10991273 -0.35035659 -1.16746072  0.13664898  0.00706592\n",
      "  -0.79970048 -0.54840882  0.96464076]\n",
      " [ 0.73271277 -0.68202047 -0.13894131  0.73681903 -0.39333035  0.2105121\n",
      "   0.84976071  0.09365847 -0.89493599]\n",
      " [ 0.79038117 -1.0255734   0.19348377  0.83192727 -0.26770303  0.00267977\n",
      "   1.00705664  1.01027802 -0.5823253 ]\n",
      " [-1.0663515   1.35591975 -0.28499102 -1.29958642  0.01091521  0.35093558\n",
      "  -1.05490257 -0.88968704  0.91343726]\n",
      " [ 1.21596508 -1.24773117  0.25723938  0.93986725 -0.09821577 -0.38557163\n",
      "   0.8960866   0.98159327 -0.71207098]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.82140612  2.1709086  -2.77484499  0.36599127  0.69041246 -3.49962234\n",
      "   1.53356059]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:98 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.84253304]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 98 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.97062252 -1.37089601  0.12005484  1.16132066 -0.23215887 -0.26513302\n",
      "   1.32651149  0.38385325 -0.74202772]\n",
      " [-0.93157966  1.10991273 -0.35250634 -1.16746072  0.13664898  0.00491617\n",
      "  -0.80185023 -0.54840882  0.96464076]\n",
      " [ 0.73504149 -0.68202047 -0.13661259  0.73681903 -0.39333035  0.21284082\n",
      "   0.85208943  0.09365847 -0.89493599]\n",
      " [ 0.79257828 -1.0255734   0.19568087  0.83192727 -0.26770303  0.00487687\n",
      "   1.00925374  1.01027802 -0.5823253 ]\n",
      " [-1.06851195  1.35591975 -0.28715147 -1.29958642  0.01091521  0.34877513\n",
      "  -1.05706302 -0.88968704  0.91343726]\n",
      " [ 1.21816776 -1.24773117  0.25944207  0.93986725 -0.09821577 -0.38336895\n",
      "   0.89828928  0.98159327 -0.71207098]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.81096046  2.18025823 -2.7736771   0.37475974  0.69960593 -3.49843658\n",
      "   1.54274313]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:98 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.08584749]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 99 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.9706065  -1.37091203  0.12005484  1.16130464 -0.23215887 -0.26513302\n",
      "   1.32651149  0.38385325 -0.74204374]\n",
      " [-0.93155904  1.10993335 -0.35250634 -1.16744011  0.13664898  0.00491617\n",
      "  -0.80185023 -0.54840882  0.96466138]\n",
      " [ 0.73512975 -0.6819322  -0.13661259  0.73690729 -0.39333035  0.21284082\n",
      "   0.85208943  0.09365847 -0.89484773]\n",
      " [ 0.79256429 -1.02558739  0.19568087  0.83191329 -0.26770303  0.00487687\n",
      "   1.00925374  1.01027802 -0.58233929]\n",
      " [-1.068429    1.35600271 -0.28715147 -1.29950347  0.01091521  0.34877513\n",
      "  -1.05706302 -0.88968704  0.91352021]\n",
      " [ 1.21800245 -1.24789648  0.25944207  0.93970194 -0.09821577 -0.38336895\n",
      "   0.89828928  0.98159327 -0.71223629]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.81432901  2.17855794 -2.77534076  0.37316389  0.69790766 -3.50003777\n",
      "   1.54089245]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:99 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.87193686]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 99 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.97190563 -1.37091203  0.12135397  1.16130464 -0.23215887 -0.26513302\n",
      "   1.32781062  0.38385325 -0.74204374]\n",
      " [-0.93302465  1.10993335 -0.35397195 -1.16744011  0.13664898  0.00491617\n",
      "  -0.80331583 -0.54840882  0.96466138]\n",
      " [ 0.73672538 -0.6819322  -0.13501696  0.73690729 -0.39333035  0.21284082\n",
      "   0.85368506  0.09365847 -0.89484773]\n",
      " [ 0.79406667 -1.02558739  0.19718325  0.83191329 -0.26770303  0.00487687\n",
      "   1.01075612  1.01027802 -0.58233929]\n",
      " [-1.06973061  1.35600271 -0.28845308 -1.29950347  0.01091521  0.34877513\n",
      "  -1.05836463 -0.88968704  0.91352021]\n",
      " [ 1.21932415 -1.24789648  0.26076377  0.93970194 -0.09821577 -0.38336895\n",
      "   0.89961099  0.98159327 -0.71223629]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.80717906  2.18512251 -2.77455088  0.37895601  0.70420345 -3.49944995\n",
      "   1.54743436]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:99 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.52386081]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 99 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.95873154 -1.38408612  0.10817988  1.16130464 -0.23215887 -0.26513302\n",
      "   1.31463653  0.38385325 -0.74204374]\n",
      " [-0.9203176   1.12264039 -0.3412649  -1.16744011  0.13664898  0.00491617\n",
      "  -0.79060879 -0.54840882  0.96466138]\n",
      " [ 0.725808   -0.69284958 -0.14593434  0.73690729 -0.39333035  0.21284082\n",
      "   0.84276768  0.09365847 -0.89484773]\n",
      " [ 0.78138821 -1.03826585  0.18450479  0.83191329 -0.26770303  0.00487687\n",
      "   0.99807766  1.01027802 -0.58233929]\n",
      " [-1.05649288  1.36924044 -0.27521535 -1.29950347  0.01091521  0.34877513\n",
      "  -1.04512689 -0.88968704  0.91352021]\n",
      " [ 1.20569053 -1.26153011  0.24713015  0.93970194 -0.09821577 -0.38336895\n",
      "   0.88597737  0.98159327 -0.71223629]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.87251253  2.13672313 -2.79237493  0.33425033  0.6567454  -3.51625406\n",
      "   1.49803106]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:99 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.78536308]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 99 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.96276875 -1.38408612  0.11221709  1.16130464 -0.23215887 -0.26513302\n",
      "   1.31463653  0.38789046 -0.74204374]\n",
      " [-0.92427498  1.12264039 -0.34522228 -1.16744011  0.13664898  0.00491617\n",
      "  -0.79060879 -0.5523662   0.96466138]\n",
      " [ 0.7285332  -0.69284958 -0.14320915  0.73690729 -0.39333035  0.21284082\n",
      "   0.84276768  0.09638367 -0.89484773]\n",
      " [ 0.78521022 -1.03826585  0.1883268   0.83191329 -0.26770303  0.00487687\n",
      "   0.99807766  1.01410003 -0.58233929]\n",
      " [-1.06004017  1.36924044 -0.27876264 -1.29950347  0.01091521  0.34877513\n",
      "  -1.04512689 -0.89323434  0.91352021]\n",
      " [ 1.2089535  -1.26153011  0.25039312  0.93970194 -0.09821577 -0.38336895\n",
      "   0.88597737  0.98485625 -0.71223629]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.85442208  2.15137849 -2.78983071  0.34623151  0.67263375 -3.51448399\n",
      "   1.51466367]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:99 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.61154583]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 99 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.97299758 -1.38408612  0.11221709  1.17153347 -0.23215887 -0.26513302\n",
      "   1.31463653  0.38789046 -0.73181491]\n",
      " [-0.9338863   1.12264039 -0.34522228 -1.17705142  0.13664898  0.00491617\n",
      "  -0.79060879 -0.5523662   0.95505006]\n",
      " [ 0.73460706 -0.69284958 -0.14320915  0.74298116 -0.39333035  0.21284082\n",
      "   0.84276768  0.09638367 -0.88877386]\n",
      " [ 0.79444559 -1.03826585  0.1883268   0.84114866 -0.26770303  0.00487687\n",
      "   0.99807766  1.01410003 -0.57310391]\n",
      " [-1.07033371  1.36924044 -0.27876264 -1.30979702  0.01091521  0.34877513\n",
      "  -1.04512689 -0.89323434  0.90322666]\n",
      " [ 1.21923948 -1.26153011  0.25039312  0.94998792 -0.09821577 -0.38336895\n",
      "   0.88597737  0.98485625 -0.70195032]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.80828198  2.18825905 -2.77853998  0.37571043  0.70667787 -3.50568902\n",
      "   1.55194024]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:99 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.04814831]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 99 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.97306708 -1.38408612  0.11221709  1.17153347 -0.23208937 -0.26506353\n",
      "   1.31463653  0.38789046 -0.73174541]\n",
      " [-0.93393089  1.12264039 -0.34522228 -1.17705142  0.13660439  0.00487158\n",
      "  -0.79060879 -0.5523662   0.95500547]\n",
      " [ 0.73469684 -0.69284958 -0.14320915  0.74298116 -0.39324058  0.21293059\n",
      "   0.84276768  0.09638367 -0.88868409]\n",
      " [ 0.79445703 -1.03826585  0.1883268   0.84114866 -0.26769159  0.00488831\n",
      "   0.99807766  1.01410003 -0.57309248]\n",
      " [-1.07038635  1.36924044 -0.27876264 -1.30979702  0.01086258  0.3487225\n",
      "  -1.04512689 -0.89323434  0.90317403]\n",
      " [ 1.21922964 -1.26153011  0.25039312  0.94998792 -0.09822562 -0.3833788\n",
      "   0.88597737  0.98485625 -0.70196016]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.8093853   2.18777765 -2.77913643  0.37525022  0.70613765 -3.50629364\n",
      "   1.55137874]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:99 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.00273424]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 99 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.97306791 -1.38408529  0.11221709  1.17153347 -0.23208854 -0.26506269\n",
      "   1.31463653  0.38789046 -0.73174458]\n",
      " [-0.9339317   1.12263958 -0.34522228 -1.17705142  0.13660358  0.00487077\n",
      "  -0.79060879 -0.5523662   0.95500466]\n",
      " [ 0.73469758 -0.69284884 -0.14320915  0.74298116 -0.39323984  0.21293134\n",
      "   0.84276768  0.09638367 -0.88868335]\n",
      " [ 0.79445779 -1.03826509  0.1883268   0.84114866 -0.26769083  0.00488907\n",
      "   0.99807766  1.01410003 -0.57309171]\n",
      " [-1.07038718  1.36923961 -0.27876264 -1.30979702  0.01086175  0.34872167\n",
      "  -1.04512689 -0.89323434  0.9031732 ]\n",
      " [ 1.21923044 -1.26152931  0.25039312  0.94998792 -0.09822482 -0.383378\n",
      "   0.88597737  0.98485625 -0.70195936]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.80938902  2.18777704 -2.77913935  0.37524924  0.70613671 -3.50629672\n",
      "   1.55137789]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:99 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.20005891]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 99 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.97150658 -1.38564662  0.11221709  1.17153347 -0.23364987 -0.26662402\n",
      "   1.3130752   0.38789046 -0.73174458]\n",
      " [-0.93218336  1.12438793 -0.34522228 -1.17705142  0.13835193  0.00661912\n",
      "  -0.78886044 -0.5523662   0.95500466]\n",
      " [ 0.73220145 -0.69534496 -0.14320915  0.74298116 -0.39573596  0.21043521\n",
      "   0.84027155  0.09638367 -0.88868335]\n",
      " [ 0.79260505 -1.04011783  0.1883268   0.84114866 -0.26954357  0.00303633\n",
      "   0.99622492  1.01410003 -0.57309171]\n",
      " [-1.06889606  1.37073073 -0.27876264 -1.30979702  0.01235287  0.35021279\n",
      "  -1.04363577 -0.89323434  0.9031732 ]\n",
      " [ 1.21782786 -1.26293188  0.25039312  0.94998792 -0.09962739 -0.38478057\n",
      "   0.88457479  0.98485625 -0.70195936]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.82539727  2.17816824 -2.78533269  0.36453744  0.69620435 -3.51277228\n",
      "   1.54194034]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:99 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.92295245]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 99 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.97178732 -1.38564662  0.11221709  1.17181421 -0.23364987 -0.26662402\n",
      "   1.31335594  0.38789046 -0.73174458]\n",
      " [-0.93257649  1.12438793 -0.34522228 -1.17744456  0.13835193  0.00661912\n",
      "  -0.78925358 -0.5523662   0.95500466]\n",
      " [ 0.73272018 -0.69534496 -0.14320915  0.74349988 -0.39573596  0.21043521\n",
      "   0.84079028  0.09638367 -0.88868335]\n",
      " [ 0.79305689 -1.04011783  0.1883268   0.84160049 -0.26954357  0.00303633\n",
      "   0.99667675  1.01410003 -0.57309171]\n",
      " [-1.069183    1.37073073 -0.27876264 -1.31008396  0.01235287  0.35021279\n",
      "  -1.04392272 -0.89323434  0.9031732 ]\n",
      " [ 1.21818809 -1.26293188  0.25039312  0.95034814 -0.09962739 -0.38478057\n",
      "   0.88493502  0.98485625 -0.70195936]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.8226578   2.18082392 -2.78518956  0.36703077  0.69875964 -3.5126857\n",
      "   1.54455623]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:99 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.76933771]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 99 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.97628239 -1.38564662  0.11221709  1.17181421 -0.2291548  -0.26212895\n",
      "   1.31785101  0.38789046 -0.73174458]\n",
      " [-0.93715652  1.12438793 -0.34522228 -1.17744456  0.13377189  0.00203908\n",
      "  -0.79383361 -0.5523662   0.95500466]\n",
      " [ 0.73726081 -0.69534496 -0.14320915  0.74349988 -0.39119533  0.21497584\n",
      "   0.84533091  0.09638367 -0.88868335]\n",
      " [ 0.79763806 -1.04011783  0.1883268   0.84160049 -0.2649624   0.0076175\n",
      "   1.00125792  1.01410003 -0.57309171]\n",
      " [-1.07370013  1.37073073 -0.27876264 -1.31008396  0.00783574  0.34569565\n",
      "  -1.04843985 -0.89323434  0.9031732 ]\n",
      " [ 1.22276094 -1.26293188  0.25039312  0.95034814 -0.09505454 -0.38020772\n",
      "   0.88950787  0.98485625 -0.70195936]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.80219146  2.19834919 -2.78168489  0.38341011  0.71556277 -3.50965711\n",
      "   1.5616378 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:99 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.57876715]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 99 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 0.9614413  -1.38564662  0.11221709  1.15697312 -0.2291548  -0.27697004\n",
      "   1.31785101  0.38789046 -0.74658567]\n",
      " [-0.9222984   1.12438793 -0.34522228 -1.16258644  0.13377189  0.01689721\n",
      "  -0.79383361 -0.5523662   0.96986278]\n",
      " [ 0.72511395 -0.69534496 -0.14320915  0.73135302 -0.39119533  0.20282898\n",
      "   0.84533091  0.09638367 -0.90083021]\n",
      " [ 0.78325817 -1.04011783  0.1883268   0.82722061 -0.2649624  -0.00676238\n",
      "   1.00125792  1.01410003 -0.5874716 ]\n",
      " [-1.0589609   1.37073073 -0.27876264 -1.29534472  0.00783574  0.36043489\n",
      "  -1.04843985 -0.89323434  0.91791243]\n",
      " [ 1.20827443 -1.26293188  0.25039312  0.93586164 -0.09505454 -0.39469423\n",
      "   0.88950787  0.98485625 -0.71644587]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.87274194  2.14471097 -2.79855431  0.33462578  0.66298064 -3.52681888\n",
      "   1.5088266 ]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:99 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.7358397]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 99 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 9.67089523e-01 -1.37999840e+00  1.12217090e-01  1.16262134e+00\n",
      "  -2.29154796e-01 -2.71321818e-01  1.32349923e+00  3.87890464e-01\n",
      "  -7.46585669e-01]\n",
      " [-9.27974441e-01  1.11871189e+00 -3.45222281e-01 -1.16826248e+00\n",
      "   1.33771895e-01  1.12211684e-02 -7.99509646e-01 -5.52366204e-01\n",
      "   9.69862778e-01]\n",
      " [ 7.30730861e-01 -6.89728048e-01 -1.43209146e-01  7.36969934e-01\n",
      "  -3.91195334e-01  2.08445891e-01  8.50947825e-01  9.63836682e-02\n",
      "  -9.00830212e-01]\n",
      " [ 7.89004849e-01 -1.03437115e+00  1.88326801e-01  8.32967288e-01\n",
      "  -2.64962403e-01 -1.01570571e-03  1.00700460e+00  1.01410003e+00\n",
      "  -5.87471599e-01]\n",
      " [-1.06467649e+00  1.36501514e+00 -2.78762641e-01 -1.30106032e+00\n",
      "   7.83573791e-03  3.54719295e-01 -1.05415544e+00 -8.93234339e-01\n",
      "   9.17912433e-01]\n",
      " [ 1.21396169e+00 -1.25724462e+00  2.50393124e-01  9.41548893e-01\n",
      "  -9.50545444e-02 -3.89006974e-01  8.95195123e-01  9.84856248e-01\n",
      "  -7.16445868e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.84706825  2.16665969 -2.79471252  0.35668655  0.68421405 -3.52275695\n",
      "   1.52932321]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:99 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.81019618]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 99 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 9.70092289e-01 -1.37699563e+00  1.12217090e-01  1.16562411e+00\n",
      "  -2.29154796e-01 -2.71321818e-01  1.32650199e+00  3.87890464e-01\n",
      "  -7.46585669e-01]\n",
      " [-9.31183498e-01  1.11550283e+00 -3.45222281e-01 -1.17147153e+00\n",
      "   1.33771895e-01  1.12211684e-02 -8.02718702e-01 -5.52366204e-01\n",
      "   9.69862778e-01]\n",
      " [ 7.33989783e-01 -6.86469126e-01 -1.43209146e-01  7.40228855e-01\n",
      "  -3.91195334e-01  2.08445891e-01  8.54206746e-01  9.63836682e-02\n",
      "  -9.00830212e-01]\n",
      " [ 7.92269013e-01 -1.03110699e+00  1.88326801e-01  8.36231452e-01\n",
      "  -2.64962403e-01 -1.01570571e-03  1.01026876e+00  1.01410003e+00\n",
      "  -5.87471599e-01]\n",
      " [-1.06769528e+00  1.36199635e+00 -2.78762641e-01 -1.30407910e+00\n",
      "   7.83573791e-03  3.54719295e-01 -1.05717423e+00 -8.93234339e-01\n",
      "   9.17912433e-01]\n",
      " [ 1.21716271e+00 -1.25404360e+00  2.50393124e-01  9.44749919e-01\n",
      "  -9.50545444e-02 -3.89006974e-01  8.98396149e-01  9.84856248e-01\n",
      "  -7.16445868e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.83247439  2.179623   -2.79260123  0.36888733  0.69634538 -3.52109966\n",
      "   1.54183527]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:99 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [1.] Net Result: [[0.84266905]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 99 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[ 9.72196136e-01 -1.37699563e+00  1.14320937e-01  1.16562411e+00\n",
      "  -2.29154796e-01 -2.69217971e-01  1.32860584e+00  3.87890464e-01\n",
      "  -7.46585669e-01]\n",
      " [-9.33332725e-01  1.11550283e+00 -3.47371508e-01 -1.17147153e+00\n",
      "   1.33771895e-01  9.07194140e-03 -8.04867929e-01 -5.52366204e-01\n",
      "   9.69862778e-01]\n",
      " [ 7.36314959e-01 -6.86469126e-01 -1.40883970e-01  7.40228855e-01\n",
      "  -3.91195334e-01  2.10771068e-01  8.56531923e-01  9.63836682e-02\n",
      "  -9.00830212e-01]\n",
      " [ 7.94464805e-01 -1.03110699e+00  1.90522592e-01  8.36231452e-01\n",
      "  -2.64962403e-01  1.18008565e-03  1.01246455e+00  1.01410003e+00\n",
      "  -5.87471599e-01]\n",
      " [-1.06985628e+00  1.36199635e+00 -2.80923645e-01 -1.30407910e+00\n",
      "   7.83573791e-03  3.52558291e-01 -1.05933523e+00 -8.93234339e-01\n",
      "   9.17912433e-01]\n",
      " [ 1.21936571e+00 -1.25404360e+00  2.52596121e-01  9.44749919e-01\n",
      "  -9.50545444e-02 -3.86803976e-01  9.00599146e-01  9.84856248e-01\n",
      "  -7.16445868e-01]]\n",
      "\n",
      "Theta two: \n",
      "[[-1.82204509  2.18895195 -2.79143048  0.37764121  0.70552033 -3.51990903\n",
      "   1.5509959 ]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:99 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "(6, 1)\n",
      "\n",
      "\t\tSigmoid Result: (6, 1)\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "(1, 7)\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "(1, 1)\n",
      "\n",
      "\t\tFinal output: \n",
      "(1, 1)\n",
      "\n",
      "Target: [0.] Net Result: [[0.08350985]]\n",
      "\t\tNetwork Error: \n",
      "(1, 1)\n",
      "\n",
      "\t\tOutput Error: \n",
      "(1,)\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "(6, 1)\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "(6, 9)\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "(1, 7)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Main:\n",
    "\n",
    "# load in fishing normalized set \n",
    "data = getArrayFromFile(\"normalizeFish.csv\")\n",
    "inputs = data[:,0:8] # get the input values\n",
    "targets = data[:, 8:9] # get the class values\n",
    "\n",
    "inputsWB = getInputs(inputs) # adds bias to the input matrix\n",
    "\n",
    "numInputNodes = inputsWB.shape[1]\n",
    "numOutputNodes = 1 if np.unique(targets).shape[0] == 2 else np.unique(targets).shape[0]\n",
    "numHiddenNodes = int((2/3)*(numInputNodes+numOutputNodes)) # + 1 for the bias node\n",
    "\n",
    "print(\"Input nodes with bias node: \" + str(numInputNodes))\n",
    "print(\"Output layer: \" + str(numOutputNodes))\n",
    "print(\"Hidden nodes with bias node: \" + str(numHiddenNodes))\n",
    "\n",
    "# inititlize weights\n",
    "# (-1/sqrt(n)) < w < (1/sqrt(n))\n",
    "lowRange = (-1/math.sqrt(numInputNodes))\n",
    "highRange = math.fabs(lowRange)\n",
    "theta1 = np.random.uniform(low=lowRange, high=highRange, size=(numHiddenNodes, numInputNodes))\n",
    "theta2 = np.random.uniform(low=lowRange, high=highRange, size=(numOutputNodes,numHiddenNodes+1))\n",
    "\n",
    "print(\"Theta1 dims: \" + str(theta1.shape))\n",
    "print(\"Theta2 dims: \" + str(theta2.shape))\n",
    "\n",
    "\n",
    "learningRate = .5\n",
    "numberProcess = 0\n",
    "batchError = []\n",
    "for r in range(0, 100): # batch\n",
    "    trackedNetError= []\n",
    "    \n",
    "    for index in range(0, inputsWB.shape[0]): # online\n",
    "        print(color.CYAN+color.BOLD+\"----------- Data for batch: \"+str(r)+\" Online Round: \"+str(index)+\" ------------\"+color.END)\n",
    "        print(\"Inputs: \\n\" + str(inputs[index])+\"\\n\")\n",
    "        print(\"Learning Rate: \\n\" + str(learningRate)+\"\\n\")\n",
    "        print(\"Theta One: \\n\" + str(theta1)+\"\\n\")\n",
    "        print(\"Theta two: \\n\" + str(theta2)+\"\\n\")\n",
    "        print(\"Target: \\n\"+ str(targets[index]) + \"\\n\")\n",
    "        print(color.YELLOW+color.BOLD+\"----------Processing on batch:\"+str(r)+\" with online instance: \"+str(index)+\"-------------\"+color.END)\n",
    "        theta1, theta2, netError = nn(learningRate, theta1, theta2, np.array([inputsWB[index]]), targets[index])\n",
    "        trackedNetError.append(netError[0][0])\n",
    "    numberProcess = numberProcess + 1 \n",
    "    batchError.append(statistics.mean(trackedNetError))\n",
    "saveNet(theta1, theta2, \"./FishWeights.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "netPlot(numberProcess, batchError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember for this set you target yes as 1 and no as 0, confusing I know\n",
    "weightMap = load_objects(\"./FishWeights.pkl\")\n",
    "theta1 = weightMap[\"theta1\"]\n",
    "theta2 = weightMap[\"theta2\"]\n",
    "\n",
    "test = getArrayFromFile(\"normalizeFishTest.csv\")\n",
    "print(test)\n",
    "print(test.shape)\n",
    "test = getInputs(test)\n",
    "\n",
    "testInstance = test\n",
    "\n",
    "classifyFish(testInstance, theta1, theta2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
