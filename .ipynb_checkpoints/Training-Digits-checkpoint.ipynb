{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports:\n",
    "import numpy as np\n",
    "import math, os, pickle\n",
    "from numpy import genfromtxt\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "class color:\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   END = '\\033[0m'\n",
    "\n",
    "def softmax(A):  \n",
    "    expA = np.exp(A)\n",
    "    return expA / expA.sum()\n",
    "    \n",
    "def mapTargetsToEncoded(targets, tMap):\n",
    "    newTargets = []\n",
    "    for item in targets.tolist():\n",
    "        newTargets.append(tMap[int(item[0])])\n",
    "    return newTargets\n",
    "\n",
    "# functions to use:\n",
    "def sigmoid(matrix):\n",
    "    #print(\"SIGMOID: \\n\" +str(matrix)+\"\\n\")\n",
    "    return 1/(1+np.exp(-matrix))\n",
    "\n",
    "def getInputs(inputs):\n",
    "    #add bias to layerOne and inputs\n",
    "    row = inputs.shape[0] if np.ndim(inputs) != 1 else 1\n",
    "    inputBias = np.ones((row,1)) if np.ndim(inputs) != 1 else np.ones((1))\n",
    "    inputsWithBias = np.append(inputBias, inputs, 1) if np.ndim(inputs) != 1 else np.append(inputBias, inputs) \n",
    "    return inputsWithBias\n",
    "\n",
    "def networkError(target, netResult):\n",
    "    print(\"Target: \" + str(target) + \" Net Result: \" + str(netResult))\n",
    "    return .5*np.square(target - netResult)\n",
    "\n",
    "def learning(weights, lr, error, activationsForLayer):\n",
    "    print(\"Weights:  \" + str(weights.shape))\n",
    "    print(\"Learning Rate:  \" + str(lr))\n",
    "    print(\"Error:  \" + str(error.shape))\n",
    "    print(\"Activations: \"+str(activationsForLayer.shape))\n",
    "    return weights+lr*error*activationsForLayer\n",
    "\n",
    "def load_objects(file):\n",
    "    with open(file, 'rb') as input:\n",
    "        return pickle.load(input)\n",
    "\n",
    "def hiddenUnitError(temp, activations, error):\n",
    "    return temp*(1-temp)*(activations*error)\n",
    "\n",
    "def outputError(target, output):\n",
    "    # Eouput = output(1-output)(target - output)\n",
    "    return output*(1-output)*(target - output)\n",
    "\n",
    "def save_it_all(obj, filename):\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    with open(filename, 'wb') as output:  # Overwrites any existing file.\n",
    "        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "def saveNet(theta1, theta2, fileName):\n",
    "    weights = {\"theta1\":theta1, \"theta2\":theta2}\n",
    "    save_it_all(weights, fileName)\n",
    "    \n",
    "def sigmoidDerivative(target, output):\n",
    "    #E = (t − y) * y *  (1− y) // note: derivative of sigmoid func\n",
    "    return (target - output) * output * (1 - output)\n",
    "\n",
    "def netPlot(instance, error):\n",
    "    instance = list(range(0, instance))\n",
    "    \n",
    "    \n",
    "    with plt.rc_context({'axes.edgecolor':'orange', 'xtick.color':'red', 'ytick.color':'green', 'figure.facecolor':'white'}):\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(instance, error)\n",
    "\n",
    "        ax.set(xlabel='instance numebr (s)', ylabel='error (net)',\n",
    "               title='Average Net Error for each batch run')\n",
    "        ax.grid()\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def load_objects(file):\n",
    "    with open(file, 'rb') as input:\n",
    "        return pickle.load(input)\n",
    "    \n",
    "def classifyFish(inputInstance, theta1, theta2):\n",
    "    # just need to feed forward \n",
    "    layerOneActivations = theta1.dot(inputInstance.T)\n",
    "    print(\"\\t\\tLayer One activations: \\n\"+str(layerOneActivations)+\"\\n\")\n",
    "    layerOneSig = sigmoid(layerOneActivations)\n",
    "    print(\"\\t\\tSigmoid Result: \"+ str(layerOneSig) + \"\\n\")\n",
    "    inputsforhiddenlayer = getInputs(layerOneSig.T) \n",
    "    print(\"\\t\\tInputs for the hiden layer is: (b,h1,h2)\\n\"+str(inputsforhiddenlayer)+\"\\n\")\n",
    "\n",
    "    outputActivation = theta2.dot(inputsforhiddenlayer.T) \n",
    "    print(\"\\t\\tActivation for output layer: (h1,h2)\\n\" + str(outputActivation)+\"\\n\")\n",
    "        # inplace of sigmoid use softmax?? http://dataaspirant.com/2017/03/07/difference-between-softmax-function-and-sigmoid-function/\n",
    "    outputFinal = sigmoid(outputActivation)\n",
    "    print(\"\\t\\tFinal output: \\n\"+ str(outputFinal)+\"\\n\")\n",
    "    \n",
    "    return \"Yes\" if outputFinal >= .5 else \"No\"\n",
    "\n",
    "def getArrayFromFile(name):\n",
    "    array = genfromtxt(name, delimiter=',')\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural Net function:\n",
    "def nn(learningRate, theta1, theta2, inputInstance, targetInstance):\n",
    "    \n",
    "    # feed forward:\n",
    "    # phase One\n",
    "    layerOneActivations = theta1.dot(inputInstance.T)\n",
    "    print(\"\\t\\tLayer One activations: \\n\"+str(layerOneActivations)+\"\\n\")\n",
    "    layerOneSig = sigmoid(layerOneActivations)\n",
    "    print(\"\\t\\tSigmoid Result: \"+ str(layerOneSig) + \"\\n\")\n",
    "    inputsforhiddenlayer = getInputs(layerOneSig.T) \n",
    "    print(\"\\t\\tInputs for the hiden layer is: (b,h1,h2)\\n\"+str(inputsforhiddenlayer)+\"\\n\")\n",
    "\n",
    "    outputActivation = theta2.dot(inputsforhiddenlayer.T) \n",
    "    print(\"\\t\\tActivation for output layer: (h1,h2)\\n\" + str(outputActivation)+\"\\n\")\n",
    "\n",
    "    # phase two\n",
    "    outputFinal = softmax(outputActivation)\n",
    "    print(\"\\t\\tFinal output (Softmax): \\n\"+ str(outputFinal)+\"\\n\")\n",
    "\n",
    "    # network error:\n",
    "    netError = networkError(targetInstance, outputFinal)\n",
    "    print(\"\\t\\tNetwork Error: \\n\" + str(netError)+\"\\n\")\n",
    "\n",
    "    # BACKPROPAGATE\n",
    "    outputErr = outputError(targetInstance, outputFinal[0])\n",
    "    print(\"\\t\\tOutput Error: \\n\"+str(outputErr)+\"\\n\")\n",
    "\n",
    "    hidUnitErr = hiddenUnitError(layerOneSig, layerOneActivations, outputErr)\n",
    "    print(\"\\t\\tHidden unit errors: \\n\"+str(hidUnitErr)+\"\\n\")\n",
    "\n",
    "    # learning:\n",
    "    theta1 = learning(theta1, learningRate, hidUnitErr, inputInstance)\n",
    "    print(\"\\t\\tNext round of weights for layerOne: (b,x1,x2) \\n\"+ str(theta1)+\"\\n\")\n",
    "\n",
    "    theta2 = learning(theta2, learningRate, outputErr, inputsforhiddenlayer)\n",
    "    print(\"\\t\\tNext round of weights for layer 2: (b,h1,h2) \\n\"+ str(theta2)+\"\\n\")\n",
    "    \n",
    "    return theta1, theta2, netError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trian the net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14, 1)\n",
      "Input node num including bias: 9\n",
      "Hidden node num including bias: 6\n",
      "Output node num: 1\n",
      "Theta1 dims: (6, 9)\n",
      "Theta2 dims: (1, 7)\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 0 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:0 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "[[-0.25955253]\n",
      " [-0.02968531]\n",
      " [ 0.15190471]\n",
      " [-0.0156538 ]\n",
      " [ 0.17572138]\n",
      " [ 0.24188717]]\n",
      "\n",
      "\t\tSigmoid Result: [[0.43547371]\n",
      " [0.49257922]\n",
      " [0.53790332]\n",
      " [0.49608663]\n",
      " [0.54381765]\n",
      " [0.56017866]]\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "[[1.         0.43547371 0.49257922 0.53790332 0.49608663 0.54381765\n",
      "  0.56017866]]\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "[[0.33497437]]\n",
      "\n",
      "\t\tFinal output (Softmax): \n",
      "[[1.]]\n",
      "\n",
      "Target: [1.] Net Result: [[1.]]\n",
      "\t\tNetwork Error: \n",
      "[[0.]]\n",
      "\n",
      "\t\tOutput Error: \n",
      "[0.]\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "[[-0.]\n",
      " [-0.]\n",
      " [ 0.]\n",
      " [-0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 0 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:0 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "[[-0.06204479]\n",
      " [ 0.27734846]\n",
      " [ 0.05836469]\n",
      " [-0.30454565]\n",
      " [ 0.12210086]\n",
      " [ 0.27693698]]\n",
      "\n",
      "\t\tSigmoid Result: [[0.48449378]\n",
      " [0.56889605]\n",
      " [0.51458703]\n",
      " [0.42444664]\n",
      " [0.53048735]\n",
      " [0.56879513]]\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "[[1.         0.48449378 0.56889605 0.51458703 0.42444664 0.53048735\n",
      "  0.56879513]]\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "[[0.34137119]]\n",
      "\n",
      "\t\tFinal output (Softmax): \n",
      "[[1.]]\n",
      "\n",
      "Target: [0.] Net Result: [[1.]]\n",
      "\t\tNetwork Error: \n",
      "[[0.5]]\n",
      "\n",
      "\t\tOutput Error: \n",
      "[-0.]\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "[[ 0.]\n",
      " [-0.]\n",
      " [-0.]\n",
      " [ 0.]\n",
      " [-0.]\n",
      " [-0.]]\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 0 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:0 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "[[-0.35687861]\n",
      " [ 0.41233336]\n",
      " [ 0.36246912]\n",
      " [-0.20155932]\n",
      " [ 0.06978535]\n",
      " [ 0.52723504]]\n",
      "\n",
      "\t\tSigmoid Result: [[0.41171538]\n",
      " [0.60164724]\n",
      " [0.58963801]\n",
      " [0.44978007]\n",
      " [0.51743926]\n",
      " [0.628838  ]]\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "[[1.         0.41171538 0.60164724 0.58963801 0.44978007 0.51743926\n",
      "  0.628838  ]]\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "[[0.3408198]]\n",
      "\n",
      "\t\tFinal output (Softmax): \n",
      "[[1.]]\n",
      "\n",
      "Target: [1.] Net Result: [[1.]]\n",
      "\t\tNetwork Error: \n",
      "[[0.]]\n",
      "\n",
      "\t\tOutput Error: \n",
      "[0.]\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "[[-0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [-0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 0 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:0 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "[[-0.37568486]\n",
      " [ 0.09994372]\n",
      " [ 0.51974105]\n",
      " [ 0.28188531]\n",
      " [ 0.10102784]\n",
      " [-0.1400135 ]]\n",
      "\n",
      "\t\tSigmoid Result: [[0.40716808]\n",
      " [0.52496515]\n",
      " [0.62708721]\n",
      " [0.57000837]\n",
      " [0.5252355 ]\n",
      " [0.4650537 ]]\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "[[1.         0.40716808 0.52496515 0.62708721 0.57000837 0.5252355\n",
      "  0.4650537 ]]\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "[[0.355973]]\n",
      "\n",
      "\t\tFinal output (Softmax): \n",
      "[[1.]]\n",
      "\n",
      "Target: [1.] Net Result: [[1.]]\n",
      "\t\tNetwork Error: \n",
      "[[0.]]\n",
      "\n",
      "\t\tOutput Error: \n",
      "[0.]\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "[[-0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [-0.]]\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 0 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:0 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "[[ 0.00366957]\n",
      " [-0.01698873]\n",
      " [ 0.63192886]\n",
      " [ 0.15363235]\n",
      " [ 0.30463123]\n",
      " [-0.37487202]]\n",
      "\n",
      "\t\tSigmoid Result: [[0.50091739]\n",
      " [0.49575292]\n",
      " [0.6529267 ]\n",
      " [0.53833272]\n",
      " [0.57557427]\n",
      " [0.4073643 ]]\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "[[1.         0.50091739 0.49575292 0.6529267  0.53833272 0.57557427\n",
      "  0.4073643 ]]\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "[[0.38268296]]\n",
      "\n",
      "\t\tFinal output (Softmax): \n",
      "[[1.]]\n",
      "\n",
      "Target: [0.] Net Result: [[1.]]\n",
      "\t\tNetwork Error: \n",
      "[[0.5]]\n",
      "\n",
      "\t\tOutput Error: \n",
      "[-0.]\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "[[-0.]\n",
      " [ 0.]\n",
      " [-0.]\n",
      " [-0.]\n",
      " [-0.]\n",
      " [ 0.]]\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 0 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:0 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "[[ 0.20117732]\n",
      " [ 0.29004504]\n",
      " [ 0.53838884]\n",
      " [-0.1352595 ]\n",
      " [ 0.25101071]\n",
      " [-0.3398222 ]]\n",
      "\n",
      "\t\tSigmoid Result: [[0.55012539]\n",
      " [0.57200716]\n",
      " [0.63143754]\n",
      " [0.46623658]\n",
      " [0.56242526]\n",
      " [0.41585267]]\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "[[1.         0.55012539 0.57200716 0.63143754 0.46623658 0.56242526\n",
      "  0.41585267]]\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "[[0.38960161]]\n",
      "\n",
      "\t\tFinal output (Softmax): \n",
      "[[1.]]\n",
      "\n",
      "Target: [0.] Net Result: [[1.]]\n",
      "\t\tNetwork Error: \n",
      "[[0.5]]\n",
      "\n",
      "\t\tOutput Error: \n",
      "[-0.]\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "[[-0.]\n",
      " [-0.]\n",
      " [-0.]\n",
      " [ 0.]\n",
      " [-0.]\n",
      " [ 0.]]\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 0 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:0 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "[[ 0.49772805]\n",
      " [-0.2199041 ]\n",
      " [ 0.01861541]\n",
      " [-0.06963844]\n",
      " [-0.20262796]\n",
      " [-0.29817524]]\n",
      "\n",
      "\t\tSigmoid Result: [[0.62192527]\n",
      " [0.44524445]\n",
      " [0.50465372]\n",
      " [0.48259742]\n",
      " [0.44951562]\n",
      " [0.42600362]]\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "[[1.         0.62192527 0.44524445 0.50465372 0.48259742 0.44951562\n",
      "  0.42600362]]\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "[[0.37827782]]\n",
      "\n",
      "\t\tFinal output (Softmax): \n",
      "[[1.]]\n",
      "\n",
      "Target: [0.] Net Result: [[1.]]\n",
      "\t\tNetwork Error: \n",
      "[[0.5]]\n",
      "\n",
      "\t\tOutput Error: \n",
      "[-0.]\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "[[-0.]\n",
      " [ 0.]\n",
      " [-0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 0 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:0 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "[[-7.91341331e-02]\n",
      " [-4.10005422e-01]\n",
      " [-3.23798009e-05]\n",
      " [ 3.47506374e-01]\n",
      " [-3.52610833e-01]\n",
      " [-9.83665425e-02]]\n",
      "\n",
      "\t\tSigmoid Result: [[0.48022678]\n",
      " [0.39891082]\n",
      " [0.49999191]\n",
      " [0.58601275]\n",
      " [0.41274944]\n",
      " [0.47542817]]\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "[[1.         0.48022678 0.39891082 0.49999191 0.58601275 0.41274944\n",
      "  0.47542817]]\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "[[0.34474159]]\n",
      "\n",
      "\t\tFinal output (Softmax): \n",
      "[[1.]]\n",
      "\n",
      "Target: [1.] Net Result: [[1.]]\n",
      "\t\tNetwork Error: \n",
      "[[0.]]\n",
      "\n",
      "\t\tOutput Error: \n",
      "[0.]\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "[[-0.]\n",
      " [-0.]\n",
      " [-0.]\n",
      " [ 0.]\n",
      " [-0.]\n",
      " [-0.]]\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 0 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:0 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "[[ 0.3002203 ]\n",
      " [-0.52693787]\n",
      " [ 0.11215543]\n",
      " [ 0.21925342]\n",
      " [-0.14900744]\n",
      " [-0.33322506]]\n",
      "\n",
      "\t\tSigmoid Result: [[0.57449637]\n",
      " [0.37123136]\n",
      " [0.5280095 ]\n",
      " [0.55459482]\n",
      " [0.46281691]\n",
      " [0.41745612]]\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "[[1.         0.57449637 0.37123136 0.5280095  0.55459482 0.46281691\n",
      "  0.41745612]]\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "[[0.37207069]]\n",
      "\n",
      "\t\tFinal output (Softmax): \n",
      "[[1.]]\n",
      "\n",
      "Target: [1.] Net Result: [[1.]]\n",
      "\t\tNetwork Error: \n",
      "[[0.]]\n",
      "\n",
      "\t\tOutput Error: \n",
      "[0.]\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "[[ 0.]\n",
      " [-0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [-0.]\n",
      " [-0.]]\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 0 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:0 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "[[-0.148686  ]\n",
      " [-0.21405268]\n",
      " [ 0.54043438]\n",
      " [ 0.5185202 ]\n",
      " [-0.18663205]\n",
      " [-0.37740152]]\n",
      "\n",
      "\t\tSigmoid Result: [[0.46289683]\n",
      " [0.44669022]\n",
      " [0.63191346]\n",
      " [0.62680167]\n",
      " [0.45347695]\n",
      " [0.40675377]]\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "[[1.         0.46289683 0.44669022 0.63191346 0.62680167 0.45347695\n",
      "  0.40675377]]\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "[[0.37389635]]\n",
      "\n",
      "\t\tFinal output (Softmax): \n",
      "[[1.]]\n",
      "\n",
      "Target: [0.] Net Result: [[1.]]\n",
      "\t\tNetwork Error: \n",
      "[[0.5]]\n",
      "\n",
      "\t\tOutput Error: \n",
      "[-0.]\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "[[ 0.]\n",
      " [ 0.]\n",
      " [-0.]\n",
      " [-0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 0 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:0 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "[[ 0.34537248]\n",
      " [-0.41696806]\n",
      " [-0.07287907]\n",
      " [ 0.29524941]\n",
      " [-0.69389124]\n",
      " [-0.30070475]]\n",
      "\n",
      "\t\tSigmoid Result: [[0.58549497]\n",
      " [0.3972425 ]\n",
      " [0.48178829]\n",
      " [0.57328079]\n",
      " [0.33316801]\n",
      " [0.42538521]]\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "[[1.         0.58549497 0.3972425  0.48178829 0.57328079 0.33316801\n",
      "  0.42538521]]\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "[[0.36869364]]\n",
      "\n",
      "\t\tFinal output (Softmax): \n",
      "[[1.]]\n",
      "\n",
      "Target: [1.] Net Result: [[1.]]\n",
      "\t\tNetwork Error: \n",
      "[[0.]]\n",
      "\n",
      "\t\tOutput Error: \n",
      "[0.]\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "[[ 0.]\n",
      " [-0.]\n",
      " [-0.]\n",
      " [ 0.]\n",
      " [-0.]\n",
      " [-0.]]\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 0 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:0 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "[[ 0.11837361]\n",
      " [-0.10297165]\n",
      " [-0.0935724 ]\n",
      " [ 0.05861452]\n",
      " [-0.40623135]\n",
      " [-0.06331673]]\n",
      "\n",
      "\t\tSigmoid Result: [[0.5295589 ]\n",
      " [0.47427981]\n",
      " [0.47662395]\n",
      " [0.51464944]\n",
      " [0.39981612]\n",
      " [0.4841761 ]]\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "[[1.         0.5295589  0.47427981 0.47662395 0.51464944 0.39981612\n",
      "  0.4841761 ]]\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "[[0.35113581]]\n",
      "\n",
      "\t\tFinal output (Softmax): \n",
      "[[1.]]\n",
      "\n",
      "Target: [1.] Net Result: [[1.]]\n",
      "\t\tNetwork Error: \n",
      "[[0.]]\n",
      "\n",
      "\t\tOutput Error: \n",
      "[0.]\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "[[ 0.]\n",
      " [-0.]\n",
      " [-0.]\n",
      " [ 0.]\n",
      " [-0.]\n",
      " [-0.]]\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 0 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:0 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "[[-0.03255367]\n",
      " [-0.34368171]\n",
      " [ 0.17259804]\n",
      " [ 0.22098109]\n",
      " [-0.11193851]\n",
      " [ 0.00449915]]\n",
      "\n",
      "\t\tSigmoid Result: [[0.4918623 ]\n",
      " [0.41491542]\n",
      " [0.54304271]\n",
      " [0.55502155]\n",
      " [0.47204456]\n",
      " [0.50112479]]\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "[[1.         0.4918623  0.41491542 0.54304271 0.55502155 0.47204456\n",
      "  0.50112479]]\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "[[0.35297247]]\n",
      "\n",
      "\t\tFinal output (Softmax): \n",
      "[[1.]]\n",
      "\n",
      "Target: [1.] Net Result: [[1.]]\n",
      "\t\tNetwork Error: \n",
      "[[0.]]\n",
      "\n",
      "\t\tOutput Error: \n",
      "[0.]\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "[[-0.]\n",
      " [-0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [-0.]\n",
      " [ 0.]]\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 0 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:0 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "[[-0.17817711]\n",
      " [ 0.40697749]\n",
      " [ 0.42620103]\n",
      " [-0.00700655]\n",
      " [ 0.04740732]\n",
      " [-0.10496369]]\n",
      "\n",
      "\t\tSigmoid Result: [[0.45557319]\n",
      " [0.60036292]\n",
      " [0.60496615]\n",
      " [0.49824837]\n",
      " [0.51184961]\n",
      " [0.47378314]]\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "[[1.         0.45557319 0.60036292 0.60496615 0.49824837 0.51184961\n",
      "  0.47378314]]\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "[[0.3625889]]\n",
      "\n",
      "\t\tFinal output (Softmax): \n",
      "[[1.]]\n",
      "\n",
      "Target: [0.] Net Result: [[1.]]\n",
      "\t\tNetwork Error: \n",
      "[[0.5]]\n",
      "\n",
      "\t\tOutput Error: \n",
      "[-0.]\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "[[ 0.]\n",
      " [-0.]\n",
      " [-0.]\n",
      " [ 0.]\n",
      " [-0.]\n",
      " [ 0.]]\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 1 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:1 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "[[-0.25955253]\n",
      " [-0.02968531]\n",
      " [ 0.15190471]\n",
      " [-0.0156538 ]\n",
      " [ 0.17572138]\n",
      " [ 0.24188717]]\n",
      "\n",
      "\t\tSigmoid Result: [[0.43547371]\n",
      " [0.49257922]\n",
      " [0.53790332]\n",
      " [0.49608663]\n",
      " [0.54381765]\n",
      " [0.56017866]]\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "[[1.         0.43547371 0.49257922 0.53790332 0.49608663 0.54381765\n",
      "  0.56017866]]\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "[[0.33497437]]\n",
      "\n",
      "\t\tFinal output (Softmax): \n",
      "[[1.]]\n",
      "\n",
      "Target: [1.] Net Result: [[1.]]\n",
      "\t\tNetwork Error: \n",
      "[[0.]]\n",
      "\n",
      "\t\tOutput Error: \n",
      "[0.]\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "[[-0.]\n",
      " [-0.]\n",
      " [ 0.]\n",
      " [-0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 1 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:1 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "[[-0.06204479]\n",
      " [ 0.27734846]\n",
      " [ 0.05836469]\n",
      " [-0.30454565]\n",
      " [ 0.12210086]\n",
      " [ 0.27693698]]\n",
      "\n",
      "\t\tSigmoid Result: [[0.48449378]\n",
      " [0.56889605]\n",
      " [0.51458703]\n",
      " [0.42444664]\n",
      " [0.53048735]\n",
      " [0.56879513]]\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "[[1.         0.48449378 0.56889605 0.51458703 0.42444664 0.53048735\n",
      "  0.56879513]]\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "[[0.34137119]]\n",
      "\n",
      "\t\tFinal output (Softmax): \n",
      "[[1.]]\n",
      "\n",
      "Target: [0.] Net Result: [[1.]]\n",
      "\t\tNetwork Error: \n",
      "[[0.5]]\n",
      "\n",
      "\t\tOutput Error: \n",
      "[-0.]\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "[[ 0.]\n",
      " [-0.]\n",
      " [-0.]\n",
      " [ 0.]\n",
      " [-0.]\n",
      " [-0.]]\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 1 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:1 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "[[-0.35687861]\n",
      " [ 0.41233336]\n",
      " [ 0.36246912]\n",
      " [-0.20155932]\n",
      " [ 0.06978535]\n",
      " [ 0.52723504]]\n",
      "\n",
      "\t\tSigmoid Result: [[0.41171538]\n",
      " [0.60164724]\n",
      " [0.58963801]\n",
      " [0.44978007]\n",
      " [0.51743926]\n",
      " [0.628838  ]]\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "[[1.         0.41171538 0.60164724 0.58963801 0.44978007 0.51743926\n",
      "  0.628838  ]]\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "[[0.3408198]]\n",
      "\n",
      "\t\tFinal output (Softmax): \n",
      "[[1.]]\n",
      "\n",
      "Target: [1.] Net Result: [[1.]]\n",
      "\t\tNetwork Error: \n",
      "[[0.]]\n",
      "\n",
      "\t\tOutput Error: \n",
      "[0.]\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "[[-0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [-0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 1 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:1 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "[[-0.37568486]\n",
      " [ 0.09994372]\n",
      " [ 0.51974105]\n",
      " [ 0.28188531]\n",
      " [ 0.10102784]\n",
      " [-0.1400135 ]]\n",
      "\n",
      "\t\tSigmoid Result: [[0.40716808]\n",
      " [0.52496515]\n",
      " [0.62708721]\n",
      " [0.57000837]\n",
      " [0.5252355 ]\n",
      " [0.4650537 ]]\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "[[1.         0.40716808 0.52496515 0.62708721 0.57000837 0.5252355\n",
      "  0.4650537 ]]\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "[[0.355973]]\n",
      "\n",
      "\t\tFinal output (Softmax): \n",
      "[[1.]]\n",
      "\n",
      "Target: [1.] Net Result: [[1.]]\n",
      "\t\tNetwork Error: \n",
      "[[0.]]\n",
      "\n",
      "\t\tOutput Error: \n",
      "[0.]\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "[[-0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [-0.]]\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 1 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:1 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "[[ 0.00366957]\n",
      " [-0.01698873]\n",
      " [ 0.63192886]\n",
      " [ 0.15363235]\n",
      " [ 0.30463123]\n",
      " [-0.37487202]]\n",
      "\n",
      "\t\tSigmoid Result: [[0.50091739]\n",
      " [0.49575292]\n",
      " [0.6529267 ]\n",
      " [0.53833272]\n",
      " [0.57557427]\n",
      " [0.4073643 ]]\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "[[1.         0.50091739 0.49575292 0.6529267  0.53833272 0.57557427\n",
      "  0.4073643 ]]\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "[[0.38268296]]\n",
      "\n",
      "\t\tFinal output (Softmax): \n",
      "[[1.]]\n",
      "\n",
      "Target: [0.] Net Result: [[1.]]\n",
      "\t\tNetwork Error: \n",
      "[[0.5]]\n",
      "\n",
      "\t\tOutput Error: \n",
      "[-0.]\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "[[-0.]\n",
      " [ 0.]\n",
      " [-0.]\n",
      " [-0.]\n",
      " [-0.]\n",
      " [ 0.]]\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 1 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:1 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "[[ 0.20117732]\n",
      " [ 0.29004504]\n",
      " [ 0.53838884]\n",
      " [-0.1352595 ]\n",
      " [ 0.25101071]\n",
      " [-0.3398222 ]]\n",
      "\n",
      "\t\tSigmoid Result: [[0.55012539]\n",
      " [0.57200716]\n",
      " [0.63143754]\n",
      " [0.46623658]\n",
      " [0.56242526]\n",
      " [0.41585267]]\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "[[1.         0.55012539 0.57200716 0.63143754 0.46623658 0.56242526\n",
      "  0.41585267]]\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "[[0.38960161]]\n",
      "\n",
      "\t\tFinal output (Softmax): \n",
      "[[1.]]\n",
      "\n",
      "Target: [0.] Net Result: [[1.]]\n",
      "\t\tNetwork Error: \n",
      "[[0.5]]\n",
      "\n",
      "\t\tOutput Error: \n",
      "[-0.]\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "[[-0.]\n",
      " [-0.]\n",
      " [-0.]\n",
      " [ 0.]\n",
      " [-0.]\n",
      " [ 0.]]\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 1 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:1 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "[[ 0.49772805]\n",
      " [-0.2199041 ]\n",
      " [ 0.01861541]\n",
      " [-0.06963844]\n",
      " [-0.20262796]\n",
      " [-0.29817524]]\n",
      "\n",
      "\t\tSigmoid Result: [[0.62192527]\n",
      " [0.44524445]\n",
      " [0.50465372]\n",
      " [0.48259742]\n",
      " [0.44951562]\n",
      " [0.42600362]]\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "[[1.         0.62192527 0.44524445 0.50465372 0.48259742 0.44951562\n",
      "  0.42600362]]\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "[[0.37827782]]\n",
      "\n",
      "\t\tFinal output (Softmax): \n",
      "[[1.]]\n",
      "\n",
      "Target: [0.] Net Result: [[1.]]\n",
      "\t\tNetwork Error: \n",
      "[[0.5]]\n",
      "\n",
      "\t\tOutput Error: \n",
      "[-0.]\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "[[-0.]\n",
      " [ 0.]\n",
      " [-0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 1 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:1 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "[[-7.91341331e-02]\n",
      " [-4.10005422e-01]\n",
      " [-3.23798009e-05]\n",
      " [ 3.47506374e-01]\n",
      " [-3.52610833e-01]\n",
      " [-9.83665425e-02]]\n",
      "\n",
      "\t\tSigmoid Result: [[0.48022678]\n",
      " [0.39891082]\n",
      " [0.49999191]\n",
      " [0.58601275]\n",
      " [0.41274944]\n",
      " [0.47542817]]\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "[[1.         0.48022678 0.39891082 0.49999191 0.58601275 0.41274944\n",
      "  0.47542817]]\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "[[0.34474159]]\n",
      "\n",
      "\t\tFinal output (Softmax): \n",
      "[[1.]]\n",
      "\n",
      "Target: [1.] Net Result: [[1.]]\n",
      "\t\tNetwork Error: \n",
      "[[0.]]\n",
      "\n",
      "\t\tOutput Error: \n",
      "[0.]\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "[[-0.]\n",
      " [-0.]\n",
      " [-0.]\n",
      " [ 0.]\n",
      " [-0.]\n",
      " [-0.]]\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 1 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:1 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "[[ 0.3002203 ]\n",
      " [-0.52693787]\n",
      " [ 0.11215543]\n",
      " [ 0.21925342]\n",
      " [-0.14900744]\n",
      " [-0.33322506]]\n",
      "\n",
      "\t\tSigmoid Result: [[0.57449637]\n",
      " [0.37123136]\n",
      " [0.5280095 ]\n",
      " [0.55459482]\n",
      " [0.46281691]\n",
      " [0.41745612]]\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "[[1.         0.57449637 0.37123136 0.5280095  0.55459482 0.46281691\n",
      "  0.41745612]]\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "[[0.37207069]]\n",
      "\n",
      "\t\tFinal output (Softmax): \n",
      "[[1.]]\n",
      "\n",
      "Target: [1.] Net Result: [[1.]]\n",
      "\t\tNetwork Error: \n",
      "[[0.]]\n",
      "\n",
      "\t\tOutput Error: \n",
      "[0.]\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "[[ 0.]\n",
      " [-0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [-0.]\n",
      " [-0.]]\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 1 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:1 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "[[-0.148686  ]\n",
      " [-0.21405268]\n",
      " [ 0.54043438]\n",
      " [ 0.5185202 ]\n",
      " [-0.18663205]\n",
      " [-0.37740152]]\n",
      "\n",
      "\t\tSigmoid Result: [[0.46289683]\n",
      " [0.44669022]\n",
      " [0.63191346]\n",
      " [0.62680167]\n",
      " [0.45347695]\n",
      " [0.40675377]]\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "[[1.         0.46289683 0.44669022 0.63191346 0.62680167 0.45347695\n",
      "  0.40675377]]\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "[[0.37389635]]\n",
      "\n",
      "\t\tFinal output (Softmax): \n",
      "[[1.]]\n",
      "\n",
      "Target: [0.] Net Result: [[1.]]\n",
      "\t\tNetwork Error: \n",
      "[[0.5]]\n",
      "\n",
      "\t\tOutput Error: \n",
      "[-0.]\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "[[ 0.]\n",
      " [ 0.]\n",
      " [-0.]\n",
      " [-0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 1 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:1 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "[[ 0.34537248]\n",
      " [-0.41696806]\n",
      " [-0.07287907]\n",
      " [ 0.29524941]\n",
      " [-0.69389124]\n",
      " [-0.30070475]]\n",
      "\n",
      "\t\tSigmoid Result: [[0.58549497]\n",
      " [0.3972425 ]\n",
      " [0.48178829]\n",
      " [0.57328079]\n",
      " [0.33316801]\n",
      " [0.42538521]]\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "[[1.         0.58549497 0.3972425  0.48178829 0.57328079 0.33316801\n",
      "  0.42538521]]\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "[[0.36869364]]\n",
      "\n",
      "\t\tFinal output (Softmax): \n",
      "[[1.]]\n",
      "\n",
      "Target: [1.] Net Result: [[1.]]\n",
      "\t\tNetwork Error: \n",
      "[[0.]]\n",
      "\n",
      "\t\tOutput Error: \n",
      "[0.]\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "[[ 0.]\n",
      " [-0.]\n",
      " [-0.]\n",
      " [ 0.]\n",
      " [-0.]\n",
      " [-0.]]\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 1 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:1 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "[[ 0.11837361]\n",
      " [-0.10297165]\n",
      " [-0.0935724 ]\n",
      " [ 0.05861452]\n",
      " [-0.40623135]\n",
      " [-0.06331673]]\n",
      "\n",
      "\t\tSigmoid Result: [[0.5295589 ]\n",
      " [0.47427981]\n",
      " [0.47662395]\n",
      " [0.51464944]\n",
      " [0.39981612]\n",
      " [0.4841761 ]]\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "[[1.         0.5295589  0.47427981 0.47662395 0.51464944 0.39981612\n",
      "  0.4841761 ]]\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "[[0.35113581]]\n",
      "\n",
      "\t\tFinal output (Softmax): \n",
      "[[1.]]\n",
      "\n",
      "Target: [1.] Net Result: [[1.]]\n",
      "\t\tNetwork Error: \n",
      "[[0.]]\n",
      "\n",
      "\t\tOutput Error: \n",
      "[0.]\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "[[ 0.]\n",
      " [-0.]\n",
      " [-0.]\n",
      " [ 0.]\n",
      " [-0.]\n",
      " [-0.]]\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 1 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:1 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "[[-0.03255367]\n",
      " [-0.34368171]\n",
      " [ 0.17259804]\n",
      " [ 0.22098109]\n",
      " [-0.11193851]\n",
      " [ 0.00449915]]\n",
      "\n",
      "\t\tSigmoid Result: [[0.4918623 ]\n",
      " [0.41491542]\n",
      " [0.54304271]\n",
      " [0.55502155]\n",
      " [0.47204456]\n",
      " [0.50112479]]\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "[[1.         0.4918623  0.41491542 0.54304271 0.55502155 0.47204456\n",
      "  0.50112479]]\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "[[0.35297247]]\n",
      "\n",
      "\t\tFinal output (Softmax): \n",
      "[[1.]]\n",
      "\n",
      "Target: [1.] Net Result: [[1.]]\n",
      "\t\tNetwork Error: \n",
      "[[0.]]\n",
      "\n",
      "\t\tOutput Error: \n",
      "[0.]\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "[[-0.]\n",
      " [-0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [-0.]\n",
      " [ 0.]]\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 1 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:1 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "[[-0.17817711]\n",
      " [ 0.40697749]\n",
      " [ 0.42620103]\n",
      " [-0.00700655]\n",
      " [ 0.04740732]\n",
      " [-0.10496369]]\n",
      "\n",
      "\t\tSigmoid Result: [[0.45557319]\n",
      " [0.60036292]\n",
      " [0.60496615]\n",
      " [0.49824837]\n",
      " [0.51184961]\n",
      " [0.47378314]]\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "[[1.         0.45557319 0.60036292 0.60496615 0.49824837 0.51184961\n",
      "  0.47378314]]\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "[[0.3625889]]\n",
      "\n",
      "\t\tFinal output (Softmax): \n",
      "[[1.]]\n",
      "\n",
      "Target: [0.] Net Result: [[1.]]\n",
      "\t\tNetwork Error: \n",
      "[[0.5]]\n",
      "\n",
      "\t\tOutput Error: \n",
      "[-0.]\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "[[ 0.]\n",
      " [-0.]\n",
      " [-0.]\n",
      " [ 0.]\n",
      " [-0.]\n",
      " [ 0.]]\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 2 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:2 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "[[-0.25955253]\n",
      " [-0.02968531]\n",
      " [ 0.15190471]\n",
      " [-0.0156538 ]\n",
      " [ 0.17572138]\n",
      " [ 0.24188717]]\n",
      "\n",
      "\t\tSigmoid Result: [[0.43547371]\n",
      " [0.49257922]\n",
      " [0.53790332]\n",
      " [0.49608663]\n",
      " [0.54381765]\n",
      " [0.56017866]]\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "[[1.         0.43547371 0.49257922 0.53790332 0.49608663 0.54381765\n",
      "  0.56017866]]\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "[[0.33497437]]\n",
      "\n",
      "\t\tFinal output (Softmax): \n",
      "[[1.]]\n",
      "\n",
      "Target: [1.] Net Result: [[1.]]\n",
      "\t\tNetwork Error: \n",
      "[[0.]]\n",
      "\n",
      "\t\tOutput Error: \n",
      "[0.]\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "[[-0.]\n",
      " [-0.]\n",
      " [ 0.]\n",
      " [-0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 2 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:2 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "[[-0.06204479]\n",
      " [ 0.27734846]\n",
      " [ 0.05836469]\n",
      " [-0.30454565]\n",
      " [ 0.12210086]\n",
      " [ 0.27693698]]\n",
      "\n",
      "\t\tSigmoid Result: [[0.48449378]\n",
      " [0.56889605]\n",
      " [0.51458703]\n",
      " [0.42444664]\n",
      " [0.53048735]\n",
      " [0.56879513]]\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "[[1.         0.48449378 0.56889605 0.51458703 0.42444664 0.53048735\n",
      "  0.56879513]]\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "[[0.34137119]]\n",
      "\n",
      "\t\tFinal output (Softmax): \n",
      "[[1.]]\n",
      "\n",
      "Target: [0.] Net Result: [[1.]]\n",
      "\t\tNetwork Error: \n",
      "[[0.5]]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tOutput Error: \n",
      "[-0.]\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "[[ 0.]\n",
      " [-0.]\n",
      " [-0.]\n",
      " [ 0.]\n",
      " [-0.]\n",
      " [-0.]]\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 2 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:2 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "[[-0.35687861]\n",
      " [ 0.41233336]\n",
      " [ 0.36246912]\n",
      " [-0.20155932]\n",
      " [ 0.06978535]\n",
      " [ 0.52723504]]\n",
      "\n",
      "\t\tSigmoid Result: [[0.41171538]\n",
      " [0.60164724]\n",
      " [0.58963801]\n",
      " [0.44978007]\n",
      " [0.51743926]\n",
      " [0.628838  ]]\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "[[1.         0.41171538 0.60164724 0.58963801 0.44978007 0.51743926\n",
      "  0.628838  ]]\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "[[0.3408198]]\n",
      "\n",
      "\t\tFinal output (Softmax): \n",
      "[[1.]]\n",
      "\n",
      "Target: [1.] Net Result: [[1.]]\n",
      "\t\tNetwork Error: \n",
      "[[0.]]\n",
      "\n",
      "\t\tOutput Error: \n",
      "[0.]\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "[[-0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [-0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 2 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:2 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "[[-0.37568486]\n",
      " [ 0.09994372]\n",
      " [ 0.51974105]\n",
      " [ 0.28188531]\n",
      " [ 0.10102784]\n",
      " [-0.1400135 ]]\n",
      "\n",
      "\t\tSigmoid Result: [[0.40716808]\n",
      " [0.52496515]\n",
      " [0.62708721]\n",
      " [0.57000837]\n",
      " [0.5252355 ]\n",
      " [0.4650537 ]]\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "[[1.         0.40716808 0.52496515 0.62708721 0.57000837 0.5252355\n",
      "  0.4650537 ]]\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "[[0.355973]]\n",
      "\n",
      "\t\tFinal output (Softmax): \n",
      "[[1.]]\n",
      "\n",
      "Target: [1.] Net Result: [[1.]]\n",
      "\t\tNetwork Error: \n",
      "[[0.]]\n",
      "\n",
      "\t\tOutput Error: \n",
      "[0.]\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "[[-0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [-0.]]\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 2 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:2 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "[[ 0.00366957]\n",
      " [-0.01698873]\n",
      " [ 0.63192886]\n",
      " [ 0.15363235]\n",
      " [ 0.30463123]\n",
      " [-0.37487202]]\n",
      "\n",
      "\t\tSigmoid Result: [[0.50091739]\n",
      " [0.49575292]\n",
      " [0.6529267 ]\n",
      " [0.53833272]\n",
      " [0.57557427]\n",
      " [0.4073643 ]]\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "[[1.         0.50091739 0.49575292 0.6529267  0.53833272 0.57557427\n",
      "  0.4073643 ]]\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "[[0.38268296]]\n",
      "\n",
      "\t\tFinal output (Softmax): \n",
      "[[1.]]\n",
      "\n",
      "Target: [0.] Net Result: [[1.]]\n",
      "\t\tNetwork Error: \n",
      "[[0.5]]\n",
      "\n",
      "\t\tOutput Error: \n",
      "[-0.]\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "[[-0.]\n",
      " [ 0.]\n",
      " [-0.]\n",
      " [-0.]\n",
      " [-0.]\n",
      " [ 0.]]\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 2 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:2 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "[[ 0.20117732]\n",
      " [ 0.29004504]\n",
      " [ 0.53838884]\n",
      " [-0.1352595 ]\n",
      " [ 0.25101071]\n",
      " [-0.3398222 ]]\n",
      "\n",
      "\t\tSigmoid Result: [[0.55012539]\n",
      " [0.57200716]\n",
      " [0.63143754]\n",
      " [0.46623658]\n",
      " [0.56242526]\n",
      " [0.41585267]]\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "[[1.         0.55012539 0.57200716 0.63143754 0.46623658 0.56242526\n",
      "  0.41585267]]\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "[[0.38960161]]\n",
      "\n",
      "\t\tFinal output (Softmax): \n",
      "[[1.]]\n",
      "\n",
      "Target: [0.] Net Result: [[1.]]\n",
      "\t\tNetwork Error: \n",
      "[[0.5]]\n",
      "\n",
      "\t\tOutput Error: \n",
      "[-0.]\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "[[-0.]\n",
      " [-0.]\n",
      " [-0.]\n",
      " [ 0.]\n",
      " [-0.]\n",
      " [ 0.]]\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 2 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:2 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "[[ 0.49772805]\n",
      " [-0.2199041 ]\n",
      " [ 0.01861541]\n",
      " [-0.06963844]\n",
      " [-0.20262796]\n",
      " [-0.29817524]]\n",
      "\n",
      "\t\tSigmoid Result: [[0.62192527]\n",
      " [0.44524445]\n",
      " [0.50465372]\n",
      " [0.48259742]\n",
      " [0.44951562]\n",
      " [0.42600362]]\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "[[1.         0.62192527 0.44524445 0.50465372 0.48259742 0.44951562\n",
      "  0.42600362]]\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "[[0.37827782]]\n",
      "\n",
      "\t\tFinal output (Softmax): \n",
      "[[1.]]\n",
      "\n",
      "Target: [0.] Net Result: [[1.]]\n",
      "\t\tNetwork Error: \n",
      "[[0.5]]\n",
      "\n",
      "\t\tOutput Error: \n",
      "[-0.]\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "[[-0.]\n",
      " [ 0.]\n",
      " [-0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 2 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:2 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "[[-7.91341331e-02]\n",
      " [-4.10005422e-01]\n",
      " [-3.23798009e-05]\n",
      " [ 3.47506374e-01]\n",
      " [-3.52610833e-01]\n",
      " [-9.83665425e-02]]\n",
      "\n",
      "\t\tSigmoid Result: [[0.48022678]\n",
      " [0.39891082]\n",
      " [0.49999191]\n",
      " [0.58601275]\n",
      " [0.41274944]\n",
      " [0.47542817]]\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "[[1.         0.48022678 0.39891082 0.49999191 0.58601275 0.41274944\n",
      "  0.47542817]]\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "[[0.34474159]]\n",
      "\n",
      "\t\tFinal output (Softmax): \n",
      "[[1.]]\n",
      "\n",
      "Target: [1.] Net Result: [[1.]]\n",
      "\t\tNetwork Error: \n",
      "[[0.]]\n",
      "\n",
      "\t\tOutput Error: \n",
      "[0.]\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "[[-0.]\n",
      " [-0.]\n",
      " [-0.]\n",
      " [ 0.]\n",
      " [-0.]\n",
      " [-0.]]\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 2 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:2 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "[[ 0.3002203 ]\n",
      " [-0.52693787]\n",
      " [ 0.11215543]\n",
      " [ 0.21925342]\n",
      " [-0.14900744]\n",
      " [-0.33322506]]\n",
      "\n",
      "\t\tSigmoid Result: [[0.57449637]\n",
      " [0.37123136]\n",
      " [0.5280095 ]\n",
      " [0.55459482]\n",
      " [0.46281691]\n",
      " [0.41745612]]\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "[[1.         0.57449637 0.37123136 0.5280095  0.55459482 0.46281691\n",
      "  0.41745612]]\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "[[0.37207069]]\n",
      "\n",
      "\t\tFinal output (Softmax): \n",
      "[[1.]]\n",
      "\n",
      "Target: [1.] Net Result: [[1.]]\n",
      "\t\tNetwork Error: \n",
      "[[0.]]\n",
      "\n",
      "\t\tOutput Error: \n",
      "[0.]\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "[[ 0.]\n",
      " [-0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [-0.]\n",
      " [-0.]]\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 2 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:2 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "[[-0.148686  ]\n",
      " [-0.21405268]\n",
      " [ 0.54043438]\n",
      " [ 0.5185202 ]\n",
      " [-0.18663205]\n",
      " [-0.37740152]]\n",
      "\n",
      "\t\tSigmoid Result: [[0.46289683]\n",
      " [0.44669022]\n",
      " [0.63191346]\n",
      " [0.62680167]\n",
      " [0.45347695]\n",
      " [0.40675377]]\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "[[1.         0.46289683 0.44669022 0.63191346 0.62680167 0.45347695\n",
      "  0.40675377]]\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "[[0.37389635]]\n",
      "\n",
      "\t\tFinal output (Softmax): \n",
      "[[1.]]\n",
      "\n",
      "Target: [0.] Net Result: [[1.]]\n",
      "\t\tNetwork Error: \n",
      "[[0.5]]\n",
      "\n",
      "\t\tOutput Error: \n",
      "[-0.]\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "[[ 0.]\n",
      " [ 0.]\n",
      " [-0.]\n",
      " [-0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 2 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:2 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "[[ 0.34537248]\n",
      " [-0.41696806]\n",
      " [-0.07287907]\n",
      " [ 0.29524941]\n",
      " [-0.69389124]\n",
      " [-0.30070475]]\n",
      "\n",
      "\t\tSigmoid Result: [[0.58549497]\n",
      " [0.3972425 ]\n",
      " [0.48178829]\n",
      " [0.57328079]\n",
      " [0.33316801]\n",
      " [0.42538521]]\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "[[1.         0.58549497 0.3972425  0.48178829 0.57328079 0.33316801\n",
      "  0.42538521]]\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "[[0.36869364]]\n",
      "\n",
      "\t\tFinal output (Softmax): \n",
      "[[1.]]\n",
      "\n",
      "Target: [1.] Net Result: [[1.]]\n",
      "\t\tNetwork Error: \n",
      "[[0.]]\n",
      "\n",
      "\t\tOutput Error: \n",
      "[0.]\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "[[ 0.]\n",
      " [-0.]\n",
      " [-0.]\n",
      " [ 0.]\n",
      " [-0.]\n",
      " [-0.]]\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 2 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:2 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "[[ 0.11837361]\n",
      " [-0.10297165]\n",
      " [-0.0935724 ]\n",
      " [ 0.05861452]\n",
      " [-0.40623135]\n",
      " [-0.06331673]]\n",
      "\n",
      "\t\tSigmoid Result: [[0.5295589 ]\n",
      " [0.47427981]\n",
      " [0.47662395]\n",
      " [0.51464944]\n",
      " [0.39981612]\n",
      " [0.4841761 ]]\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "[[1.         0.5295589  0.47427981 0.47662395 0.51464944 0.39981612\n",
      "  0.4841761 ]]\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "[[0.35113581]]\n",
      "\n",
      "\t\tFinal output (Softmax): \n",
      "[[1.]]\n",
      "\n",
      "Target: [1.] Net Result: [[1.]]\n",
      "\t\tNetwork Error: \n",
      "[[0.]]\n",
      "\n",
      "\t\tOutput Error: \n",
      "[0.]\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "[[ 0.]\n",
      " [-0.]\n",
      " [-0.]\n",
      " [ 0.]\n",
      " [-0.]\n",
      " [-0.]]\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 2 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:2 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "[[-0.03255367]\n",
      " [-0.34368171]\n",
      " [ 0.17259804]\n",
      " [ 0.22098109]\n",
      " [-0.11193851]\n",
      " [ 0.00449915]]\n",
      "\n",
      "\t\tSigmoid Result: [[0.4918623 ]\n",
      " [0.41491542]\n",
      " [0.54304271]\n",
      " [0.55502155]\n",
      " [0.47204456]\n",
      " [0.50112479]]\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "[[1.         0.4918623  0.41491542 0.54304271 0.55502155 0.47204456\n",
      "  0.50112479]]\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "[[0.35297247]]\n",
      "\n",
      "\t\tFinal output (Softmax): \n",
      "[[1.]]\n",
      "\n",
      "Target: [1.] Net Result: [[1.]]\n",
      "\t\tNetwork Error: \n",
      "[[0.]]\n",
      "\n",
      "\t\tOutput Error: \n",
      "[0.]\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "[[-0.]\n",
      " [-0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [-0.]\n",
      " [ 0.]]\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 2 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:2 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "[[-0.17817711]\n",
      " [ 0.40697749]\n",
      " [ 0.42620103]\n",
      " [-0.00700655]\n",
      " [ 0.04740732]\n",
      " [-0.10496369]]\n",
      "\n",
      "\t\tSigmoid Result: [[0.45557319]\n",
      " [0.60036292]\n",
      " [0.60496615]\n",
      " [0.49824837]\n",
      " [0.51184961]\n",
      " [0.47378314]]\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "[[1.         0.45557319 0.60036292 0.60496615 0.49824837 0.51184961\n",
      "  0.47378314]]\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "[[0.3625889]]\n",
      "\n",
      "\t\tFinal output (Softmax): \n",
      "[[1.]]\n",
      "\n",
      "Target: [0.] Net Result: [[1.]]\n",
      "\t\tNetwork Error: \n",
      "[[0.5]]\n",
      "\n",
      "\t\tOutput Error: \n",
      "[-0.]\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "[[ 0.]\n",
      " [-0.]\n",
      " [-0.]\n",
      " [ 0.]\n",
      " [-0.]\n",
      " [ 0.]]\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 3 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:3 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "[[-0.25955253]\n",
      " [-0.02968531]\n",
      " [ 0.15190471]\n",
      " [-0.0156538 ]\n",
      " [ 0.17572138]\n",
      " [ 0.24188717]]\n",
      "\n",
      "\t\tSigmoid Result: [[0.43547371]\n",
      " [0.49257922]\n",
      " [0.53790332]\n",
      " [0.49608663]\n",
      " [0.54381765]\n",
      " [0.56017866]]\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "[[1.         0.43547371 0.49257922 0.53790332 0.49608663 0.54381765\n",
      "  0.56017866]]\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "[[0.33497437]]\n",
      "\n",
      "\t\tFinal output (Softmax): \n",
      "[[1.]]\n",
      "\n",
      "Target: [1.] Net Result: [[1.]]\n",
      "\t\tNetwork Error: \n",
      "[[0.]]\n",
      "\n",
      "\t\tOutput Error: \n",
      "[0.]\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "[[-0.]\n",
      " [-0.]\n",
      " [ 0.]\n",
      " [-0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 3 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:3 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "[[-0.06204479]\n",
      " [ 0.27734846]\n",
      " [ 0.05836469]\n",
      " [-0.30454565]\n",
      " [ 0.12210086]\n",
      " [ 0.27693698]]\n",
      "\n",
      "\t\tSigmoid Result: [[0.48449378]\n",
      " [0.56889605]\n",
      " [0.51458703]\n",
      " [0.42444664]\n",
      " [0.53048735]\n",
      " [0.56879513]]\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "[[1.         0.48449378 0.56889605 0.51458703 0.42444664 0.53048735\n",
      "  0.56879513]]\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "[[0.34137119]]\n",
      "\n",
      "\t\tFinal output (Softmax): \n",
      "[[1.]]\n",
      "\n",
      "Target: [0.] Net Result: [[1.]]\n",
      "\t\tNetwork Error: \n",
      "[[0.5]]\n",
      "\n",
      "\t\tOutput Error: \n",
      "[-0.]\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "[[ 0.]\n",
      " [-0.]\n",
      " [-0.]\n",
      " [ 0.]\n",
      " [-0.]\n",
      " [-0.]]\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 3 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:3 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "[[-0.35687861]\n",
      " [ 0.41233336]\n",
      " [ 0.36246912]\n",
      " [-0.20155932]\n",
      " [ 0.06978535]\n",
      " [ 0.52723504]]\n",
      "\n",
      "\t\tSigmoid Result: [[0.41171538]\n",
      " [0.60164724]\n",
      " [0.58963801]\n",
      " [0.44978007]\n",
      " [0.51743926]\n",
      " [0.628838  ]]\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "[[1.         0.41171538 0.60164724 0.58963801 0.44978007 0.51743926\n",
      "  0.628838  ]]\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "[[0.3408198]]\n",
      "\n",
      "\t\tFinal output (Softmax): \n",
      "[[1.]]\n",
      "\n",
      "Target: [1.] Net Result: [[1.]]\n",
      "\t\tNetwork Error: \n",
      "[[0.]]\n",
      "\n",
      "\t\tOutput Error: \n",
      "[0.]\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "[[-0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [-0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 3 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:3 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "[[-0.37568486]\n",
      " [ 0.09994372]\n",
      " [ 0.51974105]\n",
      " [ 0.28188531]\n",
      " [ 0.10102784]\n",
      " [-0.1400135 ]]\n",
      "\n",
      "\t\tSigmoid Result: [[0.40716808]\n",
      " [0.52496515]\n",
      " [0.62708721]\n",
      " [0.57000837]\n",
      " [0.5252355 ]\n",
      " [0.4650537 ]]\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "[[1.         0.40716808 0.52496515 0.62708721 0.57000837 0.5252355\n",
      "  0.4650537 ]]\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "[[0.355973]]\n",
      "\n",
      "\t\tFinal output (Softmax): \n",
      "[[1.]]\n",
      "\n",
      "Target: [1.] Net Result: [[1.]]\n",
      "\t\tNetwork Error: \n",
      "[[0.]]\n",
      "\n",
      "\t\tOutput Error: \n",
      "[0.]\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "[[-0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [-0.]]\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 3 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:3 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "[[ 0.00366957]\n",
      " [-0.01698873]\n",
      " [ 0.63192886]\n",
      " [ 0.15363235]\n",
      " [ 0.30463123]\n",
      " [-0.37487202]]\n",
      "\n",
      "\t\tSigmoid Result: [[0.50091739]\n",
      " [0.49575292]\n",
      " [0.6529267 ]\n",
      " [0.53833272]\n",
      " [0.57557427]\n",
      " [0.4073643 ]]\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "[[1.         0.50091739 0.49575292 0.6529267  0.53833272 0.57557427\n",
      "  0.4073643 ]]\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "[[0.38268296]]\n",
      "\n",
      "\t\tFinal output (Softmax): \n",
      "[[1.]]\n",
      "\n",
      "Target: [0.] Net Result: [[1.]]\n",
      "\t\tNetwork Error: \n",
      "[[0.5]]\n",
      "\n",
      "\t\tOutput Error: \n",
      "[-0.]\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "[[-0.]\n",
      " [ 0.]\n",
      " [-0.]\n",
      " [-0.]\n",
      " [-0.]\n",
      " [ 0.]]\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 3 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:3 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "[[ 0.20117732]\n",
      " [ 0.29004504]\n",
      " [ 0.53838884]\n",
      " [-0.1352595 ]\n",
      " [ 0.25101071]\n",
      " [-0.3398222 ]]\n",
      "\n",
      "\t\tSigmoid Result: [[0.55012539]\n",
      " [0.57200716]\n",
      " [0.63143754]\n",
      " [0.46623658]\n",
      " [0.56242526]\n",
      " [0.41585267]]\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "[[1.         0.55012539 0.57200716 0.63143754 0.46623658 0.56242526\n",
      "  0.41585267]]\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "[[0.38960161]]\n",
      "\n",
      "\t\tFinal output (Softmax): \n",
      "[[1.]]\n",
      "\n",
      "Target: [0.] Net Result: [[1.]]\n",
      "\t\tNetwork Error: \n",
      "[[0.5]]\n",
      "\n",
      "\t\tOutput Error: \n",
      "[-0.]\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "[[-0.]\n",
      " [-0.]\n",
      " [-0.]\n",
      " [ 0.]\n",
      " [-0.]\n",
      " [ 0.]]\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 3 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:3 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "[[ 0.49772805]\n",
      " [-0.2199041 ]\n",
      " [ 0.01861541]\n",
      " [-0.06963844]\n",
      " [-0.20262796]\n",
      " [-0.29817524]]\n",
      "\n",
      "\t\tSigmoid Result: [[0.62192527]\n",
      " [0.44524445]\n",
      " [0.50465372]\n",
      " [0.48259742]\n",
      " [0.44951562]\n",
      " [0.42600362]]\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "[[1.         0.62192527 0.44524445 0.50465372 0.48259742 0.44951562\n",
      "  0.42600362]]\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "[[0.37827782]]\n",
      "\n",
      "\t\tFinal output (Softmax): \n",
      "[[1.]]\n",
      "\n",
      "Target: [0.] Net Result: [[1.]]\n",
      "\t\tNetwork Error: \n",
      "[[0.5]]\n",
      "\n",
      "\t\tOutput Error: \n",
      "[-0.]\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "[[-0.]\n",
      " [ 0.]\n",
      " [-0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 3 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:3 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "[[-7.91341331e-02]\n",
      " [-4.10005422e-01]\n",
      " [-3.23798009e-05]\n",
      " [ 3.47506374e-01]\n",
      " [-3.52610833e-01]\n",
      " [-9.83665425e-02]]\n",
      "\n",
      "\t\tSigmoid Result: [[0.48022678]\n",
      " [0.39891082]\n",
      " [0.49999191]\n",
      " [0.58601275]\n",
      " [0.41274944]\n",
      " [0.47542817]]\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "[[1.         0.48022678 0.39891082 0.49999191 0.58601275 0.41274944\n",
      "  0.47542817]]\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "[[0.34474159]]\n",
      "\n",
      "\t\tFinal output (Softmax): \n",
      "[[1.]]\n",
      "\n",
      "Target: [1.] Net Result: [[1.]]\n",
      "\t\tNetwork Error: \n",
      "[[0.]]\n",
      "\n",
      "\t\tOutput Error: \n",
      "[0.]\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "[[-0.]\n",
      " [-0.]\n",
      " [-0.]\n",
      " [ 0.]\n",
      " [-0.]\n",
      " [-0.]]\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 3 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:3 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "[[ 0.3002203 ]\n",
      " [-0.52693787]\n",
      " [ 0.11215543]\n",
      " [ 0.21925342]\n",
      " [-0.14900744]\n",
      " [-0.33322506]]\n",
      "\n",
      "\t\tSigmoid Result: [[0.57449637]\n",
      " [0.37123136]\n",
      " [0.5280095 ]\n",
      " [0.55459482]\n",
      " [0.46281691]\n",
      " [0.41745612]]\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "[[1.         0.57449637 0.37123136 0.5280095  0.55459482 0.46281691\n",
      "  0.41745612]]\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "[[0.37207069]]\n",
      "\n",
      "\t\tFinal output (Softmax): \n",
      "[[1.]]\n",
      "\n",
      "Target: [1.] Net Result: [[1.]]\n",
      "\t\tNetwork Error: \n",
      "[[0.]]\n",
      "\n",
      "\t\tOutput Error: \n",
      "[0.]\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "[[ 0.]\n",
      " [-0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [-0.]\n",
      " [-0.]]\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 3 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:3 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "[[-0.148686  ]\n",
      " [-0.21405268]\n",
      " [ 0.54043438]\n",
      " [ 0.5185202 ]\n",
      " [-0.18663205]\n",
      " [-0.37740152]]\n",
      "\n",
      "\t\tSigmoid Result: [[0.46289683]\n",
      " [0.44669022]\n",
      " [0.63191346]\n",
      " [0.62680167]\n",
      " [0.45347695]\n",
      " [0.40675377]]\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "[[1.         0.46289683 0.44669022 0.63191346 0.62680167 0.45347695\n",
      "  0.40675377]]\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "[[0.37389635]]\n",
      "\n",
      "\t\tFinal output (Softmax): \n",
      "[[1.]]\n",
      "\n",
      "Target: [0.] Net Result: [[1.]]\n",
      "\t\tNetwork Error: \n",
      "[[0.5]]\n",
      "\n",
      "\t\tOutput Error: \n",
      "[-0.]\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "[[ 0.]\n",
      " [ 0.]\n",
      " [-0.]\n",
      " [-0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 3 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:3 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "[[ 0.34537248]\n",
      " [-0.41696806]\n",
      " [-0.07287907]\n",
      " [ 0.29524941]\n",
      " [-0.69389124]\n",
      " [-0.30070475]]\n",
      "\n",
      "\t\tSigmoid Result: [[0.58549497]\n",
      " [0.3972425 ]\n",
      " [0.48178829]\n",
      " [0.57328079]\n",
      " [0.33316801]\n",
      " [0.42538521]]\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "[[1.         0.58549497 0.3972425  0.48178829 0.57328079 0.33316801\n",
      "  0.42538521]]\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "[[0.36869364]]\n",
      "\n",
      "\t\tFinal output (Softmax): \n",
      "[[1.]]\n",
      "\n",
      "Target: [1.] Net Result: [[1.]]\n",
      "\t\tNetwork Error: \n",
      "[[0.]]\n",
      "\n",
      "\t\tOutput Error: \n",
      "[0.]\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "[[ 0.]\n",
      " [-0.]\n",
      " [-0.]\n",
      " [ 0.]\n",
      " [-0.]\n",
      " [-0.]]\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 3 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:3 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "[[ 0.11837361]\n",
      " [-0.10297165]\n",
      " [-0.0935724 ]\n",
      " [ 0.05861452]\n",
      " [-0.40623135]\n",
      " [-0.06331673]]\n",
      "\n",
      "\t\tSigmoid Result: [[0.5295589 ]\n",
      " [0.47427981]\n",
      " [0.47662395]\n",
      " [0.51464944]\n",
      " [0.39981612]\n",
      " [0.4841761 ]]\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "[[1.         0.5295589  0.47427981 0.47662395 0.51464944 0.39981612\n",
      "  0.4841761 ]]\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "[[0.35113581]]\n",
      "\n",
      "\t\tFinal output (Softmax): \n",
      "[[1.]]\n",
      "\n",
      "Target: [1.] Net Result: [[1.]]\n",
      "\t\tNetwork Error: \n",
      "[[0.]]\n",
      "\n",
      "\t\tOutput Error: \n",
      "[0.]\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "[[ 0.]\n",
      " [-0.]\n",
      " [-0.]\n",
      " [ 0.]\n",
      " [-0.]\n",
      " [-0.]]\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 3 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:3 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "[[-0.03255367]\n",
      " [-0.34368171]\n",
      " [ 0.17259804]\n",
      " [ 0.22098109]\n",
      " [-0.11193851]\n",
      " [ 0.00449915]]\n",
      "\n",
      "\t\tSigmoid Result: [[0.4918623 ]\n",
      " [0.41491542]\n",
      " [0.54304271]\n",
      " [0.55502155]\n",
      " [0.47204456]\n",
      " [0.50112479]]\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "[[1.         0.4918623  0.41491542 0.54304271 0.55502155 0.47204456\n",
      "  0.50112479]]\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "[[0.35297247]]\n",
      "\n",
      "\t\tFinal output (Softmax): \n",
      "[[1.]]\n",
      "\n",
      "Target: [1.] Net Result: [[1.]]\n",
      "\t\tNetwork Error: \n",
      "[[0.]]\n",
      "\n",
      "\t\tOutput Error: \n",
      "[0.]\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "[[-0.]\n",
      " [-0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [-0.]\n",
      " [ 0.]]\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 3 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:3 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "[[-0.17817711]\n",
      " [ 0.40697749]\n",
      " [ 0.42620103]\n",
      " [-0.00700655]\n",
      " [ 0.04740732]\n",
      " [-0.10496369]]\n",
      "\n",
      "\t\tSigmoid Result: [[0.45557319]\n",
      " [0.60036292]\n",
      " [0.60496615]\n",
      " [0.49824837]\n",
      " [0.51184961]\n",
      " [0.47378314]]\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "[[1.         0.45557319 0.60036292 0.60496615 0.49824837 0.51184961\n",
      "  0.47378314]]\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "[[0.3625889]]\n",
      "\n",
      "\t\tFinal output (Softmax): \n",
      "[[1.]]\n",
      "\n",
      "Target: [0.] Net Result: [[1.]]\n",
      "\t\tNetwork Error: \n",
      "[[0.5]]\n",
      "\n",
      "\t\tOutput Error: \n",
      "[-0.]\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "[[ 0.]\n",
      " [-0.]\n",
      " [-0.]\n",
      " [ 0.]\n",
      " [-0.]\n",
      " [ 0.]]\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 4 Online Round: 0 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:4 with online instance: 0-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "[[-0.25955253]\n",
      " [-0.02968531]\n",
      " [ 0.15190471]\n",
      " [-0.0156538 ]\n",
      " [ 0.17572138]\n",
      " [ 0.24188717]]\n",
      "\n",
      "\t\tSigmoid Result: [[0.43547371]\n",
      " [0.49257922]\n",
      " [0.53790332]\n",
      " [0.49608663]\n",
      " [0.54381765]\n",
      " [0.56017866]]\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "[[1.         0.43547371 0.49257922 0.53790332 0.49608663 0.54381765\n",
      "  0.56017866]]\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "[[0.33497437]]\n",
      "\n",
      "\t\tFinal output (Softmax): \n",
      "[[1.]]\n",
      "\n",
      "Target: [1.] Net Result: [[1.]]\n",
      "\t\tNetwork Error: \n",
      "[[0.]]\n",
      "\n",
      "\t\tOutput Error: \n",
      "[0.]\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "[[-0.]\n",
      " [-0.]\n",
      " [ 0.]\n",
      " [-0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 4 Online Round: 1 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:4 with online instance: 1-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "[[-0.06204479]\n",
      " [ 0.27734846]\n",
      " [ 0.05836469]\n",
      " [-0.30454565]\n",
      " [ 0.12210086]\n",
      " [ 0.27693698]]\n",
      "\n",
      "\t\tSigmoid Result: [[0.48449378]\n",
      " [0.56889605]\n",
      " [0.51458703]\n",
      " [0.42444664]\n",
      " [0.53048735]\n",
      " [0.56879513]]\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "[[1.         0.48449378 0.56889605 0.51458703 0.42444664 0.53048735\n",
      "  0.56879513]]\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "[[0.34137119]]\n",
      "\n",
      "\t\tFinal output (Softmax): \n",
      "[[1.]]\n",
      "\n",
      "Target: [0.] Net Result: [[1.]]\n",
      "\t\tNetwork Error: \n",
      "[[0.5]]\n",
      "\n",
      "\t\tOutput Error: \n",
      "[-0.]\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "[[ 0.]\n",
      " [-0.]\n",
      " [-0.]\n",
      " [ 0.]\n",
      " [-0.]\n",
      " [-0.]]\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 4 Online Round: 2 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:4 with online instance: 2-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "[[-0.35687861]\n",
      " [ 0.41233336]\n",
      " [ 0.36246912]\n",
      " [-0.20155932]\n",
      " [ 0.06978535]\n",
      " [ 0.52723504]]\n",
      "\n",
      "\t\tSigmoid Result: [[0.41171538]\n",
      " [0.60164724]\n",
      " [0.58963801]\n",
      " [0.44978007]\n",
      " [0.51743926]\n",
      " [0.628838  ]]\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "[[1.         0.41171538 0.60164724 0.58963801 0.44978007 0.51743926\n",
      "  0.628838  ]]\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "[[0.3408198]]\n",
      "\n",
      "\t\tFinal output (Softmax): \n",
      "[[1.]]\n",
      "\n",
      "Target: [1.] Net Result: [[1.]]\n",
      "\t\tNetwork Error: \n",
      "[[0.]]\n",
      "\n",
      "\t\tOutput Error: \n",
      "[0.]\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "[[-0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [-0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 4 Online Round: 3 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:4 with online instance: 3-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "[[-0.37568486]\n",
      " [ 0.09994372]\n",
      " [ 0.51974105]\n",
      " [ 0.28188531]\n",
      " [ 0.10102784]\n",
      " [-0.1400135 ]]\n",
      "\n",
      "\t\tSigmoid Result: [[0.40716808]\n",
      " [0.52496515]\n",
      " [0.62708721]\n",
      " [0.57000837]\n",
      " [0.5252355 ]\n",
      " [0.4650537 ]]\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "[[1.         0.40716808 0.52496515 0.62708721 0.57000837 0.5252355\n",
      "  0.4650537 ]]\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "[[0.355973]]\n",
      "\n",
      "\t\tFinal output (Softmax): \n",
      "[[1.]]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: [1.] Net Result: [[1.]]\n",
      "\t\tNetwork Error: \n",
      "[[0.]]\n",
      "\n",
      "\t\tOutput Error: \n",
      "[0.]\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "[[-0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [-0.]]\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 4 Online Round: 4 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:4 with online instance: 4-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "[[ 0.00366957]\n",
      " [-0.01698873]\n",
      " [ 0.63192886]\n",
      " [ 0.15363235]\n",
      " [ 0.30463123]\n",
      " [-0.37487202]]\n",
      "\n",
      "\t\tSigmoid Result: [[0.50091739]\n",
      " [0.49575292]\n",
      " [0.6529267 ]\n",
      " [0.53833272]\n",
      " [0.57557427]\n",
      " [0.4073643 ]]\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "[[1.         0.50091739 0.49575292 0.6529267  0.53833272 0.57557427\n",
      "  0.4073643 ]]\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "[[0.38268296]]\n",
      "\n",
      "\t\tFinal output (Softmax): \n",
      "[[1.]]\n",
      "\n",
      "Target: [0.] Net Result: [[1.]]\n",
      "\t\tNetwork Error: \n",
      "[[0.5]]\n",
      "\n",
      "\t\tOutput Error: \n",
      "[-0.]\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "[[-0.]\n",
      " [ 0.]\n",
      " [-0.]\n",
      " [-0.]\n",
      " [-0.]\n",
      " [ 0.]]\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 4 Online Round: 5 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:4 with online instance: 5-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "[[ 0.20117732]\n",
      " [ 0.29004504]\n",
      " [ 0.53838884]\n",
      " [-0.1352595 ]\n",
      " [ 0.25101071]\n",
      " [-0.3398222 ]]\n",
      "\n",
      "\t\tSigmoid Result: [[0.55012539]\n",
      " [0.57200716]\n",
      " [0.63143754]\n",
      " [0.46623658]\n",
      " [0.56242526]\n",
      " [0.41585267]]\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "[[1.         0.55012539 0.57200716 0.63143754 0.46623658 0.56242526\n",
      "  0.41585267]]\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "[[0.38960161]]\n",
      "\n",
      "\t\tFinal output (Softmax): \n",
      "[[1.]]\n",
      "\n",
      "Target: [0.] Net Result: [[1.]]\n",
      "\t\tNetwork Error: \n",
      "[[0.5]]\n",
      "\n",
      "\t\tOutput Error: \n",
      "[-0.]\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "[[-0.]\n",
      " [-0.]\n",
      " [-0.]\n",
      " [ 0.]\n",
      " [-0.]\n",
      " [ 0.]]\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 4 Online Round: 6 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:4 with online instance: 6-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "[[ 0.49772805]\n",
      " [-0.2199041 ]\n",
      " [ 0.01861541]\n",
      " [-0.06963844]\n",
      " [-0.20262796]\n",
      " [-0.29817524]]\n",
      "\n",
      "\t\tSigmoid Result: [[0.62192527]\n",
      " [0.44524445]\n",
      " [0.50465372]\n",
      " [0.48259742]\n",
      " [0.44951562]\n",
      " [0.42600362]]\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "[[1.         0.62192527 0.44524445 0.50465372 0.48259742 0.44951562\n",
      "  0.42600362]]\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "[[0.37827782]]\n",
      "\n",
      "\t\tFinal output (Softmax): \n",
      "[[1.]]\n",
      "\n",
      "Target: [0.] Net Result: [[1.]]\n",
      "\t\tNetwork Error: \n",
      "[[0.5]]\n",
      "\n",
      "\t\tOutput Error: \n",
      "[-0.]\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "[[-0.]\n",
      " [ 0.]\n",
      " [-0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 4 Online Round: 7 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:4 with online instance: 7-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "[[-7.91341331e-02]\n",
      " [-4.10005422e-01]\n",
      " [-3.23798009e-05]\n",
      " [ 3.47506374e-01]\n",
      " [-3.52610833e-01]\n",
      " [-9.83665425e-02]]\n",
      "\n",
      "\t\tSigmoid Result: [[0.48022678]\n",
      " [0.39891082]\n",
      " [0.49999191]\n",
      " [0.58601275]\n",
      " [0.41274944]\n",
      " [0.47542817]]\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "[[1.         0.48022678 0.39891082 0.49999191 0.58601275 0.41274944\n",
      "  0.47542817]]\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "[[0.34474159]]\n",
      "\n",
      "\t\tFinal output (Softmax): \n",
      "[[1.]]\n",
      "\n",
      "Target: [1.] Net Result: [[1.]]\n",
      "\t\tNetwork Error: \n",
      "[[0.]]\n",
      "\n",
      "\t\tOutput Error: \n",
      "[0.]\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "[[-0.]\n",
      " [-0.]\n",
      " [-0.]\n",
      " [ 0.]\n",
      " [-0.]\n",
      " [-0.]]\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 4 Online Round: 8 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:4 with online instance: 8-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "[[ 0.3002203 ]\n",
      " [-0.52693787]\n",
      " [ 0.11215543]\n",
      " [ 0.21925342]\n",
      " [-0.14900744]\n",
      " [-0.33322506]]\n",
      "\n",
      "\t\tSigmoid Result: [[0.57449637]\n",
      " [0.37123136]\n",
      " [0.5280095 ]\n",
      " [0.55459482]\n",
      " [0.46281691]\n",
      " [0.41745612]]\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "[[1.         0.57449637 0.37123136 0.5280095  0.55459482 0.46281691\n",
      "  0.41745612]]\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "[[0.37207069]]\n",
      "\n",
      "\t\tFinal output (Softmax): \n",
      "[[1.]]\n",
      "\n",
      "Target: [1.] Net Result: [[1.]]\n",
      "\t\tNetwork Error: \n",
      "[[0.]]\n",
      "\n",
      "\t\tOutput Error: \n",
      "[0.]\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "[[ 0.]\n",
      " [-0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [-0.]\n",
      " [-0.]]\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 4 Online Round: 9 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:4 with online instance: 9-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "[[-0.148686  ]\n",
      " [-0.21405268]\n",
      " [ 0.54043438]\n",
      " [ 0.5185202 ]\n",
      " [-0.18663205]\n",
      " [-0.37740152]]\n",
      "\n",
      "\t\tSigmoid Result: [[0.46289683]\n",
      " [0.44669022]\n",
      " [0.63191346]\n",
      " [0.62680167]\n",
      " [0.45347695]\n",
      " [0.40675377]]\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "[[1.         0.46289683 0.44669022 0.63191346 0.62680167 0.45347695\n",
      "  0.40675377]]\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "[[0.37389635]]\n",
      "\n",
      "\t\tFinal output (Softmax): \n",
      "[[1.]]\n",
      "\n",
      "Target: [0.] Net Result: [[1.]]\n",
      "\t\tNetwork Error: \n",
      "[[0.5]]\n",
      "\n",
      "\t\tOutput Error: \n",
      "[-0.]\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "[[ 0.]\n",
      " [ 0.]\n",
      " [-0.]\n",
      " [-0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 4 Online Round: 10 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:4 with online instance: 10-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "[[ 0.34537248]\n",
      " [-0.41696806]\n",
      " [-0.07287907]\n",
      " [ 0.29524941]\n",
      " [-0.69389124]\n",
      " [-0.30070475]]\n",
      "\n",
      "\t\tSigmoid Result: [[0.58549497]\n",
      " [0.3972425 ]\n",
      " [0.48178829]\n",
      " [0.57328079]\n",
      " [0.33316801]\n",
      " [0.42538521]]\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "[[1.         0.58549497 0.3972425  0.48178829 0.57328079 0.33316801\n",
      "  0.42538521]]\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "[[0.36869364]]\n",
      "\n",
      "\t\tFinal output (Softmax): \n",
      "[[1.]]\n",
      "\n",
      "Target: [1.] Net Result: [[1.]]\n",
      "\t\tNetwork Error: \n",
      "[[0.]]\n",
      "\n",
      "\t\tOutput Error: \n",
      "[0.]\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "[[ 0.]\n",
      " [-0.]\n",
      " [-0.]\n",
      " [ 0.]\n",
      " [-0.]\n",
      " [-0.]]\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 4 Online Round: 11 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:4 with online instance: 11-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "[[ 0.11837361]\n",
      " [-0.10297165]\n",
      " [-0.0935724 ]\n",
      " [ 0.05861452]\n",
      " [-0.40623135]\n",
      " [-0.06331673]]\n",
      "\n",
      "\t\tSigmoid Result: [[0.5295589 ]\n",
      " [0.47427981]\n",
      " [0.47662395]\n",
      " [0.51464944]\n",
      " [0.39981612]\n",
      " [0.4841761 ]]\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "[[1.         0.5295589  0.47427981 0.47662395 0.51464944 0.39981612\n",
      "  0.4841761 ]]\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "[[0.35113581]]\n",
      "\n",
      "\t\tFinal output (Softmax): \n",
      "[[1.]]\n",
      "\n",
      "Target: [1.] Net Result: [[1.]]\n",
      "\t\tNetwork Error: \n",
      "[[0.]]\n",
      "\n",
      "\t\tOutput Error: \n",
      "[0.]\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "[[ 0.]\n",
      " [-0.]\n",
      " [-0.]\n",
      " [ 0.]\n",
      " [-0.]\n",
      " [-0.]]\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 4 Online Round: 12 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "Target: \n",
      "[1.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:4 with online instance: 12-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "[[-0.03255367]\n",
      " [-0.34368171]\n",
      " [ 0.17259804]\n",
      " [ 0.22098109]\n",
      " [-0.11193851]\n",
      " [ 0.00449915]]\n",
      "\n",
      "\t\tSigmoid Result: [[0.4918623 ]\n",
      " [0.41491542]\n",
      " [0.54304271]\n",
      " [0.55502155]\n",
      " [0.47204456]\n",
      " [0.50112479]]\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "[[1.         0.4918623  0.41491542 0.54304271 0.55502155 0.47204456\n",
      "  0.50112479]]\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "[[0.35297247]]\n",
      "\n",
      "\t\tFinal output (Softmax): \n",
      "[[1.]]\n",
      "\n",
      "Target: [1.] Net Result: [[1.]]\n",
      "\t\tNetwork Error: \n",
      "[[0.]]\n",
      "\n",
      "\t\tOutput Error: \n",
      "[0.]\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "[[-0.]\n",
      " [-0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [-0.]\n",
      " [ 0.]]\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "\u001b[96m\u001b[1m----------- Data for batch: 4 Online Round: 13 ------------\u001b[0m\n",
      "Inputs: \n",
      "[1. 1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Learning Rate: \n",
      "0.5\n",
      "\n",
      "Theta One: \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Theta two: \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n",
      "Target: \n",
      "[0.]\n",
      "\n",
      "\u001b[93m\u001b[1m----------Processing on batch:4 with online instance: 13-------------\u001b[0m\n",
      "\t\tLayer One activations: \n",
      "[[-0.17817711]\n",
      " [ 0.40697749]\n",
      " [ 0.42620103]\n",
      " [-0.00700655]\n",
      " [ 0.04740732]\n",
      " [-0.10496369]]\n",
      "\n",
      "\t\tSigmoid Result: [[0.45557319]\n",
      " [0.60036292]\n",
      " [0.60496615]\n",
      " [0.49824837]\n",
      " [0.51184961]\n",
      " [0.47378314]]\n",
      "\n",
      "\t\tInputs for the hiden layer is: (b,h1,h2)\n",
      "[[1.         0.45557319 0.60036292 0.60496615 0.49824837 0.51184961\n",
      "  0.47378314]]\n",
      "\n",
      "\t\tActivation for output layer: (h1,h2)\n",
      "[[0.3625889]]\n",
      "\n",
      "\t\tFinal output (Softmax): \n",
      "[[1.]]\n",
      "\n",
      "Target: [0.] Net Result: [[1.]]\n",
      "\t\tNetwork Error: \n",
      "[[0.5]]\n",
      "\n",
      "\t\tOutput Error: \n",
      "[-0.]\n",
      "\n",
      "\t\tHidden unit errors: \n",
      "[[ 0.]\n",
      " [-0.]\n",
      " [-0.]\n",
      " [ 0.]\n",
      " [-0.]\n",
      " [ 0.]]\n",
      "\n",
      "Weights:  (6, 9)\n",
      "Learning Rate:  0.5\n",
      "Error:  (6, 1)\n",
      "Activations: (1, 9)\n",
      "\t\tNext round of weights for layerOne: (b,x1,x2) \n",
      "[[-0.05871885  0.19750775 -0.3278457  -0.1474273   0.00492828  0.22699886\n",
      "   0.12701201  0.02968594 -0.16953871]\n",
      " [ 0.02933624  0.30703377  0.15091903 -0.22940108 -0.03233713 -0.3139964\n",
      "  -0.20994058  0.23207809  0.30000857]\n",
      " [ 0.24235557 -0.09354002  0.13662886 -0.01530823  0.07618626  0.02069333\n",
      "  -0.22707973 -0.01651532  0.2926937 ]\n",
      " [-0.0774779  -0.28889186 -0.20678416  0.15637601 -0.20851183  0.23663489\n",
      "   0.26860826  0.08270274  0.2029872 ]\n",
      " [ 0.08035484 -0.05362052  0.30552596 -0.22280625  0.26845703 -0.28765989\n",
      "  -0.21015942 -0.31609545  0.24347925]\n",
      " [-0.01203771  0.03504982  0.22855836 -0.11169535 -0.10916585 -0.23738802\n",
      "   0.02536652  0.31071438 -0.01628044]]\n",
      "\n",
      "Weights:  (1, 7)\n",
      "Learning Rate:  0.5\n",
      "Error:  (1,)\n",
      "Activations: (1, 7)\n",
      "\t\tNext round of weights for layer 2: (b,h1,h2) \n",
      "[[ 0.26812622  0.16453383 -0.02153026  0.24886776 -0.07858753 -0.06993736\n",
      "  -0.09112102]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Main:\n",
    "\n",
    "# load in fishing normalized set \n",
    "data = getArrayFromFile(\"normalizeFish.csv\")\n",
    "# data = getArrayFromFile(\"normDigit.csv\")\n",
    "inputs = data[:,0:data.shape[1]-1] # get the input values\n",
    "targets = data[:, data.shape[1]-1:data.shape[1]] # get the class values\n",
    "\n",
    "print(targets.shape)\n",
    "\n",
    "# inputs = np.array([[0.5403,-0.4161],[-0.9900,-0.6536],[0.2837,0.9602]])\n",
    "# targets = np.array([[4],[2],[3]])\n",
    "\n",
    "inputsWB = getInputs(inputs) # adds bias to the input matrix\n",
    "\n",
    "numInputNodes = inputsWB.shape[1]\n",
    "numOutputNodes = 1 if np.unique(targets).shape[0] == 2 else np.unique(targets).shape[0]\n",
    "numHiddenNodes = int((2/3)*(numInputNodes+numOutputNodes))\n",
    "\n",
    "print(\"Input node num including bias: \"+ str(numInputNodes))\n",
    "print(\"Hidden node num including bias: \"+ str(numHiddenNodes))\n",
    "print(\"Output node num: \" + str(numOutputNodes))\n",
    "\n",
    "\n",
    "\n",
    "# targets = np.array([[0,0,0,1],[0,1,0,0],[0,0,1,0]])\n",
    "\n",
    "# inititlize weights\n",
    "# (-1/sqrt(n)) < w < (1/sqrt(n))\n",
    "lowRange = (-1/math.sqrt(numInputNodes))\n",
    "highRange = math.fabs(lowRange)\n",
    "theta1 = np.random.uniform(low=lowRange, high=highRange, size=(numHiddenNodes, numInputNodes))\n",
    "theta2 = np.random.uniform(low=lowRange, high=highRange, size=(numOutputNodes,numHiddenNodes+1)) # add one for the bias node at this layer\n",
    "\n",
    "\n",
    "print(\"Theta1 dims: \" + str(theta1.shape))\n",
    "print(\"Theta2 dims: \" + str(theta2.shape))\n",
    "\n",
    "\n",
    "learningRate = .5\n",
    "numberProcess = 0\n",
    "batchError = []\n",
    "for r in range(0, 5): # batch\n",
    "    trackedNetError= []\n",
    "    for index in range(0, inputsWB.shape[0]): # online\n",
    "        print(color.CYAN+color.BOLD+\"----------- Data for batch: \"+str(r)+\" Online Round: \"+str(index)+\" ------------\"+color.END)\n",
    "        print(\"Inputs: \\n\" + str(inputsWB[index])+\"\\n\")\n",
    "        print(\"Learning Rate: \\n\" + str(learningRate)+\"\\n\")\n",
    "        print(\"Theta One: \\n\" + str(theta1)+\"\\n\")\n",
    "        print(\"Theta two: \\n\" + str(theta2)+\"\\n\")\n",
    "        print(\"Target: \\n\"+ str(targets[index]) + \"\\n\")\n",
    "        print(color.YELLOW+color.BOLD+\"----------Processing on batch:\"+str(r)+\" with online instance: \"+str(index)+\"-------------\"+color.END)\n",
    "\n",
    "        theta1, theta2, netError = nn(learningRate, theta1, theta2, np.array([inputsWB[index]]), targets[index])\n",
    "        trackedNetError.append(netError[0][0])\n",
    "    numberProcess = numberProcess + 1 \n",
    "    batchError.append(statistics.mean(trackedNetError))\n",
    "saveNet(theta1, theta2, \"./FishWeights.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "netPlot(numberProcess, batchError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.array([[0.5403,-0.4161],[-0.9900,-0.6536],[0.2837,0.9602]])\n",
    "targets = np.array([[0,0,0,1],[0,1,0,0],[0,0,1,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = np.array([[0.5403,-0.4161],[-0.9900,-0.6536],[0.2837,0.9602]])\n",
    "targets = np.array([[0,0,0,1],[0,1,0,0],[0,0,1,0]])\n",
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 4)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(targets).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Two Tutorial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in fishing normalized set \n",
    "# data = getArrayFromFile(\"normalizeFish.csv\")\n",
    "data = getArrayFromFile(\"normDigit.csv\")\n",
    "inputs = data[:,0:data.shape[1]-1] # get the input values\n",
    "targets = data[:, data.shape[1]-1:data.shape[1]] # get the class values\n",
    "\n",
    "targetMap = load_objects(\"./digitTargetCleanDict.pkl\")\n",
    "oneHotTargets = np.asarray(mapTargetsToEncoded(targets, targetMap), dtype=np.float32)\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):  \n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_der(x):  \n",
    "    return sigmoid(x) *(1-sigmoid (x))\n",
    "\n",
    "def softmax(A):  \n",
    "    expA = np.exp(A)\n",
    "    return expA / expA.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input node num: 64\n",
      "Hidden node num: 49\n",
      "Output node num: 10\n",
      "\n",
      "(64, 49)\n",
      "(49,)\n",
      "(49, 10)\n",
      "(10,)\n",
      "\n",
      "(3823, 64)\n",
      "\n",
      "------------0-----------\n",
      "derivativecost_zetaOutput\n",
      "(3823, 10)\n",
      "\n",
      "derivativeZetaOutput_derivatieWeightOutput\n",
      "(3823, 49)\n",
      "\n",
      "(49, 10)\n",
      "\n",
      "(3823, 10)\n",
      "\n",
      "(49, 10)\n",
      "\n",
      "(3823, 49)\n",
      "\n",
      "(3823, 49)\n",
      "\n",
      "(3823, 64)\n",
      "\n",
      "(64, 49)\n",
      "\n",
      "(3823, 49)\n",
      "\n",
      "(64, 49)\n",
      "\n",
      "(49,)\n",
      "\n",
      "(49, 10)\n",
      "\n",
      "(10,)\n",
      "\n",
      "Loss function value:  8929.997657978858\n",
      "------------1-----------\n",
      "derivativecost_zetaOutput\n",
      "(3823, 10)\n",
      "\n",
      "derivativeZetaOutput_derivatieWeightOutput\n",
      "(3823, 49)\n",
      "\n",
      "(49, 10)\n",
      "\n",
      "(3823, 10)\n",
      "\n",
      "(49, 10)\n",
      "\n",
      "(3823, 49)\n",
      "\n",
      "(3823, 49)\n",
      "\n",
      "(3823, 64)\n",
      "\n",
      "(64, 49)\n",
      "\n",
      "(3823, 49)\n",
      "\n",
      "(64, 49)\n",
      "\n",
      "(49,)\n",
      "\n",
      "(49, 10)\n",
      "\n",
      "(10,)\n",
      "\n",
      "Loss function value:  8917.377858781549\n",
      "------------2-----------\n",
      "derivativecost_zetaOutput\n",
      "(3823, 10)\n",
      "\n",
      "derivativeZetaOutput_derivatieWeightOutput\n",
      "(3823, 49)\n",
      "\n",
      "(49, 10)\n",
      "\n",
      "(3823, 10)\n",
      "\n",
      "(49, 10)\n",
      "\n",
      "(3823, 49)\n",
      "\n",
      "(3823, 49)\n",
      "\n",
      "(3823, 64)\n",
      "\n",
      "(64, 49)\n",
      "\n",
      "(3823, 49)\n",
      "\n",
      "(64, 49)\n",
      "\n",
      "(49,)\n",
      "\n",
      "(49, 10)\n",
      "\n",
      "(10,)\n",
      "\n",
      "Loss function value:  8905.879052130289\n",
      "------------3-----------\n",
      "derivativecost_zetaOutput\n",
      "(3823, 10)\n",
      "\n",
      "derivativeZetaOutput_derivatieWeightOutput\n",
      "(3823, 49)\n",
      "\n",
      "(49, 10)\n",
      "\n",
      "(3823, 10)\n",
      "\n",
      "(49, 10)\n",
      "\n",
      "(3823, 49)\n",
      "\n",
      "(3823, 49)\n",
      "\n",
      "(3823, 64)\n",
      "\n",
      "(64, 49)\n",
      "\n",
      "(3823, 49)\n",
      "\n",
      "(64, 49)\n",
      "\n",
      "(49,)\n",
      "\n",
      "(49, 10)\n",
      "\n",
      "(10,)\n",
      "\n",
      "Loss function value:  8895.391241078203\n",
      "------------4-----------\n",
      "derivativecost_zetaOutput\n",
      "(3823, 10)\n",
      "\n",
      "derivativeZetaOutput_derivatieWeightOutput\n",
      "(3823, 49)\n",
      "\n",
      "(49, 10)\n",
      "\n",
      "(3823, 10)\n",
      "\n",
      "(49, 10)\n",
      "\n",
      "(3823, 49)\n",
      "\n",
      "(3823, 49)\n",
      "\n",
      "(3823, 64)\n",
      "\n",
      "(64, 49)\n",
      "\n",
      "(3823, 49)\n",
      "\n",
      "(64, 49)\n",
      "\n",
      "(49,)\n",
      "\n",
      "(49, 10)\n",
      "\n",
      "(10,)\n",
      "\n",
      "Loss function value:  8885.816464466865\n",
      "------------5-----------\n",
      "derivativecost_zetaOutput\n",
      "(3823, 10)\n",
      "\n",
      "derivativeZetaOutput_derivatieWeightOutput\n",
      "(3823, 49)\n",
      "\n",
      "(49, 10)\n",
      "\n",
      "(3823, 10)\n",
      "\n",
      "(49, 10)\n",
      "\n",
      "(3823, 49)\n",
      "\n",
      "(3823, 49)\n",
      "\n",
      "(3823, 64)\n",
      "\n",
      "(64, 49)\n",
      "\n",
      "(3823, 49)\n",
      "\n",
      "(64, 49)\n",
      "\n",
      "(49,)\n",
      "\n",
      "(49, 10)\n",
      "\n",
      "(10,)\n",
      "\n",
      "Loss function value:  8877.06714764558\n",
      "------------6-----------\n",
      "derivativecost_zetaOutput\n",
      "(3823, 10)\n",
      "\n",
      "derivativeZetaOutput_derivatieWeightOutput\n",
      "(3823, 49)\n",
      "\n",
      "(49, 10)\n",
      "\n",
      "(3823, 10)\n",
      "\n",
      "(49, 10)\n",
      "\n",
      "(3823, 49)\n",
      "\n",
      "(3823, 49)\n",
      "\n",
      "(3823, 64)\n",
      "\n",
      "(64, 49)\n",
      "\n",
      "(3823, 49)\n",
      "\n",
      "(64, 49)\n",
      "\n",
      "(49,)\n",
      "\n",
      "(49, 10)\n",
      "\n",
      "(10,)\n",
      "\n",
      "Loss function value:  8869.064749572366\n",
      "------------7-----------\n",
      "derivativecost_zetaOutput\n",
      "(3823, 10)\n",
      "\n",
      "derivativeZetaOutput_derivatieWeightOutput\n",
      "(3823, 49)\n",
      "\n",
      "(49, 10)\n",
      "\n",
      "(3823, 10)\n",
      "\n",
      "(49, 10)\n",
      "\n",
      "(3823, 49)\n",
      "\n",
      "(3823, 49)\n",
      "\n",
      "(3823, 64)\n",
      "\n",
      "(64, 49)\n",
      "\n",
      "(3823, 49)\n",
      "\n",
      "(64, 49)\n",
      "\n",
      "(49,)\n",
      "\n",
      "(49, 10)\n",
      "\n",
      "(10,)\n",
      "\n",
      "Loss function value:  8861.738641550957\n",
      "------------8-----------\n",
      "derivativecost_zetaOutput\n",
      "(3823, 10)\n",
      "\n",
      "derivativeZetaOutput_derivatieWeightOutput\n",
      "(3823, 49)\n",
      "\n",
      "(49, 10)\n",
      "\n",
      "(3823, 10)\n",
      "\n",
      "(49, 10)\n",
      "\n",
      "(3823, 49)\n",
      "\n",
      "(3823, 49)\n",
      "\n",
      "(3823, 64)\n",
      "\n",
      "(64, 49)\n",
      "\n",
      "(3823, 49)\n",
      "\n",
      "(64, 49)\n",
      "\n",
      "(49,)\n",
      "\n",
      "(49, 10)\n",
      "\n",
      "(10,)\n",
      "\n",
      "Loss function value:  8855.025168576216\n",
      "------------9-----------\n",
      "derivativecost_zetaOutput\n",
      "(3823, 10)\n",
      "\n",
      "derivativeZetaOutput_derivatieWeightOutput\n",
      "(3823, 49)\n",
      "\n",
      "(49, 10)\n",
      "\n",
      "(3823, 10)\n",
      "\n",
      "(49, 10)\n",
      "\n",
      "(3823, 49)\n",
      "\n",
      "(3823, 49)\n",
      "\n",
      "(3823, 64)\n",
      "\n",
      "(64, 49)\n",
      "\n",
      "(3823, 49)\n",
      "\n",
      "(64, 49)\n",
      "\n",
      "(49,)\n",
      "\n",
      "(49, 10)\n",
      "\n",
      "(10,)\n",
      "\n",
      "Loss function value:  8848.866855906641\n"
     ]
    }
   ],
   "source": [
    "instances = inputs.shape[0]\n",
    "attributes = inputs.shape[1]\n",
    "\n",
    "numInputNodes = attributes\n",
    "numOutputNodes = 1 if np.unique(targets).shape[0] == 2 else np.unique(targets).shape[0]\n",
    "numHiddenNodes = int((2/3)*(numInputNodes+numOutputNodes))\n",
    "\n",
    "print(\"Input node num: \"+ str(numInputNodes))\n",
    "print(\"Hidden node num: \"+ str(numHiddenNodes))\n",
    "print(\"Output node num: \" + str(numOutputNodes))\n",
    "\n",
    "lowRange = (-1/math.sqrt(numInputNodes))\n",
    "highRange = math.fabs(lowRange)\n",
    "\n",
    "weightHidden = np.random.uniform(low=lowRange, high=highRange, size=(numInputNodes, numHiddenNodes, ))\n",
    "biasHidden = np.random.uniform(low=lowRange, high=highRange, size=(numHiddenNodes))\n",
    "\n",
    "weightOutput = np.random.uniform(low=lowRange, high=highRange, size=(numHiddenNodes, numOutputNodes))\n",
    "biasOutput = np.random.uniform(low=lowRange, high=highRange, size=(numOutputNodes))\n",
    "\n",
    "print()\n",
    "print(str(weightHidden.shape))\n",
    "print(str(biasHidden.shape))\n",
    "print(str(weightOutput.shape))\n",
    "print(str(biasOutput.shape))\n",
    "\n",
    "print()\n",
    "print(str(inputs.shape))\n",
    "print()\n",
    "learningRate = 10e-6\n",
    "\n",
    "errorCost = []\n",
    "\n",
    "for epoch in range(10):    \n",
    "    print(\"------------\"+str(epoch)+\"-----------\")\n",
    "    #### feedforward\n",
    "\n",
    "    # Phase 1 inputs fed through to hidden\n",
    "    zetaHidden = np.dot(inputs, weightHidden) + biasHidden\n",
    "    activationHidden = sigmoid(zetaHidden)\n",
    "    \n",
    "    # Phase 2 hidden fed through to the output\n",
    "    zetaOutput = np.dot(activationHidden, weightOutput) + biasOutput\n",
    "    activationOutput = softmax(zetaOutput)\n",
    "\n",
    "    #### backpropigate with the cross entropy cost function # bad step maybe\n",
    "    # phase one \n",
    "    derivativecost_zetaOutput = activationOutput - oneHotTargets\n",
    "    print(\"derivativecost_zetaOutput\")\n",
    "    print(derivativecost_zetaOutput.shape)\n",
    "    #print(derivativecost_zetaOutput)\n",
    "    print()\n",
    "\n",
    "    derivativeZetaOutput_derivatieWeightOutput = activationHidden\n",
    "    print(\"derivativeZetaOutput_derivatieWeightOutput\")\n",
    "    print(derivativeZetaOutput_derivatieWeightOutput.shape)\n",
    "    #print(derivativeZetaOutput_derivatieWeightOutput)\n",
    "    print()\n",
    "\n",
    "    derivativecost_weightOutput = np.dot(derivativeZetaOutput_derivatieWeightOutput.T, derivativecost_zetaOutput)\n",
    "    print(derivativecost_weightOutput.shape)\n",
    "    #print(derivativecost_weightOutput)\n",
    "    print()\n",
    "\n",
    "    derivativecost_biasOutput = derivativecost_zetaOutput\n",
    "    print(derivativecost_biasOutput.shape)\n",
    "    #print(derivativecost_biasOutput)\n",
    "    print()\n",
    "    #     # phase two\n",
    "    derivativeZetaOutput_derivativeActivationHidden = weightOutput\n",
    "    print(derivativeZetaOutput_derivativeActivationHidden.shape)\n",
    "    #print(derivativeZetaOutput_derivativeActivationHidden)\n",
    "    print()\n",
    "    derivativeCost_derivativeActivationHidden = np.dot(derivativecost_zetaOutput , derivativeZetaOutput_derivativeActivationHidden.T)\n",
    "    print(derivativeCost_derivativeActivationHidden.shape)\n",
    "    #print(derivativeCost_derivativeActivationHidden)\n",
    "    print()\n",
    "    derivativeActivationHidden_derivativeZetaHidden = sigmoid_der(zetaHidden)\n",
    "    print(derivativeActivationHidden_derivativeZetaHidden.shape)\n",
    "    #print(derivativeActivationHidden_derivativeZetaHidden)\n",
    "    print()\n",
    "    derivativeZetaHidden_derivativeWeightHidden = inputs\n",
    "    print(derivativeZetaHidden_derivativeWeightHidden.shape)\n",
    "    #print(derivativeZetaHidden_derivativeWeightHidden)\n",
    "    print()\n",
    "    derivativeCost_weightHidden = np.dot(derivativeZetaHidden_derivativeWeightHidden.T, derivativeActivationHidden_derivativeZetaHidden * derivativeCost_derivativeActivationHidden)\n",
    "    print(derivativeCost_weightHidden.shape)\n",
    "    #print(derivativeCost_weightHidden)\n",
    "    print()\n",
    "    derivativeCost_biasHidden = derivativeCost_derivativeActivationHidden * derivativeActivationHidden_derivativeZetaHidden\n",
    "    print(derivativeCost_biasHidden.shape)\n",
    "    #print(derivativeCost_biasHidden)\n",
    "    print()\n",
    "\n",
    "    #     # Update Weights ================\n",
    "\n",
    "    weightHidden -= learningRate * derivativeCost_weightHidden\n",
    "    print(weightHidden.shape)\n",
    "   # print(weightHidden)\n",
    "    print()\n",
    "    biasHidden -= learningRate * derivativeCost_biasHidden.sum(axis=0)\n",
    "    print(biasHidden.shape)\n",
    "    #print(biasHidden)\n",
    "    print()\n",
    "    weightOutput -= learningRate * derivativecost_weightOutput\n",
    "    print(weightOutput.shape)\n",
    "    #print(weightOutput)\n",
    "    print()\n",
    "    biasOutput -= learningRate * derivativecost_biasOutput.sum(axis=0)\n",
    "    print(biasOutput.shape)\n",
    "    #print(biasOutput)\n",
    "    print()\n",
    "    \n",
    "    loss = np.sum(-oneHotTargets * np.log(activationOutput))\n",
    "    print('Loss function value: ', loss)\n",
    "    errorCost.append(loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "netPlot(10, errorCost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
